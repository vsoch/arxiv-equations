In probability theory and statistics, the moment-generating function of a random variable is an alternative specification of its probability distribution. Thus, it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. There are particularly simple results for the moment-generating functions of distributions defined by the weighted sums of random variables. Note, however, that not all random variables have moment-generating functions. In addition to univariate distributions, moment-generating functions can be defined for vector- or matrix-valued random variables, and can even be extended to more general cases. The moment-generating function does not always exist even for real-valued arguments, unlike the characteristic function. There are relations between the behavior of the moment-generating function of a distribution and properties of the distribution, such as the existence of moments.
In queueing theory, a discipline within the mathematical theory of probability, Burke's theorem (sometimes the Burke's output theorem) is a theorem (stated and demonstrated by Paul J. Burke while working at Bell Telephone Laboratories) asserting that, for the M/M/1 queue, M/M/c queue or M/M/  queue in the steady state with arrivals a Poisson process with rate parameter  : The departure process is a Poisson process with rate parameter  . At time t the number of customers in the queue is independent of the departure process prior to time t.
In mathematics and in particular mathematical dynamics, discrete time and continuous time are two alternative frameworks within which to model variables that evolve over time.
In descriptive statistics, summary statistics are used to summarize a set of observations, in order to communicate the largest amount of information as simply as possible. Statisticians commonly try to describe the observations in a measure of location, or central tendency, such as the arithmetic mean a measure of statistical dispersion like the standard deviation a measure of the shape of the distribution like skewness or kurtosis if more than one variable is measured, a measure of statistical dependence such as a correlation coefficient A common collection of order statistics used as summary statistics are the five-number summary, sometimes extended to a seven-number summary, and the associated box plot. Entries in an analysis of variance table can also be regarded as summary statistics.
A mathematical or physical process is time-reversible if the dynamics of the process remain well-defined when the sequence of time-states is reversed. A deterministic process is time-reversible if the time-reversed process satisfies the same dynamic equations as the original process; in other words, the equations are invariant or symmetrical under a change in the sign of time. A stochastic process is reversible if the statistical properties of the process are the same as the statistical properties for time-reversed data from the same process.
The item-total correlation test arises in psychometrics in contexts where a number of tests or questions are given to an individual and where the problem is to construct a useful single quantity for each individual that can be used to compare that individual with others in a given population. The test is used to see if any of the tests or questions ("items") do not have responses that vary in line with those for other tests across the population. The summary measure would be an average of some form, weighted where necessary, and the item-correlation test is used to decide whether or not responses to a given test should be included in the set being averaged. In some fields of application such a summary measure is called a scale.
In signal processing, the Wiener filter is a filter used to produce an estimate of a desired or target random process by linear time-invariant (LTI) filtering of an observed noisy process, assuming known stationary signal and noise spectra, and additive noise. The Wiener filter minimizes the mean square error between the estimated random process and the desired process.
In the mathematical fields of probability and statistics, a random variate is a particular outcome of a random variable: the random variates which are other outcomes of the same random variable might have different values. Random variates are used when simulating processes driven by random influences (stochastic processes). In modern applications, such simulations would derive random variates corresponding to any given probability distribution from computer procedures designed to create random variates corresponding to a uniform distribution, where these procedures would actually provide values chosen from a uniform distribution of pseudorandom numbers. Procedures to generate random variates corresponding to a given distribution are known as procedures for random variate generation or pseudo-random number sampling. In probability theory, a random variable is a measurable function from a probability space to a measurable space of values that the variable can take on. In that context, and in statistics, those values are known as a random variates, or occasionally random deviates, and this represents a wider meaning than just that associated with pseudorandom numbers.
In statistics and econometrics, an augmented Dickey Fuller test (ADF) is a test for a unit root in a time series sample. It is an augmented version of the Dickey Fuller test for a larger and more complicated set of time series models. The augmented Dickey Fuller (ADF) statistic, used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.
A multiple baseline design is a style of research involving the careful measurement of multiple persons, traits or settings both before and after a treatment. This design is used in medical, psychological and biological research to name a few areas. It has several advantages over AB designs which only measures a single case. It is important to note that the start of treatment conditions is staggered (started at different times) across individuals. Because treatment is started at different times we can conclude that changes are due to the treatment rather than to a chance factor. By gathering data from many subjects (instances), inferences can be made about the likeliness that the measured trait generalizes to a greater population. In multiple baseline designs, the experimenter starts by measuring a trait of interest, then applying a treatment before measuring that trait again. Treatment should not begin until a stable baseline has been recorded, and should not finish until measures regain stability. If a significant change occurs across all participants the experimenter may infer that the treatment is effective. Multiple base-line experiments are most commonly used in cases where the dependent variable is not expected to return to normal after the treatment has been applied, or when medical reasons forbid the withdrawal of a treatment. They often employ particular methods or recruiting participants. Multiple base-line designs are associated with potential confounds introduced by an experimenter bias which must be addressed in order to preserve objectivity. Particularly, researchers are advised to develop all test schedules and data collection limits beforehand.
"The Strong Law of Small Numbers" is the humorous title of a popular paper by mathematician Richard K. Guy and also the so-called law that it proclaims:  "There aren't enough small numbers to meet the many demands made of them."  In other words, any given small number appears in far more contexts than may seem reasonable, leading to many apparently surprising coincidences in mathematics, simply because small numbers appear so often and yet are so few. Guy's paper gives 35 examples in support of this thesis. This can lead inexperienced mathematicians to conclude that these concepts are related, when in fact they are not. Guy's observation has since become part of mathematical folklore, and is commonly referenced by other authors.
In statistics, reification is the use of an idealized model of a statistical process. The model is then used to make inferences connecting model results, which imperfectly represent the actual process, with experimental observations. Also,  a process whereby model-derived quantities such as principal components, factors and latent variables are identified, named and treated as if they were directly measurable quantities.
The layered hidden Markov model (LHMM) is a statistical model derived from the hidden Markov model (HMM). A layered hidden Markov model (LHMM) consists of N levels of HMMs, where the HMMs on level i + 1 correspond to observation symbols or probability generators at level i. Every level i of the LHMM consists of Ki HMMs running in parallel.
A dyadic (or 2-adic) distribution is a specific type of discrete or categorical probability distribution that is of some theoretical importance in data compression.  
In statistics, a multivariate Pareto distribution is a multivariate extension of a univariate Pareto distribution. There are several different types of univariate Pareto distributions including Pareto Types I IV and Feller Pareto. Multivariate Pareto distributions have been defined for many of these types.
In statistics, the explained sum of squares (ESS), alternatively known as the model sum of squares or sum of squares due to regression ("SSR"   not to be confused with the residual sum of squares RSS), is a quantity used in describing how well a model, often a regression model, represents the data being modelled. In particular, the explained sum of squares measures how much variation there is in the modelled values and this is compared to the total sum of squares, which measures how much variation there is in the observed data, and to the residual sum of squares, which measures the variation in the modelling errors.
In statistics, a regression diagnostic is one of a set of procedures available for regression analysis that seek to assess the validity of a model in any of a number of different ways. This assessment may be an exploration of the model's underlying statistical assumptions, an examination of the structure of the model by considering formulations that have fewer, more or different explanatory variables, or a study of subgroups of observations, looking for those that are either poorly represented by the model (outliers) or that have a relatively large effect on the regression model's predictions. A regression diagnostic may take the form of a graphical result, informal quantitative results or a formal statistical hypothesis test, each of which provides guidance for further stages of a regression analysis.
In cryptography, coincidence counting is the technique (invented by William F. Friedman) of putting two texts side-by-side and counting the number of times that identical letters appear in the same position in both texts. This count, either as a ratio of the total or normalized by dividing by the expected count for a random source model, is known as the index of coincidence, or IC for short.
"Correlation does not imply causation" is a phrase used in statistics to emphasize that a correlation between two variables does not imply that one causes the other. Many statistical tests calculate correlation between variables. A few go further, using correlation as a basis for testing a hypothesis of a true causal relationship; examples are the Granger causality test and convergent cross mapping. The counter-assumption, that "correlation proves causation," is considered a questionable cause logical fallacy in that two events occurring together are taken to have a cause-and-effect relationship. This fallacy is also known as cum hoc ergo propter hoc, Latin for "with this, therefore because of this," and "false cause." A similar fallacy, that an event that follows another was necessarily a consequence of the first event, is sometimes described as post hoc ergo propter hoc (Latin for "after this, therefore because of this.") For example, in a widely studied case, numerous epidemiological studies showed that women taking combined hormone replacement therapy (HRT) also had a lower-than-average incidence of coronary heart disease (CHD), leading doctors to propose that HRT was protective against CHD. But randomized controlled trials showed that HRT caused a small but statistically significant increase in risk of CHD. Re-analysis of the data from the epidemiological studies showed that women undertaking HRT were more likely to be from higher socio-economic groups (ABC1), with better-than-average diet and exercise regimens. The use of HRT and decreased incidence of coronary heart disease were coincident effects of a common cause (i.e. the benefits associated with a higher socioeconomic status), rather than a direct cause and effect, as had been supposed. As with any logical fallacy, identifying that the reasoning behind an argument is flawed does not imply that the resulting conclusion is false. In the instance above, if the trials had found that hormone replacement therapy does in fact have a negative incidence on the likelihood of coronary heart disease the assumption of causality would have been correct, although the logic behind the assumption would still have been flawed.
Belt transects are used in biology to estimate the distribution of organisms in relation to a certain area, such as the seashore or a meadow. It records all the species found between two lines and how far they are for a certain place or area and how many of them there are. An interrupted belt transect records all the species found in quadrats (square frames) placed at certain intervals along a line. The belt transect method is similar to the line transect method but gives information on abundance as well as presence, or absence of species. It may be considered as a widening of the line transect to form a continuous belt, or series of quadrats. In this method, the transect line is laid out across the area to be surveyed and a quadrat is placed on the first marked point on the line. These marked points should be a set amount of space apart. The plants and/or animals inside the quadrat are then identified and their abundance estimated. Animals can be counted within the quadrat, or collected, while it is usual to estimate the percentage cover of plant species. Cover is the area of the quadrat occupied by the above-ground parts of a species when viewed from above. The canopies of the plants inside the quadrat will often overlap each other, so the total percentage cover of plants in a single quadrat will frequently add up to more than 100%. Quadrats are sampled all the way down the transect line, at each marked point on the line, or at some other predetermined interval (or even randomly) if time is short. It is important that the same person should do the estimations of cover in each quadrat, because the estimation is likely to vary from person to person. If different people estimate percentage cover in different quadrats, then an element of personal variation is introduced which will lead to less accurate results. Sampling should always be as least destructive as possible and trampling the surrounding area should be avoided when carrying out a survey. Not only is there a risk of damaging the area, but there is also a risk of reducing the population or percentage cover of the species being surveyed.
In probability theory, a stochastic (/sto  k st k/) process, or often random process, is a collection of random variables representing the evolution of some system of random values over time. This is the probabilistic counterpart to a deterministic process (or deterministic system). Instead of describing a process which can only evolve in one way (as in the case, for example, of solutions of an ordinary differential equation), in a stochastic, or random process, there is some indeterminacy: even if the initial condition (or starting point) is known, there are several (often infinitely many) directions in which the process may evolve. In the simple case of discrete time, as opposed to continuous time, a stochastic process is a sequence of random variables. (For example, see Markov chain, also known as discrete-time Markov chain.) The random variables corresponding to various times may be completely different, the only requirement being that these different random quantities all take values in the same space (the codomain of the function). One approach may be to model these random variables as random functions of one or several deterministic arguments (in most cases, the time parameter). Although the random values of a stochastic process at different times may be independent random variables, in most commonly considered situations they exhibit complicated statistical dependence. Familiar examples of stochastic processes include stock market and exchange rate fluctuations; signals such as speech; audio and video; medical data such as a patient's EKG, EEG, blood pressure or temperature; and random movement such as Brownian motion or random walks. A generalization, the random field, is defined by letting the variables be parametrized by members of a topological space instead of time. Examples of random fields include static images, random terrain (landscapes), wind waves and composition variations of a heterogeneous material.
A probability box (or p-box) is a characterization of an uncertain number consisting of both aleatoric and epistemic uncertainties that is often used in risk analysis or quantitative uncertainty modeling where numerical calculations must be performed. Probability bounds analysis is used to make arithmetic and logical calculations with p-boxes. An example p-box is shown in the figure at right for an uncertain number x consisting of a left (upper) bound and a right (lower) bound on the probability distribution for x. The bounds are coincident for values of x below 0 and above 24. The bounds may have almost any shapes, including step functions, so long as they are monotonically increasing and do not cross each other. A p-box is used to express simultaneously incertitude (epistemic uncertainty), which is represented by the breadth between the left and right edges of the p-box, and variability (aleatory uncertainty), which is represented by the overall slant of the p-box.
In statistics, the design effect (or estimates of unit variance) is an adjustment used in some kinds of studies, such as cluster randomised trials, to allow for the design structure. The adjustment inflates the variance of parameter estimates, and therefore their standard errors, which is necessary to allow for correlations among clusters of observations. It is similar to the variance inflation factor and is used in sample size calculations. The term was introduced by Leslie Kish in 1965.  
In population genetics, the Watterson estimator is a method for estimating the population mutation rate, , where  is the effective population size and  is the per-generation mutation rate of the population of interest (Watterson (1975)). The assumptions made are that there is a sample of n haploid individuals from the population of interest, that there are infinitely many sites capable of varying (so that mutations never overlay or reverse one another), and that . The estimate of , often denoted as , is  where K is the number of segregating sites (an example of a segregating site would be a single-nucleotide polymorphism) in the sample and  is the (n   1)th harmonic number. This estimate is based on coalescent theory. Watterson's estimator is commonly used for its simplicity. When its assumptions are met, the estimator is unbiased and the variance of the estimator decreases with increasing sample size or recombination rate. However, the estimator can be biased by population structure. For example,  is downwardly biased in an exponentially growing population. It can also be biased by violation of the infinite-sites mutational model; if multiple mutations can overwrite one another, Watterson's estimator will be biased downward.
In probability theory, the factorial moment is a mathematical quantity defined as the expectation or average of the falling factorial of a random variable. Factorial moments are useful for studying non-negative integer-valued random variables, and arise in the use of probability-generating functions to derive the moments of discrete random variables. Factorial moments serve as analytic tools in the mathematical field of combinatorics, which is the study of discrete mathematical structures.
A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The relative position and angle of the axes is typically uninformative. The radar chart is also known as web chart, spider chart, star chart, star plot, cobweb chart, irregular polygon, polar chart, or kiviat diagram.
In statistics, Hartley's test, also known as the Fmax test or Hartley's Fmax, is used in the analysis of variance to verify that different groups have a similar variance, an assumption needed for other statistical tests. It was developed by H. O. Hartley, who published it in 1950. The test involves computing the ratio of the largest group variance, max(sj2) to the smallest group variance, min(sj2). The resulting ratio, Fmax, is then compared to a critical value from a table of the sampling distribution of Fmax. If the computed ratio is less than the critical value, the groups are assumed to have similar or equal variances. Hartley's test assumes that data for each group are normally distributed, and that each group has an equal number of members. This test, although convenient, is quite sensitive to violations of the normality assumption. Alternatives to Hartley's test that are robust to violations of normality are O'Brien's procedure, and the Brown Forsythe test.
The method of iteratively reweighted least squares (IRLS) is used to solve certain optimization problems with objective functions of the form:  by an iterative method in which each step involves solving a weighted least squares problem of the form:  IRLS is used to find the maximum likelihood estimates of a generalized linear model, and in robust regression to find an M-estimator, as a way of mitigating the influence of outliers in an otherwise normally-distributed data set. For example, by minimizing the least absolute error rather than the least square error. Although not a linear regression problem, Weiszfeld's algorithm for approximating the geometric median can also be viewed as a special case of iteratively reweighted least squares, in which the objective function is the sum of distances of the estimator from the samples. One of the advantages of IRLS over linear programming and convex programming is that it can be used with Gauss Newton and Levenberg Marquardt numerical algorithms.
In statistics, a probit model is a type of regression where the dependent variable can only take two values, for example married or not married. The name is from probability + unit. The purpose of the model is to estimate the probability that an observation with particular characteristics will fall into a specific one of the categories; moreover, if estimated probabilities greater than 1/2 are treated as classifying an observation into a predicted category, the probit model is a type of binary classification model. A probit model is a popular specification for an ordinal or a binary response model. As such it treats the same set of problems as does logistic regression using similar techniques. The probit model, which employs a probit link function, is most often estimated using the standard maximum likelihood procedure, such an estimation being called a probit regression. Probit models were introduced by Chester Bliss in 1934; a fast method for computing maximum likelihood estimates for them was proposed by Ronald Fisher as an appendix to Bliss' work in 1935.
Algorithms for calculating variance play a major role in computational statistics. A key difficulty in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values.
GAUSS is a matrix programming language for mathematics and statistics, developed and marketed by Aptech Systems. Its primary purpose is the solution of numerical problems in statistics, econometrics, time-series, optimization and 2D- and 3D-visualization. It was first published in 1984 for MS-DOS and is currently also available for Linux, Mac OS X and Windows.
In probability theory, a continuity correction is an adjustment that is made when a discrete distribution is approximated by a continuous distribution.
Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s. As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include statistical relational learning and reasoning (emphasizing the importance of reasoning) and first-order probabilistic languages (emphasizing the key properties of the languages with which models are represented).  
A probability distribution function is some function that may be used to define a particular probability distribution. Depending upon which text is consulted, the term may refer to: a cumulative distribution function, a probability mass function, and/or a probability density function. The similar term probability function may mean any of the above and, in addition, a probability measure function, as in a probability space, where the domain of the function is the set of events.
Level of measurement or scale of measure is a classification that describes the nature of information within the numbers assigned to variables. Psychologist Stanley Smith Stevens developed the best known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. Other classifications include those by Chrisman and by Mosteller and Tukey. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines.
Statistical inference is the process of deducing properties of an underlying distribution by analysis of data. Inferential statistical analysis infers properties about a population: this includes testing hypotheses and deriving estimates. The population is assumed to be larger than the observed data set; in other words, the observed data is assumed to be sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and does not assume that the data came from a larger population.
The Pocock boundary is a method for determining whether to stop a clinical trial prematurely. The typical clinical trial compares two groups of patients. One group are given a placebo or conventional treatment, while the other group of patients are given the treatment that is being tested. The investigators running the clinical trial will wish to stop the trial early for ethical reasons if the treatment group clearly shows evidence of benefit. In other words, "when early results proved so promising it was no longer fair to keep patients on the older drugs for comparison, without giving them the opportunity to change." The concept was introduced by the medical statistician Stuart Pocock in 1977. The many reasons underlying when to stop a clinical trial for benefit were discussed in an editorial from his hand in 2005.
In data analysis, the self-similarity matrix is a graphical representation of similar sequences in a data series. Similarity can be explained by different measures, like spatial distance (distance matrix), correlation, or comparison of local histograms or spectral properties (e.g. IXEGRAM). This technique is also applied for the search of a given pattern in a long data series as in gene matching. A similarity plot can be the starting point for dot plots or recurrence plots.
In probability theory and statistics a Rayleigh mixture distribution is a weighted mixture of multiple probability distributions where the weightings are equal to the weightings of a Rayleigh distribution. Since the probability density function for a (standard) Rayleigh distribution is given by  Rayleigh mixture distributions have probability density functions of the form  where  is a well-defined probability density function or sampling distribution. The Rayleigh mixture distribution is one of many types of compound distributions in which the appearance of a value in a sample or population might be interpreted as a function of other underlying random variables. Mixture distributions are often used in mixture models, which are used to express probabilities of sub-populations within a larger population.
In statistics and probability theory, a median is the number separating the higher half of a data sample, a population, or a probability distribution, from the lower half. The median of a finite list of numbers can be found by arranging all the observations from lowest value to highest value and picking the middle one (e.g., the median of {3, 3, 5, 9, 11} is 5). If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values   (the median of {3, 5, 7, 9} is (5 + 7) / 2 = 6), which corresponds to interpreting the median as the fully trimmed mid-range. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large result. A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions. In a sample of data, or a finite population, there may be no member of the sample whose value is identical to the median (in the case of an even sample size); if there is such a member, there may be more than one so that the median may not uniquely identify a sample member. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid. At most, half the population have values strictly less than the median, and, at most, half have values strictly greater than the median. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if a < b < c, then the median of the list {a, b, c} is b, and, if a < b < c < d, then the median of the list {a, b, c, d} is the mean of b and c; i.e., it is (b + c)/2. The median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors. In terms of notation, some authors represent the median of a variable x either as x  or as  1/2 sometimes also M. There is no widely accepted standard notation for the median, so the use of these or other symbols for the median needs to be explicitly defined when they are introduced. The median is the 2nd quartile, 5th decile, and 50th percentile.  
In computer science, MinHash (or the min-wise independent permutations locality sensitive hashing scheme) is a technique for quickly estimating how similar two sets are. The scheme was invented by Andrei Broder (1997), and initially used in the AltaVista search engine to detect duplicate web pages and eliminate them from search results. It has also been applied in large-scale clustering problems, such as clustering documents by the similarity of their sets of words.
In statistics, the term higher-order statistics (HOS) refers to functions which use the third or higher power of a sample, as opposed to more conventional techniques of lower-order statistics, which use constant, linear, and quadratic terms (zeroth, first, and second powers). The third and higher moments, as used in the skewness and kurtosis, are examples of HOS, whereas the first and second moments, as used in the arithmetic mean, and variance are examples of low-order statistics. HOS are particularly used in estimation of shape parameters, such as skewness and kurtosis, as when measuring the deviation of a distribution from the normal distribution. On the other hand, due to the higher powers, HOS are significantly less robust than lower-order statistics. In statistical theory, one long-established approach to higher-order statistics, for univariate and multivariate distributions is through the use of cumulants and joint cumulants. In time series analysis, the extension of these is to higher order spectra, for example the bispectrum and trispectrum. An alternative to the use of HOS and higher moments is to instead uses L-moments, which are linear statistics (linear combinations of order statistics), and thus more robust than HOS.
Starting with a sample  observed from a random variable X having a given distribution law with a non-set parameter, a parametric inference problem consists of computing suitable values   call them estimates   of this parameter precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample. In turn, parameter compatibility is a probability measure that we derive from the probability distribution of the random variable to which the parameter refers. In this way we identify a random parameter   compatible with an observed sample. Given a sampling mechanism , the rationale of this operation lies in using the Z seed distribution law to determine both the X distribution law for the given  , and the   distribution law given an X sample. Hence, we may derive the latter distribution directly from the former if we are able to relate domains of the sample space to subsets of   support. In more abstract terms, we speak about twisting properties of samples with properties of parameters and identify the former with statistics that are suitable for this exchange, so denoting a well behavior w.r.t. the unknown parameters. The operational goal is to write the analytic expression of the cumulative distribution function , in light of the observed value s of a statistic S, as a function of the S distribution law when the X parameter is exactly  .
In probability theory, the law of the iterated logarithm describes the magnitude of the fluctuations of a random walk. The original statement of the law of the iterated logarithm is due to A. Y. Khinchin (1924). Another statement was given by A.N. Kolmogorov in 1929.
In probability theory, the central limit theorem (CLT) states that, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed, regardless of the underlying distribution. To illustrate what this means, suppose that a sample is obtained containing a large number of observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic average of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the computed values of the average will be distributed according to the normal distribution (commonly known as a "bell curve"). A simple example of this is that if one flips a coin many times, the probability of getting a given number of heads should follow a normal curve, with mean equal to half the total number of flips. The central limit theorem has a number of variants. In its common form, the random variables must be identically distributed. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, given that they comply with certain conditions. In more general usage, a central limit theorem is any of a set of weak-convergence theorems in probability theory. They all express the fact that a sum of many independent and identically distributed (i.i.d.) random variables, or alternatively, random variables with specific types of dependence, will tend to be distributed according to one of a small set of attractor distributions. When the variance of the i.i.d. variables is finite, the attractor distribution is the normal distribution. In contrast, the sum of a number of i.i.d. random variables with power law tail distributions decreasing as |x|   1 where 0 <   < 2 (and therefore having infinite variance) will tend to an alpha-stable distribution with stability parameter (or index of stability) of   as the number of variables grows.
Maximum entropy spectral estimation is a method of spectral density estimation. The goal is to improve the spectral quality based on the principle of maximum entropy. The method is based on choosing the spectrum which corresponds to the most random or the most unpredictable time series whose autocorrelation function agrees with the known values. This assumption, which corresponds to the concept of maximum entropy as used in both statistical mechanics and information theory, is maximally non-committal with regard to the unknown values of the autocorrelation function of the time series. It is simply the application of maximum entropy modeling to any type of spectrum and is used in all fields where data is presented in spectral form. The usefulness of the technique varies based on the source of the spectral data since it is dependent on the amount of assumed knowledge about the spectrum that can be applied to the model. In maximum entropy modeling, probability distributions are created on the basis of that which is known, leading to a type of statistical inference about the missing information which is called the maximum entropy estimate. For example, in spectral analysis the expected peak shape is often known, but in a noisy spectrum the center of the peak may not be clear. In such a case, inputting the known information allows the maximum entropy model to derive a better estimate of the center of the peak, thus improving spectral accuracy.
In statistics, regression toward (or to) the mean is the phenomenon that if a variable is extreme on its first measurement, it will tend to be closer to the average on its second measurement and if it is extreme on its second measurement, it will tend to have been closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. The conditions under which regression toward the mean occurs depend on the way the term is mathematically defined. Sir Francis Galton first observed the phenomenon in the context of simple linear regression of data points. Galton  developed the following model: pellets fall through a quincunx forming a normal distribution centered directly under their entrance point. These pellets could then be released down into a second gallery corresponding to a second measurement occasion. Galton then asked the reverse question, "From where did these pellets come "  "The answer was not 'on average directly above'. Rather it was 'on average, more towards the middle', for the simple reason that there were more pellets above it towards the middle that could wander left than there were in the left extreme that could wander to the right, inwards" (p 477)   A less restrictive approach is possible. Regression towards the mean can be defined for any bivariate distribution with identical marginal distributions. Two such definitions exist. One definition accords closely with the common usage of the term  regression towards the mean . Not all such bivariate distributions show regression towards the mean under this definition. However, all such bivariate distributions show regression towards the mean under the other definition. Historically, what is now called regression toward the mean has also been called reversion to the mean and reversion to mediocrity. In finance, the term mean reversion has a different meaning. Jeremy Siegel uses it to describe a financial time series in which "returns can be very unstable in the short run but very stable in the long run." More quantitatively, it is one in which the standard deviation of average annual returns declines faster than the inverse of the holding period, implying that the process is not a random walk, but that periods of lower returns are systematically followed by compensating periods of higher returns, in seasonal businesses for example.
The following outline is provided as an overview and guide to the variety of topics included within the subject of statistics: Statistics pertains to the collection, analysis, interpretation, and presentation of data. It is applicable to a wide variety of academic disciplines, from the physical and social sciences to the humanities; it is also used and misused for making informed decisions in all areas of business and government.  
Histograms are most commonly used as visual representations of data. However, database systems use histograms to summarize data internally and provide size estimates for queries. These histograms are not presented to users or displayed visually, so a wider range of options are available for their construction. Simple or exotic histograms are defined by four parameters, Sort Value, Source Value, Partition Class and Partition Rule. The most basic histogram is the equi-width histogram, where each bucket represents the same range of values. That histogram would be defined as having a Sort Value of Value, a Source Value of Frequency, be in the Serial Partition Class and have a Partition Rule stating that all buckets have the same range. V-optimal histograms are an example of a more "exotic" histogram. V-optimality is a Partition Rule which states that the bucket boundaries are to be placed as to minimize the cumulative weighted variance of the buckets. Implementation of this rule is a complex problem and construction of these histograms is also a complex process.
In statistics, point estimation involves the use of sample data to calculate a single value (known as a statistic) which is to serve as a "best guess" or "best estimate" of an unknown (fixed or random) population parameter. More formally, it is the application of a point estimator to the data. In general, point estimation should be contrasted with interval estimation: such interval estimates are typically either confidence intervals in the case of frequentist inference, or credible intervals in the case of Bayesian inference.
Upper and lower probabilities are representations of imprecise probability. Whereas probability theory uses a single number, the probability, to describe how likely an event is to occur, this method uses two numbers: the upper probability of the event and the lower probability of the event. Because frequentist statistics disallows metaprobabilities, frequentists have had to propose new solutions. Cedric Smith and Arthur Dempster each developed a theory of upper and lower probabilities. Glenn Shafer developed Dempster's theory further, and it is now known as Dempster Shafer theory: see also Choquet(1953). More precisely, in the work of these authors one considers in a power set, , a mass function  satisfying the conditions  In turn, a mass is associated with two non-additive continuous measures called belief and plausibility defined as follows:  In the case where  is infinite there can be  such that there is no associated mass function. See p. 36 of Halpern (2003). Probability measures are a special case of belief functions in which the mass function assigns positive mass to singletons of the event space only. A different notion of upper and lower probabilities is obtained by the lower and upper envelopes obtained from a class C of probability distributions by setting  The upper and lower probabilities are also related with probabilistic logic: see Gerla (1994). Observe also that a necessity measure can be seen as a lower probability and a possibility measure can be seen as an upper probability.
In statistics, aggregate data are data combined from several measurements. When data are aggregated, groups of observations are replaced with summary statistics based on those observations. In a data warehouse, the use of aggregate data dramatically reduces the time to query large sets of data. Developers pre-summarize queries that are regularly used, such as Weekly Sales across several dimensions such as by item hierarchy or geographical hierarchy. In economics, aggregate data or data aggregates are high-level data that are composed from a multitude or combination of other more individual data, such as: in macroeconomics, data such as the overall price level or overall inflation rate; and in microeconomics, data of an entire sector of an economy composed of many firms, or of all households in a city or region.
Sampling risk is one of the many types of risks an auditor may face when performing the necessary procedure of audit sampling. Audit sampling exists because of the impractical and costly effects of examining all or 100% of a client's records or books. As a result, a "sample" of a client's accounts are examined. Due to the negative effects produced by sampling risk, an auditor may have to perform additional procedures which in turn can impact the overall efficiency of the audit. Sampling risk represents the possibility that an auditor's conclusion based on a sample is different from that reached if the entire population were subject to audit procedure. The auditor may conclude that material misstatements exist, when in fact they do not; or material misstatements do not exist but in fact they do exist. Auditors can lower the sampling risk by increasing the sampling size. Although there are many types of risks associated with the audit process, each type primarily has an effect on the overall audit engagement. The effects produced by sampling risk generally can increase the risk of material misstatement which states that an entity's financial statements will contain a material misstatement. Sampling risk can also increase detection risk which suggests the possibility that an auditor will not find material misstatements relating to the financial statements through substantive tests and analysis.
See also: Conjoint analysis (in marketing), Rule Developing Experimentation
In evidence-based medicine, likelihood ratios are used for assessing the value of performing a diagnostic test. They use the sensitivity and specificity of the test to determine whether a test result usefully changes the probability that a condition (such as a disease state) exists. The first description of the use of likelihood ratios for decision rules was made at a symposium on information theory in 1954. In medicine, likelihood ratios were introduced between 1975 and 1980.
The hyperbolic distribution is a continuous probability distribution characterized by the logarithm of the probability density function being a hyperbola. Thus the distribution decreases exponentially, which is more slowly than the normal distribution. It is therefore suitable to model phenomena where numerically large values are more probable than is the case for the normal distribution. Examples are returns from financial assets and turbulent wind speeds. The hyperbolic distributions form a subclass of the generalised hyperbolic distributions. The origin of the distribution is the observation by Ralph Alger Bagnold, published in his book The Physics of Blown Sand and Desert Dunes (1941), that the logarithm of the histogram of the empirical size distribution of sand deposits tends to form a hyperbola. This observation was formalised mathematically by Ole Barndorff-Nielsen in a paper in 1977, where he also introduced the generalised hyperbolic distribution, using the fact the a hyperbolic distribution is a random mixture of normal distributions.
In physics and chemistry and related fields, master equations are used to describe the time-evolution of a system that can be modelled as being in exactly one of the states at any given time, and where switching between states is treated probabilistically. The equations are usually a set of differential equations for the variation over time of the probabilities that the system occupies each of the different states.
Failure rate is the frequency with which an engineered system or component fails, expressed in failures per unit of time. It is often denoted by the Greek letter   (lambda) and is highly used in reliability engineering. The failure rate of a system usually depends on time, with the rate varying over the life cycle of the system. For example, an automobile's failure rate in its fifth year of service may be many times greater than its failure rate during its first year of service. One does not expect to replace an exhaust pipe, overhaul the brakes, or have major transmission problems in a new vehicle. In practice, the mean time between failures (MTBF, 1/ ) is often reported instead of the failure rate. This is valid and useful if the failure rate may be assumed constant   often used for complex units / systems, electronics   and is a general agreement in some reliability standards (Military and Aerospace). It does in this case only relate to the flat region of the bathtub curve, also called the "useful life period". Because of this, it is incorrect to extrapolate MTBF to give an estimate of the service life time of a component, which will typically be much less than suggested by the MTBF due to the much higher failure rates in the "end-of-life wearout" part of the "bathtub curve". The reason for the preferred use for MTBF numbers is that the use of large positive numbers (such as 2000 hours) is more intuitive and easier to remember than very small numbers (such as 0.0005 per hour). The MTBF is an important system parameter in systems where failure rate needs to be managed, in particular for safety systems. The MTBF appears frequently in the engineering design requirements, and governs frequency of required system maintenance and inspections. In special processes called renewal processes, where the time to recover from failure can be neglected and the likelihood of failure remains constant with respect to time, the failure rate is simply the multiplicative inverse of the MTBF (1/ ). A similar ratio used in the transport industries, especially in railways and trucking is "mean distance between failures", a variation which attempts to correlate actual loaded distances to similar reliability needs and practices. Failure rates are important factors in the insurance, finance, commerce and regulatory industries and fundamental to the design of safe systems in a wide variety of applications.
In information theory, the asymptotic equipartition property (AEP) is a general property of the output samples of a stochastic source. It is fundamental to the concept of typical set used in theories of compression. Roughly speaking, the theorem states that although there are many series of results that may be produced by a random process, the one actually produced is most probably from a loosely defined set of outcomes that all have approximately the same chance of being the one actually realized. (This is a consequence of the law of large numbers and ergodic theory.) Although there are individual outcomes which have a higher probability than any outcome in this set, the vast number of outcomes in the set almost guarantees that the outcome will come from the set. One way of intuitively understanding the property is through Crame r's large deviation theorem, which states that the probability of a large deviation from mean decays exponentially with the number of samples. Such results are studied in large deviations theory; intuitively, it is the large deviations that would violate equipartition, but these are unlikely. In the field of pseudorandom number generation, a candidate generator of undetermined quality whose output sequence lies too far outside the typical set by some statistical criteria is rejected as insufficiently random. Thus, although the typical set is loosely defined, practical notions arise concerning sufficient typicality.
A limited dependent variable is a variable whose range of possible values is "restricted in some important way." In econometrics, the term is often used when estimation of the relationship between the limited dependent variable of interest and other variables requires methods that take this restriction into account. For example, this may arise when the variable of interest is constrained to lie between zero and one, as in the case of a probability, or is constrained to be positive, as in the case of wages or hours worked. Limited dependent variable models include: Censoring, where for some individuals in a data set, some data are missing but other data are present; Truncation, where some individuals are systematically excluded from observation (failure to take this phenomenon into account can result in selection bias); Discrete outcomes, such as binary decisions or qualitative data restricted to a small number of categories. Discrete choice models may have either unordered or ordered alternatives; ordered alternatives may take the form of count data or ordered rating responses (such as a Likert scale).
In statistics, Hoeffding's test of independence, named after Wassily Hoeffding, is a test based on the population measure of deviation from independence  where  is the joint distribution function of two random variables, and  and  are their marginal distribution functions. Hoeffding derived an unbiased estimator of  that can be used to test for independence, and is consistent for any continuous alternative. The test should only be applied to data drawn from a continuous distribution, since  has a defect for discontinuous , namely that it is not necessarily zero when . A recent paper describes both the calculation of a sample based version of this measure for use as a test statistic, and calculation of the null distribution of this test statistic.
In probability theory and statistics, an inverse distribution is the distribution of the reciprocal of a random variable. Inverse distributions arise in particular in the Bayesian context of prior distributions and posterior distributions for scale parameters. In the algebra of random variables, inverse distributions are special cases of the class of ratio distributions, in which the numerator random variable has a degenerate distribution.
In probability theory and statistics, the specific name generalized chi-squared distribution (also generalized chi-square distribution) arises in relation to one particular family of variants of the chi-squared distribution. There are several other such variants for which the same term is sometimes used, or which clearly are generalizations of the chi-squared distribution, and which are treated elsewhere: some are special cases of the family discussed here, for example the noncentral chi-squared distribution and the gamma distribution, while the generalized gamma distribution is outside this family. The type of generalisation of the chi-squared distribution that is discussed here is of importance because it arises in the context of the distribution of statistical estimates in cases where the usual statistical theory does not hold. For example, if a predictive model is fitted by least squares but the model errors have either autocorrelation or heteroscedasticity, then a statistical analysis of alternative model structures can be undertaken by relating changes in the sum of squares to an asymptotically valid generalized chi-squared distribution. More specifically, the distribution can be defined in terms of a quadratic form derived from a multivariate normal distribution.  
In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types: Agglomerative: This is a "bottom up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive: This is a "top down" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram. In the general case, the complexity of agglomerative clustering is , which makes them too slow for large data sets. Divisive clustering with an exhaustive search is , which is even worse. However, for some special cases, optimal efficient agglomerative methods (of complexity ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering.
In statistics, Gaussian process emulator is one name for a general type of statistical model that has been used in contexts where the problem is to make maximum use of the outputs of a complicated (often non-random) computer-based simulation model. Each run of the simulation model is computationally expensive and each run is based on many different controlling inputs. The variation of the outputs of the simulation model is expected to vary reasonably smoothly with the inputs, but in an unknown way. The overall analysis involves two models: the simulation model, or "simulator", and the statistical model, or "emulator", which notionally emulates the unknown outputs from the simulator. The Gaussian process emulator model treats the problem from the viewpoint of Bayesian statistics. In this approach, even though the output of the simulation model is fixed for any given set of inputs, the actual outputs are unknown unless the computer model is run and hence can be made the subject of a Bayesian analysis. The main element of the Gaussian process emulator model is that it models the outputs as a Gaussian process on a space that is defined by the model inputs. The model includes a description of the correlation or covariance of the outputs, which enables the model to encompass the idea that differences in the output will be small if there are only small differences in the inputs.
In statistics, the order of a kernel is the first non-zero moment of a kernel.
In the mathematical theory of probability, an absorbing Markov chain is a Markov chain in which every state can reach an absorbing state. An absorbing state is a state that, once entered, cannot be left. Like general Markov chains, there can be continuous-time absorbing Markov chains with an infinite state space. However, this article concentrates on the discrete-time discrete-state-space case.
In descriptive statistics, a box plot or boxplot is a convenient way of graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points. Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution. The spacings between the different parts of the box indicate the degree of dispersion (spread) and skewness in the data, and show outliers. In addition to the points themselves, they allow one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Box plots can be drawn either horizontally or vertically.
Pollyanna Creep is a phrase that originated with John Williams, a California-based economic analyst and statistician. It describes the way the U.S. government has modified the way important economic measures are calculated with the purpose of giving a better impression of economic development. This is a clear reference, in a sarcastic way, to Pollyanna's proverbial optimism. Williams and other economic analysts, such as Kevin P. Phillips, argue that such manipulations distort the perception of electors and economic factors and have ill effects on political and investment decisions.
The ziggurat algorithm is an algorithm for pseudo-random number sampling. Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables. The algorithm is used to generate values from a monotone decreasing probability distribution. It can also be applied to symmetric unimodal distributions, such as the normal distribution, by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from. It was developed by George Marsaglia and others in the 1960s. A typical value produced by the algorithm only requires the generation of one random floating-point value and one random table index, followed by one table lookup, one multiply operation and one comparison. Sometimes (2.5% of the time, in the case of a normal or exponential distribution when using typical table sizes) more computations are required. Nevertheless, the algorithm is computationally much faster than the two most commonly used methods of generating normally distributed random numbers, the Marsaglia polar method and the Box Muller transform, which require at least one logarithm and one square root calculation for each pair of generated values. However, since the ziggurat algorithm is more complex to implement it is best used when large quantities of random numbers are required. The term ziggurat algorithm dates from Marsaglia's paper with Wai Wan Tsang in 2000; it is so named because it is conceptually based on covering the probability distribution with rectangular segments stacked in decreasing order of size, resulting in a figure that resembles a ziggurat.
The Yamartino method (introduced by Robert J. Yamartino in 1984) is an algorithm for calculating an approximation to the standard deviation    of wind direction   during a single pass through the incoming data. The standard deviation of wind direction is a measure of lateral turbulence, and is used in a method for estimating the Pasquill stability category. The simple method for calculating standard deviation requires two passes through the list of values. The first pass determines the average of those values; the second pass determines the sum of the squares of the differences between the values and the average. This double-pass method requires access to all values. A single-pass method can be used for normal data but is unsuitable for angular data such as wind direction where the 0 /360  (or +180 /-180 ) discontinuity forces special consideration. For example, the directions 1 , 0 , and 359  (or -1 ) should not average to the direction 120 ! The Yamartino method solves both problems. The United States Environmental Protection Agency (EPA) has chosen it as the preferred way to compute the standard deviation of wind direction. A further discussion of the Yamartino method, along with other methods of estimating the standard deviation of wind direction can be found in Farrugia & Micallef. It should be mentioned that it is also possible to calculate the exact standard deviation in one pass. However, that method needs slightly more calculation effort.
In queueing theory, Bartlett's theorem gives the distribution of the number of customers in a given part of a system at a fixed time.
In multivariate statistics, if  is a vector of  random variables, and  is an -dimensional symmetric matrix, then the scalar quantity  is known as a quadratic form in .
In macroecology and community ecology, an occupancy frequency distribution (OFD) is the distribution of the numbers of species occupying different numbers of areas. It was first reported in 1918 by the Danish botanist Christen C. Raunki r in his study on plant communities. The OFD is also known as the species-range size distribution in literature.
The Kendall tau rank distance is a metric that counts the number of pairwise disagreements between two ranking lists. The larger the distance, the more dissimilar the two lists are. Kendall tau distance is also called bubble-sort distance since it is equivalent to the number of swaps that the bubble sort algorithm would make to place one list in the same order as the other list. The Kendall tau distance was created by Maurice Kendall.
In statistics, an exchangeable sequence of random variables (also sometimes interchangeable) is a sequence such that future samples behave like earlier samples, meaning formally that any order (of a finite number of samples) is equally likely. This formalizes the notion of "the future being predictable on the basis of past experience." It is closely related to the use of independent and identically-distributed random variables in statistical models. Exchangeable sequences of random variables arise in cases of simple random sampling.
The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between  1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and  1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2 2 contingency table  where n is the total number of observations. While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification. The MCC can be calculated directly from the confusion matrix using the formula:  In this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value. The original formula as given by Matthews was:  This is equal to the formula given above. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness ( p) and Youden's J statistic (Informedness or deltap'). Markedness and Informedness correspond to different directions of information flow and generalize Youden's J statistic, the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.
In probability theory and statistics, the noncentral chi distribution is a generalization of the chi distribution. If  are k independent, normally distributed random variables with means  and variances , then the statistic  is distributed according to the noncentral chi distribution. The noncentral chi distribution has two parameters:  which specifies the number of degrees of freedom (i.e. the number of ), and  which is related to the mean of the random variables  by:
This is a list of probability topics, by Wikipedia page. It overlaps with the (alphabetical) list of statistical topics. There are also the outline of probability and catalog of articles in probability theory. For distributions, see List of probability distributions. For journals, see list of probability journals. For contributors to the field, see list of mathematical probabilists and list of statisticians.
Analytic and enumerative statistical studies are two types of scientific studies: In any statistical study the ultimate aim is to provide a rational basis for action. Enumerative and analytic studies differ by where the action is taken. Deming summarized the distinction between enumerative and analytic studies as follows:  Enumerative study: A statistical study in which action will be taken on the material in the frame being studied.  Analytic study: A statistical study in which action will be taken on the process or cause-system that produced the frame being studied. The aim being to improve practice in the future.  (In a statistical study, the frame is the set from which the Sample (statistics) is taken.) These terms were introduced in Some Theory of Sampling (1950, Chapter 7) by W. Edwards Deming. In other words, an enumerative study is a statistical study in which the focus is on judgment of results, and an analytic study is one in which the focus is on improvement of the process or system which created the results being evaluated and which will continue creating results in the future. A statistical study can be enumerative or analytic, but it cannot be both. This distinction between enumerative and analytic studies is the theory behind the Fourteen Points for Management. Dr. Deming's philosophy is that management should be analytic instead of enumerative. In other words, management should focus on improvement of processes for the future instead of on judgment of current results.  "Use of data requires knowledge about the different sources of uncertainty. Measurement is a process. Is the system of measurement stable or unstable  Use of data requires also understanding of the distinction between enumerative studies and analytic problems."  "The interpretation of results of a test or experiment is something else. It is prediction that a specific change in a process or procedure will be a wise choice, or that no change would be better. Either way the choice is prediction. This is known as an analytic problem, or a problem of inference, prediction."  Statistician Dr. Mike Tveite has pointed out the dangers of attempting to use an enumerative study for prediction.
Univariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved.
In statistics, Dunnett's test is a multiple comparison procedure developed by Canadian statistician Charles Dunnett to compare each of a number of treatments with a single control. Multiple comparisons to a control are also referred to as many-to-one comparisons.
In probability theory and statistics, the coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. It is often expressed as a percentage, and is defined as the ratio of the standard deviation  to the mean  (or its absolute value, ). The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay. It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R.
In estimation theory and statistics, the Crame r Rao bound (CRB) or Crame r Rao lower bound (CRLB), named in honor of Harald Crame r and Calyampudi Radhakrishna Rao who were among the first to derive it, expresses a lower bound on the variance of estimators of a deterministic parameter. The bound is also known as the Crame r Rao inequality or the information inequality. In its simplest form, the bound states that the variance of any unbiased estimator is at least as high as the inverse of the Fisher information. An unbiased estimator which achieves this lower bound is said to be (fully) efficient. Such a solution achieves the lowest possible mean squared error among all unbiased methods, and is therefore the minimum variance unbiased (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU estimator exists. The Crame r Rao bound can also be used to bound the variance of biased estimators of given bias. In some cases, a biased approach can result in both a variance and a mean squared error that are below the unbiased Crame r Rao lower bound; see estimator bias.
Bayesian search theory is the application of Bayesian statistics to the search for lost objects. It has been used several times to find lost sea vessels, for example the USS Scorpion. It also played a key role in the recovery of the flight recorders in the Air France Flight 447 disaster of 2009. Currently, it is being used to locate the remains of Malaysia Airlines Flight 370.

In queueing theory, a discipline within the mathematical theory of probability, mean value analysis (MVA) is a recursive technique for computing expected queue lengths, waiting time at queueing nodes and throughput in equilibrium for a closed separable system of queues. The first approximate techniques were published independently by Schweitzer and Bard, followed later by an exact version by Lavenberg and Reiser published in 1980. It is based on the arrival theorem, which states that when one customer in an M-customer closed system arrives at a service facility he/she observes the rest of the system to be in the equilibrium state for a system with M   1 customers.
Directional statistics (also circular statistics or spherical statistics) is the subdiscipline of statistics that deals with directions (unit vectors in Rn), axes (lines through the origin in Rn) or rotations in Rn. More generally, directional statistics deals with observations on compact Riemannian manifolds.  The fact that 0 degrees and 360 degrees are identical angles, so that for example 180 degrees is not a sensible mean of 2 degrees and 358 degrees, provides one illustration that special statistical methods are required for the analysis of some types of data (in this case, angular data). Other examples of data that may be regarded as directional include statistics involving temporal periods (e.g. time of day, week, month, year, etc.), compass directions, dihedral angles in molecules, orientations, rotations and so on.
The median polish is an exploratory data analysis procedure proposed by the statistician John Tukey. It finds an additively-fit model for data in a two-way layout table (usually, results from a factorial experiment) of the form row effect + column effect + overall median.
Transmission of an infection requires three conditions: an infectious individual a susceptible individual an effective contact between them An effective contact is defined as any kind of contact between two individuals such that, if one individual is infectious and the other susceptible, then the first individual infects the second. Whether or not a particular kind of contact will be effective depends on the infectious agent and its route of transmission. The effective contact rate (denoted  ) in a given population for a given infectious disease is measured in effective contacts per unit time. This may be expressed as the total contact rate (the total number of contacts, effective or not, per unit time, denoted  ), multiplied by the risk of infection, given contact between an infectious and a susceptible individual. This risk is called the transmission risk and is denoted p. Thus:  The total contact rate,  , will generally be greater than the effective contact rate,  , since not all contacts result in infection. That is to say, p is almost always less than 1 and it can never be greater than 1, since it is effectively the probability of transmission occurring. This relation formalises the fact that the effective contact rate depends not only on the social patterns of contact in a particular society ( ) but also on the specific types of contact and the pathology of the infectious organism (p). For example, it has been shown that a concurrent sexually transmitted infection can substantially increase the probability (p) of infecting a susceptible with HIV. Therefore, one way to reduce the value of p (and hence lower HIV transmission rates) might be to treat other sexually transmitted infections. There are a number of difficulties in using this relation. The first is that it is very difficult to measure contact rates because they vary widely between individuals and groups, and within the same group at different times. For sexually transmitted infections, large scale studies of sexual behaviour have been set up to estimate the contact rate. In developed countries for serious diseases such as AIDS or tuberculosis, contact tracing is often carried out when a patient is diagnosed (the patient and medical authorities try to inform every possible contact the patient may have made since infection). This, however, is not so much a research tool and more to alert the contacts to the possibility that they may be infected and so can seek medical treatment and avoiding passing on the disease if they have contracted it. A second consideration is that it is generally thought unethical to carry out direct experiments to establish per-contact infection risks as this would require the deliberate exposure of individuals to infectious agents. The Common Cold Unit that researched cold transmission in the UK between 1946 and 1989 was a notable exception. It is also possible to estimate the transmission risk in certain circumstances where exposures to infection have been documented, for example the rate of infection among nurses who have accidentally pricked their fingers with a needle that had previously been used with contaminated blood. A more direct assessment of transmission risks can be provided by a contact study, which is often carried out because of an outbreak (such a study was carried out during the SARS outbreak of 2002 3). The first (or primary) case within a defined group (such as a school or family) is identified and people infected by this individual (called secondary cases) are documented. If the number of susceptibles in the group is n and the number of secondary cases is x, then an estimation of the transmission risk is  Here, p is the same parameter as before but it has been calculated in a different way. To reflect this, it is called the secondary attack rate (it is really a risk, of course, and not a rate, but the term is still commonly used). Even if the whole group in question is susceptible, x is generally smaller than the basic reproduction number for the disease. That is defined as the number of individuals each infected individual will go on to infect themselves, in a population with no resistance to the disease. The basic reproduction number includes all secondary cases infected by a primary case, while x is only the number of secondary cases within the group in question. Secondary attack rates are useful for comparisons between vaccinated and unvaccinated groups and hence assessing the efficacy of vaccinations against the disease under inspection. However, there are inevitably complications with such contact studies. It is not always obvious which members of the group are susceptible and distinguishing between secondary and subsequent cases (for example, those infected by the secondary cases are tertiary cases and so on) can be difficult. Also, the possibility of infection from an outsider must be ignored. Despite these problems, the parameters p and   are powerful tools in the mathematical modelling of epidemics. But it should always be remembered that a model is only as good as the assumptions on which it is based and the data from which its parameters are calculated.
In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable  is log-normally distributed, then  has a normal distribution. Likewise, if  has a normal distribution, then  has a log-normal distribution. A random variable which is log-normally distributed takes only positive real values. The distribution is occasionally referred to as the Galton distribution or Galton's distribution, after Francis Galton. The log-normal distribution also has been associated with other names, such as McAlister, Gibrat and Cobb Douglas. A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive. This is justified by considering the central limit theorem in the log domain. The log-normal distribution is the maximum entropy probability distribution for a random variate  for which the mean and variance of  are specified.
In statistics and econometrics, extremum estimators is a wide class of estimators for parametric models that are calculated through maximization (or minimization) of a certain objective function, which depends on the data. The general theory of extremum estimators was developed by Amemiya (1985).
In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. The number of independent ways by which a dynamic system can move, without violating any constraint imposed on it, is called number of degrees of freedom. In other words, the number of degrees of freedom can be defined as the minimum number of independent coordinates that can specify the position of the system completely. Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter are called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself (i.e. the sample variance has N-1 degrees of freedom, since it is computed from N random scores minus the only 1 parameter estimated as intermediate step, which is the sample mean). Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of "free" components (how many components need to be known before the vector is fully determined). The term is most often used in the context of linear models (linear regression, analysis of variance), where certain random vectors are constrained to lie in linear subspaces, and the number of degrees of freedom is the dimension of the subspace. The degrees of freedom are also commonly associated with the squared lengths (or "sum of squares" of the coordinates) of such vectors, and the parameters of chi-squared and other distributions that arise in associated statistical testing problems. While introductory textbooks may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept. Walker (1940) has stated this succinctly as "the number of observations minus the number of necessary relations among these observations."
A gamma process is a random process with independent gamma distributed increments. Often written as , it is a pure-jump increasing Le vy process with intensity measure , for positive . Thus jumps whose size lies in the interval  occur as a Poisson process with intensity  The parameter  controls the rate of jump arrivals and the scaling parameter  inversely controls the jump size. It is assumed that the process starts from a value 0 at t=0. The gamma process is sometimes also parameterised in terms of the mean () and variance () of the increase per unit time, which is equivalent to  and .
In statistics, non-linear iterative partial least squares (NIPALS) is an algorithm for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the 'omics sciences (e.g., genomics, metabolomics) it is usually only necessary to compute the first few principal components. The nonlinear iterative partial least squares (NIPALS) algorithm calculates t1 and p1' from X. The outer product, t1p1' can then be subtracted from X leaving the residual matrix E1. This can be then used to calculate subsequent principal components.  This results in a dramatic reduction in computational time since calculation of the covariance matrix is avoided.
In statistics, the Breusch Godfrey test, named after Trevor S. Breusch and Leslie G. Godfrey, is used to assess the validity of some of the modelling assumptions inherent in applying regression-like models to observed data series. In particular, it tests for the presence of serial dependence that has not been included in a proposed model structure and which, if present, would mean that incorrect conclusions would be drawn from other tests, or that sub-optimal estimates of model parameters are obtained if it is not taken into account. The regression models to which the test can be applied include cases where lagged values of the dependent variables are used as independent variables in the model's representation for later observations. This type of structure is common in econometric models. A similar assessment can be also carried out with the Durbin Watson test. Because the test is based on the idea of Lagrange multiplier testing, it is sometimes referred to as LM test for serial correlation.
In signal processing, white noise is a random signal with a constant power spectral density. The term is used, with this or similar meanings, in many scientific and technical disciplines, including physics, acoustic engineering, telecommunications, statistical forecasting, and many more. White noise refers to a statistical model for signals and signal sources, rather than to any specific signal.  In discrete time, white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance; a single realization of white noise is a random shock. Depending on the context, one may also require that the samples be independent and have the same probability distribution (in other words i.i.d is a simplest representative of the white noise). In particular, if each sample has a normal distribution with zero mean, the signal is said to be Gaussian white noise. The samples of a white noise signal may be sequential in time, or arranged along one or more spatial dimensions. In digital image processing, the pixels of a white noise image are typically arranged in a rectangular grid, and are assumed to be independent random variables with uniform probability distribution over some interval. The concept can be defined also for signals spread over more complicated domains, such as a sphere or a torus.  An infinite-bandwidth white noise signal is a purely theoretical construction. The bandwidth of white noise is limited in practice by the mechanism of noise generation, by the transmission medium and by finite observation capabilities. Thus, a random signal is considered "white noise" if it is observed to have a flat spectrum over the range of frequencies that is relevant to the context. For an audio signal, for example, the relevant range is the band of audible sound frequencies, between 20 to 20,000 Hz. Such a signal is heard as a hissing sound, resembling the /sh/ sound in "ash". In music and acoustics, the term "white noise" may be used for any signal that has a similar hissing sound. White noise draws its name from white light, although light that appears white generally does not have a flat spectral power density over the visible band. The term white noise is sometimes used in the context of phylogenetically based statistical methods to refer to a lack of phylogenetic pattern in comparative data. It is sometimes used in non technical contexts, in the metaphoric sense of "random talk without meaningful contents".
In statistics, the Jonckheere trend test (sometimes called the Jonckheere Terpstra test) is a test for an ordered alternative hypothesis within an independent samples (between-participants) design. It is similar to the Kruskal Wallis test in that the null hypothesis is that several independent samples are from the same population. However, with the Kruskal Wallis test there is no a priori ordering of the populations from which the samples are drawn. When there is an a priori ordering, the Jonckheere test has more statistical power than the Kruskal Wallis test. The null and alternative hypotheses can be conveniently expressed in terms of population medians for k populations (where k > 2). Letting  i be the population median for the ith population, the null hypothesis is:  The alternative hypothesis is that the population medians have an a priori ordering e.g.:           with at least one strict inequality.
The Allan variance (AVAR), also known as two-sample variance, is a measure of frequency stability in clocks, oscillators and amplifiers. It is named after David W. Allan. It is expressed mathematically as  The Allan deviation (ADEV) is the square root of Allan variance. It is also known as sigma-tau, and is expressed mathematically as  The M-sample variance is a measure of frequency stability using M samples, time T between measures and observation time . M-sample variance is expressed as  The Allan variance is intended to estimate stability due to noise processes and not that of systematic errors or imperfections such as frequency drift or temperature effects. The Allan variance and Allan deviation describe frequency stability, i.e. the stability in frequency. See also the section entitled "Interpretation of value" below. There are also different adaptations or alterations of Allan variance, notably the modified Allan variance MAVAR or MVAR, the total variance, and the Hadamard variance. There also exist time stability variants such as time deviation TDEV or time variance TVAR. Allan variance and its variants have proven useful outside the scope of timekeeping and are a set of improved statistical tools to use whenever the noise processes are not unconditionally stable, thus a derivative exists. The general M-sample variance remains important since it allows dead time in measurements and bias functions allows conversion into Allan variance values. Nevertheless, for most applications the special case of 2-sample, or "Allan variance" with  is of greatest interest.
In probability theory, inverse probability is an obsolete term for the probability distribution of an unobserved variable. Today, the problem of determining an unobserved variable (by whatever method) is called inferential statistics, the method of inverse probability (assigning a probability distribution to an unobserved variable) is called Bayesian probability, the "distribution" of an unobserved variable given data is rather the likelihood function (which is not a probability distribution), and the distribution of an unobserved variable, given both data and a prior distribution, is the posterior distribution. The development of the field and terminology from "inverse probability" to "Bayesian probability" is described by Fienberg (2006).  The term "inverse probability" appears in an 1837 paper of De Morgan, in reference to Laplace's method of probability (developed in a 1774 paper, which independently discovered and popularized Bayesian methods, and 1812 book), though the term "inverse probability" does not occur in these. Fisher uses the term in 1922, referring to "the fundamental paradox of inverse probability" as the source of the confusion between statistical terms that refer to the true value to be estimated, with the actual value arrived at by the estimation method, which is subject to error. , (See reprint in .) Later Jeffreys uses the term in his defense of the methods of Bayes and Laplace, in 1939. The term "Bayesian", which displaced "inverse probability", was introduced by Ronald Fisher around 1950. Inverse probability, variously interpreted, was the dominant approach to statistics until the development of frequentism in the early 20th century by Ronald Fisher, Jerzy Neyman and Egon Pearson. Following the development of frequentism, the terms frequentist and Bayesian developed to contrast these approaches, and became common in the 1950s.
The tyranny of averages is a phrase used in applied statistics to describe the often overlooked fact that the mean does not provide any information about the shape of the probability distribution of a data set or skewness, and that decisions or analysis based on only the mean as opposed to median and standard deviation may be faulty. A UN Development Program press release discusses a real world example:  A new report launched 1 July [2005] warns that in Asia and the Pacific, the rising prosperity and fast growth in populous countries like China and India is hiding widespread extreme poverty in the Least Developed Countries (LDCs). The result is potentially very debilitating to development efforts in the 14 Asia-Pacific LDCs. This  tyranny of averages  to which the report refers tends to mask the stark contrast between the Asia-Pacific LDCs  sluggish economies and the success of their far more populous neighbours.
Differential entropy (also referred to as continuous entropy) is a concept in information theory that began as an attempt by Shannon to extend the idea of (Shannon) entropy, a measure of average surprisal of a random variable, to continuous probability distributions. Unfortunately, Shannon did not derive this formula, and rather just assumed it was the correct continuous analogue of discrete entropy, but it is not. The actual continuous version of discrete entropy is the limiting density of discrete points. Differential entropy (described here) is commonly encountered in the literature, but it is a limiting case of the LDDP, and one that loses its fundamental association with discrete entropy.
GLIM (an acronym for Generalized Linear Interactive Modelling) is a statistical software program for fitting generalized linear models (GLMs). It was developed by the Royal Statistical Society's Working Party on Statistical Computing (later renamed the GLIM Working Party), chaired initially by John Nelder. It was first released in 1974 with the last major release, GLIM4, in 1993. GLIM was distributed by the Numerical Algorithms Group (NAG). GLIM was notable for being the first package capable of fitting a wide range of generalized linear models in a unified framework, and for encouraging an interactive, iterative approach to statistical modelling. GLIM used a command-line interface and allowed users to define their own macros. Many articles in academic journals were written about the use of GLIM. GLIM was reviewed in The American Statistician in 1994, along with other software for fitting generalized linear models. The GLIMPSE system was later developed to provide a knowledge based front-end for GLIM. GLIM is no longer actively developed or distributed.
In statistics Hotelling's T-squared distribution is a univariate distribution proportional to the F-distribution and arises importantly as the distribution of a set of statistics which are natural generalizations of the statistics underlying Student's t-distribution. In particular, the distribution arises in multivariate statistics in undertaking tests of the differences between the (multivariate) means of different populations, where tests for univariate problems would make use of a t-test. The distribution is named for Harold Hotelling, who developed it as a generalization of Student's t-distribution.
In probability theory, Isserlis  theorem or Wick s theorem is a formula that allows one to compute higher-order moments of the multivariate normal distribution in terms of its covariance matrix. It is named after Leon Isserlis. This theorem is particularly important in particle physics, where it is known as Wick's theorem after the work of Wick (1950). Other applications include the analysis of portfolio returns, quantum field theory and generation of colored noise.
A geometric stable distribution or geo-stable distribution is a type of leptokurtic probability distribution. Geometric stable distributions were introduced in Klebanov, L. B., Maniya, G. M., and Melamed, I. A. (1985). A problem of Zolotarev and analogs of infinitely divisible and stable distributions in a scheme for summing a random number of random variables. These distributions are analogues for stable distributions for the case when the number of summands is random, independent of the distribution of summand, and having geometric distribution. The geometric stable distribution may be symmetric or asymmetric. A symmetric geometric stable distribution is also referred to as a Linnik distribution. The Laplace distribution and asymmetric Laplace distribution are special cases of the geometric stable distribution. The Laplace distribution is also a special case of a Linnik distribution. The Mittag Leffler distribution is also a special case of a geometric stable distribution. The geometric stable distribution has applications in finance theory.
In probability theory and statistics, the negative binomial distribution is a discrete probability distribution of the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures (denoted r) occurs. For example, if we define a "1" as failure, all non-"1"s as successes, and we throw a die repeatedly until the third time  1  appears (r = three failures), then the probability distribution of the number of non- 1 s that had appeared will be a negative binomial. The Pascal distribution (after Blaise Pascal) and Polya distribution (for George Po lya) are special cases of the negative binomial. There is a convention among engineers, climatologists, and others to reserve  negative binomial  in a strict sense or  Pascal  for the case of an integer-valued stopping-time parameter r, and use  Polya  for the real-valued case. For occurrences of  contagious  discrete events, like tornado outbreaks, the Polya distributions can be used to give more accurate models than the Poisson distribution by allowing the mean and variance to be different, unlike the Poisson.  Contagious  events have positively correlated occurrences causing a larger variance than if the occurrences were independent, due to a positive covariance term.
In probability theory and statistics, the Exponential-Logarithmic (EL) distribution is a family of lifetime distributions with decreasing failure rate, defined on the interval [0,  ). This distribution is parameterized by two parameters  and .
Multilevel models (also hierarchical linear models, nested models, mixed models, random coefficient, random-effects models, random parameter models, or split-plot designs) are statistical models of parameters that vary at more than one level. An example could be a model of student performance that contains measures for individual students as well as measures for classrooms within which the students are grouped. These models can be seen as generalizations of linear models (in particular, linear regression), although they can also extend to non-linear models. These models became much more popular after sufficient computing power and software became available. Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level (i.e., nested data). The units of analysis are usually individuals (at a lower level) who are nested within contextual/aggregate units (at a higher level). While the lowest level of data in multilevel models is usually an individual, repeated measurements of individuals may also be examined. As such, multilevel models provide an alternative type of analysis for univariate or multivariate analysis of repeated measures. Individual differences in growth curves may be examined (see growth model). Furthermore, multilevel models can be used as an alternative to ANCOVA, where scores on the dependent variable are adjusted for covariates (i.e., individual differences) before testing treatment differences. Multilevel models are able to analyze these experiments without the assumptions of homogeneity-of-regression slopes that is required by ANCOVA. Multilevel models can be used on data with many levels, although 2-level models are the most common and the rest of this article deals only with these. The dependent variable must be examined at the lowest level of analysis.
In the world of finance and investments, statistical arbitrage is used in two related but distinct ways: In academic literature, "statistical arbitrage" is opposed to (deterministic) arbitrage. In deterministic arbitrage, a sure profit can be obtained from being long some securities and short others. In statistical arbitrage, there is a statistical mispricing of one or more assets based on the expected value of these assets. In other words, statistical arbitrage conjectures statistical mispricings of price relationships that are true in expectation, in the long run when repeating a trading strategy. Among those who follow the hedge fund industry, "statistical arbitrage" refers to a particular category of hedge funds (other categories include global macro, convertible arbitrage, and so on). In this narrower sense, statistical arbitrage is often abbreviated as Stat Arb or StatArb. According to Andrew Lo, StatArb "refers to highly technical short-term mean-reversion strategies involving large numbers of securities (hundreds to thousands, depending on the amount of risk capital), very short holding periods (measured in days to seconds), and substantial computational, trading, and information technology (IT) infrastructure".
In probability theory, a beta negative binomial distribution is the probability distribution of a discrete random variable X equal to the number of failures needed to get r successes in a sequence of independent Bernoulli trials where the probability p of success on each trial is constant within any given experiment but is itself a random variable following a beta distribution, varying between different experiments. Thus the distribution is a compound probability distribution. This distribution has also been called both the inverse Markov-Po lya distribution and the generalized Waring distribution. A shifted form of the distribution has been called the beta-Pascal distribution. If parameters of the beta distribution are   and  , and if  where  then the marginal distribution of X is a beta negative binomial distribution:  In the above, NB(r, p) is the negative binomial distribution and B( ,  ) is the beta distribution. Recurrence relation
In statistics, a sampling distribution or finite-sample distribution is the probability distribution of a given statistic based on a random sample. Sampling distributions are important in statistics because they provide a major simplification en route to statistical inference. More specifically, they allow analytical considerations to be based on the sampling distribution of a statistic, rather than on the joint probability distribution of all the individual sample values.
Predictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events. In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions. The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement. Predictive analytics is used in actuarial science, marketing, financial services, insurance, telecommunications, retail, travel, healthcare, child protection, pharmaceuticals, capacity planning and other fields. One of the most well known applications is credit scoring, which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.
The need for function approximations arises in many branches of applied mathematics, and computer science in particular. In general, a function approximation problem asks us to select a function among a well-defined class that closely matches ("approximates") a target function in a task-specific way. One can distinguish two major classes of function approximation problems: First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.). Second, the target function, call it g, may be unknown; instead of an explicit formula, only a set of points of the form (x, g(x)) is provided. Depending on the structure of the domain and codomain of g, several techniques for approximating g may be applicable. For example, if g is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of g is a finite set, one is dealing with a classification problem instead. A related problem, online time series approximation, is to summarize the data in one-pass and construct an approximate representation that can support a variety of timeseries queries with bounds on worst-case error. To some extent the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.
In machine learning, pattern recognition and in image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction. When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a features vector). This process is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data.
Financial econometrics is the subject of research that has been defined as the application of statistical methods to financial market data. Financial econometrics is a branch of financial economics, in the field of economics. Areas of study include capital markets , financial institutions, corporate finance and corporate governance. Topics often revolve around asset valuation of individual stocks, bonds, derivatives, currencies and other financial instruments. Financial econometrics is different from other forms of econometrics because the emphasis is usually on analyzing the prices of financial assets traded at competitive, liquid markets. People working in the finance industry or researching the finance sector often use econometric techniques in a range of activities   for example, in support of portfolio management and in the valuation of securities. Financial econometrics is essential for risk management when it is important to know how often 'bad' investment outcomes are expected to occur over future days, weeks, months and years.
In probability theory and directional statistics, the von Mises distribution (also known as the circular normal distribution or Tikhonov distribution) is a continuous probability distribution on the circle. It is a close approximation to the wrapped normal distribution, which is the circular analogue of the normal distribution. A freely diffusing angle  on a circle is a wrapped normally distributed random variable with an unwrapped variance that grows linearly in time. On the other hand, the von Mises distribution is the stationary distribution of a drift and diffusion process on the circle in a harmonic potential, i.e. with a preferred orientation. The von Mises distribution is the maximum entropy distribution for a given expectation value of . The von Mises distribution is a special case of the von Mises Fisher distribution on the N-dimensional sphere.
In statistics, the fraction of variance unexplained (FVU) in the context of a regression task is the fraction of variance of the regressand (dependent variable) Y which cannot be explained, i.e., which is not correctly predicted, by the explanatory variables X.
In economics, discrete choice models, or qualitative choice models, describe, explain, and predict choices between two or more discrete alternatives, such as entering or not entering the labor market, or choosing between modes of transport. Such choices contrast with standard consumption models in which the quantity of each good consumed is assumed to be a continuous variable. In the continuous case, calculus methods (e.g. first-order conditions) can be used to determine the optimum amount chosen, and demand can be modeled empirically using regression analysis. On the other hand, discrete choice analysis examines situations in which the potential outcomes are discrete, such that the optimum is not characterized by standard first-order conditions. Thus, instead of examining  how much  as in problems with continuous choice variables, discrete choice analysis examines  which one.  However, discrete choice analysis can also be used to examine the chosen quantity when only a few distinct quantities must be chosen from, such as the number of vehicles a household chooses to own  and the number of minutes of telecommunications service a customer decides to purchase. Techniques such as logistic regression and probit regression can be used for empirical analysis of discrete choice. Discrete choice models theoretically or empirically model choices made by people among a finite set of alternatives. The models have been used to examine, e.g., the choice of which car to buy, where to go to college, which mode of transport (car, bus, rail) to take to work among numerous other applications. Discrete choice models are also used to examine choices by organizations, such as firms or government agencies. In the discussion below, the decision-making unit is assumed to be a person, though the concepts are applicable more generally. Daniel McFadden won the Nobel prize in 2000 for his pioneering work in developing the theoretical basis for discrete choice. Discrete choice models statistically relate the choice made by each person to the attributes of the person and the attributes of the alternatives available to the person. For example, the choice of which car a person buys is statistically related to the person s income and age as well as to price, fuel efficiency, size, and other attributes of each available car. The models estimate the probability that a person chooses a particular alternative. The models are often used to forecast how people s choices will change under changes in demographics and/or attributes of the alternatives. Discrete choice models specify the probability that an individual chooses an option among a set of alternatives. The probabilistic description of discrete choice behavior is used not to reflect individual behavior that is viewed as intrinsically probabilistic. Rather, it is the lack of information that leads us to describe choice in a probabilistic fashion. In practice, we cannot know all factors affecting individual choice decisions as their determinants are partially observed or imperfectly measured. Therefore, discrete choice models rely on stochastic assumptions and specifications to account for unobserved factors related to a) choice alternatives, b) taste variation over people (interpersonal heterogeneity) and over time (intra-individual choice dynamics), and c) heterogeneous choice sets. The different formulations have been summarized and classified into groups of models.
Epi Info is public domain statistical software for epidemiology developed by Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia (USA). Epi Info has been in existence for over 20 years and is currently available for Microsoft Windows. The program allows for electronic survey creation, data entry, and analysis. Within the analysis module, analytic routines include t-tests, ANOVA, nonparametric statistics, cross tabulations and stratification with estimates of odds ratios, risk ratios, and risk differences, logistic regression (conditional and unconditional), survival analysis (Kaplan Meier and Cox proportional hazard), and analysis of complex survey data. The software is in the public domain, free, and can be downloaded from http://www.cdc.gov/epiinfo. Limited support is available. An analysis conducted in 2003 documented over 1,000,000 downloads of Epi Info from 180 countries.
Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as classification function: Sensitivity (also called the true positive rate, or the recall in some fields) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition). Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition). Thus sensitivity quantifies the avoiding of false negatives, as specificity does for false positives. For any test, there is usually a trade-off between the measures. For instance, in an airport security setting in which one is testing for potential threats to safety, scanners may be set to trigger on low-risk items like belt buckles and keys (low specificity), in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard (high sensitivity). This trade-off can be represented graphically as a receiver operating characteristic curve. A perfect predictor would be described as 100% sensitive (e.g., all sick are identified as sick) and 100% specific (e.g., no healthy are identified as sick); however, theoretically any predictor will possess a minimum error bound known as the Bayes error rate.
The Ljung Box test (named for Greta M. Ljung and George E. P. Box) is a type of statistical test of whether any of a group of autocorrelations of a time series are different from zero. Instead of testing randomness at each distinct lag, it tests the "overall" randomness based on a number of lags, and is therefore a portmanteau test. This test is sometimes known as the Ljung Box Q test, and it is closely connected to the Box Pierce test (which is named after George E. P. Box and David A. Pierce). In fact, the Ljung Box test statistic was described explicitly in the paper that led to the use of the Box-Pierce statistic, and from which that statistic takes its name. The Box-Pierce test statistic is a simplified version of the Ljung Box statistic for which subsequent simulation studies have shown poor performance. The Ljung Box test is widely applied in econometrics and other applications of time series analysis.
Statistical parsing is a group of parsing methods within natural language processing. The methods have in common that they associate grammar rules with a probability. Grammar rules are traditionally viewed in computational linguistics as defining the valid sentences in a language. Within this mindset, the idea of associating each rule with a probability then provides the relative frequency of any given grammar rule and, by deduction, the probability of a complete parse for a sentence. (The probability associated with a grammar rule may be induced, but the application of that grammar rule within a parse tree and the computation of the probability of the parse tree based on its component rules is a form of deduction.) Using this concept, statistical parsers make use of a procedure to search over a space of all candidate parses, and the computation of each candidate's probability, to derive the most probable parse of a sentence. The Viterbi algorithm is one popular method of searching for the most probable parse. "Search" in this context is an application of the very useful search algorithm in artificial intelligence. As an example, think about the sentence "The can can hold water". A reader would instantly see that there is an object called "the can" and that this object is performing the action 'can' (i.e. is able to); and the thing the object is able to do is "hold"; and the thing the object is able to hold is "water". Using more linguistic terminology, "The can" is a noun phrase composed of a determiner followed by a noun, and "can hold water" is a verb phrase which is itself composed of a verb followed by a verb phrase. But is this the only interpretation of the sentence  Certainly "The can can" is a perfectly valid noun-phrase referring to a type of dance, and "hold water" is also a valid verb-phrase, although the coerced meaning of the combined sentence is non-obvious. This lack of meaning is not seen as a problem by most linguists (for a discussion on this point, see Colorless green ideas sleep furiously) but from a pragmatic point of view it is desirable to obtain the first interpretation rather than the second and statistical parsers achieve this by ranking the interpretations based on their probability. (In this example various assumptions about the grammar have been made, such as a simple left-to-right derivation rather than head-driven, its use of noun-phrases rather than the currently fashionable determiner-phrases, and no type-check preventing a concrete noun being combined with an abstract verb phrase. None of these assumptions affect the thesis of the argument and a comparable argument can be made using any other grammatical formalism.) There are a number of methods that statistical parsing algorithms frequently use. While few algorithms will use all of these they give a good overview of the general field. Most statistical parsing algorithms are based on a modified form of chart parsing. The modifications are necessary to support an extremely large number of grammatical rules and therefore search space, and essentially involve applying classical artificial intelligence algorithms to the traditionally exhaustive search. Some examples of the optimisations are only searching a likely subset of the search space (stack search), for optimising the search probability (Baum-Welch algorithm) and for discarding parses that are too similar to be treated separately (Viterbi algorithm).
The standard error (SE) is the standard deviation of the sampling distribution of a statistic, most commonly of the mean. The term may also be used to refer to an estimate of that standard deviation, derived from a particular sample used to compute the estimate. For example, the sample mean is the usual estimator of a population mean. However, different samples drawn from that same population would in general have different values of the sample mean, so there is a distribution of sampled means (with its own mean and variance). The standard error of the mean (SEM) (i.e., of using the sample mean as a method of estimating the population mean) is the standard deviation of those sample means over all possible samples (of a given size) drawn from the population. Secondly, the standard error of the mean can refer to an estimate of that standard deviation, computed from the sample of data being analyzed at the time. In regression analysis, the term "standard error" is also used in the phrase standard error of the regression to mean the ordinary least squares estimate of the standard deviation of the underlying errors.
In mathematics, Laplace's principle is a basic theorem in large deviations theory, similar to Varadhan's lemma. It gives an asymptotic expression for the Lebesgue integral of exp(   (x)) over a fixed set A as   becomes large. Such expressions can be used, for example, in statistical mechanics to determining the limiting behaviour of a system as the temperature tends to absolute zero.
In statistics, Studentization, named after William Sealy Gosset, who wrote under the pseudonym Student, is the adjustment consisting of division of a first-degree statistic derived from a sample, by a sample-based estimate of a population standard deviation. The term is also used for the standardisation of a higher-degree statistic by another statistic of the same degree: for example, an estimate of the third central moment would be standardised by dividing by the cube of the sample standard deviation. A simple example is the process of dividing a sample mean by the sample standard deviation when data arise from a location-scale family. The consequence of "Studentization" is that the complication of treating the probability distribution of the mean, which depends on both the location and scale parameters, has been reduced to considering a distribution which depends only on the location parameter. However, the fact that a sample standard deviation is used, rather than the unknown population standard deviation, complicates the mathematics of finding the probability distribution of a Studentized statistic. In computational statistics, the idea of using Studentized statistics is of some importance in the development of confidence intervals with improved properties in the context of resampling and, in particular, bootstrapping.
In statistics and information geometry, divergence or a contrast function is a function which establishes the "distance" of one probability distribution to the other on a statistical manifold. The divergence is a weaker notion than that of the distance, in particular the divergence need not be symmetric (that is, in general the divergence from p to q is not equal to the divergence from q to p), and need not satisfy the triangle inequality.
The Kish grid or Kish selection grid is a method for selecting members within a household to be interviewed. It uses a pre-assigned table of random numbers to find the person to be interviewed. It was developed by statistician Leslie Kish in 1949. It is a technique widely used in survey research. However, in telephone surveys, the next-birthday method is sometimes preferred to the Kish grid.
A risk benefit ratio is the ratio of the risk of an action to its potential benefits. Risk benefit analysis is analysis that seeks to quantify the risk and benefits and hence their ratio. Exposure to personal risk is recognized as a normal aspect of everyday life. A certain level of risk in our lives is accepted as necessary to achieve certain benefits. With most of these risks, some sort of control over the situation is felt. For example, driving an automobile is a risk most people take daily. "The controlling factor appears to be their perception of their individual ability to manage the risk-creating situation." Analyzing the risk of a situation is, however, very dependent on the individual doing the analysis. When individuals are exposed to involuntary risk (a risk over which they have no control), they make risk aversion their primary goal. Under these circumstances individuals require the probability of risk to be as much as one thousand times smaller than for the same situation under their perceived control.
In statistics, one purpose for the analysis of variance (ANOVA) is to analyze differences in means between groups. The test statistic, F, assumes independence of observations, homogeneous variances, and population normality. ANOVA on ranks is a statistic designed for situations when the normality assumption has been violated.
Control charts, also known as Shewhart charts (after Walter A. Shewhart) or process-behavior charts, in statistical process control are tools used to determine if a manufacturing or business process is in a state of statistical control.
Lot quality assurance sampling (LQAS) is a random sampling methodology, originally developed in the 1920s  as a method of quality control in industrial production. Compared to similar sampling techniques like stratified and cluster sampling, LQAS provides less information but often requires substantially smaller sample sizes.
In statistical quality control, the p-chart is a type of control chart used to monitor the proportion of nonconforming units in a sample, where the sample proportion nonconforming is defined as the ratio of the number of nonconforming units to the sample size, n. The p-chart only accommodates "pass"/"fail"-type inspection as determined by one or more go-no go gauges or tests, effectively applying the specifications to the data before they are plotted on the chart. Other types of control charts display the magnitude of the quality characteristic under study, making troubleshooting possible directly from those charts.
In econometrics, the generalized method of moments (GMM) is a generic method for estimating parameters in statistical models. Usually it is applied in the context of semiparametric models, where the parameter of interest is finite-dimensional, whereas the full shape of the distribution function of the data may not be known, and therefore maximum likelihood estimation is not applicable. The method requires that a certain number of moment conditions were specified for the model. These moment conditions are functions of the model parameters and the data, such that their expectation is zero at the true values of the parameters. The GMM method then minimizes a certain norm of the sample averages of the moment conditions. The GMM estimators are known to be consistent, asymptotically normal, and efficient in the class of all estimators that do not use any extra information aside from that contained in the moment conditions. GMM was developed by Lars Peter Hansen in 1982 as a generalization of the method of moments which was introduced by Karl Pearson in 1894. Hansen shared the 2013 Nobel Prize in Economics in part for this work.
In statistical theory, the field of high-dimensional statistics studies data whose dimension is larger than dimensions considered in classical multivariate analysis. High-dimensional statistics relies on the theory of random vectors. In many applications, the dimension of the data vectors may be larger than the sample size.
In statistics, the inverse matrix gamma distribution is a generalization of the inverse gamma distribution to positive-definite matrices. It is a more general version of the inverse Wishart distribution, and is used similarly, e.g. as the conjugate prior of the covariance matrix of a multivariate normal distribution or matrix normal distribution. The compound distribution resulting from compounding a matrix normal with an inverse matrix gamma prior over the covariance matrix is a generalized matrix t-distribution. This reduces to the inverse Wishart distribution with .
Spatial dependence is the spatial relationship of variable values (for themes defined over space, such as rainfall) or locations (for themes defined as objects, such as cities). Spatial dependence is measured as the existence of statistical dependence in a collection of random variables, each of which is associated with a different geographical location. Spatial dependence is of importance in applications where it is reasonable to postulate the existence of corresponding set of random variables at locations that have not been included in a sample. Thus rainfall may be measured at a set of rain gauge locations, and such measurements can be considered as outcomes of random variables, but rainfall clearly occurs at other locations and would again be random. Because rainfall exhibits properties of autocorrelation, spatial interpolation techniques can be used to estimate rainfall amounts at locations near measured locations. As with other types of statistical dependence, the presence of spatial dependence generally leads to estimates of an average value from a sample being less accurate than had the samples been independent, although if negative dependence exists a sample average can be better than in the independent case. A different problem than that of estimating an overall average is that of spatial interpolation: here the problem is to estimate the unobserved random outcomes of variables at locations intermediate to places where measurements are made, on that there is spatial dependence between the observed and unobserved random variables. Tools for exploring spatial dependence include: spatial correlation, spatial covariance functions and semivariograms. Methods for spatial interpolation include Kriging, which is a type of best linear unbiased prediction. The topic of spatial dependence is of importance to geostatistics and spatial analysis.
The SAS language is a computer programming language used for statistical analysis, originated by a project at the North Carolina State University. It can read in data from common spreadsheets and databases and output the results of statistical analyses in tables, graphs, and as RTF, HTML and PDF documents. The SAS language runs under compilers that can be used on Microsoft Windows, Linux, and various other UNIX and mainframe computers. The SAS System and World Programming System are SAS language compilers.
Complete-linkage clustering is one of several methods of agglomerative hierarchical clustering. At the beginning of the process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. At each step, the two clusters separated by the shortest distance are combined. The definition of 'shortest distance' is what differentiates between the different agglomerative clustering methods. In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other. The shortest of these links that remains at any step causes the fusion of the two clusters whose elements are involved. The method is also known as farthest neighbour clustering. The result of the clustering can be visualized as a dendrogram, which shows the sequence of cluster fusion and the distance at which each fusion took place. Mathematically, the complete linkage function   the distance  between clusters  and    is described by the following expression :  where  is the distance between elements  and  ;  and  are two sets of elements (clusters) Complete linkage clustering avoids a drawback of the alternative single linkage method - the so-called chaining phenomenon, where clusters formed via single linkage clustering may be forced together due to single elements being close to each other, even though many of the elements in each cluster may be very distant to each other. Complete linkage tends to find compact clusters of approximately equal diameters.
The Moffat distribution, named after the physicist Anthony Moffat, is a continuous probability distribution based upon the Lorentzian distribution. Its particular importance in astrophysics is due to its ability to accurately reconstruct point spread functions, whose wings cannot be accurately portrayed by either a Gaussian or Lorentzian function.
A Savitzky Golay filter is a digital filter that can be applied to a set of digital data points for the purpose of smoothing the data, that is, to increase the signal-to-noise ratio without greatly distorting the signal. This is achieved, in a process known as convolution, by fitting successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares. When the data points are equally spaced, an analytical solution to the least-squares equations can be found, in the form of a single set of "convolution coefficients" that can be applied to all data sub-sets, to give estimates of the smoothed signal, (or derivatives of the smoothed signal) at the central point of each sub-set. The method, based on established mathematical procedures, was popularized by Abraham Savitzky and Marcel J. E. Golay who published tables of convolution coefficients for various polynomials and sub-set sizes in 1964. Some errors in the tables have been corrected. The method has been extended for the treatment of 2- and 3-dimensional data. Savitzky and Golay's paper is one of the most widely cited papers in the journal Analytical Chemistry and is classed by that journal as one of its "10 seminal papers" saying "it can be argued that the dawn of the computer-controlled analytical instrument can be traced to this article".
In statistics, the question of checking whether a coin is fair is one whose importance lies, firstly, in providing a simple problem on which to illustrate basic ideas of statistical inference and, secondly, in providing a simple problem that can be used to compare various competing methods of statistical inference, including decision theory. The practical problem of checking whether a coin is fair might be considered as easily solved by performing a sufficiently large number of trials, but statistics and probability theory can provide guidance on two types of question; specifically those of how many trials to undertake and of the accuracy an estimate of the probability of turning up heads, derived from a given sample of trials. A fair coin is an idealized randomizing device with two states (usually named "heads" and "tails") which are equally likely to occur. It is based on the coin flip used widely in sports and other situations where it is required to give two parties the same chance of winning. Either a specially designed chip or more usually a simple currency coin is used, although the latter might be slightly "unfair" due to an asymmetrical weight distribution, which might cause one state to occur more frequently than the other, giving one party an unfair advantage. So it might be necessary to test experimentally whether the coin is in fact "fair"   that is, whether the probability of the coin falling on either side when it is tossed is approximately 50%. It is of course impossible to rule out arbitrarily small deviations from fairness such as might be expected to affect only one flip in a lifetime of flipping; also it is always possible for an unfair (or "biased") coin to happen to turn up exactly 10 heads in 20 flips. As such, any fairness test must only establish a certain degree of confidence in a certain degree of fairness (a certain maximum bias). In more rigorous terminology, the problem is of determining the parameters of a Bernoulli process, given only a limited sample of Bernoulli trials.
In probability theory, the probability generating function of a discrete random variable is a power series representation (the generating function) of the probability mass function of the random variable. Probability generating functions are often employed for their succinct description of the sequence of probabilities Pr(X = i) in the probability mass function for a random variable X, and to make available the well-developed theory of power series with non-negative coefficients.
In multivariate statistics, the congruence coefficient is an index of the similarity between factors that have been derived in a factor analysis. It was introduced in 1948 by Cyril Burt who referred to it as unadjusted correlation. It is also called Tucker's congruence coefficient after Ledyard Tucker who popularized the technique. Its values range between -1 and +1. It can be used to study the similarity of extracted factors across different samples of, for example, test takers who have taken the same test.
Observational error (or measurement error) is the difference between a measured value of quantity and its true value. In statistics, an error is not a "mistake". Variability is an inherent part of things being measured and of the measurement process. Measurement errors can be divided into two components: random error and systematic error. Random errors are errors in measurement that lead to measurable values being inconsistent when repeated measures of a constant attribute or quantity are taken. Systematic errors are errors that are not determined by chance but are introduced by an inaccuracy (as of observation or measurement) inherent in the system. Systematic error may also refer to an error having a nonzero mean, so that its effect is not reduced when observations are averaged.
In statistical quality control, the  and s chart is a type of control chart used to monitor variables data when samples are collected at regular intervals from a business or industrial process. The chart is advantageous in the following situations: The sample size is relatively large (say, n > 10  and R charts are typically used for smaller sample sizes) The sample size is variable Computers can be used to ease the burden of calculation The "chart" actually consists of a pair of charts: One to monitor the process standard deviation and another to monitor the process mean, as is done with the  and R and individuals control charts. The  and s chart plots the mean value for the quality characteristic across all units in the sample, , plus the standard deviation of the quality characteristic across all units in the sample as follows: . The normal distribution is the basis for the charts and requires the following assumptions: The quality characteristic to be monitored is adequately modeled by a normally-distributed random variable The parameters   and   for the random variable are the same for each unit and each unit is independent of its predecessors or successors The inspection procedure is same for each sample and is carried out consistently from sample to sample The control limits for this chart type are:  (lower) and  (upper) for monitoring the process variability  for monitoring the process mean where  and  are the estimates of the long-term process mean and range established during control-chart setup and A3, B3, and B4 are sample size-specific anti-biasing constants. The anti-biasing constants are typically found in the appendices of textbooks on statistical process control. As with the  and R and individuals control charts, the  chart is only valid if the within-sample variability is constant. Thus, the s chart is examined before the  chart; if the s chart indicates the sample variability is in statistical control, then the  chart is examined to determine if the sample mean is also in statistical control. If on the other hand, the sample variability is not in statistical control, then the entire process is judged to be not in statistical control regardless of what the  chart indicates. When samples collected from the process are of unequal sizes (arising from a mistake in collecting them, for example), there are two approaches:
In probability theory a Brownian excursion process is a stochastic processes that is closely related to a Wiener process (or Brownian motion). Realisations of Brownian excursion processes are essentially just realizations of a Wiener process selected to satisfy certain conditions. In particular, a Brownian excursion process is a Wiener process conditioned to be positive and to take the value 0 at time 1. Alternatively, it is a Brownian bridge process conditioned to be positive. BEPs are important because, among other reasons, they naturally arise as the limit process of a number of conditional functional central limit theorems.
In computational statistics, reversible-jump Markov chain Monte Carlo is an extension to standard Markov chain Monte Carlo (MCMC) methodology that allows simulation of the posterior distribution on spaces of varying dimensions. Thus, the simulation is possible even if the number of parameters in the model is not known. Let  be a model indicator and  the parameter space whose number of dimensions  depends on the model . The model indication need not be finite. The stationary distribution is the joint posterior distribution of  that takes the values . The proposal  can be constructed with a mapping  of  and , where  is drawn from a random component  with density  on . The move to state  can thus be formulated as  The function  must be one to one and differentiable, and have a non-zero support:  so that there exists an inverse function  that is differentiable. Therefore, the  and  must be of equal dimension, which is the case if the dimension criterion  is met where  is the dimension of . This is known as dimension matching. If  then the dimensional matching condition can be reduced to  with  The acceptance probability will be given by  where  denotes the absolute value and  is the joint posterior probability  where  is the normalising constant.
In combinatorial mathematics, the Fishburn Shepp inequality is an inequality for the number of extensions of partial orders to linear orders, found by Fishburn (1984) and Shepp (1982). It states that if x, y, and z are incomparable elements of a finite poset, then  where P(*) is the probability that a linear order < extending the partial order has the property *. In other words the probability that x < z strictly increases if one adds the condition that x < y. In the language of conditional probability,  The proof uses the Ahlswede Daykin inequality.
In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures. In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience: by supplying a valid probability mass function or probability density function by supplying a valid cumulative distribution function or survival function by supplying a valid hazard function by supplying a valid characteristic function by supplying a rule for constructing a new random variable from other random variables whose joint probability distribution is known. A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector a set of two or more random variables taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
Rao's score test, or the score test (often known as the Lagrange multiplier test in econometrics) is a statistical test of a simple null hypothesis that a parameter of interest  is equal to some particular value . It is the most powerful test when the true value of  is close to . The main advantage of the Score-test is that it does not require an estimate of the information under the alternative hypothesis or unconstrained maximum likelihood. This constitutes a potential advantage in comparison to other tests, such as the Wald test and the generalized likelihood ratio test (GLRT). This makes testing feasible when the unconstrained maximum likelihood estimate is a boundary point in the parameter space.
In probability theory, the Chernoff bound, named after Herman Chernoff but due to Herman Rubin, gives exponentially decreasing bounds on tail distributions of sums of independent random variables. It is a sharper bound than the known first or second moment based tail bounds such as Markov's inequality or Chebyshev inequality, which only yield power-law bounds on tail decay. However, the Chernoff bound requires that the variates be independent   a condition that neither the Markov nor the Chebyshev inequalities require. It is related to the (historically prior) Bernstein inequalities, and to Hoeffding's inequality.
In probability and statistics, the Irwin Hall distribution, named after Joseph Oscar Irwin and Philip Hall, is a probability distribution for a random variable defined as the sum of a number of independent random variables, each having a uniform distribution. For this reason it is also known as the uniform sum distribution. The generation of pseudo-random numbers having an approximately normal distribution is sometimes accomplished by computing the sum of a number of pseudo-random numbers having a uniform distribution; usually for the sake of simplicity of programming. Rescaling the Irwin Hall distribution provides the exact distribution of the random variates being generated. This distribution is sometimes confused with the Bates distribution, which is the mean (not sum) of n independent random variables uniformly distributed from 0 to 1.  
Economic data or economic statistics may refer to data (quantitative measures) describing an actual economy, past or present. These are typically found in time-series form, that is, covering more than one time period (say the monthly unemployment rate for the last five years) or in cross-sectional data in one time period (say for consumption and income levels for sample households). Data may also be collected from surveys of for example individuals and firms or aggregated to sectors and industries of a single economy or for the international economy. A collection of such data in table form comprises a data set. Methodological economic and statistical elements of the subject include measurement, collection, analysis, and publication of data. 'Economic statistics' may also refer to a subtopic of official statistics produced by official organizations (e.g. statistical institutes, intergovernmental organizations such as United Nations, European Union or OECD, central banks, ministries, etc.). Economic data provide an empirical basis for economic research, whether descriptive or econometric. Data archives are also a key input for assessing the replicability of empirical findings and for use in decision making as to economic policy. At the level of an economy, many data are organized and compiled according to the methodology of national accounting. Such data include Gross National Product and its components, Gross National Expenditure, Gross National Income in the National Income and Product Accounts, and also the capital stock and national wealth. In these examples data may be stated in nominal or real values, that is, in money or inflation-adjusted terms. Other economic indicators include a variety of alternative measures of output, orders, trade, the labor force, confidence, prices, and financial series (e.g., money and interest rates). At the international level there are many series including international trade, international financial flows, direct investment flows (between countries) and exchange rates. For time-series data, reported measurements can be hourly (e.g. for stock markets), daily, monthly, quarterly, or annually. Estimates such as averages are often subjected to seasonal adjustment to remove weekly or seasonal-periodicity elements, for example, holiday-period sales and seasonal unemployment. Within a country the data are usually produced by one or more statistical organizations, e.g., a governmental or quasi-governmental organization and/or the central banks. International statistics are produced by several international bodies and firms, including the International Monetary Fund and the Bank for International Settlements. Studies in experimental economics may also generate data, rather than using data collected for other purposes. Designed randomized experiments may provide more reliable conclusions than do observational studies. Like epidemiology, economics often studies the behavior of humans over periods too long to allow completely controlled experiments, in which case economists can use observational studies or quasi-experiments; in these studies, economists collect data which are then analyzed with statistical methods (econometrics). Many methods can be used to analyse the data. These include, e.g., time-series analysis using multiple regression, Box-Jenkins analysis, seasonality analysis. Analysis may be univariate (modeling one series) or multivariate (from several series). econometricians, economic statisticians, and financial analysts formulate models, whether for past relationships or for economic forecasting. These models include both partial equilibrium microeconomics aimed at examining particular parts of an economy or economies, or they may cover a whole economic system, as in general equilibrium theory or and in macroeconomics. Economists use these models to understand past events and to forecast future events, e.g., demand, prices and employment. Methods have also been developed for analyzing or correcting results from use of incomplete data and errors in variables.
In probability and statistics, the quantile function specifies, for a given probability in the probability distribution of a random variable, the value at which the probability of the random variable being less than or equal to this value is equal to the given probability. It is also called the percent point function or inverse cumulative distribution function.
A nested case control (NCC) study is a variation of a case-control study in which only a subset of controls from the cohort are compared to the incident cases. In a case-control study, all incident cases in the cohort are compared to a random subset of participants who do not develop the disease of interest. In contrast, in a nested-case-control study, some number of controls are selected for each case from that case's matched risk set. By matching on factors such as age and selecting controls from relevant risk sets, the nested case control model is generally more efficient than a case-cohort design with the same number of selected controls. Usually, the exposure of interest is only measured among the cases and the selected controls. Thus the nested case control study is less efficient than the full cohort design. The nested case control study can be analyzed using methods for missing covariates. The NCC design is often used when the exposure of interest is difficult or expensive to obtain and when the outcome is rare. By utilizing data previously collected from a large cohort study, the time and cost of beginning a new case-control study is avoided. By only measuring the covariate in as many participants as necessary, the cost and effort of exposure assessment is reduced. This benefit is pronounced when the covariate of interest is biological, since assessments such as gene expression profiling are expensive, and because the quantity of blood available for such analysis is often limited, making it a valuable resource that should not be used unnecessarily.
In probability theory, the multidimensional Chebyshev's inequality is a generalization of Chebyshev's inequality, which puts a bound on the probability of the event that a random variable differs from its expected value by more than a specified amount. Let X be an N-dimensional random vector with expected value  and covariance matrix  If  is a positive-definite matrix, for any real number :
This article gives two concrete illustrations of the central limit theorem. Both involve the sum of independent and identically-distributed random variables and show how the probability distribution of the sum approaches the normal distribution as the number of terms in the sum increases. The first illustration involves a continuous probability distribution, for which the random variables have a probability density function. The second illustration, for which most of the computation can be done by hand, involves a discrete probability distribution, which is characterized by a probability mass function. A free full-featured interactive simulation that allows the user to set up various distributions and adjust the sampling parameters is available through the External links section at the bottom of this page.
The Elston Stewart algorithm is an algorithm for computing the likelihood of observed genotype data given a pedigree. It is due to Robert Elston and John Stewart. It can handle relatively large pedigrees providing they are (almost) outbred. Its computation time is exponential in the number of markers. It is used in the analysis of genetic linkage.
In probability theory, the normal (or Gaussian) distribution is a very common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. The normal distribution is useful because of the central limit theorem. In its most general form, under some conditions (which include finite variance), it states that averages of random variables independently drawn from independent distributions converge in distribution to the normal, that is, become normally distributed when the number of random variables is sufficiently large. Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal. Moreover, many results and methods (such as propagation of uncertainty and least squares parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed. The normal distribution is sometimes informally called the bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions). The terms Gaussian function and Gaussian bell curve are also ambiguous because they sometimes refer to multiples of the normal distribution that cannot be directly interpreted in terms of probabilities. The probability density of the normal distribution is:  Where:  is mean or expectation of the distribution (and also its median and mode).  is standard deviation  is variance A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.
Guesstimate is an informal English portmanteau of guess and estimate, first used by American statisticians in 1934 or 1935. It is defined as an estimate made without using adequate or complete information, or, more strongly, as an estimate arrived at by guesswork or conjecture. Like the words estimate and guess, guesstimate may be used as a verb or a noun (with the same change in pronunciation as estimate). A guesstimate may be a first rough approximation pending a more accurate estimate, or it may be an educated guess at something for which no better information will become available. The word may be used in a pejorative sense if information for a better estimate is available but ignored. Guesstimation techniques are used: in physics, where the use of guesstimation techniques to solve Fermi problems is taught as a useful skill to science students. in cosmology, where the Drake equation is a well-known guesstimation method. in economics, where economic forecasts and statistics are often based on guesstimates. in software engineering, where new development of features and release timelines are based on effort guesstimates of tasks. Lawrence Weinstein and John Adam's book Guesstimation: Solving the World's Problems on the Back of a Cocktail Napkin, based on the course "Physics on the Back of an Envelope" at Old Dominion University, promotes guesstimation techniques as a useful life skill. It includes many worked examples of guesstimation, including the following problems: How many total miles do Americans drive in a year   Answer: about two trillion (2x1012).  How much high-level nuclear waste does a 1 GW nuclear power plant produce in a year   Answer: about sixty tons.  ^ guess Online Etymological Dictionary ^ a b guesstimate Dictionary.com Unabridged (v 1.1) ^ guesstimate Merriam-Webster On-line Dictionary ^ guesstimate MSN Encarta Dictionary. Archived 2009-10-31. ^ guesstimate American Heritage Dictionary ^ Compact Oxford English Dictionary guesstimate ^ "Guesstimate with confidence using confidence intervals" from back cover of Statistics for Dummies ^ Guesstimate; Grades 4-6 NTTI Lesson Plan ^ Guesstimation: Solving the World's Problems on the Back of a Cocktail Napkin, Tony Mann, Times Higher Education Supplement ^ The Drake Equation WeAreNotAlone.net ^ Economic outlooks often rely on guesstimation, M. Ray Perryman, San Antonio Business Journal ^ Weinstein & Adam (2008) Problem 5.1 ^ Weinstein & Adam (2008) Problem 10.5
This is a list of scientific journals published in the field of statistics.
Morisita's overlap index, named after Masaaki Morisita, is a statistical measure of dispersion of individuals in a population. It is used to compare overlap among samples (Morisita 1959). This formula is based on the assumption that increasing the size of the samples will increase the diversity because it will include different habitats (i.e. different faunas). Formula:  xi is the number of times species i is represented in the total X from one sample. yi is the number of times species i is represented in the total Y from another sample. Dx and Dy are the Simpson's index values for the x and y samples respectively. S is the number of unique species CD = 0 if the two samples do not overlap in terms of species, and CD = 1 if the species occur in the same proportions in both samples. Horn's modification of the index is (Horn 1966):
The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set. The algorithm proceeds as follows, using two thresholds  (the loose distance) and  (the tight distance), where  . Begin with the set of data points to be clustered. Remove a point from the set, beginning a new 'canopy'. For each point left in the set, assign it to the new canopy if the distance less than the loose distance . If the distance of the point is additionally less than the tight distance , remove it from the original set. Repeat from step 2 until there are no more data points in the set to cluster. These relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm. An important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4. Since the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the curse of dimensionality. Only when a cheap and approximative   low-dimensional   distance function is available, the produced canopies will preserve the clusters produced by K-means.
The percentile rank of a score is the percentage of scores in its frequency distribution that are equal to or lower than it. For example, a test score that is greater than or equal to 75% of the scores of people taking the test is said to be at the 75th percentile, where 75 is the percentile rank. Percentile ranks are commonly used to clarify the interpretation of scores on standardized tests. For the test theory, the percentile rank of a raw score is interpreted as the percentages of examinees in the norm group who scored at or below the score of interest. Percentile ranks are not on an equal-interval scale; that is, the difference between any two scores is not the same between any other two scores whose difference in percentile ranks is the same. For example, 50   25 = 25 is not the same distance as 60   35 = 25 because of the bell-curve shape of the distribution. Some percentile ranks are closer to some than others. Percentile rank 30 is closer on the bell curve to 40 than it is to 20. The mathematical formula is  where cl is the count of all scores less than the score of interest,  i is the frequency of the score of interest, and N is the number of examinees in the sample. If the distribution is normally distributed, the percentile rank can be inferred from the standard score.
In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon Newell theorem. This method was first proposed by Jeffrey P. Buzen in 1973. Computing G(N) is required to compute the stationary probability distribution of a closed queueing network. Performing a nai ve computation of the normalising constant requires enumeration of all states. For a system with N jobs and M states there are  states. Buzen's algorithm "computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions." This is a significant improvement and allows for computations to be performed with much larger networks.
In statistics, the Davis distributions are a family of continuous probability distributions. It is named after Harold T. Davis (1892 1974), who in 1941 proposed this distribution to model income sizes. (The Theory of Econometrics and Analysis of Economic Time Series). It is a generalization of the Planck's law of radiation from statistical physics.  
Cochrane Orcutt estimation is a procedure in econometrics, which adjusts a linear model for serial correlation in the error term. It is named after statisticians Donald Cochrane and Guy Orcutt.
Signal-to-noise ratio (abbreviated SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. It is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise. While SNR is commonly quoted for electrical signals, it can be applied to any form of signal (such as isotope levels in an ice core or biochemical signaling between cells). The signal-to-noise ratio, the bandwidth, and the channel capacity of a communication channel are connected by the Shannon Hartley theorem. Signal-to-noise ratio is sometimes used informally to refer to the ratio of useful information to false or irrelevant data in a conversation or exchange. For example, in online discussion forums and other online communities, off-topic posts and spam are regarded as "noise" that interferes with the "signal" of appropriate discussion.
In the design of experiments in statistics, the lady tasting tea is a famous randomized experiment devised by Ronald Fisher and reported in his book The Design of Experiments (1935). The experiment is the original exposition of Fisher's notion of a null hypothesis, which is "never proved or established, but is possibly disproved, in the course of experimentation". The lady in question claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher proposed to give her eight cups, four of each variety, in random order. One could then ask what the probability was for her getting the specific number of cups she identified correct, but just by chance. Fisher's description is less than 10 pages in length and is notable for its simplicity and completeness regarding terminology, calculations and design of the experiment. The example is loosely based on an event in Fisher's life. The lady in question was Muriel Bristol, and the test used was Fisher's exact test.
In the analysis of data, a correlogram is an image of correlation statistics. For example, in time series analysis, a correlogram, also known as an autocorrelation plot, is a plot of the sample autocorrelations  versus  (the time lags). If cross-correlation is used, the result is called a cross-correlogram. The correlogram is a commonly used tool for checking randomness in a data set. This randomness is ascertained by computing autocorrelations for data values at varying time lags. If random, such autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero. In addition, correlograms are used in the model identification stage for Box Jenkins autoregressive moving average time series models. Autocorrelations should be near-zero for randomness; if the analyst does not check for randomness, then the validity of many of the statistical conclusions becomes suspect. The correlogram is an excellent way of checking for such randomness. Sometimes, corrgrams, color-mapped matrices of correlation strengths in multivariate analysis, are also called correlograms.
In probability and statistics, the Bates distribution, named after Grace Bates, is a probability distribution of the mean of a number of statistically independent uniformly distributed random variables on the unit interval. This distribution is sometimes confused with the Irwin Hall distribution, which is the distribution of the sum (not mean) of n independent random variables uniformly distributed from 0 to 1.
In probability and statistics, a random variable, random quantity, aleatory variable or stochastic variable is a variable whose value is subject to variations due to chance (i.e. randomness, in a mathematical sense). A random variable can take on a set of possible different values (similarly to other mathematical variables), each with an associated probability, in contrast to other mathematical variables. A random variable's possible values might represent the possible outcomes of a yet-to-be-performed experiment, or the possible outcomes of a past experiment whose already-existing value is uncertain (for example, due to imprecise measurements or quantum uncertainty). They may also conceptually represent either the results of an "objectively" random process (such as rolling a die) or the "subjective" randomness that results from incomplete knowledge of a quantity. The meaning of the probabilities assigned to the potential values of a random variable is not part of probability theory itself but is instead related to philosophical arguments over the interpretation of probability. The mathematics works the same regardless of the particular interpretation in use. The mathematical function describing the possible values of a random variable and their associated probabilities is known as a probability distribution. Random variables can be discrete, that is, taking any of a specified finite or countable list of values, endowed with a probability mass function, characteristic of a probability distribution; or continuous, taking any numerical value in an interval or collection of intervals, via a probability density function that is characteristic of a probability distribution; or a mixture of both types. The realizations of a random variable, that is, the results of randomly choosing values according to the variable's probability distribution function, are called random variates. The formal mathematical treatment of random variables is a topic in probability theory. In that context, a random variable is understood as a function defined on a sample space whose outputs are numerical values.
In probability theory, conditional probability is a measure of the probability of an event given that (by assumption, presumption, assertion or evidence) another event has occurred. If the event of interest is A and the event B is known or assumed to have occurred, "the conditional probability of A given B", or "the probability of A under the condition B", is usually written as P(A|B), or sometimes PB(A). For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person has a cold, then they are much more likely to be coughing. The conditional probability of coughing given that you have a cold might be a much higher 75%. The concept of conditional probability is one of the most fundamental and one of the most important concepts in probability theory. But conditional probabilities can be quite slippery and require careful interpretation. For example, there need not be a causal or temporal relationship between A and B. P(A|B) may or may not be equal to P(A) (the unconditional probability of A). If P(A|B) = P(A), then events A and B are said to be independent. In such a case, having learned about the event B does not change our knowledge about the event A. Also, in general, P(A|B) (the conditional probability of A given B) is not equal to P(B|A). For example, if you have cancer you might have a 90% chance of testing positive for cancer. In this case what is being measured is that the if event B "having cancer" has occurred, the probability A - test is positive given that B having cancer occurred is 90%, P(A|B) = 90%. Alternatively, you can test positive for cancer but you may have only a 10% chance of actually having cancer because cancer is very rare. In this case what is being measured is the probability of the event B - having cancer given that the event A - test is positive has occurred, P(B|A) = 10%. Falsely equating the two probabilities causes various errors of reasoning such as the base rate fallacy. Conditional probabilities can be correctly reversed using Bayes' theorem.  
In mathematics, in the area of statistical analysis, the bispectrum is a statistic used to search for nonlinear interactions.
In mathematics, the Wiener process is a continuous-time stochastic process named in honor of Norbert Wiener. It is often called standard Brownian motion, after Robert Brown. It is one of the best known Le vy processes (ca dla g stochastic processes with stationary independent increments) and occurs frequently in pure and applied mathematics, economics, quantitative finance, and physics. The Wiener process plays an important role both in pure and applied mathematics. In pure mathematics, the Wiener process gave rise to the study of continuous time martingales. It is a key process in terms of which more complicated stochastic processes can be described. As such, it plays a vital role in stochastic calculus, diffusion processes and even potential theory. It is the driving process of Schramm Loewner evolution. In applied mathematics, the Wiener process is used to represent the integral of a white noise Gaussian process, and so is useful as a model of noise in electronics engineering (see Brownian noise), instrument errors in filtering theory and unknown forces in control theory. The Wiener process has applications throughout the mathematical sciences. In physics it is used to study Brownian motion, the diffusion of minute particles suspended in fluid, and other types of diffusion via the Fokker Planck and Langevin equations. It also forms the basis for the rigorous path integral formulation of quantum mechanics (by the Feynman Kac formula, a solution to the Schro dinger equation can be represented in terms of the Wiener process) and the study of eternal inflation in physical cosmology. It is also prominent in the mathematical theory of finance, in particular the Black Scholes option pricing model.  
Pseudo-random number sampling or non-uniform pseudo-random variate generation is the numerical practice of generating pseudo-random numbers that are distributed according to a given probability distribution. Methods of sampling a non-uniform distribution are typically based on the availability of a pseudo-random number generator producing numbers X that are uniformly distributed. Computational algorithms are then used to manipulate a single random variate, X, or often several such variates, into a new random variate Y such that these values have the required distribution. Historically, basic methods of pseudo-random number sampling were developed for Monte-Carlo simulations in the Manhattan project; they were first published by John von Neumann in the early 1950s.
In mathematics and its applications, a parametric family or a parameterized family is a family of objects (a set of related objects) whose definitions depend on a set of parameters. Common examples are parametrized (families of) functions, probability distributions, curves, shapes, etc.  
A spatial distribution is the arrangement of a phenomenon across the Earth's surface and a graphical display of such an arrangement is an important tool in geographical and environmental statistics. A graphical display of a spatial distribution may summarize raw data directly or may reflect the outcome of more sophisticated data analysis. Many different aspects of a phenomenon can be shown in a single graphical display by using a suitable choice of different colours to represent differences. One example of such a display could be observations made to describe the geographic patterns of features, both physical and human across the earth. The information included could be where units of something are, how many units of the thing there are per units of area, and how sparsely or densely packed they are from each other.
In statistics, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population are less likely to be included than others. It results in a biased sample, a non-random sample of a population (or non-human factors) in which all individuals, or instances, were not equally likely to have been selected. If this is not accounted for, results can be erroneously attributed to the phenomenon under study rather than to the method of sampling. Medical sources sometimes refer to sampling bias as ascertainment bias. Ascertainment bias has basically the same definition, but is still sometimes classified as a separate type of bias.
The Akaike information criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Hence, AIC provides a means for model selection. AIC is founded on information theory: it offers a relative estimate of the information lost when a given model is used to represent the process that generates the data. In doing so, it deals with the trade-off between the goodness of fit of the model and the complexity of the model. AIC does not provide a test of a model in the sense of testing a null hypothesis; i.e. AIC can tell nothing about the quality of the model in an absolute sense. If all the candidate models fit poorly, AIC will not give any warning of that.
In the design and analysis of experiments, post hoc analysis (from Latin post hoc, "after this") consists of looking at the data after the experiment has concluded for patterns that were not specified a priori. It is sometimes called by critics data dredging to evoke the sense that the more one looks the more likely something will be found. More subtly, each time a pattern in the data is considered, a statistical test is effectively performed. This greatly inflates the total number of statistical tests and necessitates the use of multiple testing procedures to compensate. However, this is difficult to do precisely and in fact most results of post hoc analyses are reported as they are with unadjusted p-values. These p-values must be interpreted in light of the fact that they are a small and selected subset of a potentially large group of p-values. Results of post hoc analyses should be explicitly labeled as such in reports and publications to avoid misleading readers. In practice, post hoc analyses are usually concerned with finding patterns and/or relationships between subgroups of sampled populations that would otherwise remain undetected and undiscovered were a scientific community to rely strictly upon a priori statistical methods. Post hoc tests also known as a posteriori tests greatly expand the range and capability of methods that can be applied in exploratory research. Post hoc examination strengthens induction by limiting the probability that significant effects will seem to have been discovered between subgroups of a population when none actually exist. As it is, many scientific papers are published without adequate, preventative post hoc control of the type I error rate. Post hoc analysis is an important procedure without which multivariate hypothesis testing would greatly suffer, rendering the chances of discovering false positives unacceptably high. Ultimately, post hoc testing creates better informed scientists who can therefore formulate better, more efficient a priori hypotheses and research designs.
In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures. In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience: by supplying a valid probability mass function or probability density function by supplying a valid cumulative distribution function or survival function by supplying a valid hazard function by supplying a valid characteristic function by supplying a rule for constructing a new random variable from other random variables whose joint probability distribution is known. A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector a set of two or more random variables taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
In mathematics, unimodality means possessing a unique mode. More generally, unimodality means there is only a single highest value, somehow defined, of some mathematical object.
In statistics, the mean absolute scaled error (MASE) is a measure of the accuracy of forecasts . It was proposed in 2005 by statistician Rob J. Hyndman and Professor of Decision Sciences Anne B. Koehler, who described it as a "generally applicable measurement of forecast accuracy without the problems seen in the other measurements." The mean absolute scaled error is given by  where the numerator et is the forecast error for a given period, defined as the actual value (Yt) minus the forecast value (Ft) for that period: et = Yt   Ft, and the denominator is the average forecast error of the one-step "naive forecast method", which uses the actual value from the prior period as the forecast: Ft = Yt 1 This scale-free error metric "can be used to compare forecast methods on a single series and also to compare forecast accuracy between series. This metric is well suited to intermittent-demand series because it never gives infinite or undefined values except in the irrelevant case where all historical data are equal.
SigmaStat is a statistical software package, which was originally developed by Jandel Scientific Software in the 1980s. As of October 1996, Systat Software is now based in San Jose, California. SigmaStat users have the ability to compare effects among groups. This includes before and after or repeated measure studies. The users can also conduct survival analysis, analyze rates and proportions, perform regression and correlation analysis and calculate power and sample size. The program uses a wizard based interface which asks the user questions about the project and its data. After a test is run, the user receives a detailed report that interprets the results. If installed with SigmaPlot, SigmaStat integrated with SigmaPlot and SigmaPlot gained advanced statistical analysis capabilities from version 11. SigmaStat is available both as a separate product or is available integrated with SigmaPlot. On February 1st, 2016 SigmaStat version 4 was relaunched as a separate Advisory Statistics Software by Systat Software Inc. With SigmaStat version 4 users have multiple new statistical procedures like Principal Components Analysis (PCA), One-Way Analysis of Covariance (ANCOVA) with multiple covariates, Deming Regression and Cox Regression. Users can experience a new user interface which includes the ribbon interface, right-click mouse menus and drag and drop of items in the Notebook Manager. SigmaStat users can now access 36 new probability distribution functions added to the transform language and generalized weighting added to nonlinear regression which allows use of user determined error distributions and to enable robust fitting techniques. It also has profile plots for multi-factor ANOVA and normality test graphs. A new graph properties dialog box makes editing graphs much easier with instant redraw when properties are changed.
Slice sampling is a type of Markov chain Monte Carlo algorithm for pseudo-random number sampling, i.e. for drawing random samples from a statistical distribution. The method is based on the observation that to sample a random variable one can sample uniformly from the region under the graph of its density function. To visualize this motivation, imagine printing out a simple bell curve and throwing darts at it. Assume that the darts are uniformly distributed around the board. Now take off all of the darts that are outside the curve (i.e. perform rejection sampling). The x-positions of the remaining darts will be distributed according to the bell curve. This is because there is the most room for the darts to land where curve is highest and thus the probability density is greatest. Slice sampling, in its simplest form, samples uniformly from underneath the curve f(x) without the need to reject any points, as follows: Choose a starting value x0 for which f(x0)>0. Sample a y value uniformly between 0 and f(x0). Draw a horizontal line across the curve at this y position. Sample a point (x,y) from the line segments within the curve. Repeat from step 2 using the new x value. The motivation here is that one way to sample a point uniformly from within an arbitrary curve is first to draw thin uniform-height horizontal slices across the whole curve. Then, we can sample a point within the curve by randomly selecting a slice that falls at or below the curve at the x-position from the previous iteration, then randomly picking an x-position somewhere along the slice. By using the x-position from the previous iteration of the algorithm, in the long run we select slices with probabilities proportional to the lengths of their segments within the curve. Generally, the trickiest part of this algorithm is finding the bounds of the horizontal slice, which involves inverting the function describing the distribution being sampled from. This is especially problematic for multi-modal distributions, where the slice may consist of multiple discontiguous parts. It is often possible to use a form of rejection sampling to overcome this, where we sample from a larger slice that is known to include the desired slice in question, and then discard points outside of the desired slice. Note also that this algorithm can be used to sample from the area under any curve, regardless of whether the function integrates to 1. In fact, scaling a function by a constant has no effect on the sampled x-positions. This means that the algorithm can be used to sample from a distribution whose probability density function is only known up to a constant (i.e. whose normalizing constant is unknown), which is common in computational statistics.
A numerical model determines how a model state at a particular time changes into the model state at a later time. Even if the numerical model were a perfect representation of an actual system (which of course can rarely if ever be the case) in order to make a perfect forecast of the future state of the actual system the initial state of the numerical model would also have to be a perfect representation of the actual state of the system. Data assimilation or, more-or-less synonymously, data analysis is the process by which observations of the actual system are incorporated into the model state of a numerical model of that system. Applications of data assimilation arise in many fields of geosciences, perhaps most importantly in weather forecasting and hydrology. A frequently encountered problem is that the number of observations of the actual system available for analysis is orders of magnitude smaller than the number of values required to specify the model state. The initial state of the numerical model cannot therefore be determined from the available observations alone. Instead, the numerical model is used to propagate information from past observations to the current time. This is then combined with current observations of the actual system using a data assimilation method. Most commonly this leads to the numerical modelling system alternately performing a numerical forecast and a data analysis. This is known as analysis/forecast cycling. The forecast from the previous analysis to the current one is frequently called the background. The analysis combines the information in the background with that of the current observations, essentially by taking a weighted mean of the two; using estimates of the uncertainty of each to determine their weighting factors. The data assimilation procedure is invariably multivariate and includes approximate relationships between the variables. The observations are of the actual system, rather than of the model's incomplete representation of that system, and so may have different relationships between the variables from those in the model. To reduce the impact of these problems incremental analyses are often performed. That is the analysis procedure determines increments which when added to the background yield the analysis. As the increments are generally small compared to the background values this leaves the analysis less affected by 'balance' errors in the analysed increments. Even so some filtering, known as initialisation, may be required to avoid problems, such as the excitement of unphysical wave like activity or even numerical instability, when running the numerical model from the analysed initial state. As an alternative to analysis/forecast cycles, data assimilation can proceed by some sort of continuous process such as nudging, where the model equations themselves are modified to add terms that continuously push the model towards the observations.
ViSta, the Visual Statistics system is a freeware statistical system developed by Forrest W. Young of the University of North Carolina. ViSta current version maintained by Pedro M. Valero-Mora of the University of Valencia and can be found at [1]. Old versions of ViSta and of the documentation can be found at [2]. ViSta incorporates a number of special features that are of both theoretical and practical interest: The workmap keeps record of the datasets opened by the user and the subsequent statistical transformations and analysis applied to them. Spreadplots show all the relevant plots for a dataset with a given combination of types of variables. Graphics are the primary way of output in contrast with traditional statistics packages where the textual output is more important.
In computer science, all-pairs testing or pairwise testing is a combinatorial method of software testing that, for each pair of input parameters to a system (typically, a software algorithm), tests all possible discrete combinations of those parameters. Using carefully chosen test vectors, this can be done much faster than an exhaustive search of all combinations of all parameters, by "parallelizing" the tests of parameter pairs.  
S is a statistical programming language developed primarily by John Chambers and (in earlier versions) Rick Becker and Allan Wilks of Bell Laboratories. The aim of the language, as expressed by John Chambers, is "to turn ideas into software, quickly and faithfully". The two modern implementations of S are R, a part of the GNU free software project; and S-PLUS, a commercial product sold by TIBCO Software .
In control engineering, a state-space representation is a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations. "State space" refers to the space whose axes are the state variables. The state of the system can be represented as a vector within that space. To abstract from the number of inputs, outputs and states, these variables are expressed as vectors. Additionally, if the dynamical system is linear, time-invariant, and finite-dimensional, then the differential and algebraic equations may be written in matrix form. The state-space method is characterized by significant algebraization of general system theory, which makes possible to use Kronecker vector-matrix structures. The capacity of these structures can be efficiently applied to research systems with modulation or without it. The state-space representation (also known as the "time-domain approach") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With  inputs and  outputs, we would otherwise have to write down  Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions.
The principle of maximum entropy states that, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one with largest entropy. Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. Of those, the one with maximal information entropy is the proper distribution, according to this principle.
Scott's pi (named after William A. Scott) is a statistic for measuring inter-rater reliability for nominal data in communication studies. Textual entities are annotated with categories by different annotators, and various measures are used to assess the extent of agreement between the annotators, one of which is Scott's pi. Since automatically annotating text is a popular problem in natural language processing, and goal is to get the computer program that is being developed to agree with the humans in the annotations it creates, assessing the extent to which humans agree with each other is important for establishing a reasonable upper limit on computer performance. Scott's pi is similar to Cohen's kappa in that they improve on simple observed agreement by factoring in the extent of agreement that might be expected by chance. However, in each statistic, the expected agreement is calculated slightly differently. Scott's pi makes the assumption that annotators have the same distribution of responses, which makes Cohen's kappa slightly more informative. Scott's pi is extended to more than two annotators in the form of Fleiss' kappa. The equation for Scott's pi, as in Cohen's kappa, is:  However, Pr(e) is calculated using joint proportions. A worked example is given below. Confusion matrix for two annotators, three categories {Yes, No, Maybe} and 45 items rated (90 ratings for 2 annotators): To calculate the expected agreement, sum marginals across annotators and divide by the total number of ratings to obtain joint proportions. Square and total these: To calculate observed agreement, divide the number of items on which annotators agreed by the total number of items. In this case,  Given that Pr(e) = 0.369, Scott's pi is then
"Correlation does not imply causation" is a phrase used in statistics to emphasize that a correlation between two variables does not imply that one causes the other. Many statistical tests calculate correlation between variables. A few go further, using correlation as a basis for testing a hypothesis of a true causal relationship; examples are the Granger causality test and convergent cross mapping. The counter-assumption, that "correlation proves causation," is considered a questionable cause logical fallacy in that two events occurring together are taken to have a cause-and-effect relationship. This fallacy is also known as cum hoc ergo propter hoc, Latin for "with this, therefore because of this," and "false cause." A similar fallacy, that an event that follows another was necessarily a consequence of the first event, is sometimes described as post hoc ergo propter hoc (Latin for "after this, therefore because of this.") For example, in a widely studied case, numerous epidemiological studies showed that women taking combined hormone replacement therapy (HRT) also had a lower-than-average incidence of coronary heart disease (CHD), leading doctors to propose that HRT was protective against CHD. But randomized controlled trials showed that HRT caused a small but statistically significant increase in risk of CHD. Re-analysis of the data from the epidemiological studies showed that women undertaking HRT were more likely to be from higher socio-economic groups (ABC1), with better-than-average diet and exercise regimens. The use of HRT and decreased incidence of coronary heart disease were coincident effects of a common cause (i.e. the benefits associated with a higher socioeconomic status), rather than a direct cause and effect, as had been supposed. As with any logical fallacy, identifying that the reasoning behind an argument is flawed does not imply that the resulting conclusion is false. In the instance above, if the trials had found that hormone replacement therapy does in fact have a negative incidence on the likelihood of coronary heart disease the assumption of causality would have been correct, although the logic behind the assumption would still have been flawed.
In statistics, principal component regression (PCR) is a regression analysis technique that is based on principal component analysis (PCA). Typically, it considers regressing the outcome (also known as the response or, the dependent variable) on a set of covariates (also known as predictors or, explanatory variables or, independent variables) based on a standard linear regression model, but uses PCA for estimating the unknown regression coefficients in the model. In PCR, instead of regressing the dependent variable on the explanatory variables directly, the principal components of the explanatory variables are used as regressors. One typically uses only a subset of all the principal components for regression, thus making PCR some kind of a regularized procedure. Often the principal components with higher variances (the ones based on eigenvectors corresponding to the higher eigenvalues of the sample variance-covariance matrix of the explanatory variables) are selected as regressors. However, for the purpose of predicting the outcome, the principal components with low variances may also be important, in some cases even more important. One major use of PCR lies in overcoming the multicollinearity problem which arises when two or more of the explanatory variables are close to being collinear. PCR can aptly deal with such situations by excluding some of the low-variance principal components in the regression step. In addition, by usually regressing on only a subset of all the principal components, PCR can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model. This can be particularly useful in settings with high-dimensional covariates. Also, through appropriate selection of the principal components to be used for regression, PCR can lead to efficient prediction of the outcome based on the assumed model.
In queueing theory, a discipline within the mathematical theory of probability, a Jackson network (sometimes Jacksonian network) is a class of queueing network where the equilibrium distribution is particularly simple to compute as the network has a product-form solution. It was the first significant development in the theory of networks of queues, and generalising and applying the ideas of the theorem to search for similar product-form solutions in other networks has been the subject of much research, including ideas used in the development of the Internet. The networks were first identified by James R. Jackson and his paper was re-printed in the journal Management Science s  Ten Most Influential Titles of Management Sciences First Fifty Years.  Jackson was inspired by the work of Burke and Reich, though Jean Walrand notes "product-form results ... [are] a much less immediate result of the output theorem than Jackson himself appeared to believe in his fundamental paper". An earlier product-form solution was found by R. R. P. Jackson for tandem queues (a finite chain of queues where each customer must visit each queue in order) and cyclic networks (a loop of queues where each customer must visit each queue in order). A Jackson network consists of a number of nodes, where each node represents a queue in which the service rate can be both node-dependent and state-dependent. Jobs travel among the nodes following a fixed routing matrix. All jobs at each node belong to a single "class" and jobs follow the same service-time distribution and the same routing mechanism. Consequently, there is no notion of priority in serving the jobs: all jobs at each node are served on a first-come, first-served basis. Jackson networks where a finite population of jobs travel around a closed network also have a product-form solution described by the Gordon Newell theorem.  
In statistics, and specifically in the study of the Dirichlet distribution, a neutral vector of random variables is one that exhibits a particular type of statistical independence amongst its elements. In particular, when elements of the random vector must add up to certain sum, then an element in the vector is neutral with respect to the others if the distribution of the vector created by expressing the remaining elements as proportions of their total is independent of the element that was omitted.
In probability and statistics, memorylessness is a property of certain probability distributions: the exponential distributions of non-negative real numbers and the geometric distributions of non-negative integers. The property is most easily explained in terms of "waiting times." Suppose that a random variable, X, is defined to be the time elapsed in a shop from 9 am on a certain day until the arrival of the first customer: thus X is the time a server waits for the first customer. The "memoryless" property makes a comparison between the probability distributions of the time a server has to wait from 9 am onwards for his first customer, and the time that the server still has to wait for the first customer on those occasions when no customer has arrived by any given later time: the property of memorylessness is that these distributions of "time from now to the next customer" are exactly the same. As another example, suppose X is the lifetime of a car engine given in terms of number of miles driven. If the engine has lasted 200,000 miles, then, based on our intuition, it is clear that the probability that the engine lasts another 100,000 miles is not the same as the engine lasting 100,000 miles from the first time it was built. However, memorylessness states that the two probabilities are the same (And if our intuition is right, the distribution that describes the lifetime of a large set of these engines does not have the memorylessness property). In essence, we 'forget' what state the car is in. In other words, the probabilities are not influenced by how much time has elapsed.  The terms "memoryless" and "memorylessness" are used in a very different way to refer to Markov processes in which the underlying assumption of the Markov property implies that the properties of random variables related to the future depend only on relevant information about the current time, not on information from further in the past. The present article describes the use outside the Markov property, limited to conditional probability distributions.  
Energy distance is a statistical distance between probability distributions. If X and Y are independent random vectors in Rd with cumulative distribution functions F and G respectively, then the energy distance between the distributions F and G is defined to be the square root of  where X, X' are independent and identically distributed (iid), Y, Y' are iid,  is expected value, and || . || denotes the length of a vector. Energy distance satisfies all axioms of a metric thus energy distance characterizes the equality of distributions: D(F,G) = 0 if and only if F = G. Energy distance for statistical applications was introduced in 1985 by Ga bor J. Sze kely, who proved that for real-valued random variables this distance is exactly twice Harald Crame r's distance:  For a simple proof of this equivalence, see Sze kely (2002). In higher dimensions, however, the two distances are different because the energy distance is rotation invariant while Crame r's distance is not. (Notice that Crame r's distance is not the same as the distribution-free Cramer-von-Mises criterion.)
For the notion in quantum mechanics, see scattering matrix. In multivariate statistics and probability theory, the scatter matrix is a statistic that is used to make estimates of the covariance matrix, for instance of the multivariate normal distribution.
As with other noncentrality parameters, the noncentral t-distribution generalizes a probability distribution   Student's t-distribution   using a noncentrality parameter. Whereas the central distribution describes how a test statistic is distributed when the difference tested is null, the noncentral distribution also describes how t is distributed when the null is false. This leads to its use in statistics, especially calculating statistical power. The noncentral t-distribution is also known as the singly noncentral t-distribution, and in addition to its primary use in statistical inference, is also used in robust modeling for data.
In probability theory and statistics, a mixture is a combination of two or more probability distributions. The concept arises in two contexts:  A mixture defining a new probability distribution from some existing ones, as in a mixture density. Here the main problem is to derive the theoretical properties of the new distribution. A mixture used as a statistical model such as is often used for statistical classification.The model may represent the population from which observations arise as a mixture density, but the problem is that of a mixture model, in which a data classification hypothesis represents an overall distribution as a mixture of separate distributions (representing separate populations) and the task is to infer from which population each observation arises.
In probability theory, Novikov's condition is the sufficient condition for a stochastic process which takes the form of the Radon-Nikodym derivative in Girsanov's theorem to be a martingale. If satisfied together with other conditions, Girsanov's theorem may be applied to a Brownian motion stochastic process to change from the original measure to the new measure defined by the Radon-Nikodym derivative. This condition was suggested and proved by Alexander Novikov. There are other results which may be used to show that the Radon-Nikodym derivative is a martingale, such as the more general criterion Kazamaki's condition, however Novikov's condition is the most well-known result. Assume that  is a real valued adapted process on the probability space  and  is an adapted Brownian motion: If the condition  is fulfilled then the process  is a martingale under the probability measure  and the filtration . Here  denotes the Dole ans-Dade exponential.
In queueing theory, a discipline within the mathematical theory of probability, Kingman's formula is an approximation for the mean waiting time in a G/G/1 queue. The formula is the product of three terms which depend on utilization, variability and service time. It was first published by John Kingman in his 1961 paper The single server queue in heavy traffic. It is known to be generally very accurate, especially for a system operating close to saturation.
In epidemiology, a risk factor is a variable associated with an increased risk of disease or infection. Sometimes, determinant is also used, being a variable associated with either increased or decreased risk.
Cohen's kappa coefficient is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since   takes into account the agreement occurring by chance.
The half-normal distribution is a special case of the folded normal distribution. Let  follow an ordinary normal distribution, , then  follows a half-normal distribution. Thus, the half-normal distribution is a fold at the mean of an ordinary normal distribution with mean zero.
Umbrella sampling is a technique in computational physics and chemistry, used to improve sampling of a system (or different systems) where ergodicity is hindered by the form of the system's energy landscape. It was first suggested by Torrie and Valleau in 1977 [1]. It is a particular physical application of the more general importance sampling in statistics. Systems in which an energy barrier separates two regions of configuration space may suffer from poor sampling. In Metropolis Monte Carlo runs, the low probability of overcoming the potential barrier can leave inaccessible configurations poorly sampled   or even entirely unsampled   by the simulation. An easily visualised example occurs with a solid at its melting point: considering the state of the system with an order parameter Q, both liquid (low Q) and solid (high Q) phases are low in energy, but are separated by a free energy barrier at intermediate values of Q. This prevents the simulation from adequately sampling both phases. Umbrella sampling is a means of "bridging the gap" in this situation. The standard Boltzmann weighting for Monte Carlo sampling is replaced by a potential chosen to cancel the influence of the energy barrier present. The Markov chain generated has a distribution given by:  with U the potential energy, w(rN) a function chosen to promote configurations that would otherwise be inaccessible to a Boltzmann-weighted Monte Carlo run. In the example above, w may be chosen such that w = w(Q), taking high values at intermediate Q and low values at low/high Q, facilitating barrier crossing. Values for a thermodynamic property A deduced from a sampling run performed in this manner can be transformed into canonical-ensemble values by applying the formula:  with the  subscript indicating values from the umbrella-sampled simulation. The effect of introducing the weighting function w(rN) is equivalent to adding a biasing potential V(rN) to the potential energy of the system.  If the biasing potential is strictly a function of a reaction coordinate or order parameter , then the (unbiased) free energy profile on the reaction coordinate can be calculated by subtracting the biasing potential from the biased free energy profile.  where  is the free energy profile of the unbiased system and  is the free energy profile calculated for the biased, umbrella-sampled system. Series of umbrella sampling simulations can be analyzed using the weighted histogram analysis method (WHAM) or its generalization. WHAM can be derived using the Maximum likelihood method. Subtleties exist in deciding the most computationally efficient way to apply the umbrella sampling method, as described in Frenkel & Smit's book Understanding Molecular Simulation. Alternatives to umbrella sampling for computing potentials of mean force or reaction rates are free energy perturbation and transition interface sampling. A further alternative which functions in full non-equilibrium is S-PRES.
In probability theory and statistics, the Rademacher distribution (which is named after Hans Rademacher) is a discrete probability distribution where a random variate X has a 50% chance of being either +1 or -1. A series of Rademacher distributed variables can be regarded as a simple symmetrical random walk where the step size is 1.
In probability and statistics, a mean-preserving spread (MPS) is a change from one probability distribution A to another probability distribution B, where B is formed by spreading out one or more portions of A's probability density function or probability mass function while leaving the mean (the expected value) unchanged. As such, the concept of mean-preserving spreads provides a stochastic ordering of equal-mean gambles (probability distributions) according to their degree of risk; this ordering is partial, meaning that of two equal-mean gambles, it is not necessarily true that either is a mean-preserving spread of the other. A is said to be a mean-preserving contraction of B if B is a mean-preserving spread of A. Ranking gambles by mean-preserving spreads is a special case of ranking gambles by second-order stochastic dominance   namely, the special case of equal means: If B is a mean-preserving spread of A, then A is second-order stochastically dominant over B; and the converse holds if A and B have equal means. If B is a mean-preserving spread of A, then B has a higher variance than A and the expected value of A and B are identical; but the converse is not in general true, because the variance is a complete ordering while ordering by mean-preserving spreads is only partial.
In statistics, a concordant pair is a pair of observations, each on two variables, {X1,Y1} and {X2,Y2}, having the property that  where "sgn" refers to whether a number is positive, zero, or negative (its sign). Specifically, the sign function, often represented as sgn, is defined as:  That is, in a concordant pair, both elements of one pair are either greater than, equal to, or less than the corresponding elements of the other pair. In contrast, a discordant pair is a pair of two-variable observations such that  That is, if one pair contains a higher value of X then the other pair contains a higher value of Y.
In probability theory, a Pitman Yor process denoted PY(d,  , G0), is a stochastic process whose sample path is a probability distribution. A random sample from this process is an infinite discrete probability distribution, consisting of an infinite set of atoms drawn from G0, with weights drawn from a two-parameter Poisson Dirichlet distribution. The process is named after Jim Pitman and Marc Yor. The parameters governing the Pitman Yor process are: 0   d < 1 a discount parameter, a strength parameter   >  d and a base distribution G0 over a probability space  X. When d = 0, it becomes the Dirichlet process. The discount parameter gives the Pitman Yor process more flexibility over tail behavior than the Dirichlet process, which has exponential tails. This makes Pitman Yor process useful for modeling data with power-law tails (e.g., word frequencies in natural language). The exchangeable random partition induced by the Pitman Yor process is an example of a Poisson Kingman partition, and of a Gibbs type random partition.
Goodhart's law is named after the economist who originated it, Charles Goodhart. Its most popular formulation is: "When a measure becomes a target, it ceases to be a good measure." The original formulation by Goodhart, a former advisor to the Bank of England and Emeritus Professor at the London School of Economics, is this: "As soon as the government attempts to regulate any particular set of financial assets, these become unreliable as indicators of economic trends." This is because investors try to anticipate what the effect of the regulation will be, and invest so as to benefit from it. Goodhart first used it in a 1975 paper, and it later became used popularly to criticize the United Kingdom government of Margaret Thatcher for trying to conduct monetary policy on the basis of targets for broad and narrow money. However, the concept is considerably older, and closely related ideas are known under different names, e.g. Campbell's law (1976), and the Lucas critique (1976). The law is implicit in the economic idea of rational expectations. While it originated in the context of market responses, the law has profound implications for the selection of high-level targets in organisations.
Simultaneous equation models are a form of statistical model in the form of a set of linear simultaneous equations. They are often used in econometrics.
In statistics, a record value or record statistic is the largest or smallest value obtained from a sequence of random variables. The theory is closely related to that used in order statistics. The term was first introduced by K. N. Chandler in 1952.
In statistics, a power transform is a family of functions that are applied to create a monotonic transformation of data using power functions. This is a useful data transformation technique used to stabilize variance, make the data more normal distribution-like, improve the validity of measures of association such as the Pearson correlation between variables and for other data stabilization procedures.
Population dynamics is the branch of life sciences that studies the size and age composition of populations as dynamic systems, and the biological and environmental processes driving them (such as birth and death rates, and by immigration and emigration). Example scenarios are ageing populations, population growth, or population decline.
In statistical graphics, the functional boxplot is an informative exploratory tool that has been proposed for visualizing functional data. Analogous to the classical boxplot, the descriptive statistics of a functional boxplot are: the envelope of the 50% central region, the median curve and the maximum non-outlying envelope. To construct a functional boxplot, data ordering is the first step. In functional data analysis, each observation is a real function, therefore, different from the classical boxplot where data are simply ordered from the smallest sample value to the largest, in a functional boxplot, functional data, e.g. curves or images, are ordered by a notion of band depth or a modified band depth. It allows for ordering functional data from the center outwards and, thus, introduces a measure to define functional quantiles and the centrality or outlyingness of an observation. Having the ranks of functional data, the functional boxplot is a natural extension of the classical boxplot.  
Bangdiwala's B statistic was created by Dr. Shrikant Bangdiwala in 1985 and is a measure of inter-rater agreement. While not as commonly used as the kappa statistic the B test has been used by various workers. While it is principally used as a graphical aid to inter observer agreement, its asymptotic distribution is known.
In statistics, the delta method is a result concerning the approximate probability distribution for a function of an asymptotically normal statistical estimator from knowledge of the limiting variance of that estimator.
Quantitative marketing research is the application of quantitative research techniques to the field of marketing. It has roots in both the positivist view of the world, and the modern marketing viewpoint that marketing is an interactive process in which both the buyer and seller reach a satisfying agreement on the "four Ps" of marketing: Product, Price, Place (location) and Promotion. As a social research method, it typically involves the construction of questionnaires and scales. People who respond (respondents) are asked to complete the survey. Marketers use the information to obtain and understand the needs of individuals in the marketplace, and to create strategies and marketing plans.
Tajima's D is a statistical test created by and named after the Japanese researcher Fumio Tajima. The purpose of the test is to distinguish between a DNA sequence evolving randomly ("neutrally") and one evolving under a non-random process, including directional selection or balancing selection, demographic expansion or contraction, genetic hitchhiking, or introgression. A randomly evolving DNA sequence contains mutations with no effect on the fitness and survival of an organism. The randomly evolving mutations are called "neutral", while mutations under selection are "non-neutral". For example, you would expect to find that a mutation which causes prenatal death or severe disease to be under selection. When looking at the human population as a whole, we say that the population frequency of a neutral mutation fluctuates randomly (i.e. the percentage of people in the population with the mutation changes from one generation to the next, and this percentage is equally likely to go up or down) through genetic drift. Tajima's D is computed as the difference between two measures of genetic diversity: the mean number of pairwise differences and the number of segregating sites, each scaled so that they are expected to be the same in a neutrally evolving population of constant size. The strength of genetic drift depends on the population size. If a population is at a constant size with constant mutation rate, the population will reach an equilibrium of gene frequencies. This equilibrium has important properties, including the number of segregating sites , and the number of nucleotide differences between pairs sampled (these are called pairwise differences). To standardize the pairwise differences, the mean or 'average' number of pairwise differences is used. This is simply the sum of the pairwise differences divided by the number of pairs, and is signified by . The purpose of Tajima's test is to identify sequences which do not fit the neutral theory model at equilibrium between mutation and genetic drift. In order to perform the test on a DNA sequence or gene, you need to sequence homologous DNA for at least 3 individuals. Tajima's statistic computes a standardized measure of the total number of segregating sites (these are DNA sites that are polymorphic) in the sampled DNA and the average number of mutations between pairs in the sample. The two quantities whose values are compared are both method of moments estimates of the population genetic parameter theta, and so are expected to equal the same value. If these two numbers only differ by as much as one could reasonably expect by chance, then the null hypothesis of neutrality cannot be rejected. Otherwise, the null hypothesis of neutrality is rejected.
In statistics, stepwise regression includes regression models in which the choice of predictive variables is carried out by an automatic procedure.  Usually, this takes the form of a sequence of F-tests or t-tests, but other techniques are possible, such as adjusted R-square, Akaike information criterion, Bayesian information criterion, Mallows's Cp, PRESS, or false discovery rate. The frequent practice of fitting the final selected model followed by reporting estimates and confidence intervals without adjusting them to take the model building process into account has led to calls to stop using stepwise model building altogether or to at least make sure model uncertainty is correctly reflected.
In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. Instrumental variable methods allow consistent estimation when the explanatory variables (covariates) are correlated with the error terms of a regression relationship. Such correlation may occur when the dependent variable causes at least one of the covariates ("reverse" causation), when there are relevant explanatory variables which are omitted from the model, or when the covariates are subject to measurement error. In this situation, ordinary linear regression generally produces biased and inconsistent estimates. However, if an instrument is available, consistent estimates may still be obtained. An instrument is a variable that does not itself belong in the explanatory equation and is correlated with the endogenous explanatory variables, conditional on the other covariates. In linear models, there are two main requirements for using an IV: The instrument must be correlated with the endogenous explanatory variables, conditional on the other covariates. The instrument cannot be correlated with the error term in the explanatory equation (conditional on the other covariates), that is, the instrument cannot suffer from the same problem as the original predicting variable.
Cohen's kappa coefficient is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since   takes into account the agreement occurring by chance.
In statistical genetics, Felsenstein's tree-pruning algorithm (or Felsenstein's tree-peeling algorithm), due to Joseph Felsenstein, is an algorithm for computing the likelihood of an evolutionary tree from nucleic acid sequence data.  The algorithm is often used as a subroutine in a search for a maximum likelihood estimate for an evolutionary tree. Further, it can be used in a hypothesis test for whether evolutionary rates are constant (by using likelihood ratio tests). It can also be used to provide error estimates for the parameters describing an evolutionary tree.
In cognitive psychology and decision science, conservatism or conservatism bias is a bias in human information processing. This bias describes human belief revision in which persons over-weigh the prior distribution (base rate) and under-weigh new sample evidence when compared to Bayesian belief-revision. According to the theory, "opinion change is very orderly, and usually proportional to the numbers of Bayes' Theorem - but it is insufficient in amount". In other words, persons update their prior beliefs as new evidence becomes observed, but they do so more slowly than they would if they used Bayes' theorem. This bias was discussed by Ward Edwards in 1968, who reported on experiments like the following one:  There are two bookbags, one containing 700 red and 300 blue chips, the other containing 300 red and 700 blue. Take one of the bags. Now, you sample, randomly, with replacement after each chip. In 12 samples, you get 8 reds and 4 blues. what is the probability that this is the predominantly red bag   Most subjects chose an answer around .7. The correct answer according to Bayes' Theorem is closer to .97. Edwards suggested that people updated beliefs conservatively, in accordance with Bayes' Theorem more slowly. They updated from .5 incorrectly according to an observed bias in several experiments.
In statistics, when performing multiple comparisons, the term false positive ratio, also known as the false alarm ratio, usually refers to the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate (or "false alarm rate") usually refers to the expectancy of the false positive ratio.
An institutional review board (IRB), also known as an independent ethics committee (IEC), ethical review board (ERB), or research ethics board (REB), is a type of committee used in research in the United States that has been formally designated to approve, monitor, and review biomedical and behavioral research involving humans. They often conduct some form of risk-benefit analysis in an attempt to determine whether or not research should be done. The purpose of the IRB is to assure that appropriate steps are taken to protect the rights and welfare of humans participating as subjects in a research study.
In probability, statistics and related fields, a Poisson point process or Poisson process (also called a Poisson random measure, Poisson random point field or Poisson point field) is a type of random mathematical object that consists of points randomly located on a mathematical space. The process has convenient mathematical properties, which has led to it being frequently defined in Euclidean space and used as a mathematical model for seemingly random processes in numerous disciplines such as astronomy, biology, ecology, geology, physics, image processing, and telecommunications. The Poisson point process is often defined on the real line. For example, in queueing theory  it is used to model random events, such as the arrival of customers at a store or phone calls at an exchange, distributed in time. In the plane, the point process, also known as a spatial Poisson process, may represent scattered objects such as transmitters in a wireless network, particles colliding into a detector, or trees in a forest. In this setting, the process is often used in mathematical models and in the related fields of spatial point processes, stochastic geometry, spatial statistics  and continuum percolation theory. In more abstract spaces, the Poisson point process serves as an object of mathematical study in its own right. In all settings, the Poisson point process has the property that each point is stochastically independent to all the other points in the process, which is why it is sometimes called a purely or completely random process. Despite its wide use as a stochastic model of phenomena representable as points, the inherent nature of the process implies that it does not adequately describe phenomena in which there is sufficiently strong interaction between the points. This has sometimes led to the overuse of the point process in mathematical models, and has inspired other point processes, some of which are constructed via the Poisson point process, that seek to capture this interaction. The process is named after French mathematician Sime on Denis Poisson owing to the fact that if a collection of random points in some space forms a Poisson process, then the number points in a region of finite size is directly related to the Poisson distribution, but Poisson never studied the process, which independently arose in several different settings. The process is defined with a single non-negative mathematical object, which, depending on the context, may be a constant, an integrable function or, in more general settings, a Radon measure. If this object is a constant, then the resulting process is called a homogeneous  or stationary  Poisson point process. Otherwise, the parameter depends on its location in the underlying space, which leads to the inhomogeneous or nonhomogeneous Poisson point process. The word point is often omitted, but there are other Poisson processes of objects, which, instead of points, consist of more complicated mathematical objects such as lines and polygons, and such processes can be based on the Poisson point process.
In probability and statistics, a compound probability distribution is the probability distribution that results from assuming that a random variable is distributed according to some parametrized distribution, with the parameters of that distribution being assumed to be themselves random variables. The compound distribution is the result of marginalizing over the intermediate random variables that represent the parameters of the initial distribution. An important type of compound distribution occurs when the parameter being marginalized over represents the number of random variables in a summation of random variables.
Subgroup analysis, in the context of design and analysis of experiments, refers to looking for pattern in a subset of the subjects.
Omnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall. One example is the F-test in the analysis of variance. There can be legitimate significant effects within a model even if the omnibus test is not significant. For instance, in a model with two independent variables, if only one variable exerts a significant effect on the dependent variable and the other does not, then the omnibus test may be non-significant. This fact does not affect the conclusions that may be drawn from the one significant variable. In order to test effects within an omnibus test, researchers often use contrasts. In addition, Omnibus test as a general name refers to an overall or a global test. Other names include F-test or Chi-squared test. Omnibus test as a statistical test is implemented on an overall hypothesis that tends to find general significance between parameters' variance, while examining parameters of the same type, such as: Hypotheses regarding equality vs. inequality between k expectancies  1= 2=...= k  vs. at least one pair   j=  j'  , where j,j'=1,...,k and j= j', in Analysis Of Variance(ANOVA); or regarding equality between k standard deviations   1=  2=....=   k   vs. at least one pair    j=   j'   in testing equality of variances in ANOVA; or regarding coefficients   1=  2=....=  k   vs. at least one pair  j=  j'  in Multiple linear regression or in Logistic regression. Usually, it tests more than two parameters of the same type and its role is to find general significance of at least one of the parameters involved. Omnibus tests commonly refers to either one of those statistical tests: ANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure ; The omnibus multivariate F Test in ANOVA with repeated measures ; F test for equality/inequality of the regression coefficients in Multiple Regression; Chi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression. Those omnibus tests are usually conducted whenever one tends to test an overall hypothesis on a quadratic statistic (like sum of squares or variance or covariance) or rational quadratic statistic (like the ANOVA overall F test in Analysis of Variance or F Test in Analysis of covariance or the F Test in Linear Regression, or Chi-Square in Logistic Regression). While significance is founded on the omnibus test, it doesn't specify exactly where the difference is occurred, meaning, it doesn't bring specification on which parameter is significally different from the other, but it statistically determine that there is a difference, so at least two of the tested parameters are statistically different. If significance was met, none of those tests will tell specifically which mean differs from the others (in ANOVA), which coefficient differs from the others (in Regression) etc.
In numerical analysis, isotonic regression (IR) involves finding a weighted least-squares fit  to a vector  with weights vector  subject to a set of non-contradictory constraints of the kind . Such constraints define partial order or total order and can be represented as a directed graph , where N is the set of variables involved, and E is the set of pairs (i, j) for each constraint . Thus, the IR problem corresponds to the following quadratic program (QP):   In the case when  is a total order, a simple iterative algorithm for solving this QP is called the pool adjacent violators algorithm (PAVA). Best and Chakravarti (1990) have studied the problem as an active set identification problem, and have proposed a primal algorithm in O(n), the same complexity as the PAVA, which can be seen as a dual algorithm. IR has applications in statistical inference, for example, to fit of an isotonic curve to mean experimental results when an order is expected. A benefit of isotonic regression is that it does not assume any form for the target function, such as linearity assumed by linear regression. Another application is nonmetric multidimensional scaling, where a low-dimensional embedding for data points is sought such that order of distances between points in the embedding matches order of dissimilarity between points. Isotonic regression is used iteratively to fit ideal distances to preserve relative dissimilarity order. Isotonic regression is also sometimes referred to as monotonic regression. Correctly speaking, isotonic is used when the direction of the trend is increasing, while monotonic could imply a trend that is either increasing or strictly decreasing. Isotonic regression under the  for  is defined as follows:
In statistics, dispersion (also called variability, scatter, or spread) denotes how stretched or squeezed a distribution (theoretical or that underlying a statistical sample) is. Common examples of measures of statistical dispersion are the variance, standard deviation and interquartile range. Dispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.
In epidemiology, the standardized mortality ratio or SMR, is a quantity, expressed as either a ratio or percentage quantifying the increase or decrease in mortality of a study cohort with respect to the general population.
In statistics, a univariate distribution is a probability distribution of only one random variable. This is in contrast to a multivariate distribution, the probability distribution of a random vector (consisting of multiple random variables).
A field of applied statistics, survey methodology studies the sampling of individual units from a population and the associated survey data collection techniques, such as questionnaire construction and methods for improving the number and accuracy of responses to surveys. Statistical surveys are undertaken with a view towards making statistical inferences about the population being studied, and this depends strongly on the survey questions used. Polls about public opinion, public health surveys, market research surveys, government surveys and censuses are all examples of quantitative research that use contemporary survey methodology to answer questions about a population. Although censuses do not include a "sample", they do include other aspects of survey methodology, like questionnaires, interviewers, and nonresponse follow-up techniques. Surveys provide important information for all kinds of public information and research fields, e.g., marketing research, psychology, health professionals and sociology.  
Plackett Burman designs are experimental designs presented in 1946 by Robin L. Plackett and J. P. Burman while working in the British Ministry of Supply. Their goal was to find experimental designs for investigating the dependence of some measured quantity on a number of independent variables (factors), each taking L levels, in such a way as to minimize the variance of the estimates of these dependencies using a limited number of experiments. Interactions between the factors were considered negligible. The solution to this problem is to find an experimental design where each combination of levels for any pair of factors appears the same number of times, throughout all the experimental runs (refer table). A complete factorial design would satisfy this criterion, but the idea was to find smaller designs. For the case of two levels (L=2), Plackett and Burman used the method found in 1933 by Raymond Paley for generating orthogonal matrices whose elements are all either 1 or -1 (Hadamard matrices). Paley's method could be used to find such matrices of size N for most N equal to a multiple of 4. In particular, it worked for all such N up to 100 except N = 92. If N is a power of 2, however, the resulting design is identical to a fractional factorial design, so Plackett Burman designs are mostly used when N is a multiple of 4 but not a power of 2 (i.e. N = 12, 20, 24, 28, 36 ...). If one is trying to estimate less than N parameters (including the overall average), then one simply uses a subset of the columns of the matrix. For the case of more than two levels, Plackett and Burman rediscovered designs that had previously been given by Raj Chandra Bose and K. Kishen at the Indian Statistical Institute. Plackett and Burman give specifics for designs having a number of experiments equal to the number of levels L to some integer power, for L = 3, 4, 5, or 7. When interactions between factors are not negligible, they are often confounded in Plackett Burman designs with the main effects, meaning that the designs do not permit one to distinguish between certain main effects and certain interactions. This is called aliasing or confounding.
The principle of indifference (also called principle of insufficient reason) is a rule for assigning epistemic probabilities. Suppose that there are n > 1 mutually exclusive and collectively exhaustive possibilities. The principle of indifference states that if the n possibilities are indistinguishable except for their names, then each possibility should be assigned a probability equal to 1/n. In Bayesian probability, this is the simplest non-informative prior. The principle of indifference is meaningless under the frequency interpretation of probability, in which probabilities are relative frequencies rather than degrees of belief in uncertain propositions, conditional upon state information.
In statistics, familywise error rate (FWER) is the probability of making one or more false discoveries, or type I errors, among all the hypotheses when performing multiple hypotheses tests.
In critical sociology, age stratification refers to the hierarchical ranking of people into age groups within a society. Age stratification which is based on an ascribed status is a major source inequality, and thus may lead to ageism. Some of the advantages of Age Stratification are labour force calculation, estimating dependants, population growth estimation, forming appropriate government policies & planning (e.g. Budgeting more on health care when ageing population is found, increase university capacity to serve increasing numbers of young children) etc.
Mean time between failures (MTBF) is the predicted elapsed time between inherent failures of a system during operation. MTBF can be calculated as the arithmetic mean (average) time between failures of a system. The terms is used in both plant and equipment maintenance contexts. The MTBF is typically part of a model that assumes the failed system is immediately repaired (mean time to repair, or MTTR), as a part of a renewal process. This is in contrast to the mean time to failure (MTTF), which measures average time to failures with the modeling assumption that the failed system is not repaired (infinite repair time). The definition of MTBF depends on the definition of what is considered a system failure. For complex, repairable systems, failures are considered to be those out of design conditions which place the system out of service and into a state for repair. Failures which occur that can be left or maintained in an unrepaired condition, and do not place the system out of service, are not considered failures under this definition. In addition, units that are taken down for routine scheduled maintenance or inventory control are not considered within the definition of failure.
A unit in a statistical analysis refers to one member of a set of entities being studied. It is the material source for the mathematical abstraction of a "random variable". Common examples of a unit would be a single person, animal, plant, or manufactured item that belongs to a larger collection of such entities being studied. Units are often referred to as being either experimental units, sampling units or, more generally, units of observation: An "experimental unit" is typically thought of as one member of a set of objects that are initially equivalent, with each object then subjected to one of several experimental treatments. A "sampling unit" is typically thought of as an object that has been sampled from a statistical population. This term is commonly used in opinion polling and survey sampling. In most statistical studies, the goal is to generalize from the observed units to a larger set consisting of all comparable units that exist but are not directly observed. For example, if we randomly sample 100 people and ask them which candidate they intend to vote for in an election, our main interest is in the voting behavior of all eligible voters, not exclusively on the 100 observed units. In some cases, the observed units may not form a sample from any meaningful population, but rather constitute a convenience sample, or may represent the entire population of interest. In this situation, we may study the units descriptively, or we may study their dynamics over time. But it typically does not make sense to talk about generalizing to a larger population of such units. Studies involving countries or business firms are often of this type. Clinical trials also typically use convenience samples, however the aim is often to make inferences about the efficacy of treatments in other patients, and given the inclusion and exclusion criteria for some clinical trials, the sample may not be representative of the majority of patients with the condition or disease. In simple data sets, the units are in one-to-one correspondence with the data values. In more complex data sets, multiple measurements are made for each unit. For example, if blood pressure measurements are made daily for a week on each subject in a study, there would be seven data values for each statistical unit. Multiple measurements taken on an individual are not independent (they will be more alike compared to measurements taken on different individuals). Ignoring these dependencies during the analysis can lead to an inflated sample size or pseudoreplication. While a unit is often the lowest level at which observations are made, in some cases, a unit can be further decomposed as a statistical assembly. Many statistical analyses use quantitative data that have units of measurement. This is a distinct and non-overlapping use of the term "unit."
Calculus of predispositions is a basic part of predispositioning theory and belongs to the indeterministic procedures.
In probability and statistics the extended negative binomial distribution is a discrete probability distribution extending the negative binomial distribution. It is a truncated version of the negative binomial distribution for which estimation methods have been studied. In the context of actuarial science, the distribution appeared in its general form in a paper by K. Hess, A. Liewald and K.D. Schmidt when they characterized all distributions for which the extended Panjer recursion works. For the case m = 1, the distribution was already discussed by Willmot and put into a parametrized family with the logarithmic distribution and the negative binomial distribution by H.U. Gerber.  
The generalized normal distribution or generalized Gaussian distribution (GGD) is either of two families of parametric continuous probability distributions on the real line. Both families add a shape parameter to the normal distribution. To distinguish the two families, they are referred to below as "version 1" and "version 2". However this is not a standard nomenclature.
In econometrics, the seemingly unrelated regressions (SUR) or seemingly unrelated regression equations (SURE) model, proposed by Arnold Zellner in (1962), is a generalization of a linear regression model that consists of several regression equations, each having its own dependent variable and potentially different sets of exogenous explanatory variables. Each equation is a valid linear regression on its own and can be estimated separately, which is why the system is called seemingly unrelated, although some authors suggest that the term seemingly related would be more appropriate, since the error terms are assumed to be correlated across the equations. The model can be estimated equation-by-equation using standard ordinary least squares (OLS). Such estimates are consistent, however generally not as efficient as the SUR method, which amounts to feasible generalized least squares with a specific form of the variance-covariance matrix. Two important cases when SUR is in fact equivalent to OLS, are: either when the error terms are in fact uncorrelated between the equations (so that they are truly unrelated), or when each equation contains exactly the same set of regressors on the right-hand-side. The SUR model can be viewed as either the simplification of the general linear model where certain coefficients in matrix  are restricted to be equal to zero, or as the generalization of the general linear model where the regressors on the right-hand-side are allowed to be different in each equation. The SUR model can be further generalized into the simultaneous equations model, where the right-hand side regressors are allowed to be the endogenous variables as well.
In mathematics and computing, the Levenberg Marquardt algorithm (LMA), also known as the damped least-squares (DLS) method, is used to solve non-linear least squares problems. These minimization problems arise especially in least squares curve fitting. The LMA is used in many software applications for solving generic curve-fitting problems. However, as for many fitting algorithms, the LMA finds only a local minimum, which is not necessarily the global minimum. The LMA interpolates between the Gauss Newton algorithm (GNA) and the method of gradient descent. The LMA is more robust than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. LMA can also be viewed as Gauss Newton using a trust region approach. The algorithm was first published in 1944 by Kenneth Levenberg, while working at the Frankford Army Arsenal. It was rediscovered in 1963 by Donald Marquardt who worked as a statistician at DuPont and independently by Girard, Wynne and Morrison.
A truncated mean or trimmed mean is a statistical measure of central tendency, much like the mean and median. It involves the calculation of the mean after discarding given parts of a probability distribution or sample at the high and low end, and typically discarding an equal amount of both. This number of points to be discarded is usually given as a percentage of the total number of points, but may also be given as a fixed number of points. For most statistical applications, 5 to 25 percent of the ends are discarded; the 25% trimmed mean (when the lowest 25% and the highest 25% are discarded) is known as the interquartile mean. For example, given a set of 8 points, trimming by 12.5% would discard the minimum and maximum value in the sample: the smallest and largest values, and would compute the mean of the remaining 6 points. The median can be regarded as a fully truncated mean and is most robust. As with other trimmed estimators, the main advantage of the trimmed mean is robustness and higher efficiency for mixed distributions and heavy-tailed distribution (like the Cauchy distribution), at the cost of lower efficiency for some other less heavily-tailed distributions (such as the normal distribution). For intermediate distributions the differences between the efficiency of the mean and the median are not very big, e.g. for the student-t distribution with 2 degrees of freedom the variances for mean and median are nearly equal.
Econometrics is the application of mathematics, statistical methods, and computer science, to economic data and is described as the branch of economics that aims to give empirical content to economic relations. More precisely, it is "the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference." An introductory economics textbook describes econometrics as allowing economists "to sift through mountains of data to extract simple relationships." The first known use of the term "econometrics" (in cognate form) was by Polish economist Pawe  Ciompa in 1910. Ragnar Frisch is credited with coining the term in the sense in which it is used today.
In statistics, pooled variance (also known as combined, composite, or overall variance) is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same. If the populations are indexed , then the pooled variance  can be estimated by the weighted average of the sample variances   where  is the sample size of population  and  is . Use of  weighting factors instead of  comes from Bessel's correction. Under the assumption of equal population variances, the pooled sample variance provides a higher precision estimate of variance than the individual sample variances. This higher precision can lead to increased statistical power when used in statistical tests that compare the populations, such as the t-test. The square-root of a pooled variance estimator is known as a pooled standard deviation (also known as combined, composite, or overall standard deviation).
In statistics, ordered probit is a generalization of the popular probit analysis to the case of more than two outcomes of an ordinal dependent variable. Similarly, the popular logit method also has a counterpart ordered logit. For example, in clinical research, the effect a drug may have on a patient may be modeled with ordered probit regression. Independent variables may include the use or non-use of the drug as well as control variables such as age and details from medical history such as whether the patient suffers from high blood pressure, heart disease, etc. The dependent variable would be ranked from the following list: complete cure, relieve symptoms, no effect, deteriorate condition, death. The model cannot be consistently estimated using ordinary least squares; it is usually estimated using maximum likelihood. Suppose the underlying relationship to be characterized is , where  is the exact but unobserved dependent variable (perhaps the exact level of improvement by the patient);  is the vector of independent variables, and  is the vector of regression coefficients which we wish to estimate. Further suppose that while we cannot observe , we instead can only observe the categories of response:  Then the ordered probit technique will use the observations on , which are a form of censored data on , to fit the parameter vector .
The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspe  Peninsula "all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus". The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.
A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability  to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, information retrieval and other applications. In speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases "recognize speech" and "wreck a nice beach" are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model. Language models are used in information retrieval in the query likelihood model. Here a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model . Commonly, the unigram language model is used for this purpose otherwise known as the bag of words model. Data sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1.
In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, thus assigning each individual to a particular group or "category." In computer science and some branches of mathematics, categorical variables are referred to as enumerations or enumerated types. Commonly (though not in this article), each of the possible values of a categorical variable is referred to as a level. The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from either or both of observations made of qualitative data, where the observations are summarised as counts or cross tabulations, or of quantitative data, where observations might be directly observed counts of events happening or might be counts of values that occur within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term "categorical data" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables. A categorical variable that can take on exactly two values is termed a binary variable or dichotomous variable; an important special case is the Bernoulli variable. Categorical variables with more than two possible values are called polytomous variables; variables are often assumed to be polytomous unless otherwise specified. Discretization is treating continuous data as if it were categorical. Dichotomization is treating continuous data or polytomous variables as if they were binary variables. Regression analysis often treats category membership as a quantitative dummy variable.
Configural frequency analysis (CFA) is a method of exploratory data analysis, introduced by Gustav A. Lienert in 1969. The goal of a configural frequency analysis is to detect patterns in the data that occur significantly more (such patterns are called Types) or significantly less often (such patterns are called Antitypes) than expected by chance. Thus, the idea of a CFA is to provide by the identified types and antitypes some insight into the structure of the data. Types are interpreted as concepts which are constituted by a pattern of variable values. Antitypes are interpreted as patterns of variable values that do in general not occur together.
In mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of rates is desired. The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals. As a simple example, the harmonic mean of 1, 2, and 4 is  The harmonic mean H of the positive real numbers  is defined to be  From the third formula in the above equation, it is more apparent that the harmonic mean is related to the arithmetic and geometric means. It is the reciprocal dual of the arithmetic mean for positive inputs:  The harmonic mean is a Schur-concave function, and dominated by the minimum of its arguments, in the sense that for any positive set of arguments, . Thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged).
A cohort study or panel study is a quasi-experiment in the form of a longitudinal study (generally a type of observational study) used in medicine, nursing, psychology, social science, actuarial science, business analytics, and ecology. In a cohort study there is a passive follow-up of a group of people and a documentation of relevant characteristics or events related to this group of people. For instance in medicine, it is an analysis of risk factors and follows a group of people who do not have the disease, and uses correlations to determine the absolute risk of subject contraction. It is one type of clinical study design and should be compared with a cross-sectional study. Cohort studies are largely about the life histories of segments of populations, and the individual people who constitute these segments. A cohort is a group of people who share a common characteristic or experience within a defined period (e.g., are born, are exposed to a drug or vaccine or pollutant, or undergo a certain medical procedure). Thus a group of people who were born on a day or in a particular period, say 1948, form a birth cohort. The comparison group may be the general population from which the cohort is drawn, or it may be another cohort of persons thought to have had little or no exposure to the substance under investigation, but otherwise similar. Alternatively, subgroups within the cohort may be compared with each other. Randomized controlled trials (RCTs) are a superior methodology in the hierarchy of evidence in therapy, because they limit the potential for any biases by randomly assigning one patient pool to an intervention and another patient pool to non-intervention (or placebo). This minimizes the chance that the incidence of confounding (particularly unknown confounding) variables will differ between the two groups. However, it is important to note that RCTs may not be suitable in all cases and other methodologies could be much more suitable to investigate the study's objective(s). Cohort studies can either be conducted prospectively, or retrospectively from archived records.
In probability theory, Kolmogorov equations, including Kolmogorov forward equations and Kolmogorov backward equations, characterize random dynamic processes. In order to try to explain them in simple ideas, suppose we have a complete statistical description of a stochastic process x(t) a and know some transformation,a function of the stochastic variable, (for example, velocity, which is the derivative of the state variable) which defines a new process y(t) related to x(t), for the velocity example :. Then the Kolmogorov equations are a means for determining features of the stochastic process y(t), thus without needing to know the stochastic process itself, the details.
In statistics and signal processing, the orthogonality principle is a necessary and sufficient condition for the optimality of a Bayesian estimator. Loosely stated, the orthogonality principle says that the error vector of the optimal estimator (in a mean square error sense) is orthogonal to any possible estimator. The orthogonality principle is most commonly stated for linear estimators, but more general formulations are possible. Since the principle is a necessary and sufficient condition for optimality, it can be used to find the minimum mean square error estimator.
In the military science of ballistics, circular error probable (CEP) (also circular error probability or circle of equal probability) is a measure of a weapon system's precision. It is defined as the radius of a circle, centered about the mean, whose boundary is expected to include the landing points of 50% of the rounds.  
In statistics and econometrics, the multinomial probit model is a generalization of the probit model used when there are several possible categories that the dependent variable can fall into. As such, it is an alternative to the multinomial logit model as one method of multiclass classification. It is not to be confused with the multivariate probit model, which is used to model correlated binary outcomes for more than one independent variable.
Coalescent theory is a retrospective stochastic model of population genetics that relates genetic diversity in a sample to demographic history of the population from which it was taken. That is, it is a model of the effect of genetic drift, viewed backwards in time, on the genealogy of antecedents.[1] It comprises a probabilistic assessment of variation in time to common ancestry of alleles in a relatively small sample of individuals, from a much larger population. This includes consideration of all pathways of inheritance through which sampled copies of a homologous DNA element are traced back to a single ancestral copy, known as the most recent common ancestor (MRCA; sometimes also termed the coancestor to emphasize the coalescent relationship). The inheritance relationships among alleles are typically represented as a gene genealogy, or gene tree, similar in form to a phylogenetic tree. The probabilistic expectation of this gene genealogy is also known as the coalescent. Understanding the statistical properties of the coalescent under different assumptions forms the basis of coalescent theory. Because of recombination, different gene loci follow different pathways of ancestry, resulting in different gene genealogies. The coalescent is also relevant to phylogenetics, as incomplete lineage sorting between speciation events results in conflict among gene-loci in phylogenetic relationships inferred among species. The mathematical theory of the coalescent was originally developed in the early 1980s by John Kingman.[2] In the simplest case, coalescent theory assumes no recombination, no natural selection, and no gene flow or population structure. The gene genealogy is independent of the mutational process, such that changes in the DNA sequence do not affect inheritance and can be considered separately (even if all gene copies are identical in sequence they are not equally related in the gene tree). Under this model, the expected time between successive coalescence events, by which two gene copies arise from a single ancestral copy, increases almost exponentially back in time (with wide variance). Advances in coalescent theory include recombination, selection, and virtually any arbitrarily complex evolutionary or demographic model in population genetic analysis.
The rescaled range is a statistical measure of the variability of a time series introduced by the British hydrologist Harold Edwin Hurst (1880 1978). Its purpose is to provide an assessment of how the apparent variability of a series changes with the length of the time-period being considered. The rescaled range is calculated from dividing the range of the values exhibited in a portion of the time series by the standard deviation of the values over the same portion of the time series. For example, consider a time series {2, 5, 3, 7, 8, 12, 4, 2} which has a range, R, of 12 - 2 = 10. Its standard deviation, s, is 3.46, so the rescaled range is R/s = 2.89. If we consider the same time series, but increase the number of observations of it, the rescaled range will generally also increase. The increase of the rescaled range can be characterized by making a plot of the logarithm of R/s vs. the logarithm of n. The slope of this line gives the Hurst exponent, H. If the time series is generated by a random walk (or a Brownian motion process) it has the value of H =1/2. Many physical phenomena that have a long time series suitable for analysis exhibit a Hurst exponent greater than 1/2. For example, observations of the height of the Nile River measured annually over many years gives a value of H = 0.77. Several researchers (including Peters, 1991) have found that the prices of many financial instruments (such as currency exchange rates, stock values, etc.) also have H > 1/2. This means that they have a behavior that is distinct from a random walk, and therefore the time series is not generated by a stochastic process that has the nth value independent of all of the values before this. According to model  of Fractional Brownian motion this is referred to as long memory of positive linear autocorrelation. However it has been shown  that this measure is correct only for linear evaluation: complex nonlinear processes with memory need additional descriptive parameters. Several studies using Lo's  modified rescaled range statistic have contradicted Peters' results as well.
A statistical parameter is a parameter that indexes a family of probability distributions. It can be regarded as a numerical characteristic of a population or a statistical model.
This is a list of graphical methods with a mathematical basis. Included are diagram techniques, chart techniques, plot techniques, and other forms of visualization. There is also a list of computer graphics and descriptive geometry topics.
In statistical quality control, the u-chart is a type of control chart used to monitor "count"-type data where the sample size is greater than one, typically the average number of nonconformities per unit. The u-chart differs from the c-chart in that it accounts for the possibility that the number or size of inspection units for which nonconformities are to be counted may vary. Larger samples may be an economic necessity or may be necessary to increase the area of opportunity in order to track very low nonconformity levels. Examples of processes suitable for monitoring with a u-chart include: Monitoring the number of nonconformities per lot of raw material received where the lot size varies Monitoring the number of new infections in a hospital per day Monitoring the number of accidents for delivery trucks per day As with the c-chart, the Poisson distribution is the basis for the chart and requires the same assumptions. The control limits for this chart type are  where  is the estimate of the long-term process mean established during control-chart setup. The observations  are plotted against these control limits, where xi is the number of nonconformities for the ith subgroup and ni is the number of inspection units in the ith subgroup.
A statistical model embodies a set of assumptions concerning the generation of the observed data, and similar data from a larger population. A model represents, often in considerably idealized form, the data-generating process. The model assumptions describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. A model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, "a model is a formal representation of a theory" (Herman Ade r quoting Kenneth Bollen). All statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.
In probability and statistics, the Tweedie distributions are a family of probability distributions which include the purely continuous normal and gamma distributions, the purely discrete scaled Poisson distribution, and the class of mixed compound Poisson gamma distributions which have positive mass at zero, but are otherwise continuous. For any random variable Y that obeys a Tweedie distribution, the variance var(Y) relates to the mean E(Y) by the power law,  where a and p are positive constants. The Tweedie distributions were named by Bent J rgensen after Maurice Tweedie, a statistician and medical physicist at the University of Liverpool, UK, who presented the first thorough study of these distributions in 1984.
In statistics, the Behrens Fisher problem, named after Walter Ulrich Behrens and Ronald Fisher, is the problem of interval estimation and hypothesis testing concerning the difference between the means of two normally distributed populations when the variances of the two populations are not assumed to be equal, based on two independent samples.
Brownian motion or pedesis (from Ancient Greek:          /p   d  sis/ "leaping") is the random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the quick atoms or molecules in the gas or liquid. Wiener Process refers to the mathematical model used to describe such Brownian Motion, which is often called a particle theory. This transport phenomenon is named after the botanist Robert Brown. In 1827, while looking through a microscope at particles trapped in cavities inside pollen grains in water, he noted that the particles moved through the water; but he was not able to determine the mechanisms that caused this motion. Atoms and molecules had long been theorized as the constituents of matter, and Albert Einstein published a paper in 1905 that explained in precise detail how the motion that Brown had observed was a result of the pollen being moved by individual water molecules. This explanation of Brownian motion served as definitive confirmation that atoms and molecules actually exist, and was further verified experimentally by Jean Perrin in 1908. Perrin was awarded the Nobel Prize in Physics in 1926 "for his work on the discontinuous structure of matter" (Einstein had received the award five years earlier "for his services to theoretical physics" with specific citation of different research). The direction of the force of atomic bombardment is constantly changing, and at different times the particle is hit more on one side than another, leading to the seemingly random nature of the motion. The mathematical model of Brownian motion has numerous real-world applications. For instance, stock market fluctuations are often cited, although Benoit Mandelbrot rejected its applicability to stock price movements in part because these are discontinuous. Brownian motion is among the simplest of the continuous-time stochastic (or probabilistic) processes, and it is a limit of both simpler and more complicated stochastic processes (see random walk and Donsker's theorem). This universality is closely related to the universality of the normal distribution. In both cases, it is often mathematical convenience, rather than the accuracy of the models, that motivates their use.
A census is the procedure of systematically acquiring and recording information about the members of a given population. It is a regularly occurring and official count of a particular population. The term is used mostly in connection with national population and housing censuses; other common censuses include agriculture, business, and traffic censuses. The United Nations defines the essential features of population and housing censuses as "individual enumeration, universality within a defined territory, simultaneity and defined periodicity", and recommends that population censuses be taken at least every 10 years. United Nations recommendations also cover census topics to be collected, official definitions, classifications and other useful information to co-ordinate international practice. The word is of Latin origin: during the Roman Republic, the census was a list that kept track of all adult males fit for military service. The modern census is essential to international comparisons of any kind of statistics, and censuses collect data on many attributes of a population, not just how many people there are but now census takes its place within a system of surveys where it typically began as the only national demographic data collection. Although population estimates remain an important function of a census, including exactly the geographic distribution of the population, statistics can be produced about combinations of attributes e.g. education by age and sex in different regions. Current administrative data systems allow for other approaches to enumeration with the same level of detail but raise concerns about privacy and the possibility of biasing estimates. The United Nations Population Fund (UNFPA) explains that,  A traditional population and housing census requires mapping an entire country, figuring out what technologies should be employed, mobilizing and training legions of enumerators, conducting a major public campaign, canvassing all households, collecting individual information, compiling hundreds of thousands   or millions   of completed questionnaires, monitoring procedures and results, and analyzing and disseminating the data.   A census can be contrasted with sampling in which information is obtained only from a subset of a population, typically main population estimates are updated by such intercensal estimates. Modern census data are commonly used for research, business marketing, and planning, and as a baseline for designing sample surveys by providing a sampling frame such as an address register. Census counts are necessary to adjust samples to be representative of a population by weighting them as is common in opinion polling. Similarly, stratification requires knowledge of the relative sizes of different population strata which can be derived from census enumerations. In some countries, the census provides the official counts used to apportion the number of elected representatives to regions (sometimes controversially   e.g., Utah v. Evans). In many cases, a carefully chosen random sample can provide more accurate information than attempts to get a population census.
Estimation of a Rasch model is used to estimate the parameters of the Rasch model. Various techniques are employed to estimate the parameters from matrices of response data. The most common approaches are types of maximum likelihood estimation, such as joint and conditional maximum likelihood estimation. Joint maximum likelihood (JML) equations are efficient, but inconsistent for a finite number of items, whereas conditional maximum likelihood (CML) equations give consistent and unbiased item estimates. Person estimates are generally thought to have bias associated with them, although weighted likelihood estimation methods for the estimation of person parameters reduce the bias.
In econometrics, Kwiatkowski Phillips Schmidt Shin (KPSS) tests are used for testing a null hypothesis that an observable time series is stationary around a deterministic trend. Such models were proposed in 1982 by Alok Bhargava in his Ph.D. thesis where several John von Neumann- or Durbin Watson-type finite sample tests for unit roots were developed (see Bhargava, 1986). Later, Denis Kwiatkowski, Peter C. B. Phillips, Peter Schmidt and Yongcheol Shin (1992) proposed a test of the null hypothesis that an observable series is trend stationary (stationary around a deterministic trend). The series is expressed as the sum of deterministic trend, random walk, and stationary error, and the test is the Lagrange multiplier test of the hypothesis that the random walk has zero variance. KPSS-type tests are intended to complement unit root tests, such as the Dickey Fuller tests. By testing both the unit root hypothesis and the stationarity hypothesis, one can distinguish series that appear to be stationary, series that appear to have a unit root, and series for which the data (or the tests) are not sufficiently informative to be sure whether they are stationary or integrated.
In statistics, time series data is data collected at regular intervals. When there are patterns that repeat over known, fixed periods of time within the data set it is considered to be seasonality, seasonal variation, periodic variation, or periodic fluctuations. This variation can be either regular or semi-regular. Seasonality may be caused by various factors, such as weather, vacation, and holidays and usually consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series. Seasonality can repeat on a weekly, monthly or quarterly basis, these periods of time are structured and occur in a length of time less than a year. Seasonal fluctuations in a time series can be contrasted with cyclical patterns. The latter occur in a period of time that extends beyond a single year, these fluctuations are usually of at least two year and do not repeat over fixed periods of time. Organizations facing seasonal variations, such as ice-cream vendors, are often interested in knowing their performance relative to the normal seasonal variation. Seasonal variations in the labor market can be attributed to the entrance of school leavers into the job market; as they aim to contribute to the workforce during their vacations, or upon the completion of their schooling. These regular changes are of less interest to those who study employment data than the variations that occur due to the underlying state of the economy. Where their focus is on how unemployment in the workforce has changed, despite the impact of the regular seasonal variations. It is necessary for organizations to identify and measure seasonal variations within their market to help them plan for the future. This can prepare them for the temporary increases or decreases in labor requirements and inventory as demand for their product or service fluctuates over certain periods. This may require training, periodic maintenance, and so forth that can be organized in advance. Apart from these considerations, the organizations need to know if variation they have experienced have been more or less than the expected amount, beyond what the usual seasonal variations account for.  
The Howland will forgery trial was a U.S. court case in 1868 to decide Henrietta Howland Robinson's contest of the will of Sylvia Ann Howland. It is famous for the forensic use of mathematics by Benjamin Peirce as an expert witness.
Stochastic gradient descent (often shortened in SGD) is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions.
In statistics, consistency of procedures, such as computing confidence intervals or conducting hypothesis tests, is a desired property of their behaviour as the number of items in the data set to which they are applied increases indefinitely. In particular, consistency requires that the outcome of the procedure with unlimited data should identify the underlying truth. Use of the term in statistics derives from Sir Ronald Fisher in 1922. Use of the terms consistency and consistent in statistics is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow.
Acquiescence bias is a category of response bias in which respondents to a survey have a tendency to agree with all the questions or to indicate a positive connotation. Acquiescence is sometimes referred to as "yea-saying" and is the tendency of a respondent to agree with a statement when in doubt. This particularly is in the case of surveys or questionnaires that employ truisms, such as: "It is better to give than to receive" or "Never a lender nor a borrower be". Douglas N. Jackson did a demonstration of acquiescence responding on the California F-scale (a measure of authoritarianism), which contains such truisms. He created a reverse-keyed version of the California F-scale where all the items were the opposite in meaning (see the two previous examples for a pair of such contradictory statements). He administered both the original and reverse-keyed versions of the California F-scale to the same group of respondents. One would expect that the correlation between these two scales to be negative, but there was a high, positive correlation. Jackson interpreted this as evidence of acquiescence responding. Respondents were merely being agreeable to the statements, regardless of the content. Jackson and Messick, using factor analysis, also demonstrated that the two main factors explaining the majority of response variation on the Minnesota Multiphasic Personality Inventory (MMPI) were for social desirability and acquiescence responding (this would also hold true for the revised MMPI-2). One approach to dealing with acquiescence responding on surveys and questionnaires is to employ a balance of positively and negatively keyed items in terms of the intended content. For example, in trying to assess depression it would be a good idea to also include items assessing happiness and contentedness, etc. (reversed-keyed items), in addition to the usual depressive content.
In statistics, the sample maximum and sample minimum, also called the largest observation, and smallest observation, are the values of the greatest and least elements of a sample. They are basic summary statistics, used in descriptive statistics such as the five-number summary and seven-number summary and the associated box plot. The minimum and the maximum value are the first and last order statistics (often denoted X(1) and X(n) respectively, for a sample size of n). If there are outliers, they necessarily include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum need not be outliers, if they are not unusually far from other observations.
The principle of detailed balance is formulated for kinetic systems which are decomposed into elementary processes (collisions, or steps, or elementary reactions): At equilibrium, each elementary process should be equilibrated by its reverse process.
In fields such as epidemiology, social sciences, psychology and statistics, an observational study draws inferences from a sample to a population where the independent variable is not under the control of the researcher because of ethical concerns or logistical constraints. One common observational study is about the possible effect of a treatment on subjects, where the assignment of subjects into a treated group versus a control group is outside the control of the investigator. This is in contrast with experiments, such as randomized controlled trials, where each subject is randomly assigned to a treated group or a control group.
In combinatorial mathematics, a Latin rectangle is an r   n matrix that has the numbers 1, 2, 3, ..., n as its entries with no number occurring more than once in any row or column where r   n. An n   n Latin rectangle is called a Latin square. If r < n, then it is possible to append n   r rows to an r   n Latin rectangle to form a Latin square, using Hall's marriage theorem. In statistics, Latin rectangles have applications in the design of experiments.
In statistics, the correlation ratio is a measure of the relationship between the statistical dispersion within individual categories and the dispersion across the whole population or sample. The measure is defined as the ratio of two standard deviations representing these types of variation. The context here is the same as that of the intraclass correlation coefficient, whose value is the square of the correlation ratio.
In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables. The term "MARS" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open source implementations of MARS are called "Earth".
In probability theory, calculation of the sum of normally distributed random variables is an instance of the arithmetic of random variables, which can be quite complex based on the probability distributions of the random variables involved and their relationships.
In multilinear algebra, the tensor rank decomposition or canonical polyadic decomposition (CPD) may be regarded as a generalization of the matrix singular value decomposition (SVD) to tensors, which has found application in statistics, signal processing, psychometrics, linguistics and chemometrics. It was introduced by Hitchcock in 1927 and later rediscovered several times, notably in psychometrics. For this reason, the tensor rank decomposition is sometimes historically referred to as PARAFAC or CANDECOMP.
Data collection is the process of gathering and measuring information on targeted variables in an established systematic fashion, which then enables one to answer relevant questions and evaluate outcomes. The data collection component of research is common to all fields of study including physical and social sciences, humanities and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that then translates to rich data analysis and allows the building of a convincing and credible answer to questions that have been posed.
In probability theory, the Mabinogion sheep problem or Mabinogian urn is a problem in stochastic control introduced by David Williams (1991, 15.3), who named it after a herd of magic sheep in the Welsh epic Mabinogion.
In probability theory, in particular in the study of stochastic processes, a stopping time (also Markov time) is a specific type of  random time : a random variable whose value is interpreted as the time at which a given stochastic process exhibits a certain behavior of interest. A stopping time is often defined by a stopping rule, a mechanism for deciding whether to continue or stop a process on the basis of the present position and past events, and which will almost always lead to a decision to stop at some finite time. Stopping times occur in decision theory, and the optional stopping theorem is an important result in this context. Stopping times are also frequently applied in mathematical proofs to  tame the continuum of time , as Chung put it in his book (1982).
In business intelligence, data classification has close ties to data clustering, but where data clustering is descriptive, data classification is predictive. In essence data classification consists of using variables with known values to predict the unknown or future values of other variables. It can be used in e.g. direct marketing, insurance fraud detection or medical diagnosis. The first step in doing a data classification is to cluster the data set used for category training, to create the wanted number of categories. An algorithm, called the classifier, is then used on the categories, creating a descriptive model for each. These models can then be used to categorize new items in the created classification system. According to Golfarelli and Rizzi, these are the measures of effectiveness of the classifier: Predictive accuracy: How well does it predict the categories for new observations  Speed: What is the computational cost of using the classifier  Robustness: How well do the models created perform if data quality is low  Scalability: Does the classifier function efficiently with large amounts of data  Interpretability: Are the results understandable to users  Typical examples of input for data classification could be variables such as demographics, lifestyle information, or economical behaviour.
In statistics, Dixon's Q test, or simply the Q test, is used for identification and rejection of outliers. This assumes normal distribution and per Dean and Dixon, and others, this test should be used sparingly and never more than once in a data set. To apply a Q test for bad data, arrange the data in order of increasing values and calculate Q as defined:  Where gap is the absolute difference between the outlier in question and the closest number to it. If Q > Qtable, where Qtable is a reference value corresponding to the sample size and confidence level, then reject the questionable point. Note that only one point may be rejected from a data set using a Q test.
In statistics, the Durbin Watson statistic is a test statistic used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Later, John Denis Sargan and Alok Bhargava developed several von Neumann Durbin Watson type test statistics for the null hypothesis that the errors on a regression model follow a process with a unit root against the alternative hypothesis that the errors follow a stationary first order autoregression (Sargan and Bhargava, 1983). Note that the distribution of this test statistic does not depend on the estimated regression coefficients and the variance of the errors.
In statistics, the question of checking whether a coin is fair is one whose importance lies, firstly, in providing a simple problem on which to illustrate basic ideas of statistical inference and, secondly, in providing a simple problem that can be used to compare various competing methods of statistical inference, including decision theory. The practical problem of checking whether a coin is fair might be considered as easily solved by performing a sufficiently large number of trials, but statistics and probability theory can provide guidance on two types of question; specifically those of how many trials to undertake and of the accuracy an estimate of the probability of turning up heads, derived from a given sample of trials. A fair coin is an idealized randomizing device with two states (usually named "heads" and "tails") which are equally likely to occur. It is based on the coin flip used widely in sports and other situations where it is required to give two parties the same chance of winning. Either a specially designed chip or more usually a simple currency coin is used, although the latter might be slightly "unfair" due to an asymmetrical weight distribution, which might cause one state to occur more frequently than the other, giving one party an unfair advantage. So it might be necessary to test experimentally whether the coin is in fact "fair"   that is, whether the probability of the coin falling on either side when it is tossed is approximately 50%. It is of course impossible to rule out arbitrarily small deviations from fairness such as might be expected to affect only one flip in a lifetime of flipping; also it is always possible for an unfair (or "biased") coin to happen to turn up exactly 10 heads in 20 flips. As such, any fairness test must only establish a certain degree of confidence in a certain degree of fairness (a certain maximum bias). In more rigorous terminology, the problem is of determining the parameters of a Bernoulli process, given only a limited sample of Bernoulli trials.
The law of truly large numbers, attributed to Persi Diaconis and Frederick Mosteller, states that with a sample size large enough, any outrageous thing is likely to happen. Because we never find it notable when likely events occur, we highlight unlikely events and notice them more. The law seeks to debunk one element of supposed supernatural phenomenology.
A factor graph is a bipartite graph representing the factorization of a function. In probability theory and its applications, factor graphs are used to represent factorization of a probability distribution function, enabling efficient computations, such as the computation of marginal distributions through the sum-product algorithm. One of the important success stories of factor graphs and the sum-product algorithm is the decoding of capacity-approaching error-correcting codes, such as LDPC and turbo codes. Factor graphs generalize constraint graphs. A factor whose value is either 0 or 1 is called a constraint. A constraint graph is a factor graph where all factors are constraints. The max-product algorithm for factor graphs can be viewed as a generalization of the arc-consistency algorithm for constraint processing.
The term gambler's ruin is used for a number of related statistical ideas: The original meaning is that a gambler who raises his bet to a fixed fraction of bankroll when he wins, but does not reduce it when he loses, will eventually go broke, even if he has a positive expected value on each bet. Another common meaning is that a gambler with finite wealth, playing a fair game (that is, each bet has expected value zero to both sides) will eventually go broke against an opponent with infinite wealth. Such a situation can be modeled by a random walk on the real number line. In that context it is provable that the agent will return to his point of origin or go broke and is ruined an infinite number of times if the random walk continues forever. The result above is a corollary of a general theorem by Christiaan Huygens which is also known as gambler's ruin. That theorem shows how to compute the probability of each player winning a series of bets that continues until one's entire initial stake is lost, given the initial stakes of the two players and the constant probability of winning. This is the oldest mathematical idea that goes by the name gambler's ruin, but not the first idea to which the name was applied. The most common use of the term today is that a gambler playing a negative expected value game will eventually go broke, regardless of betting system. This is another corollary to Huygens' result. The ideas have specific relevance for gamblers; however they are also general theorems with wide application and many related results in probability and statistics. Huygens' result in particular led to important advances in the mathematical theory of probability.
Descriptive research is used to describe characteristics of a population or phenomenon being studied. It does not answer questions about how/when/why the characteristics occurred. Rather it addresses the "what" question (what are the characteristics of the population or situation being studied )  The characteristics used to describe the situation or population are usually some kind of categorical scheme also known as descriptive categories. For example, the periodic table categorizes the elements. Scientists use knowledge about the nature of electrons, protons and neutrons to devise this categorical scheme. We now take for granted the periodic table, yet it took descriptive research to devise it. Descriptive research generally precedes explanatory research. For example, over time the periodic table s description of the elements allowed scientists to explain chemical reaction and make sound prediction when elements were combined. Hence, descriptive research cannot describe what caused a situation. Thus, descriptive research cannot be used to as the basis of a causal relationship, where one variable affects another. In other words, descriptive research can be said to have a low requirement for internal validity. The description is used for frequencies, averages and other statistical calculations. Often the best approach, prior to writing descriptive research, is to conduct a survey investigation. Qualitative research often has the aim of description and researchers may follow-up with examinations of why the observations exist and what the implications of the findings are.
Frequentist probability or frequentism is a standard interpretation of probability; it defines an event's probability as the limit of its relative frequency in a large number of trials. This interpretation supports the statistical needs of experimental scientists and pollsters; probabilities can be found (in principle) by a repeatable objective process (and are thus ideally devoid of opinion). It does not support all needs; gamblers typically require estimates of the odds without experiments. The development of the frequentist account was motivated by the problems and paradoxes of the previously dominant viewpoint, the classical interpretation. In the classical interpretation, probability was defined in terms of the principle of indifference, based on the natural symmetry of a problem, so, e.g. the probabilities of dice games arise from the natural symmetric 6-sidedness of the cube. This classical interpretation stumbled at any statistical problem that has no natural symmetry for reasoning.
The term stochastic occurs in a wide variety of professional or academic fields to describe events or systems that are unpredictable due to the influence of a random variable. The word "stochastic" comes from the Greek word         (stokhos, "aim"). Researchers refer to physical systems in which they are uncertain about the values of parameters, measurements, expected input and disturbances as "stochastic systems". In probability theory, a purely stochastic system is one whose state is randomly determined, having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely. In this regard, it can be classified as non-deterministic (i.e., "random") so that the subsequent state of the system is determined probabilistically. Any system or process that must be analyzed using probability theory is stochastic at least in part. Stochastic systems and processes play a fundamental role in mathematical models of phenomena in many fields of science, engineering, finance and economics.
In epidemiology, the absolute risk reduction, risk difference or absolute effect is the change in the risk of an outcome of a given treatment or activity in relation to a comparison treatment or activity. It is the inverse of the number needed to treat. In general, absolute risk reduction is the difference between one treatment comparison group's event rate (EER) and another comparison group s event rate (CER). The difference is usually calculated with respect to two treatments A and B, with A typically a drug and B a placebo. For example, A could be a 5-year treatment with a hypothetical drug, and B is treatment with placebo, i.e. no treatment. A defined endpoint has to be specified, such as a survival or a response rate. For example: the appearance of lung cancer in a 5-year period. If the probabilities pA and pB of this endpoint under treatments A and B, respectively, are known, then the absolute risk reduction is computed as (pB   pA). The inverse of the absolute risk reduction, NNT, is an important measure in pharmacoeconomics. If a clinical endpoint is devastating enough (e.g. death, heart attack), drugs with a low absolute risk reduction may still be indicated in particular situations. If the endpoint is minor, health insurers may decline to reimburse drugs with a low absolute risk reduction.
A climograph is a graphical representation of basic climatic parameters, that is monthly average temperature and precipitation, at a certain location. It is used for a quick-view of the climate of a location.
k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.
In statistics, the focused information criterion (FIC) is a method for selecting the most appropriate model among a set of competitors for a given data set. Unlike most other model selection strategies, like the Akaike information criterion (AIC), the Bayesian information criterion (BIC) and the deviance information criterion (DIC), the FIC does not attempt to assess the overall fit of candidate models but focuses attention directly on the parameter of primary interest with the statistical analysis, say , for which competing models lead to different estimates, say  for model . The FIC method consists in first developing an exact or approximate expression for the precision or quality of each estimator, say  for , and then use data to estimate these precision measures, say . In the end the model with best estimated precision is selected. The FIC methodology was developed by Gerda Claeskens and Nils Lid Hjort, first in two 2003 discussion articles in Journal of the American Statistical Association and later on in other papers and in their 2008 book. The concrete formulae and implementation for FIC depend firstly on the particular parameter of interest, the choice of which does not depend on mathematics but on the scientific and statistical context. Thus the FIC apparatus may be selecting one model as most appropriate for estimating a quantile of a distribution but preferring another model as best for estimating the mean value. Secondly, the FIC formulae depend on the specifics of the models used for the observed data and also on how precision is to be measured. The clearest case is where precision is taken to be mean squared error, say  in terms of squared bias and variance for the estimator associated with model . FIC formulae are then available in a variety of situations, both for handling parametric, semiparametric and nonparametric situations, involving separate estimation of squared bias and variance, leading to estimated precision . In the end the FIC selects the model with smallest estimated mean squared error. Associated with the use of the FIC for selecting a good model is the FIC plot, designed to give a clear and informative picture of all estimates, across all candidate models, and their merit. It displays estimates on the  axis along with FIC scores on the  axis; thus estimates found to the left in the plot are associated with the better models and those found in the middle and to the right stem from models less or not adequate for the purpose of estimating the focus parameter in question. Generally speaking, complex models (with many parameters relative to sample size) tend to lead to estimators with small bias but high variance; more parsimonious models (with fewer parameters) typically yield estimators with larger bias but smaller variance. The FIC method balances the two desired data of having small bias and small variance in an optimal fashion. The main difficulty lies with the bias , as it involves the distance from the expected value of the estimator to the true underlying quantity to be estimated, and the true data generating mechanism may lie outside each of the candidate models. In situations where there is not a unique focus parameter, but rather a family of such, there are versions of average FIC (AFIC or wFIC) that find the best model in terms of suitably weighted performance measures, e.g. when searching for a regression model to perform particularly well in a portion of the covariate space. It is also possible to keep several of the best models on board, ending the statistical analysis with a data-dicated weighted average of the estimators of the best FIC scores, typically giving highest weight to estimators associated with the best FIC scores. Such schemes of model averaging extend the direct FIC selection method. The FIC methodology applies in particular to selection of variables in different forms of regression analysis, including the framework of generalised linear models and the semiparametric proportional hazards models (i.e. Cox regression).
In mathematics, Jensen's inequality, named after the Danish mathematician Johan Jensen, relates the value of a convex function of an integral to the integral of the convex function. It was proven by Jensen in 1906. Given its generality, the inequality appears in many forms depending on the context, some of which are presented below. In its simplest form the inequality states that the convex transformation of a mean is less than or equal to the mean applied after convex transformation; it is a simple corollary that the opposite is true of concave transformations. Jensen's inequality generalizes the statement that the secant line of a convex function lies above the graph of the function, which is Jensen's inequality for two points: the secant line consists of weighted means of the convex function,  while the graph of the function is the convex function of the weighted means,  In the context of probability theory, it is generally stated in the following form: if X is a random variable and   is a convex function, then
The Rubin causal model (RCM), also known as the Neyman Rubin causal model, is an approach to the statistical analysis of cause and effect based on the framework of potential outcomes, named after Donald Rubin. The name "Rubin causal model" was first coined by Rubin's graduate school colleague, Paul W. Holland. The potential outcomes framework was first proposed by Jerzy Neyman in his 1923 Master's thesis, though he discussed it only in the context of completely randomized experiments. Rubin, together with other contemporary statisticians, extended it into a general framework for thinking about causation in both observational and experimental studies.
In probability theory, stochastic drift is the change of the average value of a stochastic (random) process. A related term is the drift rate, which is the rate at which the average changes. For example, a process that counts the number of heads in a series of  coin tosses has a drift rate of 1/2 per toss. This is in contrast to the random fluctuations about this average value. The stochastic mean of that coin-toss process is 1/2 and the drift rate of the stochastic mean is 0, assuming 1=heads and 0=tails.
In statistical modeling, regression analysis is a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables   that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a function of the independent variables called the regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution. Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable; for example, correlation does not imply causation. Many techniques for carrying out regression analysis have been developed. Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data. Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional. The performance of regression analysis methods in practice depends on the form of the data generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results. In a narrower sense, regression may refer specifically to the estimation of continuous response variables, as opposed to the discrete response variables used in classification. The case of a continuous output variable may be more specifically referred to as metric regression to distinguish it from related problems.
Mean square quantization error (MSQE) is a figure of merit for the process of analog to digital conversion. In this conversion process, analog signals in a continuous range of values are converted to a discrete set of values by comparing them with a sequence of thresholds. The quantization error of a signal is the difference between the original continuous value and its discretization, and the mean square quantization error (given some probability distribution on the input values) is the expected value of the square of the quantization errors. Mathematically, suppose that the lower threshold for inputs that generate the quantized value  is , that the upper threshold is , that there are  levels of quantization, and that the probability density function for the input analog values is . Let  denote the quantized value corresponding to an input ; that is,  is the value  for which . Then
Whipple's index (or index of concentration) is a method to measure the tendency for individuals to inaccurately report their actual age or date of birth. Respondents to a census or other surveys sometimes report their age or date of birth to make it seem more culturally favorable, for example to appear younger, or to be born on a date that is considered luckier than their actual date of birth. The index was invented by American demographer George Chandler Whipple (1866 1924) is applied to detect the extent to which age data show systematic heaping on certain ages as a result of digit preference or rounding. Typically the concern is for heaping on particular ages such as those ending in 0 and 5. The index score is obtained by summing the number of persons in the age range 23 and 62 inclusive, who report ages ending in 0 and 5, dividing that sum by the total population between ages 23 and 62 years inclusive, and multiplying the result by 5. Restated as a percentage, index scores range between 100 (no preference for ages ending in 0 and 5) and 500 (all people reporting ages ending in 0 and 5). The UN recommends a standard for measuring the age heaping using Whipple's Index as follows:
In the mathematical discipline of graph theory, the expander walk sampling theorem states that sampling vertices in an expander graph by doing a random walk is almost as good as sampling the vertices independently from a uniform distribution. The earliest version of this theorem is due to Ajtai, Komlo s & Szemere di (1987), and the more general version is typically attributed to Gillman (1998).
In statistics, the antithetic variates method is a variance reduction technique used in Monte Carlo methods. Considering that the error reduction in the simulated signal (using Monte Carlo methods) has a square root convergence, a very large number of sample paths is required to obtain an accurate result. The antithetic variates method reduces the variance of the simulation results.
Experimental Design Diagram (EDD) is a diagram used in science classrooms to design an experiment. This diagram helps to identify the essential components of an experiment. It includes a title, the research hypothesis and null hypothesis, the independent variable, the levels of the independent variable, the number of trials, the dependent variable, the operational definition of the dependent variable and the constants.
The Seasonally Adjusted Annual Rate (SAAR) is a rate that is adjusted to take into account fluctuations of values in the data which might occur due to seasonality. Such data would be affected by the time of the year and thus it would be misleading to draw comparisons month-to-month all year long. An example would be occupancy rates of ski resorts, which would by default be higher during winter as compared to summer. Sales between these two seasons can only be fairly compared through seasonally adjusted rates. The SAAR is calculated by dividing the unadjusted annual rate for the month by its seasonality factor and creating an adjusted annual rate for the month.
High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be difficult to interpret. One approach to simplification is to assume that the data of interest lie on an embedded non-linear manifold within the higher-dimensional space. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.  Below is a summary of some of the important algorithms from the history of manifold learning and nonlinear dimensionality reduction (NLDR). Many of these non-linear dimensionality reduction methods are related to the linear methods listed below. Non-linear methods can be broadly classified into two groups: those that provide a mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa), and those that just give a visualisation. In the context of machine learning, mapping methods may be viewed as a preliminary feature extraction step, after which pattern recognition algorithms are applied. Typically those that just give a visualisation are based on proximity data   that is, distance measurements.
Cross-validation, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset). The goal of cross validation is to define a dataset to "test" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc. One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds. Cross-validation is important in guarding against testing hypotheses suggested by the data (called "Type III errors"), especially where further samples are hazardous, costly or impossible to collect. Furthermore, one of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that the error (e.g. Root Mean Square Error) on the training set in the conventional validation is not a useful estimator of model performance and thus the error on the test data set does not properly represent the assessment of model performance. This may be because there is not enough data available or there is not a good distribution and spread of data to partition it into separate training and test sets in the conventional validation method. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique. In summary, cross-validation combines (averages) measures of fit (prediction error) to correct for the optimistic nature of training error and derive a more accurate estimate of model prediction performance.
In probability theory and directional statistics, a wrapped exponential distribution is a wrapped probability distribution that results from the "wrapping" of the exponential distribution around the unit circle.  
In statistics, the mean integrated squared error (MISE) is used in density estimation. The MISE of an estimate of an unknown probability density is given by  where   is the unknown density,  n is its estimate based on a sample of n independent and identically distributed random variables. Here, E denotes the expected value with respect to that sample. The MISE is also known as L2 risk function.
Fowlkes Mallows index is an external evaluation method that is used to determine the similarity between two clusterings (clusters obtained after a clustering algorithm). This measure of similarity could be either between two hierarchical clusterings or a clustering and a benchmark classification. A higher value for the Fowlkes Mallows index indicates a greater similarity between the clusters and the benchmark classifications.  
Ergodic theory (Ancient Greek: ergon work, hodos way) is a branch of mathematics that studies dynamical systems with an invariant measure and related problems. Its initial development was motivated by problems of statistical physics. A central concern of ergodic theory is the behavior of a dynamical system when it is allowed to run for a long time. The first result in this direction is the Poincare  recurrence theorem, which claims that almost all points in any subset of the phase space eventually revisit the set. More precise information is provided by various ergodic theorems which assert that, under certain conditions, the time average of a function along the trajectories exists almost everywhere and is related to the space average. Two of the most important theorems are those of Birkhoff (1931) and von Neumann which assert the existence of a time average along each trajectory. For the special class of ergodic systems, this time average is the same for almost all initial points: statistically speaking, the system that evolves for a long time "forgets" its initial state. Stronger properties, such as mixing and equidistribution, have also been extensively studied. The problem of metric classification of systems is another important part of the abstract ergodic theory. An outstanding role in ergodic theory and its applications to stochastic processes is played by the various notions of entropy for dynamical systems. The concepts of ergodicity and the ergodic hypothesis are central to applications of ergodic theory. The underlying idea is that for certain systems the time average of their properties is equal to the average over the entire space. Applications of ergodic theory to other parts of mathematics usually involve establishing ergodicity properties for systems of special kind. In geometry, methods of ergodic theory have been used to study the geodesic flow on Riemannian manifolds, starting with the results of Eberhard Hopf for Riemann surfaces of negative curvature. Markov chains form a common context for applications in probability theory. Ergodic theory has fruitful connections with harmonic analysis, Lie theory (representation theory, lattices in algebraic groups), and number theory (the theory of diophantine approximations, L-functions).
In statistics, the log-rank test is a hypothesis test to compare the survival distributions of two samples. It is a nonparametric test and appropriate to use when the data are right skewed and censored (technically, the censoring must be non-informative). It is widely used in clinical trials to establish the efficacy of a new treatment in comparison with a control treatment when the measurement is the time to event (such as the time from initial treatment to a heart attack). The test is sometimes called the Mantel Cox test, named after Nathan Mantel and David Cox. The log-rank test can also be viewed as a time-stratified Cochran Mantel Haenszel test. The test was first proposed by Nathan Mantel and was named the log-rank test by Richard and Julian Peto.
Probability theory is the branch of mathematics concerned with probability, the analysis of random phenomena. The central objects of probability theory are random variables, stochastic processes, and events: mathematical abstractions of non-deterministic events or measured quantities that may either be single occurrences or evolve over time in an apparently random fashion. It is not possible to predict precisely results of random events. However, if a sequence of individual events, such as coin flipping or the roll of dice, is influenced by other factors, such as friction, it will exhibit certain patterns, which can be studied and predicted. Two representative mathematical results describing such patterns are the law of large numbers and the central limit theorem. As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of large sets of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.
Sparse binary polynomial hashing (SBPH) is a generalization of Bayesian spam filtering that can match mutating phrases as well as single words. SBPH is a way of generating a large number of features from an incoming text automatically, and then using statistics to determine the weights for each of those features in terms of their predictive values for spam/nonspam evaluation.
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics and data compression. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties. Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek         "grape") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals. Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.
In mathematics, mean has several different definitions depending on the context. In probability and statistics, mean and expected value are used synonymously to refer to one measure of the central tendency either of a probability distribution or of the random variable characterized by that distribution. In the case of a discrete probability distribution of a random variable X, the mean is equal to the sum over every possible value weighted by the probability of that value; that is, it is computed by taking the product of each possible value x of X and its probability P(x), and then adding all these products together, giving . An analogous formula applies to the case of a continuous probability distribution. Not every probability distribution has a defined mean; see the Cauchy distribution for an example. Moreover, for some distributions the mean is infinite: for example, when the probability of the value  is  for n = 1, 2, 3, .... For a data set, the terms arithmetic mean, mathematical expectation, and sometimes average are used synonymously to refer to a central value of a discrete set of numbers: specifically, the sum of the values divided by the number of values. The arithmetic mean of a set of numbers x1, x2, ..., xn is typically denoted by , pronounced "x bar". If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is termed the sample mean (denoted ) to distinguish it from the population mean (denoted  or ). For a finite population, the population mean of a property is equal to the arithmetic mean of the given property while considering every member of the population. For example, the population mean height is equal to the sum of the heights of every individual divided by the total number of individuals. The sample mean may differ from the population mean, especially for small samples. The law of large numbers dictates that the larger the size of the sample, the more likely it is that the sample mean will be close to the population mean. Outside of probability and statistics, a wide range of other notions of "mean" are often used in geometry and analysis; examples are given below.
In geometry, the Chebyshev center of a bounded set  having non-empty interior is the center of the minimal-radius ball enclosing the entire set , or alternatively (and non-equivalently) the center of largest inscribed ball of . In the field of parameter estimation, the Chebyshev center approach tries to find an estimator  for  given the feasibility set , such that  minimizes the worst possible estimation error for x (e.g. best worst case).
Projection pursuit (PP) is a type of statistical technique which involves finding the most "interesting" possible projections in multidimensional data. Often, projections which deviate more from a normal distribution are considered to be more interesting. As each projection is found, the data are reduced by removing the component along that projection, and the process is repeated to find new projections; this is the "pursuit" aspect that motivated the technique known as matching pursuit. The idea of projection pursuit is to locate the projection or projections from high-dimensional space to low-dimensional space that reveal the most details about the structure of the data set. Once an interesting set of projections has been found, existing structures (clusters, surfaces, etc.) can be extracted and analyzed separately. Projection pursuit has been widely use for blind source separation, so it is very important in independent component analysis. Projection pursuit seek one projection at a time such that the extracted signal is as non-Gaussian as possible.
In mathematics, finite-dimensional distributions are a tool in the study of measures and stochastic processes. A lot of information can be gained by studying the "projection" of a measure (or process) onto a finite-dimensional vector space (or finite collection of times).
Most real world data sets consist of data vectors whose individual components are not statistically independent. In other words, knowing the value of an element will provide information about the value of elements in the data vector. When this occurs, it can be desirable to create a factorial code of the data, i. e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent. Later supervised learning usually works much better when the raw input data is first translated into such a factorial code. For example, suppose the final goal is to classify images with highly redundant pixels. A naive Bayes classifier will assume the pixels are statistically independent random variables and therefore fail to produce good results. If the data are first encoded in a factorial way, however, then the naive Bayes classifier will achieve its optimal performance (compare Schmidhuber et al. 1996). To create factorial codes, Horace Barlow and co-workers suggested to minimize the sum of the bit entropies of the code components of binary codes (1989). Ju rgen Schmidhuber (1992) re-formulated the problem in terms of predictors and binary feature detectors, each receiving the raw data as an input. For each detector there is a predictor that sees the other detectors and learns to predict the output of its own detector in response to the various input vectors or images. But each detector uses a machine learning algorithm to become as unpredictable as possible. The global optimum of this objective function corresponds to a factorial code represented in a distributed fashion across the outputs of the feature detectors.
A Markov chain (discrete-time Markov chain or DTMC), named after Andrey Markov, is a random process that undergoes transitions from one state to another on a state space. It must possess a property that is usually characterized as "memorylessness": the probability distribution of the next state depends only on the current state and not on the sequence of events that preceded it. This specific kind of "memorylessness" is called the Markov property. Markov chains have many applications as statistical models of real-world processes.
Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between them is based on the likeness of their meaning or semantic content as opposed to similarity which can be estimated regarding their syntactical representation (e.g. their string format). These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature. The term semantic similarity is often confused with semantic relatedness. Semantic relatedness includes any relation between two terms, while semantic similarity only includes "is a" relations. For example, "car" is similar to "bus", but is also related to "road" and "driving". Computationally, semantic similarity can be estimated by defining a topological similarity, by using ontologies to define the distance between terms/concepts. For example, a naive metric for the comparison of concepts ordered in a partially ordered set and represented as nodes of a directed acyclic graph (e.g., a taxonomy), would be the shortest-path linking the two concept nodes. Based on text analyses, semantic relatedness between units of language (e.g., words, sentences) can also be estimated using statistical means such as a vector space model to correlate words and textual contexts from a suitable text corpus. An extensive survey dedicated to the notion of semantic measures and semantic similarity is proposed in: Semantic Similarity from Natural Language and Ontology Analysis.
A standard normal deviate (or standard normal variable) is a normally distributed random variable with expected value 0 and variance 1. A fuller term is standard normal random variable. Where collections of such random variables are used, there is often an associated (possibly unstated) assumption that members of such collections are statistically independent. Standard normal variables play a major role in theoretical statistics in the description of many types of model, particularly in regression analysis, the analysis of variance and time series analysis. When the term "deviate" is used, rather than "variable", there is a connotation that the value concerned is treated as the no-longer-random outcome of a standard norm random variable. The terminology here is the same as that for random variable and random variate. Standard normal deviates arise in practical statistics in two ways.  Given a model for a set of observed data, a set of manipulations of the data can result in a derived quantity which, assuming that the model is a true representation of reality, is a standard normal deviate (perhaps in an approximate sense). This enables a significance test to be made for the validity of the model. In the computer generation of a pseudorandom number sequence, the aim may be to generate random numbers having a normal distribution: these can be obtained from standard normal deviates (themselves the output of a pseudorandom number sequence) by multiplying by the scale parameter and adding the location parameter. More generally, the generation of pseudorandom number sequence having other marginal distributions may involve manipulating sequences of standard normal deviates: an example here is the chi-squared distribution, random values of which can be obtained by adding the squares of standard normal deviates (although this would seldom be the fastest method of generating such values).
A random walk is a mathematical formalization of a path that consists of a succession of random steps. For example, the path traced by a molecule as it travels in a liquid or a gas, the search path of a foraging animal, the price of a fluctuating stock and the financial status of a gambler can all be modeled as random walks, although they may not be truly random in reality. The term random walk was first introduced by Karl Pearson in 1905. Random walks have been used in many fields: ecology, economics, psychology, computer science, physics, chemistry, and biology. Random walks explain the observed behaviors of many processes in these fields, and thus serve as a fundamental model for the recorded stochastic activity. Various different types of random walks are of interest. Often, random walks are assumed to be Markov chains or Markov processes, but other, more complicated walks are also of interest. Some random walks are on graphs, others on the line, in the plane, in higher dimensions, or even curved surfaces, while some random walks are on groups. Random walks also vary with regard to the time parameter. Often, the walk is in discrete time, and indexed by the natural numbers, as in . However, some walks take their steps at random times, and in that case the position  is defined for the continuum of times . Specific cases or limits of random walks include the Le vy flight. Random walks are related to the diffusion models and are a fundamental topic in discussions of Markov processes. Several properties of random walks, including dispersal distributions, first-passage times and encounter rates, have been extensively studied.
In statistics, restricted randomization occurs in the design of experiments and in particular in the context of randomized experiments and randomized controlled trials. Restricted randomization allows intuitively poor allocations of treatments to experimental units to be avoided, while retaining the theoretical benefits of randomization. For example, in a clinical trial of a new proposed treatment of obesity compared to a control, an experimenter would want to avoid outcomes of the randomization in which the new treatment was allocated only to the heaviest patients. The concept was introduced by Frank Yates (1948) and William J. Youden (1972) "as a way of avoiding bad spatial patterns of treatments in designed experiments."
In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures. In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience: by supplying a valid probability mass function or probability density function by supplying a valid cumulative distribution function or survival function by supplying a valid hazard function by supplying a valid characteristic function by supplying a rule for constructing a new random variable from other random variables whose joint probability distribution is known. A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector a set of two or more random variables taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
Free statistical software is a practical alternative to commercial packages. In general, free statistical software gives results that are the same as the results from commercial programs, and many of the packages are fairly easy to learn, using menu systems, although a few are command-driven. These packages come from a variety of sources, including governments, nongovernmental organizations (NGOs) like UNESCO, and universities, and are also developed by individuals. Some packages are developed for specific purposes (e.g., time series analysis, factor analysis, calculators for probability distributions, etc.), while others are general packages, with a variety of statistical procedures. Others are meta-packages or statistical computing environments, which allow the user to code completely new statistical procedures. This article is a review of the general statistical packages.
Correlate summation analysis is a data mining method. It is designed to find the variables that are most covariant with all of the other variables being studied, relative to clustering. Aggregate correlate summation is the product of the totaled negative logarithm of the p-values for all of the correlations to a given variable and its (normalized) standard deviation-to-mean quotient. Discrete correlate summation is the product of the totaled absolute value of the logarithm of the p-value ratios between two groups' correlations to a given variable and its absolute value of the logarithm of the group mean ratios.
In applied statistics, total least squares is a type of errors-in-variables regression, a least squares data modeling technique in which observational errors on both dependent and independent variables are taken into account. It is a generalization of Deming regression and also of orthogonal regression, and can be applied to both linear and non-linear models. The total least squares approximation of the data is generically equivalent to the best, in the Frobenius norm, low-rank approximation of the data matrix.
In probability theory and statistics, the chi distribution is a continuous probability distribution. It is the distribution of the square root of the sum of squares of independent random variables having a standard normal distribution, or equivalently, the distribution of the Euclidean distance of the random variables from the origin. The most familiar examples are the Rayleigh distribution with chi distribution with 2 degrees of freedom, and the Maxwell distribution of (normalized) molecular speeds which is a chi distribution with 3 degrees of freedom (one for each spatial coordinate). If  are k independent, normally distributed random variables with means  and standard deviations , then the statistic  is distributed according to the chi distribution. Accordingly, dividing by the mean of the chi distribution (scaled by the square root of n   1) yields the correction factor in the unbiased estimation of the standard deviation of the normal distribution. The chi distribution has one parameter:  which specifies the number of degrees of freedom (i.e. the number of ).
In mathematics, the Marcinkiewicz Zygmund inequality, named after Jo zef Marcinkiewicz and Antoni Zygmund, gives relations between moments of a collection of independent random variables. It is a generalization of the rule for the sum of variances of independent random variables to moments of arbitrary order.
In queueing theory, a discipline within the mathematical theory of probability, the G/G/1 queue represents the queue length in a system with a single server where interarrival times have a general (meaning arbitrary) distribution and service times have a (different) general distribution. The evolution of the queue can be described by the Lindley equation. The system is described in Kendall's notation where the G denotes a general distribution for both interarrival times and service times and the 1 that the model has a single server. Different interarrival and service times are considered to be independent, and sometimes the model is denoted GI/GI/1 to emphasise this.
In descriptive statistics, the interquartile range (IQR), also called the midspread or middle fifty, is a measure of statistical dispersion, being equal to the difference between the upper and lower quartiles, IQR = Q3    Q1. In other words, the IQR is the 1st quartile subtracted from the 3rd quartile; these quartiles can be clearly seen on a box plot on the data. It is a trimmed estimator, defined as the 25% trimmed range, and is the most significant basic robust measure of scale. The interquartile range (IQR) is a measure of variability, based on dividing a data set into quartiles. Quartiles divide a rank-ordered data set into four equal parts. The values that divide each part are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3, respectively.
In finance, the Heston model, named after Steven Heston, is a mathematical model describing the evolution of the volatility of an underlying asset. It is a stochastic volatility model: such a model assumes that the volatility of the asset is not constant, nor even deterministic, but follows a random process.
A discrepancy function is a mathematical function which describes how closely a structural model conforms to observed data. Larger values of the discrepancy function indicate a poor fit of the model to data. In general, the parameter estimates for a given model are chosen so as to make the discrepancy function for that model as small as possible. There are several basic types of discrepancy functions, including maximum likelihood (ML), generalized least squares (GLS), and ordinary least squares (OLS), which are considered the "classical" discrepancy functions. Discrepancy functions all meet the following basic criteria: They are non-negative, i.e., always greater than or equal to zero. They are zero only if the fit is perfect, i.e., if the model and parameter estimates perfectly reproduce the observed data. The discrepancy function is a continuous function of the elements of S, the sample covariance matrix, and  ( ), the "reproduced" estimate of S obtained by using the parameter estimates and the structural model. In order for "maximum likelihood" to meet the first criterion, it is used in a revised form as the deviance.
A location test is a statistical hypothesis test that compares the location parameter of a statistical population to a given constant, or that compares the location parameters of two statistical populations to each other. Most commonly, the location parameter (or parameters) of interest are expected values, but location tests based on medians or other measures of location are also used.
In applied statistics, a partial residual plot is a graphical technique that attempts to show the relationship between a given independent variable and the response variable given that other independent variables are also in the model.
In probability theory, a martingale difference sequence (MDS) is related to the concept of the martingale. A stochastic series X is an MDS if its expectation with respect to the past is zero. Formally, consider an adapted sequence  on a probability space .  is an MDS if it satisfies the following two conditions: , and , for all . By construction, this implies that if  is a martingale, then  will be an MDS hence the name. The MDS is an extremely useful construct in modern probability theory because it implies much milder restrictions on the memory of the sequence than independence, yet most limit theorems that hold for an independent sequence will also hold for an MDS.
In epidemiology and biostatistics, the experimental event rate (EER) is a measure of how often a particular statistical event (such as response to a drug, adverse event or death) occurs within the experimental group (non-control group) of an experiment. This value is very useful in determining the therapeutic benefit or risk to patients in experimental groups, in comparison to patients in placebo or traditionally treated control groups. Three statistical terms rely on EER for their calculation: absolute risk reduction, relative risk reduction and number needed to treat.
In statistics and computer software, a convolution random number generator is a pseudo-random number sampling method that can be used to generate random variates from certain classes of probability distribution. The particular advantage of this type of approach is that it allows advantage to be taken of existing software for generating random variates from other, usually non-uniform, distributions. However, faster algorithms may be obtainable for the same distributions by other more complicated approaches. A number of distributions can be expressed in terms of the (possibly weighted) sum of two or more random variables from other distributions. (The distribution of the sum is the convolution of the distributions of the individual random variables).
In economics, the Lorenz curve is a graphical representation of the distribution of income or of wealth. It was developed by Max O. Lorenz in 1905 for representing inequality of the wealth distribution. The curve is a graph showing the proportion of overall income or wealth assumed by the bottom x% of the people, although this is not rigorously true for a finite population (see below). It is often used to represent income distribution, where it shows for the bottom x% of households, what percentage (y%) of the total income they have. The percentage of households is plotted on the x-axis, the percentage of income on the y-axis. It can also be used to show distribution of assets. In such use, many economists consider it to be a measure of social inequality. The concept is useful in describing inequality among the size of individuals in ecology and in studies of biodiversity, where the cumulative proportion of species is plotted against the cumulative proportion of individuals. It is also useful in business modeling: e.g., in consumer finance, to measure the actual percentage y% of delinquencies attributable to the x% of people with worst risk scores.
Fuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1, considered to be "fuzzy". By contrast, in Boolean logic, the truth values of variables may only be 0 or 1, often called "crisp" values. Fuzzy logic has been extended to handle the concept of partial truth, where the truth value may range between completely true and completely false. Furthermore, when linguistic variables are used, these degrees may be managed by specific (membership) functions. The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by Lotfi Zadeh. Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. Fuzzy logic had however been studied since the 1920s, as infinite-valued logic notably by  ukasiewicz and Tarski.
In statistics, a data point or observation is a set of one or more measurements on a single member of a statistical population. For example, in a study of the determinants of money demand with the unit of observation being the individual, a data point might be the values of income, wealth, age of individual, number of dependents. Statistical inference about the population would be conducted using a statistical sample consisting of various such data points. In addition, in statistical graphics, a "data point" may be an individual item with a statistical display; such points may relate to either a single member of a population or to a summary statistic calculated for a given subpopulation.
Cointegration is a statistical property of a collection (X1,X2,...,Xk) of time series variables. First, all of the series must be integrated of order 1 (see Order of Integration). Next, if a linear combination of this collection is integrated of order zero, then the collection is said to be co-integrated. Formally, if (X,Y,Z) are each integrated of order 1, and there exist coefficients a,b,c such that aX+bY+cZ is integrated of order 0, then X,Y, and Z are cointegrated. Cointegration has become an important property in contemporary time series analysis. Time series often have trends -either deterministic or stochastic. In an influential paper, Charles Nelson and Charles Plosser (1982) provided statistical evidence that many US macroeconomic time series (like GNP, wages, employment, etc.) have stochastic trends these are also called unit root processes, or processes integrated of order 1 I(1). They also showed that unit root processes have non-standard statistical properties, so that conventional econometric theory methods do not apply to them.
In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. When data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data is not labeled or when only some data is labeled as a preprocessing for a classification pass.
Wike's law of low odd primes is a methodological principle to help design sound experiments in psychology. It is: "If the number of experimental treatments is a low odd prime number, then the experimental design is unbalanced and partially confounded" (Wike, 1973, pp. 192 193). This law was stated by Edwin Wike in a humorous article in which he also admits that the association of his name with the law is an example of Stigler's law of eponymy. The lowest odd prime number is three. Wike illustrates how this yields an unbalanced design with an invented study in which researchers investigated the effects on sexual satisfaction of water beds. The fictitious researchers randomly assigned couples to three groups: those having sex on a conventional bed, those having sex on a water bed, and those having sex on a water bed having also taken a sea sickness pill. Wike pointed out that any differences in sexual satisfaction among the three groups could be due to the water bed or to the sea sickness pill. It requires a fourth group, couples taking the pill and using a conventional bed, to balance the design and to allow the researchers to attribute any differences in sexual satisfaction among the groups to the sort of bed, to the pill, or to their interaction.
The Erlang distribution is a two parameter family of continuous probability distributions with support . The two parameters are: a positive integer 'shape'  a positive real 'rate' ; sometimes the scale , the inverse of the rate is used. The Erlang distribution with shape parameter  equal to 1 simplifies to the exponential distribution. It is a special case of the Gamma distribution. It is the distribution of a sum of  independent exponential variables with mean  each. The Erlang distribution was developed by A. K. Erlang to examine the number of telephone calls which might be made at the same time to the operators of the switching stations. This work on telephone traffic engineering has been expanded to consider waiting times in queueing systems in general. The distribution is now used in the fields of stochastic processes and of biomathematics.
In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.
In probability theory, Foster's theorem, named after F. G. Foster, is used to draw conclusions about the positive recurrence of Markov chains with countable state spaces. It uses the fact that positive recurrent Markov chains exhibit a notion of "Lyapunov stability" in terms of returning to any state while starting from it within a finite time interval. Consider an irreducible discrete-time Markov chain on a countable state space S having a transition probability matrix P with elements pij for pairs i, j in S. Foster's theorem states that the Markov chain is positive recurrent if and only if there exists a Lyapunov function V: S   R, such that  and  for   for all  for some finite set F and strictly positive  .  
Analysis of molecular variance (AMOVA), is a statistical model for the molecular variation in a single species, typically biological. The name and model are inspired by ANOVA. The method was developed by Laurent Excoffier, Peter Smouse and Joseph Quattro at Rutgers University in 1992. Since developing AMOVA, Excoffier has written a program for running such analyses. This program, which runs on Windows is called Arlequin, and is freely available on Excoffier's website. There is also an implementation by Sandrine Pavoine in R language in the ade4 package available on CRAN (Comprehensive R Archive Network). Another implementation is in Info-Gen, which also runs on Windows. The student version is free and fully functional. Native language of the application is Spanish but an English version is also available. An additional free statistical package, GenAlEx, is geared toward teaching as well as research and allows for complex genetic analyses to be employed and compared within the commonly used Microsoft Excel interface. This software allows for calculation of analyses such as AMOVA, as well as comparisons with other types of closely related statistics including F-statistics and Shannon's index, and more.
In machine learning, a probabilistic classifier is a classifier that is able to predict, given a sample input, a probability distribution over a set of classes, rather than only outputting the most likely class that the sample should belong to. Probabilistic classifiers provide classification with a degree of certainty, which can be useful in its own right, or when combining classifiers into ensembles.
In statistics, the Marcum-Q-function  is defined as   is also defined as  with modified Bessel function  of order M   1. The Marcum Q-function is used for example as a cumulative distribution function (more precisely, as a survivor function) for noncentral chi, noncentral chi-squared and Rice distributions. The Marcum Q-function is monotonic and log-concave.
In mathematics, a degenerate distribution or deterministic distribution is the probability distribution of a random variable which only takes a single value. Examples include a two-headed coin and rolling a die whose sides all show the same number. This distribution satisfies the definition of "random variable" even though it does not appear random in the everyday sense of the word; hence it is considered degenerate. In the case of a real-valued random variable, the degenerate distribution is localized at a point k0 on the real line. The probability mass function equals 1 at this point and 0 elsewhere. The distribution can be viewed as the limiting case of a continuous distribution whose variance goes to 0 causing the probability density function to be a delta function at k0, with infinite height there but area equal to 1. The cumulative distribution function of the degenerate distribution is:
In statistics, in the theory relating to sampling from finite populations, the inclusion probability of an element or member of the population is its probability of becoming part of the sample during the drawing of a single sample. Each element of the population may have a different probability of being included in the sample. The inclusion probability is also termed the "first-order inclusion probability" to distinguish it from the "second-order inclusion probability", i.e. the probability of including a pair of elements. Generally, the first-order inclusion probability of the ith element of the population is denoted by the symbol  i and the second-order inclusion probability that a pair consisting of the ith and jth element of the population that is sampled is included in a sample during the drawing of a single sample is denoted by  ij.
A cepstrum (/ k pstr m   s pstr m /) is the result of taking the Inverse Fourier transform (IFT) of the logarithm of the estimated spectrum of a signal. It may be pronounced in the two ways given, the second having the advantage of avoiding confusion with 'kepstrum' which also exists (see below). There is a complex cepstrum, a real cepstrum, a power cepstrum, and a phase cepstrum. The power cepstrum in particular finds applications in the analysis of human speech. The name "cepstrum" was derived by reversing the first four letters of "spectrum". Operations on cepstra are labelled quefrency analysis (aka quefrency alanysis), liftering, or cepstral analysis.
In epidemiology, the relative risk reduction is a measure calculated by dividing the absolute risk reduction by the control event rate. Like many other epidemiological measures, the same equations can be used to measure a benefit or a harm (although the signs may need to be adjusted, depending upon how the data was collected.)
Reproducibility is the ability of an entire experiment or study to be duplicated, either by the same researcher or by someone else working independently. Reproducing an experiment is called replicating it. Reproducibility is one of the main principles of the scientific method. The values obtained from distinct experimental trials are said to be commensurate if they are obtained according to the same reproducible experimental description and procedure. The basic idea can be seen in Aristotle's dictum that there is no scientific knowledge of the individual, where the word used for individual in Greek had the connotation of the idiosyncratic, or wholly isolated occurrence. Thus all knowledge, all science, necessarily involves the formation of general concepts and the invocation of their corresponding symbols in language (cf. Turner). Aristotle s conception about the knowledge of the individual being considered unscientific is due to lack of the field of statistics in his time, so he could not appeal to statistical averaging by the individual. A particular experimentally obtained value is said to be reproducible if there is a high degree of agreement between measurements or observations conducted on replicate specimens in different locations by different people that is, if the experimental value is found to have a high precision.
Environment statistics is the application of statistical methods to environmental science. It covers procedures for dealing with questions concerning both the natural environment in its undisturbed state and the interaction of humanity with the environment. Thus weather, climate, air and water quality are included, as are studies of plant and animal populations. The United Nations Framework for the Development of Environment Statistics (FDES) defines the scope of environment statistics as follows: The scope of environment statistics covers biophysical aspects of the environment and those aspects of the socio-economic system that directly influence and interact with the environment. The scope of environment, social and economic statistics overlap. It is not easy   or necessary   to draw a clear line dividing these areas. Social and economic statistics that describe processes or activities with a direct impact on, or direct interaction with, the environment are used widely in environment statistics. They are within the scope of the FDES. Environmental statistics covers a number of types of study: Baseline studies to document the present state of an environment to provide background in case of unknown changes in the future; Targeted studies to describe the likely impact of changes being planned or of accidental occurrences; Regular monitoring to attempt to detect changes in the environment.
In statistics, the Generalized Additive Model for Location, Scale and Shape (GAMLSS) is a class of statistical model developed by Rigby and Stasinopoulos. These models provide extended capabilities beyond the simpler generalized linear models and generalized additive models. These simpler models allow the typical values of a quantity being modelled to be related to whatever explanatory variables are available. Here the "typical value" is more formally a location parameter, which only describes a limited aspect of the probability distribution of the dependent variable. The GAMLSS approach allows other parameters of the distribution to be related to the explanatory variables; where these other parameters might be interpreted as scale and shape parameters of the distribution, although the approach is not limited to such parameters.
A moral graph is a concept in graph theory, used to find the equivalent undirected form of a directed acyclic graph. It is a key step of the junction tree algorithm, used in belief propagation on graphical models.  The moralized counterpart of a directed acyclic graph is formed by adding edges between all pairs of nodes that have a common child, and then making all edges in the graph undirected. Equivalently, a moral graph of a directed acyclic graph G is an undirected graph in which each node of the original G is now connected to its Markov blanket. The name stems from the fact that, in a moral graph, two nodes that have a common child are required to be married by sharing an edge. For any vertex v of the directed acyclic graph G, the set of incoming neighbors of v forms a clique in the moralization of G. Therefore, a topological ordering of G is automatically the reverse of a perfect elimination ordering of the moralization. Because the moralization has a perfect elimination ordering, it is necessarily a chordal graph. Moralization may also be applied to mixed graphs, called in this context "chain graphs". In a chain graph, a connected component of the undirected subgraph is called a chain. Moralization adds an undirected edge between any two vertices that both have outgoing edges to the same chain, and then forgets the orientation of the directed edges of the graph. For this generalization, the resulting moral graph is not always chordal.
The Atkinson index (also known as the Atkinson measure or Atkinson inequality measure) is a measure of income inequality developed by British economist Anthony Barnes Atkinson. The measure is useful in determining which end of the distribution contributed most to the observed inequality.
The Gittins index is a measure of the reward that can be achieved by a random process bearing a termination state and evolving from its present state onward, under the option of terminating the said process at every later stage with the accrual of the probabilistic expected reward from that stage up to the attainment of its termination state. It is a real scalar value associated to the state of a stochastic process with a reward function and a probability of termination.
The St. Petersburg paradox or St. Petersburg lottery is a paradox related to probability and decision theory in economics. It is based on a particular (theoretical) lottery game that leads to a random variable with infinite expected value (i.e., infinite expected payoff) but nevertheless seems to be worth only a very small amount to the participants. The St. Petersburg paradox is a situation where a naive decision criterion which takes only the expected value into account predicts a course of action that presumably no actual person would be willing to take. Several resolutions are possible. The paradox takes its name from its resolution by Daniel Bernoulli, one-time resident of the eponymous Russian city, who published his arguments in the Commentaries of the Imperial Academy of Science of Saint Petersburg (Bernoulli 1738). However, the problem was invented by Daniel's brother Nicolas Bernoulli who first stated it in a letter to Pierre Raymond de Montmort on September 9, 1713 (de Montmort 1713).
In statistics, Spearman's rank correlation coefficient or Spearman's rho, named after Charles Spearman and often denoted by the Greek letter  (rho) or as , is a nonparametric measure of statistical dependence between two variables. It assesses how well the relationship between two variables can be described using a monotonic function. If there are no repeated data values, a perfect Spearman correlation of +1 or  1 occurs when each of the variables is a perfect monotone function of the other. Spearman's coefficient, like any correlation calculation, is appropriate for both continuous and discrete variables, including ordinal variables. Spearman's  and Kendall's  can be formulated as special cases of a more general correlation coefficient.
In statistics and signal processing, the method of empirical orthogonal function (EOF) analysis is a decomposition of a signal or data set in terms of orthogonal basis functions which are determined from the data. It is the same as performing a principal components analysis on the data, except that the EOF method finds both time series and spatial patterns. The term is also interchangeable with the geographically weighted PCAs in geophysics. The i th basis function is chosen to be orthogonal to the basis functions from the first through i   1, and to minimize the residual variance. That is, the basis functions are chosen to be different from each other, and to account for as much variance as possible. The method of EOF is similar in spirit to harmonic analysis, but harmonic analysis typically uses predetermined orthogonal functions, for example, sine and cosine functions at fixed frequencies. In some cases the two methods may yield essentially the same results. The basis functions are typically found by computing the eigenvectors of the covariance matrix of the data set. A more advanced technique is to form a kernel out of the data, using a fixed kernel. The basis functions from the eigenvectors of the kernel matrix are thus non-linear in the location of the data (see Mercer's theorem and the kernel trick for more information).
The mode is the value that appears most often in a set of data. The mode of a discrete probability distribution is the value x at which its probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled. The mode of a continuous probability distribution is the value x at which its probability density function has its maximum value, so the mode is at the peak. Like the statistical mean and median, the mode is a way of expressing, in a single number, important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions. The mode is not necessarily unique, since the probability mass function or probability density function may take the same maximum value at several points x1, x2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently. When a probability density function has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal (as opposed to unimodal). In symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. For samples, if it is known that they are drawn from a symmetric distribution, the sample mean can be used as an estimate of the population mode.
In statistics, explained variation measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set. Often, variation is quantified as variance; then, the more specific term explained variance can be used. The complementary part of the total variation is called unexplained or residual.
Consensus-based assessment expands on the common practice of consensus decision-making and the theoretical observation that expertise can be closely approximated by large numbers of novices or journeymen. It creates a method for determining measurement standards for very ambiguous domains of knowledge, such as emotional intelligence, politics, religion, values and culture in general. From this perspective, the shared knowledge that forms cultural consensus can be assessed in much the same way as expertise or general intelligence.
With the application of probability sampling in the 1930s, surveys became a standard tool for empirical research in social sciences, marketing, and official statistics. The methods involved in survey data collection are any of a number of ways in which data can be collected for a statistical survey. These are methods that are used to collect information from a sample of individuals in a systematic way. First there was the change from traditional paper-and-pencil interviewing (PAPI) to computer-assisted interviewing (CAI). Now, face-to-face surveys (CAPI), telephone surveys (CATI), and mail surveys (CASI, CSAQ) are increasingly replaced by web surveys.
In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. Missing data can occur because of nonresponse: no information is provided for several items or no information is provided for a whole unit. Some items are more sensitive for nonresponse than others, for example items about private subjects such as income. Dropout is a type of missingness that occurs mostly when studying development over time. In this type of study the measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing. Sometimes missing values are caused by the researcher for example, when data collection is done improperly or mistakes are made in data entry. Data often are missing in research in economics, sociology, and political science because governments choose not to, or fail to, report critical statistics.
For a less technical introduction, see Utility. In microeconomics, the utility maximization problem is the problem consumers face: "how should I spend my money in order to maximize my utility " It is a type of optimal decision problem.
In probability theory the hypoexponential distribution or the generalized Erlang distribution is a continuous distribution, that has found use in the same fields as the Erlang distribution, such as queueing theory, teletraffic engineering and more generally in stochastic processes. It is called the hypoexponetial distribution as it has a coefficient of variation less than one, compared to the hyper-exponential distribution which has coefficient of variation greater than one and the exponential distribution which has coefficient of variation of one.  
A probabilistic proposition is a proposition with a measured probability of being true for an arbitrary person at an arbitrary time. These are some examples of probabilistic propositions collected by the Mindpixel project:  You are not human 0.17 Color and colour are the same word spelled differently 0.95 You do not think the sun will rise tomorrow 0.15 You have never seen the sky 0.13 You are a rock 0.01 Is westlife a pop group  0.50 I exist 0.98 Bread is raw toast 0.89 Do you know how to talk  0.89
Order of integration, denoted I(d), is a summary statistic for a time series. It reports the minimum number of differences required to obtain a covariance stationary series.
A self-organizing map (SOM) or self-organising feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map. Self-organizing maps are different from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.  This makes SOMs useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on work on biologically neural models from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s Like most artificial neural networks, SOMs operate in two modes: training and mapping. "Training" builds the map using input examples (a competitive process, also called vector quantization), while "mapping" automatically classifies a new input vector. A self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. The usual arrangement of nodes is a two-dimensional regular spacing in a hexagonal or rectangular grid. The self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. The procedure for placing a vector from data space onto the map is to find the node with the closest (smallest distance metric) weight vector to the data space vector. While it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation. Useful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes. It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character. It is also common to use the U-Matrix. The U-Matrix value of a particular node is the average distance between the node's weight vector and that of its closest neighbors. In a square grid, for instance, we might consider the closest 4 or 8 nodes (the Von Neumann and Moore neighborhoods, respectively), or six nodes in a hexagonal grid. Large SOMs display emergent properties. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself.
A dot chart or dot plot is a statistical chart consisting of data points plotted on a fairly simple scale, typically using filled in circles. There are two common, yet very different, versions of the dot chart. The first has been used in hand-drawn (pre-computer era) graphs to depict distributions going back to 1884. The other version is described by William S. Cleveland as an alternative to the bar chart, in which dots are used to depict the quantitative values (e.g. counts) associated with categorical variables.
Multiple-try Metropolis is a sampling method that is a modified form of the Metropolis-Hastings method, first presented by Liu, Liang, and Wong in 2000. It is designed to help the sampling trajectory converge faster, by increasing both the step size and the acceptance rate.
Political forecasting aims at predicting the outcome of elections.
In statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model. In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition). Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations. Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175  C, the median of the data will be between 20 and 25  C but the mean temperature will be between 35.5 and 40  C. In this case, the median better reflects the temperature of a randomly sampled object than the mean; naively interpreting the mean as "a typical sample", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set. Estimators capable of coping with outliers are said to be robust: the median is a robust statistic, while the mean is not.
An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact "F-tests" mainly arise when the models have been fitted to the data using least squares. The name was coined by George W. Snedecor, in honour of Sir Ronald A. Fisher. Fisher initially developed the statistic as the variance ratio in the 1920s.
A weight function is a mathematical device used when performing a sum, integral, or average to give some elements more "weight" or influence on the result than other elements in the same set. The result of this application of a weight function is a weighted sum or weighted average. Weight functions occur frequently in statistics and analysis, and are closely related to the concept of a measure. Weight functions can be employed in both discrete and continuous settings. They can be used to construct systems of calculus called "weighted calculus" and "meta-calculus".
Jump diffusion is a stochastic process that involves jumps and diffusion. It has important applications in magnetic reconnection, coronal mass ejections, condensed matter physics and in option pricing.
In statistics, the inverse Mills ratio, named after John P. Mills, is the ratio of the probability density function to the cumulative distribution function of a distribution.
Analyse-it is a statistical analysis add-in for Microsoft Excel. Analyse-it is the successor to Astute, developed in 1992 for Excel 4 and the first statistical analysis add-in for Microsoft Excel. Analyse-it provides a range of standard parametric and non-parametric procedures, including Descriptive statistics, ANOVA, Mann Whitney, Wilcoxon, chi-square, correlation, linear regression, logistic regression, polynomial regression and advanced model fitting. Analyse-it Method Validation edition provides the standard Analyse-it statistical analyses above, plus procedures for method evaluation, validation and demonstration, including Bland-Altman bias plots, Linear regression, Weighted Linear regression, Deming regression, Weighted Deming regression and Passing Bablok for method comparison, Precision, Linearity, Reference intervals and Receiver operating characteristic analysis supporting Delong, Delong Clarke-Pearson comparisons.(see references below). Version 4.00 added support for CLSI guidelines EP5-A3, EP6-A, EP9-A3, EP10-A3, EP12-A2, EP15-A3, EP17-A2, EP21-A, EP24-A2 (formerly GP10-A), and EP28-A3C (formerly C28-A3C). Analyse-it Quality and Process Improvement edition provides the standard Analyse-it statistical analyses above, plus procedures for statistical process control, including Shewhart, Levey-Jennings, CUSUM, and EWMA control charts, process capability analysis, and pareto analysis. Analyse-it is compatible with Microsoft Excel 2007, 2010, 2013 and 2016 (both 32- and 64-bit versions).
In mathematics, the convex hull or convex envelope of a set X of points in the Euclidean plane or Euclidean space is the smallest convex set that contains X. For instance, when X is a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around X. Formally, the convex hull may be defined as the intersection of all convex sets containing X or as the set of all convex combinations of points in X. With the latter definition, convex hulls may be extended from Euclidean spaces to arbitrary real vector spaces; they may also be generalized further, to oriented matroids. The algorithmic problem of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces is one of the fundamental problems of computational geometry.
Fiducial inference is one of a number of different types of statistical inference. These are rules, intended for general application, by which conclusions can be drawn from samples of data. In modern statistical practice, attempts to work with fiducial inference have fallen out of fashion in favour of frequentist inference, Bayesian inference and decision theory. However, fiducial inference is important in the history of statistics since its development led to the parallel development of concepts and tools in theoretical statistics that are widely used. Some current research in statistical methodology is either explicitly linked to fiducial inference or is closely connected to it.  
In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification. The RVM has an identical functional form to the support vector machine, but provides probabilistic classification. It is actually equivalent to a Gaussian process model with covariance function:  where  is the kernel function (usually Gaussian),'s as the variances of the prior on the weight vector  ,and  are the input vectors of the training set. Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem). The relevance vector machine is patented in the United States by Microsoft.
In probability theory and statistics, covariance is a measure of how much two variables change together, and the covariance function, or kernel, describes the spatial covariance of a random variable process or field. For a random field or stochastic process Z(x) on a domain D, a covariance function C(x, y) gives the covariance of the values of the random field at the two locations x and y:  The same C(x, y) is called the autocovariance function in two instances: in time series (to denote exactly the same concept except that x and y refer to locations in time rather than in space), and in multivariate random fields (to refer to the covariance of a variable with itself, as opposed to the cross covariance between two different variables at different locations, Cov(Z(x1), Y(x2))).
In statistics, the Mann Whitney U test (also called the Mann Whitney Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon Mann Whitney test) is a nonparametric test of the null hypothesis that two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other. Unlike the t-test it does not require the assumption of normal distributions. It is nearly as efficient as the t-test on normal distributions.
In statistics, study heterogeneity is a problem that can arise when attempting to undertake a meta-analysis. Ideally, the studies whose results are being combined in the meta-analysis should all be undertaken in the same way and to the same experimental protocols: study heterogeneity is a term used to indicate that this ideal is not fully met.
A nominal category or a nominal group is a group of objects or ideas that can be collectively grouped on the basis of shared, arbitrary characteristic.
In mathematics, the term path space refers to any topological space of paths from one specified set into another. In particular, it may refer to the classical Wiener space of continuous paths; the Skorokhod space of ca dla g paths. the loop space, the space of loops in a topological space. for the usage in algebraic topology, see Glossary of algebraic topology#path space.
In probability and statistics, a Bernoulli process is a finite or infinite sequence of binary random variables, so it is a discrete-time stochastic process that takes only two values, canonically 0 and 1. The component Bernoulli variables Xi are identical and independent. Prosaically, a Bernoulli process is a repeated coin flipping, possibly with an unfair coin (but with consistent unfairness). Every variable Xi in the sequence is associated with a Bernoulli trial or experiment. They all have the same Bernoulli distribution. Much of what can be said about the Bernoulli process can also be generalized to more than two outcomes (such as the process for a six-sided dice); this generalization is known as the Bernoulli scheme. The problem of determining the process, given only a limited sample of the Bernoulli trials, may be called the problem of checking whether a coin is fair.
In statistics, propagation of uncertainty (or propagation of error) is the effect of variables' uncertainties (or errors, more specifically random errors) on the uncertainty of a function based on them. When the variables are the values of experimental measurements they have uncertainties due to measurement limitations (e.g., instrument precision) which propagate to the combination of variables in the function. The uncertainty u can be expressed in a number of ways. It may be defined by the absolute error  x. Uncertainties can also be defined by the relative error ( x)/x, which is usually written as a percentage. Most commonly, the uncertainty on a quantity is quantified in terms of the standard deviation,  , the positive square root of variance,  2. The value of a quantity and its error are then expressed as an interval x   u. If the statistical probability distribution of the variable is known or can be assumed, it is possible to derive confidence limits to describe the region within which the true value of the variable may be found. For example, the 68% confidence limits for a one-dimensional variable belonging to a normal distribution are   one standard deviation from the value, that is, there is approximately a 68% probability that the true value lies in the region x    . If the uncertainties are correlated then covariance must be taken into account. Correlation can arise from two different sources. First, the measurement errors may be correlated. Second, when the underlying values are correlated across a population, the uncertainties in the group averages will be correlated.
Minimum viable population (MVP) is a lower bound on the population of a species, such that it can survive in the wild. This term is used in the fields of biology, ecology, and conservation biology. More specifically, MVP is the smallest possible size at which a biological population can exist without facing extinction from natural disasters or demographic, environmental, or genetic stochasticity. The term "population" rarely refers to an entire species. For example, the undomesticated dromedary camel is extinct in its natural wild habitat; but there is a domestic population in captivity and an additional feral population in Australia. Two groups of house cats in separate houses which are not allowed outdoors are also technically distinct populations. Typically, however, MVP is used to refer solely to a wild population, such as the red wolf.
Probability is the measure of the likelihood that an event will occur. Probability is quantified as a number between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty). The higher the probability of an event, the more certain we are that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is unbiased, the two outcomes ("head" and "tail") are equally probable; the probability of "head" equals the probability of "tail." Since no other outcome is possible, the probability is 1/2 (or 50%) of either "head" or "tail". In other words, the probability of "head" is 1 out of 2 outcomes and the probability of "tail" is also, 1 out of 2 outcomes. These concepts have been given an axiomatic mathematical formalization in probability theory (see probability axioms), which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.
In economics, the To rnqvist index is a price or quantity index. In practice ,To rnqvist index values are calculated for consecutive periods, then these are strung together, or "chained". Thus, the core calculation does not refer to a single base year.
Calculating demand forecast accuracy is the process of determining the accuracy of forecasts made regarding customer demand for a product.
Statistical inference is the process of deducing properties of an underlying distribution by analysis of data. Inferential statistical analysis infers properties about a population: this includes testing hypotheses and deriving estimates. The population is assumed to be larger than the observed data set; in other words, the observed data is assumed to be sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and does not assume that the data came from a larger population.
In the design of experiments and analysis of variance, a main effect is the effect of an independent variable on a dependent variable averaging across the levels of any other independent variables. The term is frequently used in the context of factorial designs and regression models to distinguish main effects from interaction effects. Relative to a factorial design, under an analysis of variance, a main effect test will test the hypotheses expected such as H0, the null hypothesis. Running a hypothesis for a main effect will test whether there is evidence of an effect of different treatments. However a main effect test is nonspecific and will not allow for a localization of specific mean pairwise comparisons (simple effects). A main effect test will merely look at whether overall there is something about a particular factor that is making a difference. In other words a test examining differences amongst the levels of a single factor (averaging over the other factor and/or factors). Main effects are essentially the overall effect of a factor.
In probability theory, one says that an event happens almost surely (sometimes abbreviated as a.s.) if it happens with probability one. In other words, the set of possible exceptions may be non-empty, but it has probability zero. The concept is precisely the same as the concept of "almost everywhere" in measure theory. In probability experiments on a finite sample space there is no difference between almost surely and surely, but the distinction becomes important when the sample space is an infinite set (because an infinite set can have non-empty subsets of probability zero). Some examples of the use of this concept include the strong and uniform versions of the law of large numbers, and the continuity of the paths of Brownian motion. The terms almost certainly (a.c.) and almost always (a.a.) are also used. Almost never describes the opposite of almost surely: an event that happens with probability zero happens almost never.
Homogenization in climate research means the removal of non-climatic changes. Next to changes in the climate itself, raw climate records also contain non-climatic jumps and changes for example due to relocations or changes in instrumentation. The most used principle to remove these inhomogeneities is the relative homogenization approach in which a candidate stations is compared to a reference time series based on one or more neighboring stations. The candidate and reference station(s) experience about the same climate, non-climatic changes that happen only in one station can thus be identified and removed.
A Manhattan plot is a type of scatter plot, usually used to display data with a large number of data-points - many of non-zero amplitude, and with a distribution of higher-magnitude values, for instance in genome-wide association studies (GWAS). In GWAS Manhattan plots, genomic coordinates are displayed along the X-axis, with the negative logarithm of the association P-value for each single nucleotide polymorphism (SNP) displayed on the Y-axis, meaning that each dot on the Manhattan plot signifies a SNP. Because the strongest associations have the smallest P-values (e.g., 10 15), their negative logarithms will be the greatest (e.g., 15). It gains its name from the similarity of such a plot to the Manhattan skyline: a profile of skyscrapers towering above the lower level "buildings" which vary around a lower height.
In statistics, ignorability is a feature of an experiment design whereby the method of data collection (and the nature of missing data) do not depend on the missing data. A missing data mechanism such as a treatment assignment or survey sampling strategy is "ignorable" if the missing data matrix, which indicates which variables are observed or missing, is independent of the missing data conditional on the observed data. This idea is part of the Rubin Causal Inference Model, developed by Donald Rubin in collaboration with Paul Rosenbaum in the early 1970s. Pearl [2000] devised a simple graphical criterion, called back-door, that entails ignorability and identifies sets of covariates that achieve this condition.
In statistics, maximum-likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given data. The method of maximum likelihood corresponds to many well-known estimation methods in statistics. For example, one may be interested in the heights of adult female penguins, but be unable to measure the height of every single penguin in a population due to cost or time constraints. Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model. In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the "agreement" of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum-likelihood estimation gives a unified approach to estimation, which is well-defined in the case of the normal distribution and many other problems.
The Lincoln index is a statistical measure used in several fields to estimate the number of cases that have not yet been observed, based on two independent sets of observed cases. Described by Frederick Charles Lincoln in 1930, it is also sometimes known as the Lincoln-Petersen method after C.G. Johannes Petersen who was the first to use the related mark and recapture method.
Probability and statistics are two related but separate academic disciplines. Statistical analysis often uses probability distributions, and the two topics are often studied together. However, probability theory contains much that is mostly of mathematical interest and not directly relevant to statistics. Moreover, many topics in statistics are independent of probability theory.
Player wins is a stat used to estimate the number of games a player won for his team developed by Dean Oliver, the first full-time statistical analyst in the NBA. The formula used to calculate player wins is Player Games * Player Winning Percentage.
In epidemiological research, recall bias is a systematic error caused by differences in the accuracy or completeness of the recollections retrieved ("recalled") by study participants regarding events or experiences from the past. Sometimes also referred to as response bias, responder bias or reporting bias, this type of measurement bias can be a methodological issue in research that involves interviews or questionnaires (potentially leading to differential misclassification of various types of exposure). Recall bias can be a particular concern in retrospective studies that use a case-control design to investigate the etiology of a disease or psychiatric condition. For example, in studies of risk factors for breast cancer, women who have had the disease may search their memories more thoroughly than unaffected controls to try to recall exposure to factors that have been mentioned in the press, such as use of oral contraceptives.
Energy distance is a statistical distance between probability distributions. If X and Y are independent random vectors in Rd with cumulative distribution functions F and G respectively, then the energy distance between the distributions F and G is defined to be the square root of  where X, X' are independent and identically distributed (iid), Y, Y' are iid,  is expected value, and || . || denotes the length of a vector. Energy distance satisfies all axioms of a metric thus energy distance characterizes the equality of distributions: D(F,G) = 0 if and only if F = G. Energy distance for statistical applications was introduced in 1985 by Ga bor J. Sze kely, who proved that for real-valued random variables this distance is exactly twice Harald Crame r's distance:  For a simple proof of this equivalence, see Sze kely (2002). In higher dimensions, however, the two distances are different because the energy distance is rotation invariant while Crame r's distance is not. (Notice that Crame r's distance is not the same as the distribution-free Cramer-von-Mises criterion.)
Demand forecasting is the art and science of forecasting customer demand to drive holistic execution of such demand by corporate supply chain and business management. Demand forecasting involves techniques including both informal methods, such as educated guesses, and quantitative methods, such as the use of historical sales data and statistical techniques or current data from test markets. Demand forecasting may be used in production planning, inventory management, and at times in assessing future capacity requirements, or in making decisions on whether to enter a new market Demand forecasting is predicting future demand for the product. In other words it refers to the prediction of probable demand for a product or a service on the basis of the past events and prevailing trends in the present.
In statistical analysis, the rule of three states that if a certain event did not occur in a sample with n subjects (), the interval from 0 to 3/n is a 95% confidence interval for the rate of occurrences in the population. When n is greater than 30, this is a good approximation to results from more sensitive tests. For example, a pain-relief drug is tested on 1500 human subjects, and no adverse event is recorded. From the rule of three, it can be concluded with 95% confidence that fewer than 1 person in 500 (or 3/1500) will experience an adverse event. By symmetry, one could expect for only successes (), the 95% confidence interval is [1-3/n,1]. The rule is useful in the interpretation of clinical trials generally, particularly in phase II and phase III where often there are limitations in duration or statistical power. The rule of three applies well beyond medical research, to any trial done n times. If 300 parachutes are randomly tested and all open successfully, then it is concluded with 95% confidence that fewer than 1 in 100 parachutes with the same characteristics (3/300) will fail.
A forest plot, also known as a blobbogram, is a graphical display of estimated results from a number of scientific studies addressing the same question, along with the overall results. It was developed for use in medical research as a means of graphically representing a meta-analysis of the results of randomized controlled trials. In the last twenty years, similar meta-analytical techniques have been applied in observational studies (e.g. environmental epidemiology) and forest plots are often used in presenting the results of such studies also. Although forest plots can take several forms, they are commonly presented with two columns. The left-hand column lists the names of the studies (frequently randomized controlled trials or epidemiological studies), commonly in chronological order from the top downwards. The right-hand column is a plot of the measure of effect (e.g. an odds ratio) for each of these studies (often represented by a square) incorporating confidence intervals represented by horizontal lines. The graph may be plotted on a natural logarithmic scale when using odds ratios or other ratio-based effect measures, so that the confidence intervals are symmetrical about the means from each study and to ensure undue emphasis is not given to odds ratios greater than 1 when compared to those less than 1. The area of each square is proportional to the study's weight in the meta-analysis. The overall meta-analysed measure of effect is often represented on the plot as a dashed vertical line. This meta-analysed measure of effect is commonly plotted as a diamond, the lateral points of which indicate confidence intervals for this estimate. A vertical line representing no effect is also plotted. If the confidence intervals for individual studies overlap with this line, it demonstrates that at the given level of confidence their effect sizes do not differ from no effect for the individual study. The same applies for the meta-analysed measure of effect: if the points of the diamond overlap the line of no effect the overall meta-analysed result cannot be said to differ from no effect at the given level of confidence. Forest plots date back to at least the 1970s. One plot is shown in a 1985 book about meta-analysis. The first use in print of the expression "forest plot" may be in an abstract for a poster at the Pittsburgh (US) meeting of the Society for Clinical Trials in May 1996. An informative investigation on the origin of the notion "forest plot" was published in 2001. The name refers to the forest of lines produced. In September 1990, Richard Peto joked that the plot was named after a breast cancer researcher called Pat Forrest and as a result the name has sometimes been spelled "forrest plot".
In statistics, the method of support is a technique that is used to make inferences from datasets. According to A. W. F. Edwards, the method of support aims to make inferences about unknown parameters in terms of the relative support, or log likelihood, induced by a set of data for a particular parameter value. The technique may be used whether or not prior information is available. The method of maximum likelihood is part of the method of support, but note that the method of support also provides confidence regions that are defined in terms of their support. Notable proponents of the method of support include A. W. F. Edwards.
In statistics, the term "error" arises in two ways. Firstly, it arises in the context of decision making, where the probability of error may be considered as being the probability of making a wrong decision and which would have a different value for each type of error. Secondly, it arises in the context of statistical modelling (for example regression) where the model's predicted value may be in error regarding the observed outcome and where the term probability of error may refer to the probabilities of various amounts of error occurring.
In statistics the Maxwell Boltzmann distribution is a particular probability distribution named after James Clerk Maxwell and Ludwig Boltzmann. It was first defined and used in physics (in particular in statistical mechanics) for describing particle speeds in idealized gases where the particles move freely inside a stationary container without interacting with one another, except for very brief collisions in which they exchange energy and momentum with each other or with their thermal environment. Particle in this context refers to gaseous particles (atoms or molecules), and the system of particles is assumed to have reached thermodynamic equilibrium. While the distribution was first derived by Maxwell in 1860 on heuristic grounds, Boltzmann later carried out significant investigations into the physical origins of this distribution. A particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another. The distribution depends on the temperature of the system and the mass of the particle. The Maxwell Boltzmann distribution applies to the classical ideal gas, which is an idealization of real gases. In real gases, there are various effects (e.g., van der Waals interactions, vortical flow, relativistic speed limits, and quantum exchange interactions) that can make their speed distribution different from the Maxwell Boltzmann form. However, rarefied gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. Thus, it forms the basis of the Kinetic theory of gases, which provides a simplified explanation of many fundamental gaseous properties, including pressure and diffusion.
The Lander Green algorithm is an algorithm, due to Eric Lander and Philip Green for computing the likelihood of observed genotype data given a pedigree. It is appropriate for relatively small pedigrees and a large number of markers. It is used in the analysis of genetic linkage.
In regression analysis, partial leverage is a measure of the contribution of the individual independent variables to the leverage of each observation. That is, if hi is the ith element of the diagonal of the hat matrix, the partial leverage is a measure of how hi changes as a variable is added to the regression model. The partial leverage is computed as:  where j = index of independent variable i = index of observation Xj [j] = residuals from regressing Xj against the remaining independent variables Note that the partial leverage is the leverage of the ith point in the partial regression plot for the jth variable. Data points with large partial leverage for an independent variable can exert undue influence on the selection of that variable in automatic regression model building procedures. In statistics, high-leverage points are those that are outliers with respect to the independent variables. In other words, high-leverage points have no neighbouring points in  space, where p is the number of independent variables in a regression model. This makes the fitted model likely to pass close to a high leverage observation. Hence high-leverage points have the potential to cause large changes in the parameter estimates when they are deleted i.e., to be influential points. Although an influential point will typically have high leverage, a high leverage point is not necessarily an influential point. The leverage is typically defined as the diagonal of the hat matrix, which is
A phase-type distribution is a probability distribution constructed by a convolution or mixture of exponential distributions. It results from a system of one or more inter-related Poisson processes occurring in sequence, or phases. The sequence in which each of the phases occur may itself be a stochastic process. The distribution can be represented by a random variable describing the time until absorption of a Markov process with one absorbing state. Each of the states of the Markov process represents one of the phases. It has a discrete time equivalent the discrete phase-type distribution. The set of phase-type distributions is dense in the field of all positive-valued distributions, that is, it can be used to approximate any positive-valued distribution.
In probability theory, an indecomposable distribution is a probability distribution that cannot be represented as the distribution of the sum of two or more non-constant independent random variables: Z =  X + Y. If it can be so expressed, it is decomposable: Z = X + Y. If, further, it can be expressed as the distribution of the sum of two or more independent identically distributed random variables, then it is divisible: Z = X1 + X2.
In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.). Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, softmax regression, multinomial logit, maximum entropy (MaxEnt) classifier, conditional maximum entropy model.
A Doob martingale (also known as a Levy martingale) is a mathematical construction of a stochastic process which approximates a given random variable and has the martingale property with respect to the given filtration. It may be thought of as the evolving sequence of best approximations to the random variable based on information accumulated up to a certain time. When analyzing sums, random walks, or other additive functions of independent random variables, one can often apply the central limit theorem, law of large numbers, Chernoff's inequality, Chebyshev's inequality or similar tools. When analyzing similar objects where the differences are not independent, the main tools are martingales and Azuma's inequality.
In the analysis of designed experiments, the Friedman test is the most common non-parametric test for complete block designs. The Durbin test is a nonparametric test for balanced incomplete designs that reduces to the Friedman test in the case of a complete block design.
Epidemiology is the study and analysis of the patterns, causes, and effects of health and disease conditions in defined populations. It is the cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences. Major areas of epidemiological study include disease etiology, transmission, outbreak investigation, disease surveillance, forensic epidemiology and screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.
In mathematics, the law of a stochastic process is the measure that the process induces on the collection of functions from the index set into the state space. The law encodes a lot of information about the process; in the case of a random walk, for example, the law is the probability distribution of the possible trajectories of the walk.
A Likert scale (/ l k. rt/ LIK- rt but more commonly pronounced / la .k rt/ LY-k rt) is a psychometric scale commonly involved in research that employs questionnaires. It is the most widely used approach to scaling responses in survey research, such that the term is often used interchangeably with rating scale, or more accurately the Likert-type scale, even though the two are not synonymous. The scale is named after its inventor, psychologist Rensis Likert. Likert distinguished between a scale proper, which emerges from collective responses to a set of items (usually eight or more), and the format in which responses are scored along a range. Technically speaking, a Likert scale refers only to the former. The difference between these two concepts has to do with the distinction Likert made between the underlying phenomenon being investigated and the means of capturing variation that points to the underlying phenomenon. When responding to a Likert questionnaire item, respondents specify their level of agreement or disagreement on a symmetric agree-disagree scale for a series of statements. Thus, the range captures the intensity of their feelings for a given item. A scale can be created as the simple sum of questionnaire responses over the full range of the scale. In so doing, Likert scaling assumes distances between each item are equal. Importantly, "All items are assumed to be replications of each other or in other words items are considered to be parallel instruments"  (p. 197). By contrast modern test theory treats the difficulty of each item (the ICCs) as information to be incorporated in scaling items.
In statistics, Mallows's Cp, named for Colin Lingwood Mallows, is used to assess the fit of a regression model that has been estimated using ordinary least squares. It is applied in the context of model selection, where a number of predictor variables are available for predicting some outcome, and the goal is to find the best model involving a subset of these predictors. A small value of Cp means that the model is relatively precise. Mallows's Cp has been shown to be equivalent to Akaike information criterion in the special case of Gaussian linear regression.
Observational equivalence is the property of two or more underlying entities being indistinguishable on the basis of their observable implications. Thus, for example, two scientific theories are observationally equivalent if all of their empirically testable predictions are identical, in which case empirical evidence cannot be used to distinguish which is closer to being correct; indeed, it may be that they are actually two different perspectives on one underlying theory. In econometrics, two parameter values (or two structures, from among a class of statistical models) are considered observationally equivalent if they both result in the same probability distribution of observable data. This term often arises in relation to the identification problem. In the formal semantics of programming languages, two terms M and N are observationally equivalent if and only if, in all contexts C[...] where C[M] is a valid term, it is the case that C[N] is also a valid term with the same value. Thus it is not possible, within the system, to distinguish between the two terms. This definition can be made precise only with respect to a particular calculus, one that comes with its own specific definitions of term, context, and the value of a term.
In statistics, a k-statistic is a minimum-variance unbiased estimator of a cumulant.
Data Desk is a software program for visual data analysis, visual data exploration, and statistics. It carries out Exploratory Data Analysis (EDA) and standard statistical analyses by means of dynamically linked graphic data displays that update any change simultaneously.
In statistics, marginal models (Heagerty & Zeger, 2000) are a technique for obtaining regression estimates in multilevel modeling, also called hierarchical linear models. People often want to know the effect of a predictor/explanatory variable X, on a response variable Y. One way to get an estimate for such effects is through regression analysis.
In survival analysis, the hazard ratio (HR) is the ratio of the hazard rates corresponding to the conditions described by two levels of an explanatory variable. For example, in a drug study, the treated population may die at twice the rate per unit time as the control population. The hazard ratio would be 2, indicating higher hazard of death from the treatment. Or in another study, men receiving the same treatment may suffer a certain complication ten times more frequently per unit time than women, giving a hazard ratio of 10. Hazard ratios differ from relative risks in that the latter are cumulative over an entire study, using a defined endpoint, while the former represent instantaneous risk over the study time period, or some subset thereof. Hazard ratios suffer somewhat less from selection bias with respect to the endpoints chosen and can indicate risks that happen before the endpoint.
Hellin's Law is the principle that one in about 89 natural pregnancies ends in the birth of twins, triplets once in 892 births, and quadruplets once in 893 births.
JMulTi is an open-source interactive software for econometric analysis, specialised in univariate and multivariate time series analysis. It has a Java graphical user interface. The motivation for its designed was to provide the means by which some time-series econometric procedures that were difficult or unavailable in other packages could be undertaken. Such procedures include Impulse Response Analysis with bootstrapped confidence intervals for VAR/VEC modelling.
Length time bias is a form of selection bias, a statistical distortion of results which can lead to incorrect conclusions about the data. Length time bias can occur when the lengths of intervals are analysed by selecting intervals that occupy randomly chosen points in time or space. This process favors longer intervals, thus skewing the data. Length time bias is often discussed in the context of the benefits of cancer screening, where it can lead to the perception that screening leads to better outcomes when in reality it has no effect. Fast-growing tumors generally have a shorter asymptomatic phase than slower-growing tumors. This means that there is a shorter period of time when the cancer is present in the body (and therefore might be detected by screening) but not yet large enough to cause symptoms, which would cause the patient to seek medical care and be diagnosed without screening. As a result, if the same number of slow-growing and fast-growing tumors appear in a year, the screening test will detect more slow-growers than fast-growers. If these slow growing tumors are less likely to be fatal than the fast growers are, the people whose cancer is detected by screening will do better, on average, than the people whose tumors are detected from symptoms (or at autopsy), even if there is no real benefit to catching the cancer earlier. This can give the impression that detecting cancers through screening causes cancers to be less dangerous, when the reality is that less dangerous cancers are simply more likely to be detected by screening.
Odds are a numerical expression, usually expressed as a pair of numbers, used in both gambling and statistics. In statistics, the odds for or odds of some event reflect the likelihood that the event will take place. Odds against reflect the likelihood that a particular event will not take place. In gambling, the odds are the ratio of payoff to stake, and do not necessarily reflect exactly the probabilities. Odds are expressed in several ways (see below), and sometimes the term is used incorrectly to mean simply the probability of an event. Conventionally, gambling odds are expressed in the form "X to Y", where X and Y are numbers, and it is implied that the odds are odds against the event on which the gambler is considering wagering. In both gambling and statistics, the 'odds' are a numerical expression of the likelihood of some possible event. In gambling, odds represent the ratio between the amounts staked by parties to a wager or bet. Thus, odds of 6 to 1 mean the first party (normally a bookmaker) stakes six times the amount staked by the second party. In statistics, the odds for an event E are defined as a simple function of the probability of that possible event E. One drawback of expressing the uncertainty of this possible event as odds for is that to regain the probability requires a calculation. The natural way to interpret odds for (without calculating anything) is as the ratio of events to non-events in the long run. A simple example is that the (statistical) odds for rolling six with a fair die (one of a pair of dice) are 1 to 5. This is because, if one rolls the die many times, and keeps a tally of the results, one expects 1 six event for every 5 times the die does not show six. For example, if we roll the fair die 600 times, we would very much expect something in the neighborhood of 100 sixes, and 500 of the other five possible outcomes. That is a ratio of 100 to 500, or simply 1 to 5. To express the (statistical) odds against, the order of the pair is reversed. Hence the odds against rolling a six with a fair die are 5 to 1. The probability of rolling a six with a fair die is the single number 1/6 or approximately 16.7%. The gambling and statistical uses of odds are closely interlinked. If a bet is a fair one, then the odds offered to the gamblers will perfectly reflect relative probabilities. A fair bet that a fair die will roll a six will pay the gambler $5 for a $1 wager (and return the bettor his or her wager) in the case of a six and nothing in any other case. The terms of the bet are fair, because on average, five rolls result in something other than a six, at a cost of $5, for every roll that results in a six and a net payout of $5. The profit and the expense exactly offset one another and so there is no disadvantage to gambling over the long run. If the odds being offered to the gamblers do not correspond to probability in this way then one of the parties to the bet has an advantage over the other. Casinos, for example, offer odds that place themselves at an advantage, which is how they guarantee themselves a profit and survive as businesses. The fairness of a particular gamble is more clear in a game involving relatively pure chance, such as the ping-pong ball method used in state lotteries in the United States. It is much harder to judge the fairness of the odds offered in a wager on a sporting event such as a football match.
In statistics, the Hodges Lehmann estimator is a robust and nonparametric estimator of a population's location parameter. For populations that are symmetric about one median, such as the (Gaussian) normal distribution or the Student t-distribution, the Hodges Lehmann estimator is a consistent and median-unbiased estimate of the population median. For non-symmetric populations, the Hodges Lehmann estimator estimates the "pseudo median", which is closely related to the population median. The Hodges Lehmann estimator was proposed originally for estimating the location parameter of one-dimensional populations, but it has been used for many more purposes. It has been used to estimate the differences between the members of two populations. It has been generalized from univariate populations to multivariate populations, which produce samples of vectors. It is based on the Wilcoxon signed-rank statistic. In statistical theory, it was an early example of an rank-based estimator, an important class of estimators both in nonparametric statistics and in robust statistics. The Hodges Lehmann estimator was proposed in 1963 independently by Pranab Kumar Sen and by Joseph Hodges and Erich Lehmann, and so it is also called the "Hodges Lehmann Sen estimator".
In probability theory, a random measure is a measure-valued random element. Let X be a complete separable metric space and  the  -algebra of its Borel sets. A Borel measure   on X is boundedly finite if  (A) <   for every bounded Borel set A. Let  be the space of all boundedly finite measures on . Let ( , F, P) be a probability space, then a random measure maps from this probability space to the measurable space (, ).A measure generally might be decomposed as:  Here  is a diffuse measure without atoms, while  is a purely atomic measure.
In physics, engineering, and applied mathematics, Welch's method, named after P.D. Welch, is used for estimating the power of a signal at different frequencies: that is, it is an approach to spectral density estimation. The method is based on the concept of using periodogram spectrum estimates, which are the result of converting a signal from the time domain to the frequency domain. Welch's method is an improvement on the standard periodogram spectrum estimating method and on Bartlett's method, in that it reduces noise in the estimated power spectra in exchange for reducing the frequency resolution. Due to the noise caused by imperfect and finite data, the noise reduction from Welch's method is often desired.
In statistics, the concordance correlation coefficient measures the agreement between two variables, e.g., to evaluate reproducibility or for inter-rater reliability.
In probability theory and statistics, a Gaussian process is a statistical distribution where observations occur in a continuous domain, e.g. time or space. In a Gaussian process, every point in some continuous input space is associated with a normally distributed random variable. Moreover, every finite collection of those random variables has a multivariate normal distribution. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space. The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions. Gaussian processes are important in statistical modelling because of properties inherited from the normal. For example, if a random process is modeled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times.
Ecological studies are studies of risk-modifying factors on health or other outcomes based on populations defined either geographically or temporally. Both risk-modifying factors and outcomes are averaged for the populations in each geographical or temporal unit and then compared using standard statistical methods. Ecological studies have often found links between risk-modifying factors and health outcomes well in advance of other epidemiological or laboratory approaches.
The Gerschenkron effect was developed by Alexander Gerschenkron, and claims that changing the base year for an index determines the growth rate of the index. This description is from the OECD website:
In statistics, the root mean square (abbreviated RMS or rms), also known as the quadratic mean, is defined as the square root of the arithmetic mean of the squares of a set of numbers. RMS can also be defined for a continuously varying function in terms of an integral of the squares of the instantaneous values during a cycle. For a cyclically alternating electric current, RMS is equal to the value of the direct current that would produce the same power dissipation in a resistive load. In econometrics the root mean square error of an estimator is a measure of the imperfection of the fit of the estimator to the data. The root mean square is a particular case of the generalized mean, with exponent 2.
The theory of conjoint measurement (also known as conjoint measurement or additive conjoint measurement) is a general, formal theory of continuous quantity. It was independently discovered by the French economist Ge rard Debreu (1960) and by the American mathematical psychologist R. Duncan Luce and statistician John Tukey (Luce & Tukey 1964). The theory concerns the situation where at least two natural attributes, A and X, non-interactively relate to a third attribute, P. It is not required that A, X or P are known to be quantities. Via specific relations between the levels of P, it can be established that P, A and X are continuous quantities. Hence the theory of conjoint measurement can be used to quantify attributes in empirical circumstances where it is not possible to combine the levels of the attributes using a side-by-side operation or concatenation. The quantification of psychological attributes such as attitudes, cognitive abilities and utility is therefore logically plausible. This means that the scientific measurement of psychological attributes is possible. That is, like physical quantities, a magnitude of a psychological quantity may possibly be expressed as the product of a real number and a unit magnitude. Application of the theory of conjoint measurement in psychology, however, has been limited. It has been argued that this is due to the high level of formal mathematics involved (e.g., Cliff 1992) and that the theory cannot account for the "noisy" data typically discovered in psychological research (e.g., Perline, Wright & Wainer 1979). It has been argued that the Rasch model is a stochastic variant of the theory of conjoint measurement (e.g., Brogden 1977; Embretson & Reise 2000; Fischer 1995; Keats 1967; Kline 1998; Scheiblechner 1999), however, this has been disputed (e.g., Karabatsos, 2001; Kyngdon, 2008). Order restricted methods for conducting probabilistic tests of the cancellation axioms of conjoint measurement have been developed in the past decade (e.g., Karabatsos, 2001; Davis-Stober, 2009). The theory of conjoint measurement is (different but) related to conjoint analysis, which is a statistical-experiments methodology employed in marketing to estimate the parameters of additive utility functions. Different multi-attribute stimuli are presented to respondents, and different methods are used to measure their preferences about the presented stimuli. The coefficients of the utility function are estimated using alternative regression-based tools.
In probability theory, an empirical process is a stochastic process that describes the proportion of objects in a system in a given state. For a process in a discrete state space a population continuous time Markov chain or Markov population model is a process which counts the number of objects in a given state (without rescaling). In mean field theory, limit theorems (as the number of objects becomes large) are considered and generalise the central limit theorem for empirical measures. Applications of the theory of empirical processes arise in non-parametric statistics.
Medoids are representative objects of a data set or a cluster with a data set whose average dissimilarity to all the objects in the cluster is minimal. Medoids are similar in concept to means or centroids, but medoids are always members of the data set. Medoids are most commonly used on data when a mean or centroid cannot be defined such as 3-D trajectories or in the gene expression context. The term is used in computer science in data clustering algorithms. For some data sets there may be more than one medoid, as with medians. A common application of the medoid is the k-medoids clustering algorithm, which is similar to the k-means algorithm but works when a mean or centroid is not definable. This algorithm basically works as follows. First, a set of medoids is chosen at random. Second, the distances to the other points are computed. Third, data are clustered according to the medoid they are most similar to. Fourth, the medoid set is optimized via an iterative process. Note that a medoid is not equivalent to a median or a geometric median. A median is only defined on 1-dimensional data, and it only minimizes dissimilarity to other points for a specific distance metric (Manhattan norm). A geometric median is defined in any dimension, but is not necessarily a point from within the original dataset.
The term ceiling effect has two distinct meanings, referring to the level at which an independent variable no longer has an effect on a dependent variable, or to the level above which variance in an independent variable is no longer measured or estimated. An example of the first meaning, a ceiling effect in treatment, is pain relief by some kinds of analgesic drugs, which have no further effect on pain above a particular dosage level (see also: ceiling effect in pharmacology). An example of the second meaning, a ceiling effect in data-gathering, is a survey that groups all respondents into income categories, not distinguishing incomes of respondents above the highest level asked about in the survey instrument.
The Jaccard index, also known as the Jaccard similarity coefficient (originally coined coefficient de communaute  by Paul Jaccard), is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:  (If A and B are both empty, we define J(A,B) = 1.)  The MinHash min-wise independent permutations locality sensitive hashing scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a hash function. The Jaccard distance, which measures dissimilarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:  An alternate interpretation of the Jaccard distance is as the ratio of the size of the symmetric difference  to the union. This distance is a metric on the collection of all finite sets. There is also a version of the Jaccard distance for measures, including probability measures. If  is a measure on a measurable space , then we define the Jaccard coefficient by , and the Jaccard distance by . Care must be taken if  or , since these formulas are not well defined in that case.
In applied statistics, a variance-stabilizing transformation is a data transformation that is specifically chosen either to simplify considerations in graphical exploratory data analysis or to allow the application of simple regression-based or analysis of variance techniques. The aim behind the choice of a variance-stabilizing transformation is to find a simple function   to apply to values x in a data set to create new values y =  (x) such that the variability of the values y is not related to their mean value. For example, suppose that the values x are realizations from different Poisson distributions: i.e. the distributions each have different mean values  . Then, because for the Poisson distribution the variance is identical to the mean, the variance varies with the mean. However, if the simple variance-stabilizing transformation  is applied, the sampling variance associated with observation will be nearly constant: see Anscombe transform for details and some alternative transformations. While variance-stabilizing transformations are well known for certain parametric families of distributions, such as the Poisson and the binomial distribution, some types of data analysis proceed more empirically: for example by searching among power transformations to find a suitable fixed transformation. Alternatively, if data analysis suggests a functional form for the relation between variance and mean, this can be used to deduce a variance-stabilizing transformation. Thus if, for a mean  ,  a suitable basis for a variance stabilizing transformation would be  where the arbitrary constant of integration can be chosen for convenience.
The ensemble Kalman filter (EnKF) is a recursive filter suitable for problems with a large number of variables, such as discretizations of partial differential equations in geophysical models. The EnKF originated as a version of the Kalman filter for large problems (essentially, the covariance matrix is replaced by the sample covariance), and it is now an important data assimilation component of ensemble forecasting. EnKF is related to the particle filter (in this context, a particle is the same thing as ensemble member) but the EnKF makes the assumption that all probability distributions involved are Gaussian; when it is applicable, it is much more efficient than the particle filter.
In mathematics, the inverse relation of a binary relation is the relation that occurs when the order of the elements is switched in the relation. For example, the inverse of the relation 'child of' is the relation 'parent of'. In formal terms, if X and Y are sets and L   X   Y is a relation from X to Y, then L-1 is the relation defined so that y L-1 x if and only if x L y. In set-builder notation, L-1 = {(y, x)   Y   X | (x, y)   L}. The notation is analogous with that for an inverse function. Although many functions do not have an inverse; every relation does have a unique inverse. Despite the notation and terminology, the inverse relation is not an inverse in the sense of group inverse. However, the unary operation that maps a relation to the inverse relation is an involution, so it induces the structure of a semigroup with involution on the binary relations on a set, or, more generally, induces a dagger category on the category of relations as detailed below. As a unary operation, taking the inverse (sometimes called inversion) commutes with the order-related operations of relation algebra, i.e., it commutes with union, intersection, complement etc. The inverse relation is also called the converse relation or transpose relation  the latter in view of its similarity with the transpose of a matrix. It has also been called the opposite or dual of the original relation. Other notations for the inverse relation include LC, LT, L~ or  or L  or L .
In statistics, the standard score is the signed number of standard deviations an observation or datum is above the mean. A positive standard score indicates a datum above the mean, while a negative standard score indicates a datum below the mean. It is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This conversion process is called standardizing or normalizing (however, "normalizing" can refer to many types of ratios; see normalization (statistics) for more). Standard scores are also called z-values, z-scores, normal scores, and standardized variables; the use of "Z" is because the normal distribution is also known as the "Z distribution". They are most frequently used to compare a sample to a standard normal deviate, though they can be defined without assumptions of normality. The z-score is only defined if one knows the population parameters; if one only has a sample set, then the analogous computation with sample mean and sample standard deviation yields the Student's t-statistic.
In statistics, the bootstrap error-adjusted single-sample technique (BEST or the BEAST) is a non-parametric method that is intended to allow an assessment to be made of the validity of a single sample. It is based on estimating a probability distribution representing what can be expected from valid samples. This is done use a statistical method called bootstrapping, applied to previous samples that are known to be valid.
In statistics, additive smoothing, also called Laplace smoothing (not to be confused with Laplacian smoothing), or Lidstone smoothing, is a technique used to smooth categorical data. Given an observation x = (x1, ..., xd) from a multinomial distribution with N trials and parameter vector   = ( 1, ...,  d), a "smoothed" version of the data gives the estimator:  where the pseudocount   > 0 is the smoothing parameter (  = 0 corresponds to no smoothing). Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical estimate xi / N, and the uniform probability 1/d. Using Laplace's rule of succession, some authors have arguedthat   should be 1 (in which case the term add-one smoothing is also used), though in practice a smaller value is typically chosen. From a Bayesian point of view, this corresponds to the expected value of the posterior distribution, using a symmetric Dirichlet distribution with parameter   as a prior. In the special case where the number of categories is 2, this is equivalent to using a Beta distribution as the conjugate prior for the parameters of Binomial distribution.
In statistics, the mean signed difference, deviation, or error (MSD or MSE) is a sample statistic that summarises how well an estimator  matches the quantity  that it is supposed to estimate. It is one of a number of statistics that can be used to assess an estimation procedure, and it would often be used in conjunction with a sample version of the mean square error.  
Accidental sampling (sometimes known as grab, convenience sampling or opportunity sampling) is a type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand. That is, a sample population selected because it is readily available and convenient, as researchers are drawing on relationships or networks to which they have easy access. The researcher using such a sample cannot scientifically make generalizations about the total population from this sample because it would not be representative enough. For example, if the interviewer was to conduct such a survey at a shopping center early in the morning on a given day, the people that he/she could interview would be limited to those given there at that given time, which would not represent the views of other members of society in such an area, if the survey was to be conducted at different times of day and several times per week. This type of sampling is most useful for pilot testing. Credibility of a researcher's results by convenience sampling will depend on convincing the reader that the sample chosen equates to a large degree of the population from which they are drawn.
In statistics, benchmarking is a method of using auxiliary information to adjust the sampling weights used in an estimation process, in order to yield more accurate estimates of totals. Suppose we have a population where each unit  has a "value"  associated with it. For example,  could be a wage of an employee , or the cost of an item . Suppose we want to estimate the sum  of all the . So we take a sample of the , get a sampling weight W(k) for all sampled , and then sum up  for all sampled . One property usually common to the weights  described here is that if we sum them over all sampled , then this sum is an estimate of the total number of units  in the population (for example, the total employment, or the total number of items). Because we have a sample, this estimate of the total number of units in the population will differ from the true population total. Similarly, the estimate of total  (where we sum  for all sampled ) will also differ from true population total. We do not know what the true population total  value is (if we did, there would be no point in sampling!). Yet often we do know what the sum of the  are over all units in the population. For example, we may not know the total earnings of the population or the total cost of the population, but often we know the total employment or total volume of sales. And even if we don't know these exactly, there often are surveys done by other organizations or at earlier times, with very accurate estimates of these auxiliary quantities. One important function of a population census is to provide data that can be used for benchmarking smaller surveys. The benchmarking procedure begins by first breaking the population into benchmarking cells. Cells are formed by grouping units together that share common characteristics, for example, similar , yet anything can be used that enhances the accuracy of the final estimates. For each cell , we let  be the sum of all , where the sum is taken over all sampled  in the cell . For each cell , we let  be the auxiliary value for cell , which is commonly called the "benchmark target" for cell . Next, we compute a benchmark factor . Then, we adjust all weights  by multiplying it by its benchmark factor , for its cell . The net result is that the estimated  [formed by summing ] will now equal the benchmark target total . But the more important benefit is that the estimate of the total of  [formed by summing ] will tend to be more accurate.
Process Window Index (PWI) is a statistical measure that quantifies the robustness of a manufacturing process, e.g. one which involves heating and cooling, known as a thermal process. In manufacturing industry, PWI values are used to calibrate the heating and cooling of soldering jobs (known as a thermal profile) while baked in a reflow oven. PWI measures how well a process fits into a user-defined process limit known as the specification limit. The specification limit is the tolerance allowed for the process and may be statistically determined. Industrially, these specification limits are known as the process window, and values that a plotted inside or outside this window are known as the process window index. Using PWI values, processes can be accurately measured, analyzed, compared, and tracked at the same level of statistical process control and quality control available to other manufacturing processes.
In probability theory, the craps principle is a theorem about event probabilities under repeated iid trials. Let  and  denote two mutually exclusive events which might occur on a given trial. Then the probability that  occurs before  equals the conditional probability that  occurs given that  or  occur on the next trial, which is  The events  and  need not be collectively exhaustive (if they are, the result is trivial).
Randomization is the process of making something random; in various contexts this involves, for example: generating a random permutation of a sequence (such as when shuffling cards); selecting a random sample of a population (important in statistical sampling); allocating experimental units via random assignment to a treatment or control condition; generating random numbers (see Random number generation); or transforming a data stream (such as when using a scrambler in telecommunications). Randomization is not haphazard. Instead, a random process is a sequence of random variables describing a process whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. For example, a random sample of individuals from a population refers to a sample where every individual has a known probability of being sampled. This would be contrasted with nonprobability sampling where arbitrary individuals are selected.
In information theory, the Shannon Hartley theorem tells the maximum rate at which information can be transmitted over a communications channel of a specified bandwidth in the presence of noise. It is an application of the noisy-channel coding theorem to the archetypal case of a continuous-time analog communications channel subject to Gaussian noise. The theorem establishes Shannon's channel capacity for such a communication link, a bound on the maximum amount of error-free digital data (that is, information) that can be transmitted with a specified bandwidth in the presence of the noise interference, assuming that the signal power is bounded, and that the Gaussian noise process is characterized by a known power or power spectral density. The law is named after Claude Shannon and Ralph Hartley.
In robust statistics, robust regression is a form of regression analysis designed to circumvent some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable. Certain widely used methods of regression, such as ordinary least squares, have favourable properties if their underlying assumptions are true, but can give misleading results if those assumptions are not true; thus ordinary least squares is said to be not robust to violations of its assumptions. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. In particular, least squares estimates for regression models are highly sensitive to (not robust against) outliers. While there is no precise definition of an outlier, outliers are observations which do not follow the pattern of the other observations. This is not normally a problem if the outlier is simply an extreme observation drawn from the tail of a normal distribution, but if the outlier results from non-normal measurement error or some other violation of standard ordinary least squares assumptions, then it compromises the validity of the regression results if a non-robust regression technique is used.
Engineering statistics combines engineering and statistics: Design of Experiments (DOE) is a methodology for formulating scientific and engineering problems using statistical models. The protocol specifies a randomization procedure for the experiment and specifies the primary data-analysis, particularly in hypothesis testing. In a secondary analysis, the statistical analyst further examines the data to suggest other questions and to help plan future experiments. In engineering applications, the goal is often to optimize a process or product, rather than to subject a scientific hypothesis to test of its predictive adequacy. The use of optimal (or near optimal) designs reduces the cost of experimentation. Quality control and process control use statistics as a tool to manage conformance to specifications of manufacturing processes and their products. Time and methods engineering use statistics to study repetitive operations in manufacturing in order to set standards and find optimum (in some sense) manufacturing procedures. Reliability engineering which measures the ability of a system to perform for its intended function (and time) and has tools for improving performance. Probabilistic design involving the use of probability in product and system design System identification uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models. ^ a b c Box, G. E., Hunter,W.G., Hunter, J.S., Hunter,W.G., "Statistics for Experimenters: Design, Innovation, and Discovery", 2nd Edition, Wiley, 2005, ISBN 0-471-71813-0 ^ a b c d e Wu, C. F. Jeff and Hamada, Michael (2002). Experiments: Planning, Analysis, and Parameter Design Optimization. Wiley. ISBN 0-471-25511-4.  CS1 maint: Multiple names: authors list (link) ^ a b c Logothetis, N. and Wynn, H. P (1989). Quality Through Design: Experimental Design, Off-line Quality Control, and Taguchi's Contributions. Oxford U. P. ISBN 0-19-851993-1.  CS1 maint: Multiple names: authors list (link) ^ Hogg, Robert V. and Ledolter, J. (1992). Applied Statistics for Engineers and Physical Scientists. Macmillan, New York. ^ Walpole, Ronald; Myers, Raymond; Ye, Keying. Probability and Statistics for Engineers and Scientists. Pearson Education, 2002, 7th edition, pg. 237 ^ Atkinson, A. C. and Donev, A. N. and Tobias, R. D. (2007). Optimum Experimental Designs, with SAS. Oxford University Press. pp. 511+xvi. ISBN 978-0-19-929660-6.  CS1 maint: Multiple names: authors list (link) ^ Barlow, Richard E. (1998). Engineering reliability. ASA-SIAM Series on Statistics and Applied Probability. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; American Statistical Association, Alexandria, VA. pp. xx+199. ISBN 0-89871-405-2. MR 1621421.  ^ Nelson, Wayne B., (2004), Accelerated Testing - Statistical Models, Test Plans, and Data Analysis, John Wiley & Sons, New York, ISBN 0-471-69736-2 ^ LogoWynn ^ Goodwin, Graham C. and Payne, Robert L. (1977). Dynamic System Identification: Experiment Design and Data Analysis. Academic Press. ISBN 0-12-289750-1.  CS1 maint: Multiple names: authors list (link) ^ Walter, E ric and Pronzato, Luc (1997). Identification of Parametric Models from Experimental Data. Springer.  CS1 maint: Multiple names: authors list (link)
In probability theory, Le vy s continuity theorem, named after the French mathematician Paul Le vy, connects convergence in distribution of the sequence of random variables with pointwise convergence of their characteristic functions. An alternative name sometimes used is Le vy s convergence theorem. This theorem is the basis for one approach to prove the central limit theorem and it is one of the major theorems concerning characteristic functions.
In statistics, a Yates analysis is an approach to analyzing data obtained from a designed experiment, where a factorial design has been used. Full- and fractional-factorial designs are common in designed experiments for engineering and scientific applications. In these designs, each factor is assigned two levels. These are typically called the low and high levels. For computational purposes, the factors are scaled so that the low level is assigned a value of -1 and the high level is assigned a value of +1. These are also commonly referred to as "-" and "+". A full factorial design contains all possible combinations of low/high levels for all the factors. A fractional factorial design contains a carefully chosen subset of these combinations. The criterion for choosing the subsets is discussed in detail in the fractional factorial designs article. Formalized by Frank Yates, a Yates analysis exploits the special structure of these designs to generate least squares estimates for factor effects for all factors and all relevant interactions. The Yates analysis can be used to answer the following questions: What is the ranked list of factors  What is the goodness-of-fit (as measured by the residual standard deviation) for the various models  The mathematical details of the Yates analysis are given in chapter 10 of Box, Hunter, and Hunter (1978). The Yates analysis is typically complemented by a number of graphical techniques such as the dex mean plot and the dex contour plot ("dex" stands for "design of experiments").  
Not to be confused with Kernel principal component analysis. Kernel regression is a non-parametric technique in statistics to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y. In any nonparametric regression, the conditional expectation of a variable  relative to a variable  may be written:  where  is an unknown function.
In statistics, the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model. However, the term is also used in time series analysis with a different meaning. In each case, the designation "linear" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible.
Taguchi methods (Japanese:          ) are statistical methods developed by Genichi Taguchi to improve the quality of manufactured goods, and more recently also applied to engineering, biotechnology, marketing and advertising. Professional statisticians have welcomed the goals and improvements brought about by Taguchi methods, particularly by Taguchi's development of designs for studying variation, but have criticized the inefficiency of some of Taguchi's proposals. Taguchi's work includes three principal contributions to statistics: A specific loss function The philosophy of off-line quality control; and Innovations in the design of experiments.
The field of system identification  uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models as well as model reduction.
In probability theory and information theory, the Kullback Leibler divergence (also information divergence, information gain, relative entropy, KLIC, or KL divergence) is a measure of the difference between two probability distributions P and Q. It is not symmetric in P and Q. In applications, P typically represents the "true" distribution of data, observations, or a precisely calculated theoretical distribution, while Q typically represents a theory, model, description, or approximation of P. Specifically, the Kullback Leibler divergence of Q from P, denoted DKL(P Q), is a measure of the information gained when one revises ones beliefs from the prior probability distribution Q to the posterior probability distribution P. In other words, it is the amount of information lost when Q is used to approximate P. Kullback Leibler divergence also measures the expected number of extra bits required to code samples from P using a code optimized for Q rather than the code optimized for P. Although it is often intuited as a way of measuring the distance between probability distributions, the Kullback Leibler divergence is not a true metric. It does not obey the triangle inequality, and in general DKL(P Q) does not equal DKL(Q P). However, its infinitesimal form, specifically its Hessian, gives a metric tensor known as the Fisher information metric. The Kullback Leibler divergence is a special case of a broader class of divergences called f-divergences as well as the class of Bregman divergences. It is the only such divergence over probabilities that is a member of both classes. The Kullback Leibler divergence was originally introduced by Solomon Kullback and Richard Leibler in 1951 as the directed divergence between two distributions. It is discussed in Kullback's historic text, Information Theory and Statistics. The Kullback Leibler divergence is sometimes also called the information gain achieved if P is used instead of Q. It is also called the relative entropy of P with respect to Q, and written H(P|Q).
In probability theory and logic, a set of events is jointly or collectively exhaustive if at least one of the events must occur. For example, when rolling a six-sided die, the outcomes 1, 2, 3, 4, 5, and 6 are collectively exhaustive, because they encompass the entire range of possible outcomes. Another way to describe collectively exhaustive events, is that their union must cover all the events within the entire sample space. For example, events A and B are said to be collectively exhaustive if  where S is the sample space... Compare this to the concept of a set of mutually exclusive events. In such a set no more than one event can occur at a given time. (In some forms of mutual exclusion only one event can ever occur.) The set of all possible die rolls is both collectively exhaustive and mutually exclusive. The outcomes 1 and 6 are mutually exclusive but not collectively exhaustive. The outcomes "even" (2,4 or 6) and "not-6" (1,2,3,4, or 5) are collectively exhaustive but not mutually exclusive. In some forms of mutual exclusion only one event can ever occur, whether collectively exhaustive or not. For example, tossing a particular biscuit for a group of several dogs cannot be repeated, no matter which dog snaps it up. One example of an event that is both collectively exhaustive and mutually exclusive is tossing a coin. The outcome must be either heads or tails, or p (heads or tails) = 1, so the outcomes are collectively exhaustive. When heads occurs, tails can't occur, or p (heads and tails) = 0, so the outcomes are also mutually exclusive.
Grubbs' test (named after Frank E. Grubbs, who published the test in 1950), also known as the maximum normed residual test or extreme studentized deviate test, is a statistical test used to detect outliers in a univariate data set assumed to come from a normally distributed population.
In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator measures the average of the squares of the errors or deviations, that is, the difference between the estimator and what is estimated. MSE is a risk function, corresponding to the expected value of the squared error loss or quadratic loss. The difference occurs because of randomness or because the estimator doesn't account for information that could produce a more accurate estimate. The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator and its bias. For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard deviation.
In statistics, a mixed-design analysis of variance model (also known as a split-plot ANOVA) is used to test for differences between two or more independent groups whilst subjecting participants to repeated measures. Thus, in a mixed-design ANOVA model, one factor (a fixed effects factor) is a between-subjects variable and the other (a random effects factor) is a within-subjects variable. Thus, overall, the model is a type of mixed effect model. A repeated measures design is used when multiple independent variables or measures exist in a data set, but all participants have been measured on each variable.
In financial econometrics, an autoregressive conditional duration (ACD, Engle and Russell (1998)) model considers irregularly spaced and autocorrelated intertrade durations. ACD is analogous to GARCH. Indeed, in a continuous double auction (a common trading mechanism in many financial markets) waiting times between two consecutive trades vary at random.
Statistics is the theory and application of mathematics to the scientific method including hypothesis generation, experimental design, sampling, data collection, data summarization, estimation, prediction and inference from those results to the population from which the experimental sample was drawn. This article lists statisticians who have been instrumental in the development of theoretical and applied statistics.
Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. It refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix. An MDS algorithm aims to place each object in N-dimensional space such that the between-object distances are preserved as well as possible. Each object is then assigned coordinates in each of the N dimensions. The number of dimensions of an MDS plot N can exceed 2 and is specified a priori. Choosing N=2 optimizes the object locations for a two-dimensional scatterplot.
In spatial statistics the theoretical variogram  is a function describing the degree of spatial dependence of a spatial random field or stochastic process . As a concrete example from the field of gold mining, a variogram will give a measure of how much two samples taken from the mining area will vary in gold percentage depending on the distance between those samples. Samples taken far apart will vary more than samples taken close to each other.
In mathematics the signal-to-noise statistic distance between two vectors a and b with mean values  and  and standard deviation  and  respectively is:  In the case of Gaussian-distributed data and unbiased class distributions, this statistic can be related to classification accuracy given an ideal linear discrimination, and a decision boundary can be derived. This distance is frequently used to identify vectors that have significant difference. One usage is in bioinformatics to locate genes that are differential expressed on microarray experiments.
In probability theory and statistics, the poly-Weibull distribution is a continuous probability distribution. The distribution is defined to be that of a random variable defined to be the smallest of a number of statistically independent random variables having non-identical Weibull distributions.  
In mathematical statistics, Crame r's theorem (or Crame r s decomposition theorem) is one of several theorems of Harald Crame r, a Swedish statistician and probabilist.
LaplacesDemon is an open-source statistical package that is intended to provide a complete environment for Bayesian inference. LaplacesDemon has been used in numerous fields. The user writes their own model specification function and selects a numerical approximation algorithm to update their Bayesian model. Some numerical approximation families of algorithms include Laplace's method (Laplace approximation), numerical integration (iterative quadrature), Markov chain Monte Carlo (MCMC), and Variational Bayes. The base package, LaplacesDemon, is written entirely in the R programming language, and is largely self-contained, though it does require the parallel package for high performance computing via parallelism. Big data is also supported. An extension package called LaplacesDemonCpp is in development to provide C++ functionality. The software was named after the concept of Laplace's demon, which refers to a hypothetical being capable of predicting the universe. Pierre-Simon Laplace alluded to this hypothetical being in the introduction to his Philosophical Essay on Probabilities.
In the statistics of time series, and in particular the analysis of financial time series for stock trading purposes, a moving-average crossover occurs when, on plotting two moving averages each based on different degrees of smoothing, the traces of these moving averages cross. It does not predict future direction but shows trends. This indicator uses two (or more) moving averages, a slower moving average and a faster moving average. The faster moving average is a short term moving average. For end-of-day stock markets, for example, it may be 5, 10 or 25 day period while the slower moving average is medium or long term moving average (e.g. 50, 100 or 200 day period). A short term moving average is faster because it only considers prices over short period of time and is thus more reactive to daily price changes. On the other hand, a long term moving average is deemed slower as it encapsulates prices over a longer period and is more lethargic. However, it tends to smooth out price noises which are often reflected in short term moving averages. A moving average, as a line by itself, is often overlaid in price charts to indicate price trends. A crossover occurs when a faster moving average (i.e., a shorter period moving average) crosses a slower moving average (i.e. a longer period moving average). In other words, this is when the shorter period moving average line crosses a longer period moving average line. In stock investing, this meeting point is used either to enter (buy or sell) or exit (sell or buy) the market. The particular case where simple equally weighted moving-averages are used is sometimes called a simple moving-average (SMA) crossover. Such a crossover can be used to signal a change in trend and can be used to trigger a trade in a Black Box trading system.
In statistics and computational geometry, the notion of centerpoint is a generalization of the median to data in higher-dimensional Euclidean space. Given a set of points in d-dimensional space, a centerpoint of the set is a point such that any hyperplane that goes through that point divides the set of points in two roughly equal subsets: the smaller part should have at least a 1/(d + 1) fraction of the points. Like the median, a centerpoint need not be one of the data points. Every non-empty set of points (with no duplicates) has at least one centerpoint.
GenStat is a general statistical package. Early versions were developed for large mainframe computers. Up until version 5 (released 1987), there was a Unix binary available, and this continues to be used by many universities and research institutions. Later versions are available for the Windows environment only. GenStat was originally developed at the then Rothamsted Experimental Station (now Rothamsted Research) by a team led by John Nelder, and is programmed in Fortran. GenStat Discovery Edition is a free version of the statistical software GenStat. GenStat Discovery Edition 3 is available to non-commercial users throughout Africa and a range of developing countries outside Africa. Examples of users who qualify are students and lecturers of universities and staff of government research organisations or NGOs. When you register for a license you will be asked for information about the status of the organisation which will be using the software to determine whether it qualifies for the free version. It is developed by VSN International which is owned by The Numerical Algorithms Group and Rothamsted Research.
In statistics and in particular in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations. High-leverage points are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation. Modern computer packages for statistical analysis include, as part of their facilities for regression analysis, various quantitative measures for identifying influential observations: among these measures is partial leverage, a measure of how a variable contributes to the leverage of a datum.
In probability theory and statistics, a continuous-time stochastic process, or a continuous-space-time stochastic process is a stochastic process for which the index variable takes a continuous set of values, as contrasted with a discrete-time process for which the index variable takes only distinct values. An alternative terminology uses continuous parameter as being more inclusive. A more restricted class of processes are the continuous stochastic processes: here the term often (but not always) implies both that the index variable is continuous and that sample paths of the process are continuous. Given the possible confusion, caution is needed. Continuous-time stochastic processes that are constructed from discrete-time processes via a waiting time distribution are called continuous-time random walks.  
Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made. LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.
Seismic to simulation is the process and associated techniques used to develop highly accurate static and dynamic 3D models of hydrocarbon reservoirs for use in predicting future production, placing additional wells, and evaluating alternative reservoir management scenarios. The process is successful if the model accurately reflects the original well logs, seismic data and production history.
A cyclostationary process is a signal having statistical properties that vary cyclically with time. A cyclostationary process can be viewed as multiple interleaved stationary processes. For example, the maximum daily temperature in New York City can be modeled as a cyclostationary process: the maximum temperature on July 21 is statistically different from the temperature on December 20; however, it is a reasonable approximation that the temperature on December 20 of different years has identical statistics. Thus, we can view the random process composed of daily maximum temperatures as 365 interleaved stationary processes, each of which takes on a new value once per year.  
In statistics, the studentized range is the difference between the largest and smallest data in a sample measured in units of sample standard deviations. The studentized range, q, is named for William Sealy Gosset (who wrote under the pseudonym "Student"), and was initially evoked by him (1927). The concept was later presented by a number of actual students, Newman (1939)  and Keuls (1952)  and John Tukey in some unpublished notes. q is the basic statistic for the studentized range distribution, which is used for multiple comparison procedures, such as the single step procedure Tukey's range test , the Newman Keuls method, and the Duncan's step down procedure, and establishing confidence intervals that are still valid after data snooping has occurred.  
In mathematics, two variables are proportional if a change in one is always accompanied by a change in the other, and if the changes are always related by use of a constant multiplier. The constant is called the coefficient of proportionality or proportionality constant. If one variable is always the product of the other and a constant, the two are said to be directly proportional. x and y are directly proportional if the ratio y/x is constant. If the product of the two variables is always a constant, the two are said to be inversely proportional. x and y are inversely proportional if the product xy is constant. To express the statement "y is (directly) proportional to x" mathematically, we write an equation y = cx, for some real constant, c. Symbolically, this is written y   x. To express the statement "y is inversely proportional to x" mathematically, we write an equation y = c/x. We can equivalently write "y is proportional to 1/x", which y = c/x would represent. If a linear function transforms 0, a and b into 0, c and d, and if the product a b c d is not zero, we say a and b are proportional to c and d. An equality of two ratios such as a/c = b/d, where no term is zero, is called a proportion.
Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made. LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.
In statistics, shrinkage has two meanings: In relation to the general observation that, in regression analysis, a fitted relationship appears to perform less well on a new data set than on the data set used for fitting. In particular the value of the coefficient of determination 'shrinks'. This idea is complementary to overfitting and, separately, to the standard adjustment made in the coefficient of determination to compensate for the subjunctive effects of further sampling, like controlling for the potential of new explanatory terms improving the model by chance: that is, the adjustment formula itself provides "shrinkage." But the adjustment formula yields an artificial shrinkage, in contrast to the first definition. To describe general types of estimators, or the effects of some types of estimation, whereby a naive or raw estimate is improved by combining it with other information (see shrinkage estimator). The term relates to the notion that the improved estimate is at a reduced distance from the value supplied by the 'other information' than is the raw estimate. In this sense, shrinkage is used to regularize ill-posed inference problems. A common idea underlying both of these meanings is the reduction in the effects of sampling variation.
In probability theory, a continuous stochastic process is a type of stochastic process that may be said to be "continuous" as a function of its "time" or index parameter. Continuity is a nice property for (the sample paths of) a process to have, since it implies that they are well-behaved in some sense, and, therefore, much easier to analyze. It is implicit here that the index of the stochastic process is a continuous variable. Note that some authors define a "continuous (stochastic) process" as only requiring that the index variable be continuous, without continuity of sample paths: in some terminology, this would be a continuous-time stochastic process, in parallel to a "discrete-time process". Given the possible confusion, caution is needed.
In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the "cocktail party problem" of listening in on one person's speech in a noisy room.
An ecological fallacy (or ecological inference fallacy) is a logical fallacy in the interpretation of statistical data where inferences about the nature of individuals are deduced from inference for the group to which those individuals belong. Ecological fallacy sometimes refers to the fallacy of division which is not a statistical issue. The four common statistical ecological fallacies are: confusion between ecological correlations and individual correlations, confusion between group average and total average, Simpson's paradox, and confusion between higher average and higher likelihood.
In probability and statistics, the truncated normal distribution is the probability distribution of a normally distributed random variable whose value is either bounded below or above (or both). The truncated normal distribution has wide applications in statistics and econometrics. For example, it is used to model the probabilities of the binary outcomes in the probit model and to model censored data in the Tobit model.
In statistical hypothesis testing, statistical significance (or a statistically significant result) is attained when a p-value is less than the significance level (denoted  , alpha). The p-value is the probability of obtaining at least as extreme results given that the null hypothesis is true whereas the significance level   is the probability of rejecting the null hypothesis given that it is true. Equivalently, when the null hypothesis specifies the value of a parameter, the data are said to be statistically significant at given confidence level   = 1     when the computed confidence interval for that parameter fails to contain the value specified by the null hypothesis. As a matter of good scientific practice, a significance level is chosen before data collection and is often set to 0.05 (5%). Other significance levels (e.g., 0.01) may be used, depending on the field of study. In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. But if the p-value is less than the significance level (e.g., p < 0.05), then an investigator may conclude that the observed effect actually reflects the characteristics of the population rather than just sampling error. Investigators may then report that the result attains statistical significance, thereby rejecting the null hypothesis. The present-day concept of statistical significance originated with Ronald Fisher when he developed statistical hypothesis testing based on p-values in the early 20th century. It was Jerzy Neyman and Egon Pearson who later recommended that the significance level be set ahead of time, prior to any data collection. The term significance does not imply importance and the term statistical significance is not the same as research, theoretical, or practical significance. For example, the term clinical significance refers to the practical importance of a treatment effect.
In finance, marginal conditional stochastic dominance is a condition under which a portfolio can be improved in the eyes of all risk-averse investors by incrementally moving funds out of one asset (or one sub-group of the portfolio's assets) and into another. Each risk-averse investor is assumed to maximize the expected value of an increasing, concave von Neumann-Morgenstern utility function. All such investors prefer portfolio B over portfolio A if the portfolio return of B is second-order stochastically dominant over that of A; roughly speaking this means that the density function of A's return can be formed from that of B's return by pushing some of the probability mass of B's return to the left (which is disliked by all increasing utility functions) and then spreading out some of the density mass (which is disliked by all concave utility functions). If a portfolio A is marginally conditionally stochastically dominated by some incrementally different portfolio B, then it is said to be inefficient in the sense that it is not the optimal portfolio for anyone. Note that this context of portfolio optimization is not limited to situations in which mean-variance analysis applies. The presence of marginal conditional stochastic dominance is sufficient, but not necessary, for a portfolio to be inefficient. This is because marginal conditional stochastic dominance only considers incremental portfolio changes involving two sub-groups of assets   one whose holdings are decreased and one whose holdings are increased. It is possible for an inefficient portfolio to not be second-order stochastically dominated by any such one-for-one shift of funds, and yet to by dominated by a shift of funds involving three or more sub-groups of assets.
In probability theory and statistics, the U-quadratic distribution is a continuous probability distribution defined by a unique quadratic function with lower limit a and upper limit b.
In mathematics, unimodality means possessing a unique mode. More generally, unimodality means there is only a single highest value, somehow defined, of some mathematical object.
In statistical hypothesis testing, a uniformly most powerful (UMP) test is a hypothesis test which has the greatest power 1     among all possible tests of a given size  . For example, according to the Neyman Pearson lemma, the likelihood-ratio test is UMP for testing simple (point) hypotheses.
In statistics, Poisson regression is a form of regression analysis used to model count data and contingency tables. Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables. Poisson regression models are generalized linear models with the logarithm as the (canonical) link function, and the Poisson distribution function as the assumed probability distribution of the response.
The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:  where  refers to the rank position of the first relevant document for the i-th query. The reciprocal value of the mean reciprocal rank corresponds to the harmonic mean of the ranks.
In statistics, a truncated distribution is a conditional distribution that results from restricting the domain of some other probability distribution. Truncated distributions arise in practical statistics in cases where the ability to record, or even to know about, occurrences is limited to values which lie above or below a given threshold or within a specified range. For example, if the dates of birth of children in a school are examined, these would typically be subject to truncation relative to those of all children in the area given that the school accepts only children in a given age range on a specific date. There would be no information about how many children in the locality had dates of birth before or after the school's cutoff dates if only a direct approach to the school were used to obtain information. Where sampling is such as to retain knowledge of items that fall outside the required range, without recording the actual values, this is known as censoring, as opposed to the truncation here.
In probability theory and statistics, subindependence is a weak form of independence. Two random variables X and Y are said to be subindependent if the characteristic function of their sum is equal to the product of their marginal characteristic functions. Symbolically:  This is a weakening of the concept of independence of random variables, i.e. if two random variables are independent then they are subindependent, but not conversely. If two random variables are subindependent, and if their covariance exists, then they are uncorrelated. Subindependence has some peculiar properties: for example, there exist random variables X and Y that are subindependent, but X and  Y are not subindependent when   =  1 and therefore X and Y are not independent.
In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, thus assigning each individual to a particular group or "category." In computer science and some branches of mathematics, categorical variables are referred to as enumerations or enumerated types. Commonly (though not in this article), each of the possible values of a categorical variable is referred to as a level. The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from either or both of observations made of qualitative data, where the observations are summarised as counts or cross tabulations, or of quantitative data, where observations might be directly observed counts of events happening or might be counts of values that occur within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term "categorical data" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables. A categorical variable that can take on exactly two values is termed a binary variable or dichotomous variable; an important special case is the Bernoulli variable. Categorical variables with more than two possible values are called polytomous variables; variables are often assumed to be polytomous unless otherwise specified. Discretization is treating continuous data as if it were categorical. Dichotomization is treating continuous data or polytomous variables as if they were binary variables. Regression analysis often treats category membership as a quantitative dummy variable.
In probability theory, lumpability is a method for reducing the size of the state space of some continuous-time Markov chains, first published by Kemeny and Snell.
In economics, structural change is a shift or change in the basic ways a market or economy functions or operates. Such change can be caused by such factors as economic development, global shifts in capital and labor, changes in resource availability due to war or natural disaster or discovery or depletion of natural resources, or a change in political system. For example, a subsistence economy may be transformed into a manufacturing economy, or a regulated mixed economy may be liberalized. A current driver of structural change in the world economy is globalization. Structural change is possible because of the dynamic nature of the economic system. Patterns and changes in sectoral employment drive demand shifts through the income elasticity. Shifting demand for both locally sourced goods and for imported products is a fundamental part of development. The structural changes that move countries through the development process are often viewed in terms of shifts from primary, to secondary and finally, to tertiary production. Technical progress is seen as crucial in the process of structural change as it involves the obsolescence of skills, vocations, and permanent changes in spending and production resulting in structural unemployment.
The sequential probability ratio test (SPRT) is a specific sequential hypothesis test, developed by Abraham Wald. Neyman and Pearson's 1933 result inspired Wald to reformulate it as a sequential analysis problem. The Neyman-Pearson lemma, by contrast, offers a rule of thumb for when all the data is collected (and its likelihood ratio known). While originally developed for use in quality control studies in the realm of manufacturing, SPRT has been formulated for use in the computerized testing of human examinees as a termination criterion.
In applied probability, a regenerative process is a class of stochastic process with the property that certain portions of the process can be treated as being statistically independent of each other. This property can be used in the derivation of theoretical properties of such processes.
In descriptive statistics, summary statistics are used to summarize a set of observations, in order to communicate the largest amount of information as simply as possible. Statisticians commonly try to describe the observations in a measure of location, or central tendency, such as the arithmetic mean a measure of statistical dispersion like the standard deviation a measure of the shape of the distribution like skewness or kurtosis if more than one variable is measured, a measure of statistical dependence such as a correlation coefficient A common collection of order statistics used as summary statistics are the five-number summary, sometimes extended to a seven-number summary, and the associated box plot. Entries in an analysis of variance table can also be regarded as summary statistics.
Lusser's law in systems engineering is a prediction of reliability. Named after engineer Robert Lusser, and also known as Lusser's product law or the probability product law of series components, it states that the reliability of a series system is equal to the product of the reliability of its component subsystems, if their failure modes are known to be statistically independent. For a series of n components, this is expressed as:  where Rs is the overall reliability of the system, and rn is the reliability of the nth component. Lusser's law has been described as the idea that a series system is "weaker than its weakest link", as the product reliability of a series of components can be less than the lowest-value component. For example, given a series system of two components with different reliabilities   one of 0.95 and the other of 0.8   Lusser's law will predict a reliability of  which is lower than either of the individual components.
A confidence band is used in statistical analysis to represent the uncertainty in an estimate of a curve or function based on limited or noisy data. Similarly, a prediction band is used to represent the uncertainty about the value of a new data-point on the curve, but subject to noise. Confidence and prediction bands are often used as part of the graphical presentation of results of a regression analysis. Confidence bands are closely related to confidence intervals, which represent the uncertainty in an estimate of a single numerical value. "As confidence intervals, by construction, only refer to a single point, they are narrower (at this point) than a confidence band which is supposed to hold simultaneously at many points."
Starting with a sample  observed from a random variable X having a given distribution law with a set of non fixed parameters which we denote with a vector , a parametric inference problem consists of computing suitable values   call them estimates   of these parameters precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In Algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample. In this framework, resampling methods are aimed at generating a set of candidate values to replace the unknown parameters that we read as compatible replicas of them. They represent a population of specifications of a random vector   compatible with an observed sample, where the compatibility of its values has the properties of a probability distribution. By plugging parameters into the expression of the questioned distribution law, we bootstrap entire populations of random variables compatible with the observed sample. The rationale of the algorithms computing the replicas, which we denote population bootstrap procedures, is to identify a set of statistics  exhibiting specific properties, denoting a well behavior, w.r.t. the unknown parameters. The statistics are expressed as functions of the observed values , by definition. The  may be expressed as a function of the unknown parameters and a random seed specification  through the sampling mechanism , in turn. Then, by plugging the second expression in the former, we obtain  expressions as functions of seeds and parameters   the master equations   that we invert to find values of the latter as a function of: i) the statistics, whose values in turn are fixed at the observed ones; and ii) the seeds, which are random according to their own distribution. Hence from a set of seed samples we obtain a set of parameter replicas.
A population model is a type of mathematical model that is applied to the study of population dynamics. Models allow a better understanding of how complex interactions and processes work. Modeling of dynamic interactions in nature can provide a manageable way of understanding how numbers change over time or in relation to each other. Ecological population modeling is concerned with the changes in population size and age distribution within a population as a consequence of interactions of organisms with the physical environment, with individuals of their own species, and with organisms of other species (biophysical env.). The world is full of interactions that range from simple to dynamic. Many, if not all, of Earth s processes affect human life. The Earth s processes are greatly stochastic and seem chaotic to the naked eye. However, a plethora of patterns can be noticed and are brought forth by using population modeling as a tool. Population models are used to determine maximum harvest for agriculturists, to understand the dynamics of biological invasions, and have numerous environmental conservation implications. Population models are also used to understand the spread of parasites, viruses, and disease. The realization of our dependence on environmental health has created a need to understand the dynamic interactions of the earth s flora and fauna. Methods in population modeling have greatly improved our understanding of ecology and the natural world.
In descriptive statistics, the quartiles of a ranked set of data values are the three points that divide the data set into four equal groups, each group comprising a quarter of the data. A quartile is a type of quantile. The first quartile (Q1) is defined as the middle number between the smallest number and the median of the data set. The second quartile (Q2) is the median of the data. The third quartile (Q3) is the middle value between the median and the highest value of the data set. In applications of statistics such as epidemiology, sociology and finance, the quartiles of a ranked set of data values are the four subsets whose boundaries are the three quartile points. Thus an individual item might be described as being "in the upper quartile".
In statistics, polynomial regression is a form of linear regression in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y | x), and has been used to describe nonlinear phenomena such as the growth rate of tissues, the distribution of carbon isotopes in lake sediments, and the progression of disease epidemics. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data. For this reason, polynomial regression is considered to be a special case of multiple linear regression. The predictors resulting from the polynomial expansion of the "baseline" predictors are known as interaction features. Such predictors/features are also used in classification settings.
The friendship paradox is the phenomenon first observed by the sociologist Scott L. Feld in 1991 that most people have fewer friends than their friends have, on average. It can be explained as a form of sampling bias in which people with greater numbers of friends have an increased likelihood of being observed among one's own friends. In contradiction to this, most people believe that they have more friends than their friends have. The same observation can be applied more generally to social networks defined by other relations than friendship: for instance, most people's sexual partners have had (on the average) a greater number of sexual partners than they have.  
As applied in the field of computer vision, graph cuts can be employed to efficiently solve a wide variety of low-level computer vision problems (early vision), such as image smoothing, the stereo correspondence problem, and many other computer vision problems that can be formulated in terms of energy minimization. Such energy minimization problems can be reduced to instances of the maximum flow problem in a graph (and thus, by the max-flow min-cut theorem, define a minimal cut of the graph). Under most formulations of such problems in computer vision, the minimum energy solution corresponds to the maximum a posteriori estimate of a solution. Although many computer vision algorithms involve cutting a graph (e.g., normalized cuts), the term "graph cuts" is applied specifically to those models which employ a max-flow/min-cut optimization (other graph cutting algorithms may be considered as graph partitioning algorithms). "Binary" problems (such as denoising a binary image) can be solved exactly using this approach; problems where pixels can be labeled with more than two different labels (such as stereo correspondence, or denoising of a grayscale image) cannot be solved exactly, but solutions produced are usually near the global optimum.
The term dilution assay is generally used to designate a special type of bioassay in which one or more preparations (e.g. a drug) are administered to experimental units at different dose levels inducing a measurable biological response. The dose levels are prepared by dilution in a diluent that is inert in respect of the response. The experimental units can for example be cell-cultures, tissues, organs or living animals. The biological response may be quantal (e.g. positive/negative) or quantitative (e.g. growth). The goal is to relate the response to the dose, usually by interpolation techniques, and in many cases to express the potency/activity of the test preparation(s) relative to a standard of known potency/activity. Dilution assays can be direct or indirect. In a direct dilution assay the amount of dose needed to produce a specific (fixed) response is measured, so that the dose is a stochastic variable defining the tolerance distribution. Conversely, in an indirect dilution assay the dose levels are administered at fixed dose levels, so that the response is a stochastic variable.
In statistics and econometrics, a cross-sectional regression is a type of regression in which the explained and explanatory variables are associated with one period or point in time. This type of cross-sectional analysis is in contrast to a time-series regression or longitudinal regression in which the variables are considered to be associated with a sequence of points in time. For example, in economics a regression to explain and predict money demand (how much people choose to hold in the form of the most liquid assets) could be conducted with either cross-sectional or time series data. A cross-sectional regression would have as each data point an observation on a particular individual's money holdings, income, and perhaps other variables at a single point in time, and different data points would reflect different individuals at the same point in time. In contrast, a regression using time series would have as each data point an entire economy's money holdings, income, etc. at one point in time, and different data points would be drawn on the same economy but at different points in time.
An Event study is a statistical method to assess the impact of an event on the value of a firm. For example, the announcement of a merger between two business entities can be analyzed to see whether investors believe the merger will create or destroy value. The basic idea is to find the abnormal return attributable to the event being studied by adjusting for the return that stems from the price fluctuation of the market as a whole. As the event methodology can be used to elicit the effects of any type of event on the direction and magnitude of stock price changes, it is very versatile. Event studies are thus common to various research areas, such as accounting and finance, management, economics, marketing, information technology, law, and political science. One aspect often used to structure the overall body of event studies is the breadth of the studied event types. On the one hand, there is research investigating the stock market responses to economy-wide events (i.e., market shocks, such as regulatory changes, or catastrophic events). On the other hand, event studies are used to investigate the stock market responses to corporate events, such as mergers and acquisitions, earnings announcements, debt or equity issues, corporate reorganisations, investment decisions and corporate social responsibility (MacKinlay 1997; McWilliams & Siegel, 1997).
Mathematical models can project how infectious diseases progress to show the likely outcome of an epidemic and help inform public health interventions. Models use some basic assumptions and mathematics to find parameters for various infectious diseases and use those parameters to calculate the effects of possible interventions, like mass vaccination programmes.
In epidemiology, the absolute risk reduction, risk difference or absolute effect is the change in the risk of an outcome of a given treatment or activity in relation to a comparison treatment or activity. It is the inverse of the number needed to treat. In general, absolute risk reduction is the difference between one treatment comparison group's event rate (EER) and another comparison group s event rate (CER). The difference is usually calculated with respect to two treatments A and B, with A typically a drug and B a placebo. For example, A could be a 5-year treatment with a hypothetical drug, and B is treatment with placebo, i.e. no treatment. A defined endpoint has to be specified, such as a survival or a response rate. For example: the appearance of lung cancer in a 5-year period. If the probabilities pA and pB of this endpoint under treatments A and B, respectively, are known, then the absolute risk reduction is computed as (pB   pA). The inverse of the absolute risk reduction, NNT, is an important measure in pharmacoeconomics. If a clinical endpoint is devastating enough (e.g. death, heart attack), drugs with a low absolute risk reduction may still be indicated in particular situations. If the endpoint is minor, health insurers may decline to reimburse drugs with a low absolute risk reduction.
Common and special causes are the two distinct origins of variation in a process, as defined in the statistical thinking and methods of Walter A. Shewhart and W. Edwards Deming. Briefly, "common causes", also called Natural patterns, are the usual, historical, quantifiable variation in a system, while "special causes" are unusual, not previously observed, non-quantifiable variation. The distinction is fundamental in philosophy of statistics and philosophy of probability, with different treatment of these issues being a classic issue of probability interpretations, being recognised and discussed as early as 1703 by Gottfried Leibniz; various alternative names have been used over the years. The distinction has been particularly important in the thinking of economists Frank Knight, John Maynard Keynes and G. L. S. Shackle.
In probability theory, a standard probability space, also called Lebesgue Rokhlin probability space or just Lebesgue space (the latter term is ambiguous) is a probability space satisfying certain assumptions introduced by Vladimir Rokhlin in 1940. Informally, it is a probability space consisting of an interval and/or a finite or countable number of atoms. The theory of standard probability spaces was started by von Neumann in 1932 and shaped by Vladimir Rokhlin in 1940. Rokhlin showed that the unit interval endowed with the Lebesgue measure has important advantages over general probability spaces, yet can be effectively substituted for many of these in probability theory. The dimension of the unit interval is not an obstacle, as was clear already to Norbert Wiener. He constructed the Wiener process (also called Brownian motion) in the form of a measurable map from the unit interval to the space of continuous functions.
The Chow test is a statistical and econometric test of whether the coefficients in two linear regressions on different data sets are equal. The Chow test was invented by economist Gregory Chow in 1960. In econometrics, the Chow test is most commonly used in time series analysis to test for the presence of a structural break. In program evaluation, the Chow test is often used to determine whether the independent variables have different impacts on different subgroups of the population. Suppose that we model our data as  If we split our data into two groups, then we have  and  The null hypothesis of the Chow test asserts that , , and , and there is the assumption that the model errors  are independent and identically distributed from a normal distribution with unknown variance. Let  be the sum of squared residuals from the combined data,  be the sum of squared residuals from the first group, and  be the sum of squared residuals from the second group.  and  are the number of observations in each group and  is the total number of parameters (in this case, 3). Then the Chow test statistic is  The test statistic follows the F distribution with  and  degrees of freedom.  Remarks - The global sum of squares (SSE) if often called Restricted Sum of Squares (RSSM) as we basically test a constrained model where we have 2K assumptions (with K the number of regressors). - Some software like SAS will use a predictive Chow test when the size of a subsample is less than the number of regressors.
In decision theory, a decision rule is said to dominate another if the performance of the former is sometimes better, and never worse, than that of the latter. Formally, let  and  be two decision rules, and let  be the risk of rule  for parameter . The decision rule  is said to dominate the rule  if  for all , and the inequality is strict for some . This defines a partial order on decision rules; the maximal elements with respect to this order are called admissible decision rules.
In statistics, McKay's approximation of the coefficient of variation is a statistic based on a sample from a normally distributed population. It was introduced in 1932 by A. T. McKay. Statistical methods for the coefficient of variation often utilizes McKay's approximation. Let ,  be  independent observations from a  normal distribution. The population coefficient of variation is . Let  and  denote the sample mean and the sample standard deviation, respectively. Then  is the sample coefficient of variation. McKay s approximation is  Note that in this expression, the first factor includes the population coefficient of variation, which is usually unknown. When  is smaller than 1/3, then  is approximately chi-square distributed with  degrees of freedom. In the original article by McKay, the expression for  looks slightly different, since McKay defined  with denominator  instead of . McKay's approximation, , for the coefficient of variation is approximately chi-square distributed, but exactly noncentral beta distributed .
In probability theory, a Fleming Viot process (F V process) is a member of a particular subset of probability-measure-valued Markov processes on compact metric spaces, as defined in the 1979 paper by Wendell Helms Fleming and Michel Viot. Such processes are martingales and diffusions. The Fleming Viot processes have proved to be important to the development of a mathematical basis for the theories behind allele drift. They are generalisations of the Wright Fisher process and arise as infinite population limits of suitably rescaled variants of Moran processes.  
In probability theory and statistics, the Conway Maxwell Poisson (CMP or COM-Poisson) distribution is a discrete probability distribution named after Richard W. Conway, William L. Maxwell, and Sime on Denis Poisson that generalizes the Poisson distribution by adding a parameter to model overdispersion and underdispersion. It is a member of the exponential family, has the Poisson distribution and geometric distribution as special cases and the Bernoulli distribution as a limiting case. The COM-Poisson distribution was originally proposed by Conway and Maxwell in 1962 as a solution to handling queueing systems with state-dependent service rates. The probabilistic and statistical properties of the distribution were published by Shmueli et al. (2005). The COM-Poisson is defined to be the distribution with probability mass function  for x = 0,1,2,...,  and    0, where  The function  serves as a normalization constant so the probability mass function sums to one. Note that  does not have a closed form. The additional parameter  which does not appear in the Poisson distribution allows for adjustment of the rate of decay. This rate of decay is a non-linear decrease in ratios of successive probabilities, specifically  When , the COM-Poisson distribution becomes the standard Poisson distribution and as , the distribution approaches a Bernoulli distribution with parameter . When  the CoM-Poisson distribution reduces to a geometric distribution with probability of success  provided . For the COM-Poisson distribution, moments can be found through the recursive formula  
In economics and finance, an index is a statistical measure of changes in a representative group of individual data points. These data may be derived from any number of sources, including company performance, prices, productivity, and employment. Economic indices track economic health from different perspectives. Influential global financial indices such as the Global Dow, and the NASDAQ Composite track the performance of selected large and powerful companies in order to evaluate and predict economic trends. The Dow Jones Industrial Average and the S&P 500 primarily track U.S. markets, though some legacy international companies are included. The consumer price index tracks the variation in prices for different consumer goods and services over time in a constant geographical location, and is integral to calculations used to adjust salaries, bond interest rates, and tax thresholds for inflation. The GDP Deflator Index, or real GDP, measures the level of prices of all new, domestically produced, final goods and services in an economy. Market performance indices include the labour market index/job index and proprietary stock market index investment instruments offered by brokerage houses. Some indices display market variations that cannot be captured in other ways. For example, the Economist provides a Big Mac Index that expresses the adjusted cost of a globally ubiquitous Big Mac as a percentage over or under the cost of a Big Mac in the U.S. in USD (estimated: $3.57). The least relatively expensive Big Mac price occurs in Hong Kong, at a 52% reduction from U.S. prices, or $1.71 U.S. Such indices can be used to help forecast currency values. From this example, it would be assumed that Hong Kong currency is undervalued, and provides a currency investment opportunity.
Simultaneous equation models are a form of statistical model in the form of a set of linear simultaneous equations. They are often used in econometrics.
A quadratic classifier is used in machine learning and statistical classification to separate measurements of two or more classes of objects or events by a quadric surface. It is a more general version of the linear classifier.
This page is concerned with the stochastic modelling as applied to the insurance industry. For other stochastic modelling applications, please see Monte Carlo method and Stochastic asset models. For mathematical definition, please see Stochastic process.
In statistics, a polykay, or generalised k-statistic, (denoted ) is a statistic defined as a linear combination of sample moments.
In finance, a T-forward measure is a pricing measure absolutely continuous with respect to a risk-neutral measure but rather than using the money market as numeraire, it uses a bond with maturity T. The use of the forward measure was pioneered by Farshid Jamshidian (1987), and later used as a means of calculating the price of options on bonds.
Astrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory. Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference.
The Ljung Box test (named for Greta M. Ljung and George E. P. Box) is a type of statistical test of whether any of a group of autocorrelations of a time series are different from zero. Instead of testing randomness at each distinct lag, it tests the "overall" randomness based on a number of lags, and is therefore a portmanteau test. This test is sometimes known as the Ljung Box Q test, and it is closely connected to the Box Pierce test (which is named after George E. P. Box and David A. Pierce). In fact, the Ljung Box test statistic was described explicitly in the paper that led to the use of the Box-Pierce statistic, and from which that statistic takes its name. The Box-Pierce test statistic is a simplified version of the Ljung Box statistic for which subsequent simulation studies have shown poor performance. The Ljung Box test is widely applied in econometrics and other applications of time series analysis.
In statistics the mean squared prediction error of a smoothing or curve fitting procedure is the expected value of the squared difference between the fitted values  and the (unobservable) function g. If the smoothing procedure has operator matrix L, then  The MSPE can be decomposed into two terms (just like mean squared error is decomposed into bias and variance); however for MSPE one term is the sum of squared biases of the fitted values and another the sum of variances of the fitted values:  Note that knowledge of g is required in order to calculate MSPE exactly.
This a list of statistical procedures which can be used for the analysis of categorical data, also known as data on the nominal scale and as categorical variables.  
In probability and statistics, the inverse-chi-squared distribution (or inverted-chi-square distribution) is a continuous probability distribution of a positive-valued random variable. It is closely related to the chi-squared distribution and its specific importance is that it arises in the application of Bayesian inference to the normal distribution, where it can be used as the prior and posterior distribution for an unknown variance.
Elastic maps provide a tool for nonlinear dimensionality reduction. By their construction, they are a system of elastic springs embedded in the data space. This system approximates a low-dimensional manifold. The elastic coefficients of this system allow the switch from completely unstructured k-means clustering (zero elasticity) to the estimators located closely to linear PCA manifolds (for high bending and low stretching modules). With some intermediate values of the elasticity coefficients, this system effectively approximates non-linear principal manifolds. This approach is based on a mechanical analogy between principal manifolds, that are passing through "the middle" of the data distribution, and elastic membranes and plates. The method was developed by A.N. Gorban, A.Y. Zinovyev and A.A. Pitenko in 1996 1998.
In statistics and mathematics, linear least squares is an approach fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system. Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called linear least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator. In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See outline of regression analysis for an outline of the topic.
Survival rate is a part of survival analysis, indicating the percentage of people in a study or treatment group who are alive for a given period of time after diagnosis. Survival rates are important for prognosis, but because this rate is based on the population as a whole, an individual prognosis may be different depending on newer treatments since the last statistical analysis as well as the overall general health of the patient. There are various types of survival rates (discussed below). They often serve as endpoints of clinical trials, and should not be confused with mortality rates, a population metric.
The Beverton Holt model is a classic discrete-time population model which gives the expected number n t+1 (or density) of individuals in generation t + 1 as a function of the number of individuals in the previous generation,  Here R0 is interpreted as the proliferation rate per generation and K = (R0   1) M is the carrying capacity of the environment. The Beverton Holt model was introduced in the context of fisheries by Beverton & Holt (1957). Subsequent work has derived the model under other assumptions such as contest competition (Bra nnstro m & Sumpter 2005) or within-year resource limited competition (Geritz & Kisdi 2004). The Beverton Holt model can be generalized to include scramble competition (see the Ricker model, the Hassell model and the Maynard Smith Slatkin model). It is also possible to include a parameter reflecting the spatial clustering of individuals (see Bra nnstro m & Sumpter 2005). Despite being nonlinear, the model can be solved explicitly, since it is in fact an inhomogeneous linear equation in 1/n. The solution is  Because of this structure, the model can be considered as the discrete-time analogue of the continuous-time logistic equation for population growth introduced by Verhulst; for comparison, the logistic equation is  and its solution is
In statistics, survey sampling describes the process of selecting a sample of elements from a target population to conduct a survey. The term "survey" may refer to many different types or techniques of observation. In survey sampling it most often involves a questionnaire used to measure the characteristics and/or attitudes of people. Different ways of contacting members of a sample once they have been selected is the subject of survey data collection. The purpose of sampling is to reduce the cost and/or the amount of work that it would take to survey the entire target population. A survey that measures the entire target population is called a census. Survey samples can be broadly divided into two types: probability samples and non-probability samples. Probability-based samples implement a sampling plan with specified probabilities (perhaps adapted probabilities specified by an adaptive procedure). Probability-based sampling allows design-based inference about the target population. The inferences are based on a known objective probability distribution that was specified in the study protocol. Inferences from probability-based surveys may still suffer from many types of bias. Surveys that are not based on probability sampling have greater difficulty measuring their bias or sampling error. Surveys based on non-probability samples often fail to represent the people in the target population. In academic and government survey research, probability sampling is a standard procedure. In the USA, the Office of Management and Budget's "List of Standards for Statistical Surveys" states that federally funded surveys must be performed:  selecting samples using generally accepted statistical methods (e.g., probabilistic methods that can provide estimates of sampling error). Any use of nonprobability sampling methods (e.g., cut-off or model-based samples) must be justified statistically and be able to measure estimation error.  Besides, random sampling and design-based inference are supplemented by other statistical methods, such as model-assisted sampling and model-based sampling. For example, many surveys have substantial amounts of nonresponse. Even though the units are initially chosen with known probabilities, the nonresponse mechanisms are unknown. For surveys with substantial nonresponse, statisticians have proposed statistical models, with which data sets are analyzed. Issues related to survey sampling are discussed in several sources including Salant and Dillman (1994).
Reliability engineering is engineering that emphasizes dependability in the lifecycle management of a product. Dependability, or reliability, describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability may also describe the ability to function at a specified moment or interval of time (Availability). Reliability engineering represents a sub-discipline within systems engineering. Reliability is theoretically defined as the probability of success (Reliability=1-Probability of Failure), as the frequency of failures; or in terms of availability, as a probability derived from reliability, testability and maintainability. Testability, Maintainability and maintenance are often defined as a part of "reliability engineering" in Reliability Programs. Reliability plays a key role in the cost-effectiveness of systems. Reliability engineering deals with the estimation, prevention and management of high levels of "lifetime" engineering uncertainty and risks of failure. Although stochastic parameters define and affect reliability, according to some expert authors on reliability engineering (e.g. P. O'Conner, J. Moubray and A. Barnard), reliability is not (solely) achieved by mathematics and statistics. You cannot really find a root cause (needed to effectively prevent failures) by only looking at statistics. "Nearly all teaching and literature on the subject emphasize these aspects, and ignore the reality that the ranges of uncertainty involved largely invalidate quantitative methods for prediction and measurement." Reliability engineering relates closely to safety engineering and to system safety, in that they use common methods for their analysis and may require input from each other. Reliability engineering focuses on costs of failure caused by system downtime, cost of spares, repair equipment, personnel, and cost of warranty claims. Safety engineering normally emphasizes not cost, but preserving life and nature, and therefore deals only with particular dangerous system-failure modes. High reliability (safety factor) levels also result from good engineering and from attention to detail, and almost never from only reactive failure management (reliability accounting / statistics). A former United States Secretary of Defense, economist James R. Schlesinger, once stated: "Reliability is, after all, engineering in its most practical form."
Medical statistics deals with applications of statistics to medicine and the health sciences, including epidemiology, public health, forensic medicine, and clinical research. Medical statistics has been a recognized branch of statistics in the United Kingdom for more than 40 years but the term has not come into general use in North America, where the wider term 'biostatistics' is more commonly used. However, "biostatistics" more commonly connotes all applications of statistics to biology. Medical Statistics are a sub discipline of Statistics. "It is the science of summarizing, collecting, presenting and interpreting data in medical practice, and using them to estimate the magnitude of associations and test hypotheses. It has a central role in medical investigations. It not only provides a way of organizing information on a wider and more formal basis than relying on the exchange of anecdotes and personal experience, but also takes into account the intrinsic variation inherent in most biological processes."   
In stochastic processes, chaos theory and time series analysis, detrended fluctuation analysis (DFA) is a method for determining the statistical self-affinity of a signal. It is useful for analysing time series that appear to be long-memory processes (diverging correlation time, e.g. power-law decaying autocorrelation function) or 1/f noise. The obtained exponent is similar to the Hurst exponent, except that DFA may also be applied to signals whose underlying statistics (such as mean and variance) or dynamics are non-stationary (changing with time). It is related to measures based upon spectral techniques such as autocorrelation and Fourier transform. Peng et al. introduced DFA in 1994 in a paper that has been cited over 2000 times as of 2013 and represents an extension of the (ordinary) fluctuation analysis (FA), which is affected by non-stationarities.
Simfit is a free Open Source Windows package for simulation, curve fitting, statistics, and plotting, using a library of models or user-defined equations. Simfit has been in continuous development for many years by Bill Bardsley of the University of Manchester. Although it is written for Windows, it can easily be installed and used on Linux machines via WINE. Simfit is developed using Silverfrost Limited's FTN95 Fortran Compiler and is currently featured on their website as a showcased application. The graphical functionality in Simfit has been released as a Fortran library called Simdem which allows the programmer to produce charts and graphs with just a few lines of Fortran. A version of Simdem is shipped with the Windows version of the NAG Fortran Builder. A Spanish-language version of Simfit is maintained by a team in Salamanca.
In statistics, robust Bayesian analysis, also called Bayesian sensitivity analysis, is a type of sensitivity analysis applied to the outcome from Bayesian inference or Bayesian optimal decisions.
In statistics, a volcano plot is a type of scatter-plot that is used to quickly identify changes in large datasets composed of replicate data. It plots significance versus fold-change on the y- and x-axes, respectively. These plots are increasingly common in omic experiments such as genomics, proteomics, and metabolomics where one often has a list of many thousands of replicate datapoints between two conditions and one wishes to quickly identify the most-meaningful changes. A volcano plot combines a measure of statistical significance from a statistical test (e.g., a p-value from an ANOVA model) with the magnitude of the change enabling quick visual identification of those data-points (genes, etc.) that display large-magnitude changes that are also statistically significant. A volcano plot is constructed by plotting the negative log of the p-value on the y-axis (usually base 10). This results in datapoints with low p-values (highly significant) appearing toward the top of the plot. The x-axis is the log of the fold change between the two conditions. The log of the fold-change is used so that changes in both directions appear equidistant from the center. Plotting points in this way results in two regions of interest in the plot: those points that are found toward the top of the plot that are far to either the left- or the right-hand side. These represent values that display large magnitude fold changes (hence being left- or right- of center) as well as high statistical significance (hence being toward the top). Additional information can be added by coloring the points according to a third dimension of data (such as signal-intensity) but this is not uniformly employed. Volcano plot is also used to graphically display a significance analysis of microarrays (SAM) gene selection criterion, an example of regularization. The concept of volcano plot can be generalized to other applications, where the x-axis is related to a measure of the strength of a statistical signal, and y-axis is related to a measure of the statistical significance of the signal. For example, in a genetic association case-control study, such as Genome-wide association study, a point in a volcano plot represents a single-nucleotide polymorphism. Its x value can be the odds ratio and its y value can be -log10 of the p-value from Chi-square test or a Chi-square test statistic.
In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or even undefined. The qualitative interpretation of the skew is complicated. For a unimodal distribution, negative skew indicates that the tail on the left side of the probability density function is longer or fatter than the right side   it does not distinguish these shapes. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value indicates that the tails on both sides of the mean balance out, which is the case for a symmetric distribution, but is also true for an asymmetric distribution where the asymmetries even out, such as one tail being long but thin, and the other being short but fat. Further, in multimodal distributions and discrete distributions, skewness is also difficult to interpret. Importantly, the skewness does not determine the relationship of mean and median.
In statistics, generalized least squares (GLS) is a technique for estimating the unknown parameters in a linear regression model. GLS can be used to perform linear regression when there is a certain degree of correlation between the residuals in a regression model. In these cases, ordinary least squares and weighted least squares can be statistically inefficient, or even give misleading inferences. GLS was first described by Alexander Aitken in 1934.
The science of epidemiology has matured significantly from the times of Hippocrates, Semmelweis and John Snow. The techniques for gathering and analyzing epidemiological data vary depending on the type of disease being monitored but each study will have overarching similarities.
In signal processing, a window function (also known as an apodization function or tapering function) is a mathematical function that is zero-valued outside of some chosen interval. For instance, a function that is constant inside the interval and zero elsewhere is called a rectangular window, which describes the shape of its graphical representation. When another function or waveform/data-sequence is multiplied by a window function, the product is also zero-valued outside the interval: all that is left is the part where they overlap, the "view through the window". Applications of window functions include spectral analysis, filter design, and beamforming. In typical applications, the window functions used are non-negative, smooth, "bell-shaped" curves. Rectangle, triangle, and other functions can also be used. A more general definition of window functions does not require them to be identically zero outside an interval, as long as the product of the window multiplied by its argument is square integrable, and, more specifically, that the function goes sufficiently rapidly toward zero.  
In telecommunication and computer engineering, the queuing delay or queueing delay is the time a job waits in a queue until it can be executed. It is a key component of network delay. In a switched network, the time between the completion of signaling by the call originator and the arrival of a ringing signal at the call receiver. Queues may be caused by delays at the originating switch, intermediate switches, or the call receiver servicing switch. In a data network, the sum of the delays between the request for service and the establishment of a circuit to the called data terminal equipment (DTE). In a packet-switched network, the sum of the delays encountered by a packet between the time of insertion into the network and the time of delivery to the addressee.  This term is most often used in reference to routers. When packets arrive at a router, they have to be processed and transmitted. A router can only process one packet at a time. If packets arrive faster than the router can process them (such as in a burst transmission) the router puts them into the queue (also called the buffer) until it can get around to transmitting them. Delay can also vary from packet to packet so averages and statistics are usually generated when measuring and evaluating queuing delay.  As a queue begins to fill up due to traffic arriving faster than it can be processed, the amount of delay a packet experiences going through the queue increases. The speed at which the contents of a queue can be processed is a function of the transmission rate of the facility. This leads to the classic delay curve. The average delay any given packet is likely to experience is given by the formula 1/( - ) where   is the number of packets per second the facility can sustain and   is the average rate at which packets are arriving to be serviced.  This formula can be used when no packets are dropped from the queue. The maximum queuing delay is proportional to buffer size. The longer the line of packets waiting to be transmitted, the longer the average waiting time is. The router queue of packets waiting to be sent also introduces a potential cause of packet loss. Since the router has a finite amount of buffer memory to hold the queue, a router which receives packets at too high a rate may experience a full queue. In this case, the router has no other option than to simply discard excess packets. When the transmission protocol uses the dropped-packets symptom of filled buffers to regulate its transmit rate, as the Internet's TCP does, bandwidth is fairly shared at near theoretical capacity with minimal network congestion delays. Absent this feedback mechanism the delays become both unpredictable and rise sharply, a symptom also seen as freeways approach capacity; metered onramps are the most effective solution there, just as TCP's self-regulation is the most effective solution when the traffic is packets instead of cars). This result is both hard to model mathematically and quite counterintuitive to people who lack experience with mathematics or real networks. Failing to drop packets, choosing instead to buffer an ever-increasing number of them, produces bufferbloat. In Kendall's notation, the M/M/1/K queuing model, where K is the size of the buffer, may be used to analyze the queuing delay in a specific system. Kendall's notation should be used to calculate the queuing delay when packets are dropped from the queue. The M/M/1/K queuing model is the most basic and important queuing model for network analysis.
Wide and narrow (sometimes un-stacked and stacked) are terms used to describe two different presentations for tabular data.
A blind   or blinded   experiment is an experiment in which information about the test is masked (kept) from the participant, to reduce or eliminate bias, until after a trial outcome is known. It is understood that bias may be intentional or unconscious, thus no dishonesty is implied by blinding. If both tester and subject are blinded, the trial is called a double-blind experiment. Blind testing is used wherever items are to be compared without influences from testers' preferences or expectations, for example in clinical trials to evaluate the effectiveness of medicinal drugs and procedures without placebo effect, nocebo effect, observer bias, or conscious deception; and comparative testing of commercial products to objectively assess user preferences without being influenced by branding and other properties not being tested. Blinding can be imposed on researchers, technicians, subjects, and funders. The opposite of a blind trial is an open trial. Blind experiments are an important tool of the scientific method, in many fields of research medicine, psychology and the social sciences, natural sciences such as physics and biology, applied sciences such as market research, and many others. In some disciplines, such as medicinal drug testing, blind experiments are considered essential. In some cases, while blind experiments would be useful, they are impractical or unethical; an example is in the field of developmental psychology: although it would be informative to raise children under arbitrary experimental conditions, such as on a remote island with a fabricated enculturation, it is a violation of ethics and human rights. The terms blind (adjective) or to blind (transitive verb) when used in this sense are figurative extensions of the literal idea of blindfolding someone. The terms masked or to mask may be used for the same concept; this is commonly the case in ophthalmology, where the word 'blind' is often used in the literal sense.
In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset. In application to image segmentation, spectral clustering is known as segmentation-based object categorization.
Non-response bias occurs in statistical surveys if the answers of respondents differ from the potential answers of those who did not answer. It may occur due to several factors as outlined in Deming (1990).
Bayesian tool for methylation analysis, also known as BATMAN, is a statistical tool for analyzing methylated DNA immunoprecipitation (MeDIP) profiles. It can be applied to large datasets generated using either oligonucleotide arrays (MeDIP-chip) or next-generation sequencing (MeDIP-seq), providing a quantitative estimation of absolute methylation state in a region of interest.
In statistical mechanics, the Fokker Planck equation is a partial differential equation that describes the time evolution of the probability density function of the velocity of a particle under the influence of drag forces and random forces, as in Brownian motion. The equation can be generalized to other observables as well. It is named after Adriaan Fokker and Max Planck and is also known as the Kolmogorov forward equation (diffusion), named after Andrey Kolmogorov, who first introduced it in a 1931 paper. When applied to particle position distributions, it is better known as the Smoluchowski equation (after Marian Smoluchowski). The case with zero diffusion is known in statistical mechanics as the Liouville equation. The first consistent microscopic derivation of the Fokker Planck equation in the single scheme of classical and quantum mechanics was performed by Nikolay Bogoliubov and Nikolay Krylov. The Smoluchowski equation is the Fokker Planck equation for the probability density function of the particle positions of Brownian particles.
Local convex hull (LoCoH) is a method for estimating size of the home range of an animal or a group of animals (e.g. a pack of wolves, a pride of lions, or herd of buffaloes), and for constructing a utilization distribution.  The latter is a probability distribution that represents the probabilities of finding an animal within a given area of its home range at any point in time; or, more generally, at points in time for which the utilization distribution has been constructed. In particular, different utilization distributions can be constructed from data pertaining to particular periods of a diurnal or seasonal cycle. Utilization distributions are constructed from data providing the location of an individual or several individuals in space at different points in time by associating a local distribution function with each point and then summing and normalizing these local distribution functions to obtain a distribution function that pertains to the data as a whole. If the local distribution function is a parametric distribution, such as a symmetric bivariate normal distribution then the method is referred to as a kernel method, but more correctly should be designated as a parametric kernel method. On the other hand, if the local kernel element associated with each point is a local convex polygon constructed from the point and its k-1 nearest neighbors, then the method is nonparametric and referred to as a k-LoCoH or fixed point LoCoH method. This is in contrast to r-LoCoH (fixed radius) and a-LoCoH (adaptive radius) methods. In the case of LoCoH utilization distribution constructions, the home range can be taken as the outer boundary of the distribution (i.e. the 100th percentile). In the case of utilization distributions constructed from unbounded kernel elements, such as bivariate normal distributions, the utilization distribution is itself unbounded. In this case the most often used convention is to regard the 95th percentile of the utilization distribution as the boundary of the home range. To construct a k-LoCoH utilization distribution: Locate the k   1 nearest neighbors for each point in the dataset. Construct a convex hull for each set of nearest neighbors and the original data point. Merge these hulls together from smallest to largest. Divide the merged hulls into isopleths where the 10% isopleth contains 10% of the original data points, the 100% isopleth contains all the points, etc. In this sense, LoCoH methods are a generalization of the home range estimator method based on constructing the minimum convex polygon (MCP) associated with the data. The LoCoH method has a number of advantages over parametric kernel methods. In particular: As more data are added, the estimates of the home range become more accurate than for bivariate normal kernel constructions. LoCoH handles 'sharp' features such as lakes and fences much better than parametric kernel constructions. As mentioned above, the home range is a finite region without having to resort to an ad-hoc choice, such as the 95th percentile to obtain bounded region. LoCoH has a number of implementations including a LoCoH Web Application. LoCoH was formerly known as k-NNCH, for k-nearest neighbor convex hulls. It has recently been shown that the a-LoCoH is the best of the three LoCoH methods mentioned above (see Getz et al. in the references below).
A spaghetti plot (also known as a spaghetti chart, spaghetti diagram, or spaghetti model) is a method of viewing data to visualize possible flows through systems. Flows depicted in this manner appear like noodles, hence the coining of this term. This method of statistics was first used to track routing through factories. Visualizing flow in this manner can reduce inefficiency within the flow of a system. In regards to animal populations and weather buoys drifting through the ocean, they are drawn to study distribution and migration patterns. Within meteorology, these diagrams can help determine confidence in a specific weather forecast, as well as positions and intensities of high and low pressure systems. They are composed of deterministic forecasts from atmospheric models or their various ensemble members. Within medicine, they can illustrate the effects of drugs on patients during drug trials.
In probability theory, Eaton's inequality is a bound on the largest values of a linear combination of bounded random variables. This inequality was described in 1974 by Morris L. Eaton.
In probability theory, the rule of succession is a formula introduced in the 18th century by Pierre-Simon Laplace in the course of treating the sunrise problem. The formula is still used, particularly to estimate underlying probabilities when there are few observations, or for events that have not been observed to occur at all in (finite) sample data. Assigning events a zero probability contravenes Cromwell's rule, which can never be strictly justified in physical situations, albeit sometimes must be assumed in practice.
Exact statistics, such as that described in exact test, is a branch of statistics that was developed to provide more accurate results pertaining to statistical testing and interval estimation by eliminating procedures based on asymptotic and approximate statistical methods. The main characteristic of exact methods is that statistical tests and confidence intervals are based on exact probability statements that are valid for any sample size. Exact statistical methods help avoid some of the unreasonable assumptions of traditional statistical methods, such as the assumption of equal variances in classical ANOVA. They also allow exact inference on variance components of mixed models. When exact p-values and confidence intervals are computed under a certain distribution, such as the normal distribution, then the underlying methods are referred to as exact parametric methods. The exact methods that do not make any distributional assumptions are referred to as exact nonparametric methods. The latter has the advantage of making fewer assumptions whereas, the former tend to yield more powerful tests when the distributional assumption is reasonable. For advanced methods such as higher-way ANOVA regression analysis, and mixed models, only exact parametric methods are available. When the sample size is small, asymptotic results given by some traditional methods may not be valid. In such situations, the asymptotic p-values may differ substantially from the exact p-values. Hence asymptotic and other approximate results may lead to unreliable and misleading conclusions.
A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if  parent nodes represent  Boolean variables then the probability function could be represented by a table of  entries, one entry for each of the  possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks. Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
The Hoover index, also known as the Robin Hood index, but better known as the Schutz index, is a measure of income metrics. It is equal to the portion of the total community income that would have to be redistributed (taken from the richer half of the population and given to the poorer half) for there to be income uniformity. It can be graphically represented as the longest vertical distance between the Lorenz curve, or the cumulative portion of the total income held below a certain income percentile, and the 45 degree line representing perfect equality. The Hoover index is typically used in applications related to socio-economic class (SES) and health. It is conceptually one of the simplest inequality indices used in econometrics. A better known inequality measure is the Gini coefficient which is also based on the Lorenz curve.
Forecasting is the process of making predictions of the future based on past and present data and analysis of trends. A commonplace example might be estimation of some variable of interest at some specified future date. Prediction is a similar, but more general term. Both might refer to formal statistical methods employing time series, cross-sectional or longitudinal data, or alternatively to less formal judgmental methods. Usage can differ between areas of application: for example, in hydrology, the terms "forecast" and "forecasting" are sometimes reserved for estimates of values at certain specific future times, while the term "prediction" is used for more general estimates, such as the number of times floods will occur over a long period. Risk and uncertainty are central to forecasting and prediction; it is generally considered good practice to indicate the degree of uncertainty attaching to forecasts. In any case, the data must be up to date in order for the forecast to be as accurate as possible.
The so-called Berlin procedure (BV) is a mathematical procedure for time series decomposition and seasonal adjustment of monthly and quarterly economic time series. The mathematical foundations of the procedure were developed in 1960's at the Technical University of Berlin and the German Institute for Economic Research (DIW). The most important user of the procedure is the Federal Statistical Office of Germany. For the latest version 4.1 of BV a BV4.1 software is available as freeware for non-commercial purposes.
In statistical significance testing, a one-tailed test and a two-tailed test are alternative ways of computing the statistical significance of a parameter inferred from a data set, in terms of a test statistic. A two-tailed test is used if deviations of the estimated parameter in either direction from some benchmark value are considered theoretically possible; in contrast, a one-tailed test is used if only deviations in one direction are considered possible. Alternative names are one-sided and two-sided tests; the terminology "tail" is used because the extreme portions of distributions, where observations lead to rejection of the null hypothesis, are small and often "tail off" toward zero as in the normal distribution or "bell curve", pictured above right.
In linguistics, variable rules analysis is a set of statistical analysis methods commonly used in sociolinguistics and historical linguistics to describe patterns of variation between alternative forms in language use. It is also sometimes known as Varbrul analysis, after the name of a software package dedicated to carrying out the relevant statistical computations (Varbrul, from "variable rule".) The method goes back to a theoretical approach developed by the sociolinguist William Labov in the late 1960s and early 1970s, and its mathematical implementation was developed by Henrietta Cedergren and David Sankoff in 1974. A variable rules analysis is designed to provide a quantitative model of a situation where speakers alternate between different forms that have the same meaning and stand in free variation, but in such a way that the probability of choice of either the one or the other form is conditioned by a variety of context factors or social characteristics. Such a situation, where variation is not entirely random but rule-governed, is also known as "structured variation". A variable rules analysis computes a multivariate statistical model, on the basis of observed token counts, such that each determining factor is assigned a numerical factor weight that describes how it influences the probabilities of choice of either form. This is done by means of stepwise logistic regression, using a maximum likelihood algorithm. Although the necessary computations required for a variable rules analysis can be carried out with the help of mainstream general-purpose statistics software packages such as SPSS, it is more often done by means of a specialised software dedicated to the needs of linguists, called Varbrul. It was originally written by David Sankoff and currently exists in freeware implementations for Mac OS and Microsoft Windows, under the title of Goldvarb X. There are also versions implemented in the statistical language R and therefore available on most platforms. These include R-Varb and Rbrul. Variable rules approaches are commonly employed for the analysis of data in sociolinguistic research, especially in studies that aim to investigate how reflexes of linguistic change through time appear in the shape of structured variation patterns within a speech community.
A quasi-maximum likelihood estimate (QMLE, also known as a "pseudo-likelihood estimate" or a "composite likelihood estimate") is an estimate of a parameter   in a statistical model that is formed by maximizing a function that is related to the logarithm of the likelihood function, but is not equal to it. In contrast, the maximum likelihood estimate maximizes the actual log likelihood function for the data and model. The function that is maximized to form a QMLE is often a simplified form of the actual log likelihood function. A common way to form such a simplified function is to use the log-likelihood function of a misspecified model that treats certain data values as being independent, even when in actuality they may not be. This removes any parameters from the model that are used to characterize these dependencies. Doing this only makes sense if the dependency structure is a nuisance parameter with respect to the goals of the analysis. As long as the quasi-likelihood function that is maximized is not oversimplified, the QMLE (or composite likelihood estimate) is consistent and asymptotically normal. It is less efficient than the maximum likelihood estimate, but may only be slightly less efficient if the quasi-likelihood is constructed so as to minimize the loss of information relative to the actual likelihood. Standard approaches to statistical inference that are used with maximum likelihood estimates, such as the formation of confidence intervals, and statistics for model comparison, can be generalized to the quasi-maximum likelihood setting.
GraphPad Prism is a commercial scientific 2D graphing and statistics software published by GraphPad Software, Inc., a privately held California corporation. Prism is available for both Windows and Macintosh computers. Its main free software competitors are SciDAVis and interfaces to R.
In probability theory relating to stochastic processes, a Feller process is a particular kind of Markov process.
Data binning or bucketing is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall in a given small interval, a bin, are replaced by a value representative of that interval, often the central value. It is a form of quantization. Statistical data binning is a way to group a number of more or less continuous values into a smaller number of "bins". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals.  It can also be used in multivariate statistics, binning in several dimensions at once.
In statistics, a matrix gamma distribution is a generalization of the gamma distribution to positive-definite matrices. It is a more general version of the Wishart distribution, and is used similarly, e.g. as the conjugate prior of the precision matrix of a multivariate normal distribution and matrix normal distribution. The compound distribution resulting from compounding a matrix normal with a matrix gamma prior over the precision matrix is a generalized matrix t-distribution. This reduces to the Wishart distribution with   
In probability theory and statistics, a categorical distribution (also called a "generalized Bernoulli distribution", "multinoulli distribution"  or, less precisely, a "discrete distribution") is a probability distribution that describes the possible results of a random event that can take on one of K possible outcomes, with the probability of each outcome separately specified. There is not necessarily an underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution, (e.g. 1 to K). Note that the K-dimensional categorical distribution is the most general distribution over a K-way event; any other discrete distribution over a size-K sample space is a special case. The parameters specifying the probabilities of each possible outcome are constrained only by the fact that each must be in the range 0 to 1, and all must sum to 1. The categorical distribution is the generalization of the Bernoulli distribution for a categorical random variable, i.e. for a discrete variable with more than two possible outcomes, such as the roll of a die.
In statistics, inter-rater reliability, inter-rater agreement, or concordance is the degree of agreement among raters. It gives a score of how much homogeneity, or consensus, there is in the ratings given by judges. It is useful in refining the tools given to human judges, for example by determining if a particular scale is appropriate for measuring a particular variable. If various raters do not agree, either the scale is defective or the raters need to be re-trained. There are a number of statistics which can be used to determine inter-rater reliability. Different statistics are appropriate for different types of measurement. Some options are: joint-probability of agreement, Cohen's kappa and the related Fleiss' kappa, inter-rater correlation, concordance correlation coefficient and intra-class correlation.
In mathematics, a moment is a specific quantitative measure, used in both mechanics and statistics, of the shape of a set of points. If the points represent mass, then the zeroth moment is the total mass, the first moment divided by the total mass is the center of mass, and the second moment is the rotational inertia. If the points represent probability density, then the zeroth moment is the total probability (i.e. one), the first moment is the mean, the second central moment is the variance, the third moment is the skewness, and the fourth moment (with normalization and shift) is the kurtosis. The mathematical concept is closely related to the concept of moment in physics. For a bounded distribution of mass or probability, the collection of all the moments (of all orders, from 0 to  ) uniquely determines the distribution.
In electronics, control systems engineering, and statistics, the frequency domain refers to the analysis of mathematical functions or signals with respect to frequency, rather than time. Put simply, a time-domain graph shows how a signal changes over time, whereas a frequency-domain graph shows how much of the signal lies within each given frequency band over a range of frequencies. A frequency-domain representation can also include information on the phase shift that must be applied to each sinusoid in order to be able to recombine the frequency components to recover the original time signal. A given function or signal can be converted between the time and frequency domains with a pair of mathematical operators called a transform. An example is the Fourier transform, which converts the time function into a sum of sine waves of different frequencies, each of which represents a frequency component. The 'spectrum' of frequency components is the frequency domain representation of the signal. The inverse Fourier transform converts the frequency domain function back to a time function. A spectrum analyzer is the tool commonly used to visualize real-world signals in the frequency domain. Some specialized signal processing techniques use transforms that result in a joint time-frequency domain, with the instantaneous frequency being a key link between the time domain and the frequency domain.
Natural process variation, sometimes just called process variation, is the statistical description of natural fluctuations in process outputs.
In statistics and image processing, to smooth a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In smoothing, the data points of a signal are modified so individual points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Smoothing may be used in two important ways that can aid in data analysis (1) by being able to extract more information from the data as long as the assumption of smoothing is reasonable and (2) by being able to provide analyses that are both flexible and robust. Many different algorithms are used in smoothing. Smoothing may be distinguished from the related and partially overlapping concept of curve fitting in the following ways: curve fitting often involves the use of an explicit function form for the result, whereas the immediate results from smoothing are the "smoothed" values with no later use made of a functional form if there is one; the aim of smoothing is to give a general idea of relatively slow changes of value with little attention paid to the close matching of data values, while curve fitting concentrates on achieving as close a match as possible. smoothing methods often have an associated tuning parameter which is used to control the extent of smoothing. Curve fitting will adjust any number of parameters of the function to obtain the 'best' fit. However, the terminology used across applications is mixed. For example, use of an interpolating spline fits a smooth curve exactly through the given data points and is sometimes called "smoothing".
In probability and statistics, density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population. A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram.
A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be presented as the simplest dynamic Bayesian network. The mathematics behind the HMM were developed by L. E. Baum and coworkers. It is closely related to an earlier work on the optimal nonlinear filtering problem by Ruslan L. Stratonovich, who was the first to describe the forward-backward procedure. In simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters. In a hidden Markov model, the state is not directly visible, but the output, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states. The adjective 'hidden' refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a 'hidden' Markov model even if these parameters are known exactly. Hidden Markov models are especially known for their application in temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. A hidden Markov model can be considered a generalization of a mixture model where the hidden variables (or latent variables), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. Recently, hidden Markov models have been generalized to pairwise Markov models and triplet Markov models which allow consideration of more complex data structures  and the modelling of nonstationary data.
Fisher's inequality, is a necessary condition for the existence of a balanced incomplete block design which satisfies certain prescribed conditions in combinatorial mathematics. Outlined by Ronald Fisher, a population geneticist and statistician, it concerns with the design of experiments studying the differences among several different varieties of plants, under each of a number of different growing conditions, called "blocks". Let: v be the number of varieties of plants; b be the number of blocks. It was required that: k different varieties are in each block, k < v; no variety occurs twice in any one block; any two varieties occur together in exactly   blocks; each variety occurs in exactly r blocks. Fisher's inequality states simply that
In statistics, the Wishart distribution is a generalization to multiple dimensions of the chi-squared distribution, or, in the case of non-integer degrees of freedom, of the gamma distribution. It is named in honor of John Wishart, who first formulated the distribution in 1928. It is a family of probability distributions defined over symmetric, nonnegative-definite matrix-valued random variables ( random matrices ). These distributions are of great importance in the estimation of covariance matrices in multivariate statistics. In Bayesian statistics, the Wishart distribution is the conjugate prior of the inverse covariance-matrix of a multivariate-normal random-vector.
An N of 1 trial is a clinical trial in which a single patient is the entire trial, a single case study. A trial in which random allocation can be used to determine the order in which an experimental and a control intervention are given to a patient is an N of 1 randomized controlled trial. The order of experimental and control interventions can also be fixed by the researcher. This type of study has enabled practitioners to achieve experimental progress without the overwhelming work of designing a group comparison study. It can be very effective in confirming causality. This can be achieved in many ways. One of the most common procedures is the ABA withdrawal experimental design, where the patient problem is measured before a treatment is introduced (baseline) and then measured again during the treatment and finally when the treatment has terminated. If the problem vanished during the treatment it can be established that the treatment was effective. But the N=1 study can also be executed in an AB quasi experimental way; this means that causality cannot be definitively demonstrated. Another variation is non-concurrent experimental design where different points in time are compared with one another. This experimental design also has a problem with causality. But the replication of studies has a great value and can be as effective as a group study. The single case experimental design has been very effective in psychology and other field of behavior science.
Statistical regularity is a notion in statistics and probability theory that random events exhibit regularity when repeated enough times or that enough sufficiently similar random events exhibit regularity. It is an umbrella term that covers the law of large numbers, all central limit theorems and ergodic theorems. If one throws a die once, it is difficult to predict the outcome, but if we repeat this experiment many times, we will see that the number of times each result occurs divided by the number of throws will eventually stabilize towards a specific value. Repeating a series of trials will produce similar, but not identical, results for each series: the average, the standard deviation and other distributional characteristics will be around the same for each series of trials. The notion is used in games of chance, demographic statistics, quality control of a manufacturing process, and in many other parts of our lives. Observations of this phenomenon provided the initial motivation for the concept of what is now known as frequency probability. This phenomenon should not be confused with the Gambler's fallacy, it only concerns regularity in the (possibly very) long run. Gambler's fallacy does not apply to statistical regularity because the latter considers the whole rather than individual cases.
In statistics, the RV coefficient is a multivariate generalization of the squared Pearson correlation coefficient (because the RV coefficient takes values between 0 and 1). It measures the closeness of two set of points that may each be represented in a matrix. The major approaches within statistical multivariate data analysis can all be brought into a common framework in which the RV coefficient is maximised subject to relevant constraints. Specifically, these statistical methodologies include:  principal component analysis canonical correlation analysis multivariate regression statistical classification (linear discrimination).  One application of the RV coefficient is in functional neuroimaging where it can measure the similarity between two subjects' series of brain scans or between different scans of a same subject.
In statistics, an F-test for the null hypothesis that two normal populations have the same variance is sometimes used, although it needs to be used with caution as it can be sensitive to the assumption that the variables have this distribution. Notionally, any F-test can be regarded as a comparison of two variances, but the specific case being discussed in this article is that of two populations, where the test statistic used is the ratio of two sample variances. This particular situation is of importance in mathematical statistics since it provides a basic exemplar case in which the F-distribution can be derived. For application in applied statistics, there is concern that the test is so sensitive to the assumption of normality that it would be inadvisable to use it as a routine test for the equality of variances. In other words, this is a case where "approximate normality" (which in similar contexts would often be justified using the central limit theorem), is not good enough to make the test procedure approximately valid to an acceptable degree.
In the statistical analysis of the results from factorial experiments, the sparsity-of-effects principle states that a system is usually dominated by main effects and low-order interactions. Thus it is most likely that main (single factor) effects and two-factor interactions are the most significant responses in a factorial experiment. In other words, higher order interactions such as three-factor interactions are very rare. This is sometimes referred to as the hierarchical ordering principle. The sparsity-of-effects principle actually refers to the idea that only a few effects in a factorial experiment will be statistically significant. This principle is only valid on the assumption of a factor space far from a stationary point.
In Bayesian statistics, a hyperprior is a prior distribution on a hyperparameter, that is, on a parameter of a prior distribution. As with the term hyperparameter, the use of hyper is to distinguish it from a prior distribution of a parameter of the model for the underlying system. They arise particularly in the use of conjugate priors. For example, if one is using a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: The Bernoulli distribution (with parameter p) is the model of the underlying system; p is a parameter of the underlying system (Bernoulli distribution); The beta distribution (with parameters   and  ) is the prior distribution of p;   and   are parameters of the prior distribution (beta distribution), hence hyperparameters; A prior distribution of   and   is thus a hyperprior. In principle, one can iterate the above: if the hyperprior itself has hyperparameters, these may be called hyperhyperparameters, and so forth. One can analogously call the posterior distribution on the hyperparameter the hyperposterior, and, if these are in the same family, call them conjugate hyperdistributions or a conjugate hyperprior. However, this rapidly becomes very abstract and removed from the original problem.
In statistics, a confounding variable (also confounding factor, a confound, or confounder) is an extraneous variable in a statistical model that correlates (directly or inversely) with both the dependent variable and the independent variable. A spurious relationship is a perceived relationship between an independent variable and a dependent variable that has been estimated incorrectly because the estimate fails to account for a confounding factor. The incorrect estimation suffers from omitted-variable bias. While specific definitions may vary, in essence a confounding variable fits the following four criteria, here given in a hypothetical situation with variable of interest "V", confounding variable "C" and outcome of interest "O": C is associated (inversely or directly) with O C is associated with O, independent of V C is associated (inversely or directly) with V C is not in the causal pathway of V to O (C is not a direct consequence of V, not a way by which V produces O) The preceding correlation-based definition, however, is metaphorical at best   a growing number of analysts agree that confounding is a causal concept, and as such, cannot be described in terms of correlations nor associations  (see causal definition).  
The observer-expectancy effect (also called the experimenter-expectancy effect, expectancy bias, observer effect, or experimenter effect) is a form of reactivity in which a researcher's cognitive bias causes them to subconsciously influence the participants of an experiment. Confirmation bias can lead to the experimenter interpreting results incorrectly because of the tendency to look for information that conforms to their hypothesis, and overlook information that argues against it. It is a significant threat to a study's internal validity, and is therefore typically controlled using a double-blind experimental design. An example of the observer-expectancy effect is demonstrated in music backmasking, in which hidden verbal messages are said to be audible when a recording is played backwards. Some people expect to hear hidden messages when reversing songs, and therefore hear the messages, but to others it sounds like nothing more than random sounds. Often when a song is played backwards, a listener will fail to notice the "hidden" lyrics until they are explicitly pointed out, after which they are obvious. Other prominent examples include facilitated communication and dowsing. In research, experimenter bias occurs when experimenter expectancies regarding study results bias the research outcome. Examples of experimenter bias include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.
The chain rule for Kolmogorov complexity is an analogue of the chain rule for information entropy, which states:  That is, the combined randomness of two sequences X and Y is the sum of the randomness of X plus whatever randomness is left in Y once we know X. This follows immediately from the definitions of conditional and joint entropy fact from probability theory that the joint probability is the product of the marginal and conditional probability:  The equivalent statement for Kolmogorov complexity does not hold exactly; it is true only up to a logarithmic term:  (An exact version, KP(x, y) = KP(x) + KP(y|x*) + O(1), holds for the prefix complexity KP, where x* is a shortest program for x.) It states that the shortest program printing X and Y is obtained by concatenating a shortest program printing X with a program printing Y given X, plus at most a logarithmic factor. The results implies that algorithmic mutual information, an analogue of mutual information for Kolmogorov complexity is symmetric: I(x:y) = I(y:x) + O(log K(x,y)) for all x,y.
In statistics, a full factorial experiment is an experiment whose design consists of two or more factors, each with discrete possible values or "levels", and whose experimental units take on all possible combinations of these levels across all such factors. A full factorial design may also be called a fully crossed design. Such an experiment allows the investigator to study the effect of each factor on the response variable, as well as the effects of interactions between factors on the response variable. For the vast majority of factorial experiments, each factor has only two levels. For example, with two factors each taking two levels, a factorial experiment would have four treatment combinations in total, and is usually called a 2 2 factorial design. If the number of combinations in a full factorial design is too high to be logistically feasible, a fractional factorial design may be done, in which some of the possible combinations (usually at least half) are omitted.
TPL Tables is a cross tabulation system used to generate statistical tables for analysis or publication.
In statistics, omitted-variable bias (OVB) occurs when a model is created which incorrectly leaves out one or more important factors. The "bias" is created when the model compensates for the missing factor by over- or underestimating the effect of one of the other factors. More specifically, OVB is the bias that appears in the estimates of parameters in a regression analysis, when the assumed specification is incorrect in that it omits an independent variable that is correlated with both the dependent variable and one or more included independent variables.
In probability theory and statistics, the normal-gamma distribution (or Gaussian-gamma distribution) is a bivariate four-parameter family of continuous probability distributions. It is the conjugate prior of a normal distribution with unknown mean and precision.
Empirical evidence, also known as sense experience, is a collective term for the knowledge or source of knowledge acquired by means of the senses, particularly by observation and experimentation. The term comes from the Greek word for experience,            (empeiri a). After Immanuel Kant, it is common in philosophy to call the knowledge thus gained a posteriori knowledge. This is contrasted with a priori knowledge.  
In econometrics, ridit scoring is a statistical method used to analyze ordered qualitative measurements. The tools of ridit analysis were developed and first applied by Bross, who coined the term "ridit" by analogy with other statistical transformations such as probit and logit.
Gy's sampling theory is a theory about the sampling of materials, developed by Pierre Gy from the 1950s to beginning 2000s in articles and books including: (1960) Sampling nomogram (1979) Sampling of particulate materials; theory and practice (1982) Sampling of particulate materials; theory and practice; 2nd edition (1992) Sampling of Heterogeneous and Dynamic Material Systems: Theories of Heterogeneity, Sampling and Homogenizing (1998) Sampling for Analytical Purposes The abbreviation "TOS" is also used to denote Gy's sampling theory. Gy's sampling theory uses a model in which the sample taking is represented by independent Bernoulli trials for every particle in the parent population from which the sample is drawn. The two possible outcomes of each Bernoulli trial are: (1) the particle is selected and (2) the particle is not selected. The probability of selecting a particle may be different during each Bernoulli trial. The model used by Gy is mathematically equivalent to Poisson sampling. Using this model, the following equation for the variance of the sampling error in the mass concentration in a sample was derived by Gy:  in which V is the variance of the sampling error, N is the number of particles in the population (before the sample was taken), q i is the probability of including the ith particle of the population in the sample (i.e. the first-order inclusion probability of the ith particle), m i is the mass of the ith particle of the population and a i is the mass concentration of the property of interest in the ith particle of the population. It is noted that the above equation for the variance of the sampling error is an approximation based on a linearization of the mass concentration in a sample. In the theory of Gy, correct sampling is defined as a sampling scenario in which all particles have the same probability of being included in the sample. This implies that q i no longer depends on i, and can therefore be replaced by the symbol q. Gy's equation for the variance of the sampling error becomes:  where abatch is the concentration of the property of interest in the population from which the sample is to be drawn and Mbatch is the mass of the population from which the sample is to be drawn. It has been noted that a similar equation had already been derived in 1935 by Kassel and Guy.
Noise reduction is the process of removing noise from a signal. All recording devices, both analog and digital, have traits that make them susceptible to noise. Noise can be random or white noise with no coherence, or coherent noise introduced by the device's mechanism or processing algorithms. In electronic recording devices, a major form of noise is hiss caused by random electrons that, heavily influenced by heat, stray from their designated path. These stray electrons influence the voltage of the output signal and thus create detectable noise. In the case of photographic film and magnetic tape, noise (both visible and audible) is introduced due to the grain structure of the medium. In photographic film, the size of the grains in the film determines the film's sensitivity, more sensitive film having larger sized grains. In magnetic tape, the larger the grains of the magnetic particles (usually ferric oxide or magnetite), the more prone the medium is to noise. To compensate for this, larger areas of film or magnetic tape may be used to lower the noise to an acceptable level.
In probability theory, a probability space or a probability triple is a mathematical construct that models a real-world process (or "experiment") consisting of states that occur randomly. A probability space is constructed with a specific kind of situation or experiment in mind. One proposes that each time a situation of that kind arises, the set of possible outcomes is the same and the probabilities are also the same. A probability space consists of three parts: A sample space, , which is the set of all possible outcomes. A set of events , where each event is a set containing zero or more outcomes. The assignment of probabilities to the events; that is, a function  from events to probabilities. An outcome is the result of a single execution of the model. Since individual outcomes might be of little practical use, more complex events are used to characterize groups of outcomes. The collection of all such events is a  -algebra . Finally, there is a need to specify each event's likelihood of happening. This is done using the probability measure function, . Once the probability space is established, it is assumed that  nature  makes its move and selects a single outcome, , from the sample space . All the events in  that contain the selected outcome  (recall that each event is a subset of ) are said to  have occurred . The selection performed by nature is done in such a way that if the experiment were to be repeated an infinite number of times, the relative frequencies of occurrence of each of the events would coincide with the probabilities prescribed by the function . The Russian mathematician Andrey Kolmogorov introduced the notion of probability space, together with other axioms of probability, in the 1930s. Nowadays alternative approaches for axiomatization of probability theory exist; see  Algebra of random variables , for example. This article is concerned with the mathematics of manipulating probabilities. The article probability interpretations outlines several alternative views of what "probability" means and how it should be interpreted. In addition, there have been attempts to construct theories for quantities that are notionally similar to probabilities but do not obey all their rules; see, for example, Free probability, Fuzzy logic, Possibility theory, Negative probability and Quantum probability.  
In the statistical theory of estimation, the German tank problem involves estimating the maximum of a discrete uniform distribution from sampling without replacement, due to its application in World War II to the estimation of the number of German tanks. This analysis shows the approach that was used and illustrates the difference between frequentist inference and Bayesian inference. Estimating the population maximum based on a single sample yields divergent results, while the estimation based on multiple samples is an instructive practical estimation question whose answer is simple but not obvious.
Pareto interpolation is a method of estimating the median and other properties of a population that follows a Pareto distribution. It is used in economics when analysing the distribution of incomes in a population, when one must base estimates on a relatively small random sample taken from the population. The family of Pareto distributions is parameterized by a positive number   that is the smallest value that a random variable with a Pareto distribution can take. As applied to distribution of incomes,   is the lowest income of any person in the population; and a positive number   the "Pareto index"; as this increases, the tail of the distribution gets thinner. As applied to distribution of incomes, this means that the larger the value of the Pareto index   the smaller the proportion of incomes many times as big as the smallest incomes. Pareto interpolation can be used when the available information includes the proportion of the sample that falls below each of two specified numbers a < b. For example, it may be observed that 45% of individuals in the sample have incomes below a = $35,000 per year, and 55% have incomes below b = $40,000 per year. Let Pa = proportion of the sample that lies below a; Pb = proportion of the sample that lies below b. Then the estimates of   and   are  and  The estimate of the median would then be  since the actual population median is
A one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year. The 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.
Neyman construction is a frequentist method to construct an interval at a confidence level  that if we repeat the experiment many times the interval will contain the true value a fraction  of the time.
In queueing theory, a discipline within the mathematical theory of probability, an D/M/1 queue represents the queue length in a system having a single server, where arrivals occur at fixed regular intervals and job service requirements are random with an exponential distribution. The model name is written in Kendall's notation. Agner Krarup Erlang first published a solution to the stationary distribution of a D/M/1 and D/M/k queue, the model with k servers, in 1917 and 1920.
In statistics, Samuelson's inequality, named after the economist Paul Samuelson, also called the Laguerre Samuelson inequality, after the mathematician Edmond Laguerre, states that every one of any collection x1, ..., xn, is within  (n   1) sample standard deviations of their sample mean.
In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set "uncorrelated" principle variables. It can be divided into feature selection and feature extraction.
In mathematics, Sazonov's theorem, named after Vyacheslav Vasilievich Sazonov (                              ), is a theorem in functional analysis. It states that a bounded linear operator between two Hilbert spaces is  -radonifying if it is Hilbert Schmidt. The result is also important in the study of stochastic processes and the Malliavin calculus, since results concerning probability measures on infinite-dimensional spaces are of central importance in these fields. Sazonov's theorem also has a converse: if the map is not Hilbert Schmidt, then it is not  -radonifying.
In physics, the ARGUS distribution, named after the particle physics experiment ARGUS, is the probability distribution of the reconstructed invariant mass of a decayed particle candidate in continuum background.  
The grand mean is the mean of the means of several subsamples, as long as the subsamples have the same number of data points. For example, consider several lots, each containing several items. The items from each lot are sampled for a measure of some variable and the means of the measurements from each lot are computed. The mean of the measures from each lot constitutes the subsample mean. The mean of these subsample means is then the grand mean.
In economics, Knightian uncertainty is risk that is immeasurable, not possible to calculate. Knightian uncertainty is named after University of Chicago economist Frank Knight (1885 1972), who distinguished risk and uncertainty in his work Risk, Uncertainty, and Profit: "Uncertainty must be taken in a sense radically distinct from the familiar notion of Risk, from which it has never been properly separated.... The essential fact is that 'risk' means in some cases a quantity susceptible of measurement, while at other times it is something distinctly not of this character; and there are far-reaching and crucial differences in the bearings of the phenomena depending on which of the two is really present and operating.... It will appear that a measurable uncertainty, or 'risk' proper, as we shall use the term, is so far different from an unmeasurable one that it is not in effect an uncertainty at all."
In statistics, econometrics, and related fields, multidimensional analysis is a data analysis process that groups data into two categories: data dimensions and measurements. For example, a data set consisting of the number of wins for a single football team at each of several years is a single-dimensional (in this case, longitudinal) data set. A data set consisting of the number of wins for several football teams in a single year is also a single-dimensional (in this case, cross-sectional) data set. A data set consisting of the number of wins for several football teams over several years is a two-dimensional data set. In many disciplines, two-dimensional data sets are also called panel data. While, strictly speaking, two- and higher- dimensional data sets are "multi-dimensional," the term "multidimensional" tends to be applied only to data sets with three or more dimensions. For example, some forecast data sets provide forecasts for multiple target periods, conducted by multiple forecasters, and made at multiple horizons. The three dimensions provide more information than can be gleaned from two dimensional panel data sets.
The Hosmer Lemeshow test is a statistical test for goodness of fit for logistic regression models. It is used frequently in risk prediction models. The test assesses whether or not the observed event rates match expected event rates in subgroups of the model population. The Hosmer Lemeshow test specifically identifies subgroups as the deciles of fitted risk values. Models for which expected and observed event rates in subgroups are similar are called well calibrated. The Hosmer Lemeshow test statistic is given by:  Here O1g, E1g, O0g, E0g, Ng, and  g denote the observed Y=1 events, expected Y=1 events, observed Y=0 events, expected Y=0 events, total observations, predicted risk for the gth risk decile group, and G is the number of groups. The test statistic asymptotically follows a  distribution with G   2 degrees of freedom. The number of risk groups may be adjusted depending on how many fitted risks are determined by the model. This helps to avoid singular decile groups.
Pensim2 is a dynamic microsimulation model to simulate the income of pensioners, owned by the British Department for Work and Pensions. Pensim2 is the second version of Pensim which was developed in the 1990s. The time horizon of the model is 100 years, by which time today's school leavers will retire. Pensim2 uses a lot of external alignment figures. Pensim2 uses data from different data sources, e.g. BHPS, LLMDB and the Family Resources Survey.
Face validity is the extent to which a test is subjectively viewed as covering the concept it purports to measure. It refers to the transparency or relevance of a test as it appears to test participants. In other words, a test can be said to have face validity if it "looks like" it is going to measure what it is supposed to measure. For instance, if a test is prepared to measure whether students can perform multiplication, and the people to whom it is shown all agree that it looks like a good test of multiplication ability, this demonstrates face validity of the test. Face validity is often contrasted with content validity and construct validity. Some people use the term face validity to refer only to the validity of a test to observers who are not expert in testing methodologies. For instance, if a test is designed to measure whether children are good spellers, and parents are asked whether the test is a good test, this measures the face validity of the test. If an expert is asked instead, some people would argue that this does not measure face validity. This distinction seems too careful for most applications. Generally, face validity means that the test "looks like" it will work, as opposed to "has been shown to work".
Expectation propagation (EP) is a technique in Bayesian machine learning. EP finds approximations to a probability distribution. It uses an iterative approach that leverages the factorization structure of the target distribution. It differs from other Bayesian approximation approaches such as Variational Bayesian methods.
The Berkson error model is a description of random error (or misclassification) in measurement. Unlike classical error, Berkson error causes little or no bias in the measurement. It was proposed by Joseph Berkson in an article entitled  Are there two regressions ,  published in 1950. An example of Berkson error arises in exposure assessment in epidemiological studies. Berkson error may predominate over classical error in cases where exposure data are highly aggregated. While this kind of error reduces the power of a study, risk estimates themselves are not themselves attenuated (as would be the case where random error predominates).
In statistics, a P P plot (probability probability plot or percent percent plot) is a probability plot for assessing how closely two data sets agree, which plots the two cumulative distribution functions against each other. P-P plots are vastly used to evaluate the skewness of a distribution. The Q Q plot is more widely used, but they are both referred to as "the" probability plot, and are potentially confused.
This list of statisticians lists people who have made notable contributions to the theories or application of statistics, or to the related fields of probability or machine learning. Also included are actuaries and demographers.
Constant elasticity of substitution (CES), in economics, is a property of some production functions and utility functions. Specifically, it arises in a particular type of aggregator function which combines two or more types of consumption, or two or more types of productive inputs into an aggregate quantity. This aggregator function exhibits constant elasticity of substitution.
Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method. LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made. LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.
LOESS and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. "LOESS" is a later generalization of LOWESS; although it is not a true initialism, it may be understood as standing for "LOcal regrESSion". LOESS and LOWESS thus build on "classical" methods, such as linear and nonlinear least squares regression. They address situations in which the classical procedures do not perform well or cannot be effectively applied without undue labor. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data. The trade-off for these features is increased computation. Because it is so computationally intensive, LOESS would have been practically impossible to use in the era when least squares regression was being developed. Most other modern methods for process modeling are similar to LOESS in this respect. These methods have been consciously designed to use our current computational ability to the fullest possible advantage to achieve goals not easily achieved by traditional approaches. A smooth curve through a set of data points obtained with this statistical technique is called a Loess Curve, particularly when each smoothed value is given by a weighted quadratic least squares regression over the span of values of the y-axis scattergram criterion variable. When each smoothed value is given by a weighted linear least squares regression over the span, this is known as a Lowess curve; however, some authorities treat Lowess and Loess as synonyms.
In psychology and sociology, the Thurstone scale was the first formal technique to measure an attitude. It was developed by Louis Leon Thurstone in 1928, as a means of measuring attitudes towards religion. It is made up of statements about a particular issue, and each statement has a numerical value indicating how favorable or unfavorable it is judged to be. People check each of the statements to which they agree, and a mean score is computed, indicating their attitude.  
The Will Rogers phenomenon is obtained when moving an element from one set to another set raises the average values of both sets. It is based on the following quote, attributed (perhaps incorrectly) to comedian Will Rogers: When the Okies left Oklahoma and moved to California, they raised the average intelligence level in both states. The effect will occur when both of these conditions are met: The element being moved is below average for its current set. Removing it will, by definition, raise the average of the remaining elements. The element being moved is above the current average of the set it is entering. Adding it to the new set will, by definition, raise the average.
A Brownian bridge is a continuous-time stochastic process B(t) whose probability distribution is the conditional probability distribution of a Wiener process W(t) (a mathematical model of Brownian motion) subject to the condition that W(1) = 0, so that the process is pinned at the origin at both t=0 and t=1. More precisely:  The expected value of the bridge is zero, with variance t(1   t), implying that the most uncertainty is in the middle of the bridge, with zero uncertainty at the nodes. The covariance of B(s) and B(t) is s(1   t) if s < t. The increments in a Brownian bridge are not independent.
Statistical signal processing is an area of Applied Mathematics and Signal Processing that treats signals as stochastic processes, dealing with their statistical properties (e.g., mean, covariance, etc.). Because of its very broad range of application Statistical signal processing is taught at the graduate level in either Electrical Engineering, Applied Mathematics, Pure Mathematics/Statistics, or even Biomedical Engineering and Physics departments around the world, although important applications exist in almost all scientific fields.
In signal processing, the impulse response, or impulse response function (IRF), of a dynamic system is its output when presented with a brief input signal, called an impulse. More generally, an impulse response refers to the reaction of any dynamic system in response to some external change. In both cases, the impulse response describes the reaction of the system as a function of time (or possibly as a function of some other independent variable that parameterizes the dynamic behavior of the system). In all these cases, the dynamic system and its impulse response may be actual physical objects, or may be mathematical systems of equations describing such objects. Since the impulse function contains all frequencies, the impulse response defines the response of a linear time-invariant system for all frequencies.
In statistics and quantitative research methodology, a data sample is a set of data collected and/or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Typically, the population is very large, making a census or a complete enumeration of all the values in the population impractical or impossible. The sample usually represents a subset of manageable size. Samples are collected and statistics are calculated from the samples so that one can make inferences or extrapolations from the sample to the population. The data sample may be drawn from a population without replacement, in which case it is a subset of a population; or with replacement, in which case it is a multisubset.
The Smearing retransformation is used in regression analysis, after estimating the logarithm of a variable. Estimating the logarithm of a variable instead of the variable itself is a common technique to more closely approximate normality. In order to retransform the variable back to level from log, the Smearing retransformation is used. If the log-transformed variable y is normally distributed with mean  and variance  then, the expected value of y is given by:
The Delaporte distribution is a discrete probability distribution that has received attention in actuarial science. It can be defined using the convolution of a negative binomial distribution with a Poisson distribution. Just as the negative binomial distribution can be viewed as a Poisson distribution where the mean parameter is itself a random variable with a gamma distribution, the Delaporte distribution can be viewed as a compound distribution based on a Poisson distribution, where there are two components to the mean parameter: a fixed component, which has the  parameter, and a gamma-distributed variable component, which has the  and  parameters. The distribution is named for Pierre Delaporte, who analyzed it in relation to automobile accident claim counts in 1959, although it appeared in a different form as early as 1934 in a paper by Rolf von Lu ders, where it was called the Formel II distribution.
In probability theory and statistics, a covariance matrix (also known as dispersion matrix or variance covariance matrix) is a matrix whose element in the i, j position is the covariance between the i th and j th elements of a random vector. A random vector is a random variable with multiple dimensions. Each element of the vector is a scalar random variable. Each element has either a finite number of observed empirical values or a finite or infinite number of potential values. The potential values are specified by a theoretical joint probability distribution. Intuitively, the covariance matrix generalizes the notion of covariance to multiple dimensions. As an example, let's consider two vectors  and . There are four covariances to consider:  with ,  with ,  with , and  with . These variances cannot be summarized in a scalar. Of course, a 2 2 matrix is the most natural choice to describe the covariance: the first row containing the covariances of  with  and , and the second row containing the covariances of  with  and . Because the covariance of the i th random variable with itself is simply that random variable's variance, each element on the principal diagonal of the covariance matrix is just the variance of each of the elements in the vector. Because , every covariance matrix is symmetric. In addition, every covariance matrix is positive semi-definite.
Rank-size distribution is the distribution of size by rank, in decreasing order of size. For example, if a data set consists of items of sizes 5, 100, 5, and 8, the rank-size distribution is 100, 8, 5, 5 (ranks 1 through 4). This is also known as the rank-frequency distribution, when the source data are from a frequency distribution. These are particularly of interest when the data vary significantly in scale, such as city size or word frequency. These distributions frequently follow a power law distribution, or less well-known ones such as a stretched exponential function or parabolic fractal distribution, at least approximately for certain ranges of ranks; see below. A rank-size distribution is not a probability distribution or cumulative distribution function. Rather, it is a discrete form of a quantile function (inverse cumulative distribution) in reverse order, giving the size of the element at a given rank.
In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. Instrumental variable methods allow consistent estimation when the explanatory variables (covariates) are correlated with the error terms of a regression relationship. Such correlation may occur when the dependent variable causes at least one of the covariates ("reverse" causation), when there are relevant explanatory variables which are omitted from the model, or when the covariates are subject to measurement error. In this situation, ordinary linear regression generally produces biased and inconsistent estimates. However, if an instrument is available, consistent estimates may still be obtained. An instrument is a variable that does not itself belong in the explanatory equation and is correlated with the endogenous explanatory variables, conditional on the other covariates. In linear models, there are two main requirements for using an IV: The instrument must be correlated with the endogenous explanatory variables, conditional on the other covariates. The instrument cannot be correlated with the error term in the explanatory equation (conditional on the other covariates), that is, the instrument cannot suffer from the same problem as the original predicting variable.
Evolutionary data mining, or genetic data mining is an umbrella term for any data mining using evolutionary algorithms. While it can be used for mining data from DNA sequences, it is not limited to biological contexts and can be used in any classification-based prediction scenario, which helps "predict the value ... of a user-specified goal attribute based on the values of other attributes." For instance, a banking institution might want to predict whether a customer's credit would be "good" or "bad" based on their age, income and current savings. Evolutionary algorithms for data mining work by creating a series of random rules to be checked against a training dataset. The rules which most closely fit the data are selected and are mutated. The process is iterated many times and eventually, a rule will arise that approaches 100% similarity with the training data. This rule is then checked against a test dataset, which was previously invisible to the genetic algorithm.  
There are two main uses of the term calibration in statistics that denote special types of statistical inference problems. Thus "calibration" can mean  A reverse process to regression, where instead of a future dependent variable being predicted from known explanatory variables, a known observation of the dependent variables is used to predict a corresponding explanatory variable. Procedures in statistical classification to determine class membership probabilities which assess the uncertainty of a given new observation belonging to each of the already established classes.  In addition, "calibration" is used in statistics with the usual general meaning of calibration. For example, model calibration can be also used to refer to Bayesian inference about the value of a model's parameters, given some data set, or more generally to any type of fitting of a statistical model.
In probability theory and statistics, Wallenius' noncentral hypergeometric distribution (named after Kenneth Ted Wallenius) is a generalization of the hypergeometric distribution where items are sampled with bias. This distribution can be illustrated as an urn model with bias. Assume, for example, that an urn contains m1 red balls and m2 white balls, totalling N = m1 + m2 balls. Each red ball has the weight  1 and each white ball has the weight  2. We will say that the odds ratio is   =  1 /  2. Now we are taking n balls, one by one, in such a way that the probability of taking a particular ball at a particular draw is equal to its proportion of the total weight of all balls that lie in the urn at that moment. The number of red balls x1 that we get in this experiment is a random variable with Wallenius' noncentral hypergeometric distribution. The matter is complicated by the fact that there is more than one noncentral hypergeometric distribution. Wallenius' noncentral hypergeometric distribution is obtained if balls are sampled one by one in such a way that there is competition between the balls. Fisher's noncentral hypergeometric distribution is obtained if the balls are sampled simultaneously or independently of each other. Unfortunately, both distributions are known in the literature as "the" noncentral hypergeometric distribution. It is important to be specific about which distribution is meant when using this name. The two distributions are both equal to the (central) hypergeometric distribution when the odds ratio is 1. It is far from obvious why these two distributions are different. See the Wikipedia entry on noncentral hypergeometric distributions for a more detailed explanation of the difference between these two probability distributions.  
In statistics, the observed information, or observed Fisher information, is the negative of the second derivative (the Hessian matrix) of the "log-likelihood" (the logarithm of the likelihood function). It is a sample-based version of the Fisher information.
In statistics, probability theory, and information theory, a statistical distance quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points. A distance between populations can be interpreted as measuring the distance between two probability distributions and hence they are essentially measures of distances between probability measures. Where statistical distance measures relate to the differences between random variables, these may have statistical dependence, and hence these distances are not directly related to measures of distances between probability measures. Again, a measure of distance between random variables may relate to the extent of dependence between them, rather than to their individual values. Statistical distance measures are mostly not metrics and they need not be symmetric. Some types of distance measures are referred to as (statistical) divergences.
In statistics, an interaction may arise when considering the relationship among three or more variables, and describes a situation in which the simultaneous influence of two variables on a third is not additive. Most commonly, interactions are considered in the context of regression analyses. The presence of interactions can have important implications for the interpretation of statistical models. If two variables of interest interact, the relationship between each of the interacting variables and a third "dependent variable" depends on the value of the other interacting variable. In practice, this makes it more difficult to predict the consequences of changing the value of a variable, particularly if the variables it interacts with are hard to measure or difficult to control. The notion of "interaction" is closely related to that of "moderation" that is common in social and health science research: the interaction between an explanatory variable and an environmental variable suggests that the effect of the explanatory variable has been moderated or modified by the environmental variable.
In probability theory, Boole's inequality, also known as the union bound, says that for any finite or countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events. Boole's inequality is named after George Boole. Formally, for a countable set of events A1, A2, A3, ..., we have  In measure-theoretic terms, Boole's inequality follows from the fact that a measure (and certainly any probability measure) is  -sub-additive.
In probability theory, Kolmogorov's zero one law, named in honor of Andrey Nikolaevich Kolmogorov, specifies that a certain type of event, called a tail event, will either almost surely happen or almost surely not happen; that is, the probability of such an event occurring is zero or one. Tail events are defined in terms of infinite sequences of random variables. Suppose  is an infinite sequence of independent random variables (not necessarily identically distributed). Let  be the  -algebra generated by the . Then, a tail event  is an event which is probabilistically independent of each finite subset of these random variables. (Note:  belonging to  implies that membership in  is uniquely determined by the values of the  but the latter condition is strictly weaker and does not suffice to prove the zero-one law.) For example, the event that the sequence converges, and the event that its sum converges are both tail events. In an infinite sequence of coin-tosses, a sequence of 100 consecutive heads occurring infinitely many times is a tail event. In many situations, it can be easy to apply Kolmogorov's zero one law to show that some event has probability 0 or 1, but surprisingly hard to determine which of these two extreme values is the correct one.
An open-label trial or open trial is a type of clinical trial in which both the researchers and participants know which treatment is being administered. This contrasts with single blind and double blind experimental designs, where participants are not aware of what treatment they are receiving (researchers are also unaware in a double blind trial). Open-label trials may be appropriate for comparing two very similar treatments to determine which is most effective. An open-label trial may be unavoidable under some circumstances, such as comparing the effectiveness of a medication to intensive physical therapy sessions. An open-label trial may still be randomized. Open-label trials may also be uncontrolled, with all participants receiving the same treatment.
In marketing, Bayesian inference allows for decision making and market research evaluation under uncertainty and with limited data.
The generalized gamma distribution is a continuous probability distribution with three parameters. It is a generalization of the two-parameter gamma distribution. Since many distributions commonly used for parametric models in survival analysis (such as the Exponential distribution, the Weibull distribution and the Gamma distribution) are special cases of the generalized gamma, it is sometimes used to determine which parametric model is appropriate for a given set of data.
In probability theory and statistics, the exponential distribution (a.k.a. negative exponential distribution) is the probability distribution that describes the time between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson processes, it is found in various other contexts. The exponential distribution is not the same as the class of exponential families of distributions, which is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes the normal distribution, binomial distribution, gamma distribution, Poisson, and many others.
In probability theory, the Wick product is a particular way of defining an adjusted product of a set of random variables. In the lowest order product the adjustment corresponds to subtracting off the mean value, to leave a result whose mean is zero. For the higher order products the adjustment involves subtracting off lower order (ordinary) products of the random variables, in a symmetric way, again leaving a result whose mean is zero. The Wick product is a polynomial function of the random variables, their expected values, and expected values of their products. The definition of the Wick product immediately leads to the Wick power of a single random variable and this allows analogues of other functions of random variables to be defined on the basis of replacing the ordinary powers in a power-series expansions by the Wick powers. The Wick product is named after physicist Gian-Carlo Wick, cf. Wick's theorem.
In mathematics, a moment problem arises as the result of trying to invert the mapping that takes a measure   to the sequences of moments  More generally, one may consider  for an arbitrary sequence of functions Mn.
In probability theory and statistics, the beta-binomial distribution is a family of discrete probability distributions on a finite support of non-negative integers arising when the probability of success in each of a fixed or known number of Bernoulli trials is either unknown or random. The beta-binomial distribution is the binomial distribution in which the probability of success at each trial is not fixed but random and follows the beta distribution. It is frequently used in Bayesian statistics, empirical Bayes methods and classical statistics as an overdispersed binomial distribution. It reduces to the Bernoulli distribution as a special case when n = 1. For   =   = 1, it is the discrete uniform distribution from 0 to n. It also approximates the binomial distribution arbitrarily well for large   and  . The beta-binomial is a one-dimensional version of the Dirichlet-multinomial distribution, as the binomial and beta distributions are univariate versions of the multinomial and Dirichlet distributions, respectively.
In statistics, the Newcastle Ottawa scale is a tool used for assessing the quality of non-randomized studies included in a systematic review and/or meta-analyses. Using the tool, each study is judged on eight items, categorized into three groups: the selection of the study groups; the comparability of the groups; and the ascertainment of either the exposure or outcome of interest for case-control or cohort studies respectively. Stars awarded for each quality item serve as a quick visual assessment. Stars are awarded such that the highest quality studies are awarded up to nine stars. The method was developed as a collaboration between the Universities of Newcastle, Australia and Ottawa, Canada using a Delphi process to define variables for data extraction. The scale was then tested on systematic reviews and further refined. Separate tools were developed for cohort and case control studies. It has also been adapted for prevalence studies.
A funnel plot is a graph designed to check for the existence of publication bias; funnel plots are commonly used in systematic reviews and meta-analyses. In the absence of publication bias, it assumes that the largest studies will be plotted near the average, and smaller studies will be spread evenly on both sides of the average, creating a roughly funnel-shaped distribution. Deviation from this shape can indicate publication bias.
Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966). The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.  
In mathematics, Wiener deconvolution is an application of the Wiener filter to the noise problems inherent in deconvolution. It works in the frequency domain, attempting to minimize the impact of deconvolved noise at frequencies which have a poor signal-to-noise ratio. The Wiener deconvolution method has widespread use in image deconvolution applications, as the frequency spectrum of most visual images is fairly well behaved and may be estimated easily. Wiener deconvolution is named after Norbert Wiener.
Multiple Discriminant Analysis (MDA) is a method for compressing a multivariate signal to yield a lower-dimensional signal amenable to classification. MDA is not directly used to perform classification. It merely supports classification by yielding a compressed signal amenable to classification. The method described in Duda et al. (2001)  3.8.3 projects the multivariate signal down to an M 1 dimensional space where M is the number of categories. MDA is useful because most classifiers are strongly affected by the curse of dimensionality. In other words, when signals are represented in very-high-dimensional spaces, the classifier's performance is catastrophically impaired by the overfitting problem. This problem is reduced by compressing the signal down to a lower-dimensional space as MDA does. MDA has been used to reveal neural codes.
In statistics, Tschuprow's T is a measure of association between two nominal variables, giving a value between 0 and 1 (inclusive). It is closely related to Crame r's V, coinciding with it for square contingency tables. It was published by Alexander Tschuprow (alternative spelling: Chuprov) in 1939.
A statistical parameter is a parameter that indexes a family of probability distributions. It can be regarded as a numerical characteristic of a population or a statistical model.
Reliability theory describes the probability of a system completing its expected function during an interval of time. It is the basis of reliability engineering, which is an area of study focused on optimizing the reliability, or probability of successful functioning, of systems, such as airplanes, linear accelerators, and any other product. It developed apart from the mainstream of probability and statistics. It was originally a tool to help nineteenth century maritime insurance and life insurance companies compute fair-value rates to charge their customers. Even today, the terms "failure rate" and "hazard rate" are often used interchangeably. The failure of mechanical devices such as ships, trains, and cars, is similar in many ways to the life or death of biological organisms. Statistical models appropriate for any of these topics are generically called "time-to-event" models. Death or failure is called an "event", and the goal is to project or forecast the rate of events for a given population or the probability of an event for an individual. When reliability is considered from the perspective of the consumer of a technology or service, actual reliability measures may differ dramatically from perceived reliability. One bad experience can be magnified in the mind of the customer, inflating the perceived unreliability of the product. One plane crash where hundreds of passengers die will immediately instill fear in a large percentage of the flying consumer population, regardless of actual reliability data about the safety of air travel. Reliability period of any object is measured within the durability period of that object.
Regret is the negative emotion experienced when learning that an alternative course of action would have resulted in a more favorable outcome. The theory of regret aversion or anticipated regret proposes that when facing a decision, individuals may anticipate the possibility of feeling regret after the uncertainty is resolved and thus incorporate in their choice their desire to eliminate or reduce this possibility.
Formalized by John Tukey, the Tukey lambda distribution is a continuous probability distribution defined in terms of its quantile function. It is typically used to identify an appropriate distribution (see the comments below) and not used in statistical models directly. The Tukey lambda distribution has a single shape parameter  . As with other probability distributions, the Tukey lambda distribution can be transformed with a location parameter,  , and a scale parameter,  . Since the general form of probability distribution can be expressed in terms of the standard distribution, the subsequent formulas are given for the standard form of the function.
In probability theory and statistics, the Poisson binomial distribution is the discrete probability distribution of a sum of independent Bernoulli trials that are not necessarily identically distributed. The concept is named after Sime on Denis Poisson. In other words, it is the probability distribution of the number of successes in a sequence of n independent yes/no experiments with success probabilities . The ordinary binomial distribution is a special case of the Poisson binomial distribution, when all success probabilities are the same, that is .
The spectral coherence is a statistic that can be used to examine the relation between two signals or data sets. It is commonly used to estimate the power transfer between input and output of a linear system. If the signals are ergodic, and the system function linear, it can be used to estimate the causality between the input and output.
Censored regression models commonly arise in econometrics in cases where the variable of interest is only observable under certain conditions. A common example is labor supply. Data are frequently available on the hours worked by employees, and a labor supply model estimates the relationship between hours worked and characteristics of employees such as age, education and family status. However, such estimates undertaken using linear regression will be biased by the fact that for people who are unemployed it is not possible to observe the number of hours they would have worked had they had employment. Still we know age, education and family status for those observations. A model commonly used to deal with censored data is the Tobit model, including variations such as the Tobit Type II, Type III, and Type IV models. These and other censored regression models are often confused with truncated regression models. Truncated regression models are used for data where whole observations are missing so that the values for the dependent and the independent variables are unknown. Censored regression models are used for data where only the value for the dependent variable (hours of work in the example above) is unknown while the values of the independent variables (age, education, family status) are still available. Censored regression models are usually estimated using maximum likelihood estimation. The general validity of this approach has been shown by Schnedler in 2005, who also provides a method to find the likelihood for a broad class of applications.
In statistics, the t-statistic is a ratio of the departure of an estimated parameter from its notional value and its standard error. It is used in hypothesis testing, for example in the Student s t-test, in the augmented Dickey Fuller test, and in bootstrapping.
Local independence is the underlying assumption of latent variable models. The observed items are conditionally independent of each other given an individual score on the latent variable(s). This means that the latent variable explains why the observed items are related to another. This can be explained by the following example.
In statistics, Bessel's correction, named after Friedrich Bessel, is the use of n   1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This corrects the bias in the estimation of the population variance, and some (but not all) of the bias in the estimation of the population standard deviation, but often increases the mean squared error in these estimations. That is, when estimating the population variance and standard deviation from a sample when the population mean is unknown, the sample variance estimated as the mean of the squared deviations of sample values from their mean (that is, using a multiplicative factor 1/n) is a biased estimator of the population variance, and for the average sample underestimates it. Multiplying the standard sample variance as computed in that fashion by n/n   1 (equivalently, using 1/n   1 instead of 1/n in the estimator's formula) corrects for this, and gives an unbiased estimator of the population variance. In some terminology, the factor n/n   1 is itself called Bessel's correction. One can understand Bessel's correction intuitively as the degrees of freedom in the residuals vector (residuals, not errors, because the population mean is unknown):  where  is the sample mean. While there are n independent samples, there are only n   1 independent residuals, as they sum to 0.  
The prosecutor's fallacy is a fallacy of statistical reasoning, typically used by the prosecution to argue for the guilt of a defendant during a criminal trial. Although it is named after prosecutors it is not specific to them, and some variants of the fallacy can be utilized by defense lawyers arguing for the innocence of their client. At its heart the fallacy involves assuming that the prior probability of a random match is equal to the probability that the defendant is innocent. For instance, if a perpetrator is known to have the same blood type as a defendant and 10% of the population share that blood type, then to argue on that basis alone that the probability of the defendant being guilty is 90% makes the prosecutor's fallacy (in a very simple form).
In economics, the consumption distribution is an alternative to the income distribution for judging economic inequality, comparing levels of consumption rather than income or wealth.
The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical tendency of something to occur or is it a measure of how strongly one believes it will occur, or does it draw on both these elements  In answering such questions, mathematicians interpret the probability values of probability theory. There are two broad categories of probability interpretations which can be called "physical" and "evidential" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or "relative frequency", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer). Evidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom). Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of "frequentist" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the existence and importance of physical probabilities, but also consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference. The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word "frequentist" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, "frequentist probability" is just another name for physical (or objective) probability. Those who promote Bayesian inference view "frequentist statistics" as an approach to statistical inference that recognises only physical probabilities. Also the word "objective", as applied to probability, sometimes means exactly what "physical" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.  It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis.
In probability theory and statistics, the inverse gamma distribution is a two-parameter family of continuous probability distributions on the positive real line, which is the distribution of the reciprocal of a variable distributed according to the gamma distribution. Perhaps the chief use of the inverse gamma distribution is in Bayesian statistics, where the distribution arises as the marginal posterior distribution for the unknown variance of a normal distribution if an uninformative prior is used; and as an analytically tractable conjugate prior if an informative prior is required. However, it is common among Bayesians to consider an alternative parametrization of the normal distribution in terms of the precision, defined as the reciprocal of the variance, which allows the gamma distribution to be used directly as a conjugate prior. Other Bayesians prefer to parametrize the inverse gamma distribution differently, as a scaled inverse chi-squared distribution.
A dendrogram (from Greek dendro "tree" and gramma "drawing") is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. Dendrograms are often used in computational biology to illustrate the clustering of genes or samples, sometimes on top of heatmaps.
IBM SPSS Modeler is a data mining and text analytics software application built by IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming. IBM SPSS Modeler was originally named Clementine by its creators, Integral Solutions Limited. This name continued for a while after SPSS's acquisition of the product. SPSS later changed the name to SPSS Clementine, and then later to PASW Modeler. Following IBM's 2009 acquisition of SPSS, the product was renamed IBM SPSS Modeler, its current name.
Stationary Subspace Analysis (SSA) is a blind source separation algorithm which factorizes a multivariate time series into stationary and non-stationary components.
Predictive inference is an approach to statistical inference that emphasizes the prediction of future observations based on past observations. Initially, predictive inference was based on observable parameters and it was the main purpose of studying probability, but it fell out of favor in the 20th century due to a new parametric approach pioneered by Bruno de Finetti. The approach modeled phenomena as a physical system observed with error (e.g., celestial mechanics). De Finetti's idea of exchangeability that future observations should behave like past observations came to the attention of the English-speaking world with the 1974 translation from French of his 1937 paper, and has since been propounded by such statisticians as Seymour Geisser.
In probability theory, the Bapat Beg theorem gives the joint probability distribution of order statistics of independent but not necessarily identically distributed random variables in terms of the cumulative distribution functions of the random variables. Bapat and Beg published the theorem in 1989, though they did not offer a proof. A simple proof was offered by Hande in 1994. Often, all elements of the sample are obtained from the same population and thus have the same probability distribution. The Bapat Beg theorem describes the order statistics when each element of the sample is obtained from a different statistical population and therefore has its own probability distribution.
In statistics, sequential estimation refers to estimation methods in sequential analysis where the sample size is not fixed in advance. Instead, data is evaluated as it is collected, and further sampling is stopped in accordance with a pre-defined stopping rule as soon as significant results are observed.
In probability theory, a distribution is said to be stable (or a random variable is said to be stable) if a linear combination of two independent copies of a random sample has the same distribution, up to location and scale parameters. The stable distribution family is also sometimes referred to as the Le vy alpha-stable distribution, after Paul Le vy, the first mathematician to have studied it. Of the four parameters defining the family, most attention has been focused on the stability parameter,   (see panel). Stable distributions have 0 <     2, with the upper bound corresponding to the normal distribution, and   = 1 to the Cauchy distribution. The distributions have undefined variance for   < 2, and undefined mean for     1. The importance of stable probability distributions is that they are "attractors" for properly normed sums of independent and identically-distributed (iid) random variables. The normal distribution defines a family of stable distributions. By the classical central limit theorem the properly normed sum of a set of random variables, each with finite variance, will tend towards a normal distribution as the number of variables increases. Without the finite variance assumption the limit may be a stable distribution. Mandelbrot referred to stable distributions that are non-normal as "stable Paretian distributions", after Vilfredo Pareto. Mandelbrot referred to "positive" stable distributions (meaning maximally skewed in the positive direction) with 1< <2 as "Pareto-Levy distributions". He also regarded this range as relevant for the description of stock and commodity prices. q-analogs of all symmetric stable distributions have been defined, and these recover the usual symmetric stable distributions in the limit of q   1.
In statistics, the Tukey Duckworth test is a two-sample location test   a statistical test of whether one of two samples was significantly greater than the other. It was introduced by John Tukey, who aimed to answer a request by W. E. Duckworth for a test simple enough to be remembered and applied in the field without recourse to tables, let alone computers. Given two groups of measurements of roughly the same size, where one group contains the highest value and the other the lowest value, then (i) count the number of values in the one group exceeding all values in the other, (ii) count the number of values in the other group falling below all those in the one, and (iii) sum these two counts (we require that neither count be zero). The critical values of the total count are, roughly, 7, 10, and 13, i.e. 7 for a two sided 5% level, 10 for a two sided 1% level, and 13 for a two sided 0.1% level. The test loses some accuracy if the samples are quite large (greater than 30) or much different in size (ratio more than 4:3). Tukey's paper describes adjustments for these conditions.
A credal set is a set of probability distributions or, equivalently, a set of probability measures. A credal set is often assumed or constructed to be a closed convex set. It is intended to express uncertainty or doubt about the probability model that should be used, or to convey the beliefs of a Bayesian agent about the possible states of the world. Let  denote a categorical variable,  a probability mass function over , and  a credal set over . If  is convex, the credal set can be equivalently described by its extreme points . The expectation for a function  of  with respect to the credal set  can be characterised only by its lower and upper bounds. For the lower bound,  Notably, such an inference problem can be equivalently obtained by considering only the extreme points of the credal set. It is easy to see that a credal set over a Boolean variable cannot have more than two vertices, while no bounds can be provided for credal sets over variables with three or more values.
In probability theory and directional statistics, a wrapped Le vy distribution is a wrapped probability distribution that results from the "wrapping" of the Le vy distribution around the unit circle.  
In probability and statistics, a realization, or observed value, of a random variable is the value that is actually observed (what actually happened). The random variable itself should be thought of as the process how the observation comes about. Statistical quantities computed from realizations without deploying a statistical model are often called "empirical", as in empirical distribution function or empirical probability. Conventionally, upper case letters denote random variables; the corresponding lower case letters denote their realizations. Confusion results when this important convention is not strictly observed. In more formal probability theory, a random variable is a function X defined from a sample space   to a measurable space called the state space. If an element in   is sent to an element in state space by X, then that element in state space is a realization. (In fact, a random variable cannot be an arbitrary function and it needs to satisfy another condition: it needs to be measurable.) Elements of the sample space can be thought of as all the different possibilities that could happen; while a realization (an element of the state space) can be thought of as the value X attains when one of the possibilities did happen. Probability is a mapping that assigns numbers between zero and one to certain subsets of the sample space. Subsets of the sample space that contain only one element are called elementary events. The value of the random variable (that is, the function) X at a point      ,  is called a realization of X.
In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. Missing data can occur because of nonresponse: no information is provided for several items or no information is provided for a whole unit. Some items are more sensitive for nonresponse than others, for example items about private subjects such as income. Dropout is a type of missingness that occurs mostly when studying development over time. In this type of study the measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing. Sometimes missing values are caused by the researcher for example, when data collection is done improperly or mistakes are made in data entry. Data often are missing in research in economics, sociology, and political science because governments choose not to, or fail to, report critical statistics.
Cox's theorem, named after the physicist Richard Threlkeld Cox, is a derivation of the laws of probability theory from a certain set of postulates. This derivation justifies the so-called "logical" interpretation of probability. As the laws of probability derived by Cox's theorem are applicable to any proposition, logical probability is a type of Bayesian probability. Other forms of Bayesianism, such as the subjective interpretation, are given other justifications.  
In combinatorial mathematics, a block design is a set together with a family of subsets (repeated subsets are allowed at times) whose members are chosen to satisfy some set of properties that are deemed useful for a particular application. These applications come from many areas, including experimental design, finite geometry, software testing, cryptography, and algebraic geometry. Many variations have been examined, but the most intensely studied are the balanced incomplete block designs (BIBDs or 2-designs) which historically were related to statistical issues in the design of experiments. A block design in which all the blocks have the same size is called uniform. The designs discussed in this article are all uniform. Pairwise balanced designs (PBDs) are examples of block designs that are not necessarily uniform.
The Pareto principle (also known as the 80 20 rule, the law of the vital few, and the principle of factor sparsity) states that, for many events, roughly 80% of the effects come from 20% of the causes. Management consultant Joseph M. Juran suggested the principle and named it after Italian economist Vilfredo Pareto, who, while at the University of Lausanne in 1896, published his first paper "Cours d'e conomie politique." Essentially, Pareto showed that approximately 80% of the land in Italy was owned by 20% of the population; Pareto developed the principle by observing that 20% of the peapods in his garden contained 80% of the peas. It is a common rule of thumb in business; e.g., "80% of your sales come from 20% of your clients." Mathematically, the 80 20 rule is roughly followed by a power law distribution (also known as a Pareto distribution) for a particular set of parameters, and many natural phenomena have been shown empirically to exhibit such a distribution. The Pareto principle is only tangentially related to Pareto efficiency. Pareto developed both concepts in the context of the distribution of income and wealth among the population.
In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The common exponential distribution and chi-squared distribution are special cases of the gamma distribution. There are three different parametrizations in common use: With a shape parameter k and a scale parameter  . With a shape parameter   = k and an inverse scale parameter   = 1/ , called a rate parameter. With a shape parameter k and a mean parameter   = k/ . In each of these three forms, both parameters are positive real numbers. The parameterization with k and   appears to be more common in econometrics and certain other applied fields, where e.g. the gamma distribution is frequently used to model waiting times. For instance, in life testing, the waiting time until death is a random variable that is frequently modeled with a gamma distribution. The parameterization with   and   is more common in Bayesian statistics, where the gamma distribution is used as a conjugate prior distribution for various types of inverse scale (aka rate) parameters, such as the   of an exponential distribution or a Poisson distribution   or for that matter, the   of the gamma distribution itself. (The closely related inverse gamma distribution is used as a conjugate prior for scale parameters, such as the variance of a normal distribution.) If k is a positive integer, then the distribution represents an Erlang distribution; i.e., the sum of k independent exponentially distributed random variables, each of which has a mean of  . The gamma distribution is the maximum entropy probability distribution for a random variable X for which E[X] = k  =  /  is fixed and greater than zero, and E[ln(X)] =  (k) + ln( ) =  ( )   ln( ) is fixed (  is the digamma function).
In probability theory, an empirical measure is a random measure arising from a particular realization of a (usually finite) sequence of random variables. The precise definition is found below. Empirical measures are relevant to mathematical statistics. The motivation for studying empirical measures is that it is often impossible to know the true underlying probability measure . We collect observations  and compute relative frequencies. We can estimate , or a related distribution function  by means of the empirical measure or empirical distribution function, respectively. These are uniformly good estimates under certain conditions. Theorems in the area of empirical processes provide rates of this convergence.
In statistics, D Agostino s K2 test, named for Ralph D'Agostino, is a goodness-of-fit measure of departure from normality, that is the test aims to establish whether or not the given sample comes from a normally distributed population. The test is based on transformations of the sample kurtosis and skewness, and has power only against the alternatives that the distribution is skewed and/or kurtic.  
In statistics, binomial regression is a technique in which the response (often referred to as Y) is the result of a series of Bernoulli trials, or a series of one of two possible disjoint outcomes (traditionally denoted "success" or 1, and "failure" or 0). In binomial regression, the probability of a success is related to explanatory variables: the corresponding concept in ordinary regression is to relate the mean value of the unobserved response to explanatory variables. Binomial regression models are essentially the same as binary choice models, one type of discrete choice model. The primary difference is in the theoretical motivation: Discrete choice models are motivated using utility theory so as to handle various types of correlated and uncorrelated choices, while binomial regression models are generally described in terms of the generalized linear model, an attempt to generalize various types of linear regression models. As a result, discrete choice models are usually described primarily with a latent variable indicating the "utility" of making a choice, and with randomness introduced through an error variable distributed according to a specific probability distribution. Note that the latent variable itself is not observed, only the actual choice, which is assumed to have been made if the net utility was greater than 0. Binary regression models, however, dispense with both the latent and error variable and assume that the choice itself is a random variable, with a link function that transforms the expected value of the choice variable into a value that is then predicted by the linear predictor. It can be shown that the two are equivalent, at least in the case of binary choice models: the link function corresponds to the quantile function of the distribution of the error variable, and the inverse link function to the cumulative distribution function (CDF) of the error variable. The latent variable has an equivalent if one imagines generating a uniformly distributed number between 0 and 1, subtracting from it the mean (in the form of the linear predictor transformed by the inverse link function), and inverting the sign. One then has a number whose probability of being greater than 0 is the same as the probability of success in the choice variable, and can be thought of as a latent variable indicating whether a 0 or 1 was chosen. In machine learning, binomial regression is considered a special case of probabilistic classification, and thus a generalization of binary classification.
Ogden tables are a set of statistical tables and other information for use in court cases in the UK. Their purpose is to make it easier to calculate future losses in personal injury and fatal accident cases. The tables take into account life expectancy and provide a range of discount rates from -2.0% to 3.0% in steps of 0.5%. The discount rate is fixed by the Lord Chancellor and is currently 2.5%. The most recent edition of the tables (7th Edition) makes changes to the discount rate range (previously 0.0% to 5.0% revised to -2.0% to 3.0%) to allow for a revision of the discount rate by the Lord Chancellor (currently under consideration as at 24 October 2011) and to provide for the implications of the case of Helmot -v- Simon. The Civil Evidence Act 1995 permitted their use in the UK and they were finally used by the House of Lords in Wells v Wells in July 1999. The full, and official, name of the tables is 'Actuarial Tables with explanatory notes for use in Personal Injury and Fatal Accident Cases' but the unofficial name became common parlance following the Civil Evidence Act 1995, where this shorthand name was used as a subheading - Sir Michael Ogden QC having been the chairman of the Working Party for the first four editions Using the Ogden Tables There are 28 tables of data in the Ogden Tables. Table 1 (Males) and Table 2 (Females) are for life expectancy and loss for life. Tables 3 to 14 are for loss of earnings up to various retirement ages. Tables 15 to 26 are for loss or pension from various retirement ages. Table 27 is for discounting to a period in the future and Table 28 is for a recurring loss over a period of time. How to calculate life expectancy To calculate life expectancy, you need to use Table 1 (for males) or Table 2 (for females) and use the data in the 0% column. So for a 45 years old female, using Table 2 you would look down the first column to find 45 and then across to the 0% column which gives a figure of 43.93. In cases where the age is not a whole number, i.e. female who is 45.75 years, then you use the figure for 45 years (43.93) and the figure for 46 years (42.87) and interpolate between the two (46-45.75) x 43.93 + (45.75-45) x 42.87 to give 43.14 years. How to calculate multiplier for lifetime loss If the claimant is to suffer a loss that will last their entire life, you need to use Table 1 (for males) or Table 2 (for females) and use the data in the 2.5% column. So for a 50-year-old male, using Table 1 you would look down to first column to find 50 and then across to the 2.5% column which gives a figure of 22.69. How to calculate value of a single loss in the future If the claimant needs to pay for something in the future, then the present value can be worked out using Table 27. Look up the period in the future in the first column and then across to the 2.5% column for the multiplier. For example, a purchase required in 10 years time would need to multiplier by 0.7812. How to calculate multiplier for loss over a period If the claimant has a recurring loss over a period of say 15 years, then use Table 28 looking up 15 in the first column and then across to the 2.5% column which gives a multiplier of 12.54. If the loss does not start until some time in the future, then you can combine Table 27 and Table 28 to give an overall multiplier. For example a loss over a period of 15 years that starts in 10 years time would have a Table 27 multiplier of 0.7812 and a Table 28 multiplier of 12.54 giving an overall multiplier of 9.80.
Non-linear least squares is the form of least squares analysis used to fit a set of m observations with a model that is non-linear in n unknown parameters (m > n). It is used in some forms of non-linear regression. The basis of the method is to approximate the model by a linear one and to refine the parameters by successive iterations. There are many similarities to linear least squares, but also some significant differences.
In statistical theory, the Pitman closeness criterion, named after E. J. G. Pitman, is a way of comparing two candidate estimators for the same parameter. Under this criterion, estimator A is preferred to estimator B if the probability that estimator A is closer to the true value than estimator B is greater than one half. Here the meaning of closer is determined by the absolute difference in the case of a scalar parameter, or by the Mahalanobis distance for a vector parameter.
Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of "positive" on 60% of instances, and a class label of "negative" on 40% of instances. The optimal Bayesian decision strategy (to maximize the number of correct predictions, see Duda, Hart & Stork (2001)) in such a case is to always predict "positive" (i.e., predict the majority category in the absence of other information), which has 60% chance of winning rather than matching which has 52% of winning (where p is the probability of positive realization, the result of matching would be , here ). The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to Thompson sampling).
In statistics, overdispersion is the presence of greater variability (statistical dispersion) in a data set than would be expected based on a given statistical model. A common task in applied statistics is choosing a parametric model to fit a given set of empirical observations. This necessitates an assessment of the fit of the chosen model. It is usually possible to choose the model parameters in such a way that the theoretical population mean of the model is approximately equal to the sample mean. However, especially for simple models with few parameters, theoretical predictions may not match empirical observations for higher moments. When the observed variance is higher than the variance of a theoretical model, overdispersion has occurred. Conversely, underdispersion means that there was less variation in the data than predicted. Overdispersion is a very common feature in applied data analysis because in practice, populations are frequently heterogeneous (non-uniform) contrary to the assumptions implicit within widely used simple parametric models.
The false positive paradox is a statistical result where false positive tests are more probable than true positive tests, occurring when the overall population has a low incidence of a condition and the incidence rate is lower than the false positive rate. The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. When the incidence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall. So, in a society with very few infected people fewer proportionately than the test gives false positives there will actually be more who test positive for a disease incorrectly and don't have it than those who test positive accurately and do. The paradox has surprised many. It is especially counter-intuitive when interpreting a positive result in a test on a low-incidence population after having dealt with positive results drawn from a high-incidence population. If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-incidence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred. Not adjusting to the scarcity of the condition in the new population, and concluding that a positive test result probably indicates a positive subject, even though population incidence is below the false positive rate, is a "base rate fallacy".
A transect is a path along which one counts and records occurrences of the species of study (e.g. plants). It requires an observer to move along a fixed path and to count occurrences along the path and, at the same time (in some procedures), obtain the distance of the object from the path. This results in an estimate of the area covered and an estimate of the way in which detectability increases from probability 0 (far from the path) towards 1 (near the path). Using the raw count and this probability function, one can arrive at an estimate of the actual density of objects.  The estimation of the abundance of populations (such as terrestrial mammal species) can be achieved using a number of different types of transect methods, such as strip transects, line transects, belt transects, point transects and curved line transects.
In statistics, the concept of a concomitant, also called the induced order statistic, arises when one sorts the members of a random sample according to corresponding values of another random sample. Let (Xi, Yi), i = 1, . . ., n be a random sample from a bivariate distribution. If the sample is ordered by the Xi, then the Y-variate associated with Xr:n will be denoted by Y[r:n] and termed the concomitant of the rth order statistic. Suppose the parent bivariate distribution having the cumulative distribution function F(x,y) and its probability density function f(x,y), then the probability density function of rth concomitant  for  is  If all  are assumed to be i.i.d., then for , the joint density for  is given by  That is, in general, the joint concomitants of order statistics  is dependent, but are conditionally independent given  for all k where . The conditional distribution of the joint concomitants can be derived from the above result by comparing the formula in marginal distribution and hence
In statistics and in probability theory, distance correlation is a measure of statistical dependence between two random variables or two random vectors of arbitrary, not necessarily equal dimension. An important property is that this measure of dependence is zero if and only if the random variables are statistically independent. This measure is derived from a number of other quantities that are used in its specification, specifically: distance variance, distance standard deviation and distance covariance. These take the same roles as the ordinary moments with corresponding names in the specification of the Pearson product-moment correlation coefficient. These distance-based measures can be put into an indirect relationship to the ordinary moments by an alternative formulation (described below) using ideas related to Brownian motion, and this has led to the use of names such as Brownian covariance and Brownian distance covariance.
In probability theory, Kolmogorov's criterion, named after Andrey Kolmogorov, is a theorem giving a necessary and sufficient condition for a Markov chain or continuous-time Markov chain to be stochastically identical to its time-reversed version.
In statistics, the Mann Whitney U test (also called the Mann Whitney Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon Mann Whitney test) is a nonparametric test of the null hypothesis that two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other. Unlike the t-test it does not require the assumption of normal distributions. It is nearly as efficient as the t-test on normal distributions.
In mathematical or statistical modelling a threshold model is any model where a threshold value, or set of threshold values, is used to distinguish ranges of values where the behaviour predicted by the model varies in some important way. A particularly important instance arises in toxicology, where the model for the effect of a drug may be that there is zero effect for a dose below a critical or threshold value, while an effect of some significance exists above that value. Certain types of regression model may include threshold effects. Classic threshold models were developed by Schelling, Axelrod, and Granovetter to model collective behavior. Schelling attempted to model the dynamics of segregation motivated by individual interactions in America (Schelling, 1971) by constructing two simulation models. Schelling demonstrated that  there is no simple correspondence of individual incentive to collective results,  and that the dynamics of movement influenced patterns of segregation. In doing so Schelling highlighted the significance of  a general theory of  tipping  . Mark Granovetter, following Schelling, proposed the threshold model (Granovetter & Soong, 1983, 1986, 1988), which assumes that individuals  behavior depends on the number of other individuals already engaging in that behavior (both Schelling and Granovetter classify their term of  threshold  as behavioral threshold.). He used the threshold model to explain the riot, residential segregation, and the spiral of silence. In the spirit of Granovetter s threshold model, the  threshold  is  the number or proportion of others who must make one decision before a given actor does so . It is necessary to emphasize the determinants of threshold. A threshold is different for individuals, and it may be influenced by many factors: social economic status, education, age, personality, etc. Further, Granovetter relates  threshold  with utility one gets from participating collective behavior or not, using the utility function, each individual will calculate his cost and benefit from undertaking an action. And situation may change the cost and benefit of the behavior, so threshold is situation-specific. The distribution of the thresholds determines the outcome of the aggregate behavior (for example, public opinion).
In statistics, an inherent zero is a reference point used to describe data sets which are indicative of magnitude of an absolute or relative nature. Inherent zeros are used in the "ratio level" of "levels of measurement" and imply  none. 
Mark and recapture is a method commonly used in ecology to estimate an animal population's size. A portion of the population is captured, marked, and released. Later, another portion is captured and the number of marked individuals within the sample is counted. Since the number of marked individuals within the second sample should be proportional to the number of marked individuals in the whole population, an estimate of the total population size can be obtained by dividing the number of marked individuals by the proportion of marked individuals in the second sample. The method is most useful when it is not practical to count all the individuals in the population. Other names for this method, or closely related methods, include capture-recapture, capture-mark-recapture, mark-recapture, sight-resight, mark-release-recapture, multiple systems estimation, band recovery, the Petersen method, and the Lincoln method. Another major application for these methods is in epidemiology, where they are used to estimate the completeness of ascertainment of disease registers. Typical applications include estimating the number of people needing particular services (i.e. services for children with learning disabilities, services for medically frail elderly living in the community), or with particular conditions(i.e. illegal drug addicts, people infected with HIV, etc.).
The term Tsallis statistics usually refers to the collection of mathematical functions and associated probability distributions that were originated by Constantino Tsallis. Using that collection, it is possible to derive Tsallis distributions from the optimization of the Tsallis entropic form. A continuous real parameter q can be used to adjust the distributions, so that distributions which have properties intermediate to that of Gaussian and Le vy distributions can be created. The parameter q represents the degree of non-extensivity of the distribution. Tsallis statistics are useful for characterising complex, anomalous diffusion.
In probability theory, a pairwise independent collection of random variables is a set of random variables any two of which are independent. Any collection of mutually independent random variables is pairwise independent, but some pairwise independent collections are not mutually independent. Pairwise independent random variables with finite variance are uncorrelated. A pair of random variables X and Y are independent if and only if the random vector (X, Y) with joint cumulative distribution function (CDF)  satisfies  or equivalently, their joint density  satisfies  That is, the joint distribution is equal to the product of the marginal distributions. Unless it is not clear in context, in practice the modifier "mutual" is usually dropped so that independence means mutual independence. A statement such as " X, Y, Z are independent random variables" means that X, Y, Z are mutually independent.
In probability theory and statistics, the geometric standard deviation describes how spread out are a set of numbers whose preferred average is the geometric mean. For such data, it may be preferred to the more usual standard deviation. Note that unlike the usual arithmetic standard deviation, the geometric standard deviation is a multiplicative factor, and thus is dimensionless, rather than having the same dimension as the input values.
In statistics, Procrustes analysis is a form of statistical shape analysis used to analyse the distribution of a set of shapes. The name Procrustes (Greek:             ) refers to a bandit from Greek mythology who made his victims fit his bed either by stretching their limbs or cutting them off. To compare the shapes of two or more objects, the objects must be first optimally "superimposed". Procrustes superimposition (PS) is performed by optimally translating, rotating and uniformly scaling the objects. In other words, both the placement in space and the size of the objects are freely adjusted. The aim is to obtain a similar placement and size, by minimizing a measure of shape difference called the Procrustes distance between the objects. This is sometimes called full, as opposed to partial PS, in which scaling is not performed (i.e. the size of the objects is preserved). Notice that, after full PS, the objects will exactly coincide if their shape is identical. For instance, with full PS two spheres with different radii will always coincide, because they have exactly the same shape. Conversely, with partial PS they will never coincide. This implies that, by the strict definition of the term shape in geometry, shape analysis should be performed using full PS. A statistical analysis based on partial PS is not a pure shape analysis as it is not only sensitive to shape differences, but also to size differences. Both full and partial PS will never manage to perfectly match two objects with different shape, such as a cube and a sphere, or a right hand and a left hand. In some cases, both full and partial PS may also include reflection. Reflection allows, for instance, a successful (possibly perfect) superimposition of a right hand to a left hand. Thus, partial PS with reflection enabled preserves size but allows translation, rotation and reflection, while full PS with reflection enabled allows translation, rotation, scaling and reflection. In mathematics: an orthogonal Procrustes problem is a method which can be used to find out the optimal rotation and/or reflection (i.e., the optimal orthogonal linear transformation) for the PS of an object with respect to another. a constrained orthogonal Procrustes problem, subject to det(R) = 1 (where R is a rotation matrix), is a method which can be used to determine the optimal rotation for the PS of an object with respect to another (reflection is not allowed). In some contexts, this method is called the Kabsch algorithm. Optimal translation and scaling are determined with much simpler operations (see below). When a shape is compared to another, or a set of shapes is compared to an arbitrarily selected reference shape, Procrustes analysis is sometimes further qualified as classical or ordinary, as opposed to Generalized Procrustes analysis (GPA), which compares three or more shapes to an optimally determined "mean shape".
Randomness is the lack of pattern or predictability in events. A random sequence of events, symbols or steps has no order and does not follow an intelligible pattern or combination. Individual random events are by definition unpredictable, but in many cases the frequency of different outcomes over a large number of events (or "trials") is predictable. For example, when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will occur twice as often as 4. In this view, randomness is a measure of uncertainty of an outcome, rather than haphazardness, and applies to concepts of chance, probability, and information entropy. The fields of mathematics, probability, and statistics use formal definitions of randomness. In statistics, a random variable is an assignment of a numerical value to each possible outcome of an event space. This association facilitates the identification and the calculation of probabilities of the events. Random variables can appear in random sequences. A random process is a sequence of random variables whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. These and other constructs are extremely useful in probability theory and the various applications of randomness. Randomness is most often used in statistics to signify well-defined statistical properties. Monte Carlo methods, which rely on random input (such as from random number generators or pseudorandom number generators), are important techniques in science, as, for instance, in computational science. By analogy, quasi-Monte Carlo methods use quasirandom number generators. Random selection is a method of selecting items (often called units) from a population where the probability of choosing a specific item is the proportion of those items in the population. For example, with a bowl containing just 10 red marbles and 90 blue marbles, a random selection mechanism would choose a red marble with probability 1/10. Note that a random selection mechanism that selected 10 marbles from this bowl would not necessarily result in 1 red and 9 blue. In situations where a population consists of items that are distinguishable, a random selection mechanism requires equal probabilities for any item to be chosen. That is, if the selection process is such that each member of a population, of say research subjects, has the same probability of being chosen then we can say the selection process is random.
In probability theory, the Vysochanskij Petunin inequality gives a lower bound for the probability that a random variable with finite variance lies within a certain number of standard deviations of the variable's mean, or equivalently an upper bound for the probability that it lies further away. The sole restrictions on the distribution are that it be unimodal and have finite variance. (This implies that it is a continuous probability distribution except at the mode, which may have a non-zero probability.) The theorem applies even to heavily skewed distributions and puts bounds on how much of the data is, or is not, "in the middle."
In statistics, least-angle regression (LARS) is an algorithm for fitting linear regression models to high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. Suppose we expect a response variable to be determined by a linear combination of a subset of potential covariates. Then the LARS algorithm provides a means of producing an estimate of which variables to include, as well as their coefficients. Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one's correlations with the residual. The advantages of the LARS method are: It is computationally just as fast as forward selection. It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model. If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable. It is easily modified to produce solutions for other estimators, like the lasso. It is effective in contexts where p >> n (IE, when the number of dimensions is significantly greater than the number of points). The disadvantages of the LARS method include: With any amount of noise in the dependent variable and with high dimensional multicollinear independent variables, there is no reason to believe that the selected variables will have a high probability of being the actual underlying causal variables. This problem is not unique to LARS, as it is a general problem with variable selection approaches that seek to find underlying deterministic components. Yet, because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article. Weisberg provides an empirical example based upon re-analysis of data originally used to validate LARS that the variable selection appears to have problems with highly correlated variables. Since almost all high dimensional data in the real world will just by chance exhibit some fair degree of collinearity across at least some variables, the problem that LARS has with correlated variables may limit its application to high dimensional data.  ^ Efron, Bradley; Hastie, Trevor; Johnstone, Iain; Tibshirani, Robert (2004). "Least Angle Regression" (PDF). Annals of Statistics 32 (2): pp. 407 499. doi:10.1214/009053604000000067. MR 2060166.  ^ See Discussion by Weisberg following Efron, Bradley; Hastie, Trevor; Johnstone, Iain; Tibshirani, Robert (2004). "Least Angle Regression" (PDF). Annals of Statistics 32 (2): pp. 407 499. doi:10.1214/009053604000000067. MR 2060166.
The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the mistaken belief that, if something happens more frequently than normal during some period, it will happen less frequently in the future, or that, if something happens less frequently than normal during some period, it will happen more frequently in the future (presumably as a means of balancing nature). In situations where what is being observed is truly random (i.e., independent trials of a random process), this belief, though appealing to the human mind, is false. This fallacy can arise in many practical situations although it is most strongly associated with gambling where such mistakes are common among players. The use of the term Monte Carlo fallacy originates from the most famous example of this phenomenon, which occurred in a Monte Carlo Casino in 1913.
In probability theory   specifically, in stochastic analysis   a killed process is a stochastic process that is forced to assume an undefined or "killed" state at some (possibly random) time.
In probability theory and statistics, the normal-exponential-gamma distribution (sometimes called the NEG distribution) is a three-parameter family of continuous probability distributions. It has a location parameter , scale parameter  and a shape parameter  .
In mathematical statistics, the Fisher information (sometimes simply called information) is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter   of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information. In Bayesian statistics, the asymptotic distribution of the posterior mode depends on the Fisher information and not on the prior (according to the Bernstein von Mises theorem, which was anticipated by Laplace for exponential families). The role of the Fisher information in the asymptotic theory of maximum-likelihood estimation was emphasized by the statistician Ronald Fisher (following some initial results by Francis Ysidro Edgeworth). The Fisher information is also used in the calculation of the Jeffreys prior, which is used in Bayesian statistics. The Fisher-information matrix is used to calculate the covariance matrices associated with maximum-likelihood estimates. It can also be used in the formulation of test statistics, such as the Wald test. Statistical systems of a scientific nature (physical, biological, etc.) whose likelihood functions obey shift invariance have been shown to obey maximum Fisher information. The level of the maximum depends upon the nature of the system constraints.
A Bayesian average is a method of estimating the mean of a population consistent with Bayesian interpretation, where instead of estimating the mean strictly from any or all available data set, other existing information related to that data set may also be incorporated into the calculation in order to minimize the impact of large deviations, or to assert a default value when the data set is small. Calculating the Bayesian average uses the prior mean m and a constant C. C is assigned a value that is proportional to the typical data set size. The value is larger when the expected variation between data sets (within the larger population) is small. It is smaller when the data sets are expected to vary substantially from one another.
In medical research and social science, a cross-sectional study (also known as a cross-sectional analysis, transversal study, prevalence study) is a type of observational study that involves the analysis of data collected from a population, or a representative subset, at one specific point in time that is, cross-sectional data. In economics,cross-sectional studies typically involve the use of cross-sectional regression, in order to sort out the existence and magnitude of causal effects of one or more independent variables upon a dependent variable of interest at a given point in time. They differ from time series analysis, in which the behavior of one or more economic aggregates is traced through time. In medical research, cross-sectional studies differ from case-control studies in that they aim to provide data on the entire population under study, whereas case-control studies typically include only individuals with a specific characteristic, with a sample, often a tiny minority, of the rest of the population. Cross-sectional studies are descriptive studies (neither longitudinal nor experimental). Unlike case-control studies, they can be used to describe, not only the odds ratio, but also absolute risks and relative risks from prevalences (sometimes called prevalence risk ratio, or PRR). They may be used to describe some feature of the population, such as prevalence of an illness, or they may support inferences of cause and effect. Longitudinal studies differ from both in making a series of observations more than once on members of the study population over a period of time.
Challenge dechallenge rechallenge (CDR) is a medical testing protocol in which a medicine or drug is administered, withdrawn, then re-administered, while being monitored for adverse effects at each stage. The protocol is used when statistical testing is inappropriate due to an idiosyncratic reaction by a specific individual, or a lack of sufficient test subjects and unit of analysis is the individual. During the withdraw phase, the medication is allowed to wash out of the system in order to determine what effect the medication is having on an individual.
Constraint in information theory refers to the degree of statistical dependence between or among variables. Garner provides a thorough discussion of various forms of constraint (internal constraint, external constraint, total constraint) with application to pattern recognition and psychology.  
A plot is a graphical technique for representing a data set, usually as a graph showing the relationship between two or more variables. The plot can be drawn by hand or by a mechanical or electronic plotter. Graphs are a visual representation of the relationship between variables, very useful for humans who can quickly derive an understanding which would not come from lists of values. Graphs can also be used to read off the value of an unknown variable plotted as a function of a known one. Graphs of functions are used in mathematics, sciences, engineering, technology, finance, and other areas.
The Recursive least squares (RLS) is an adaptive filter which recursively finds the coefficients that minimize a weighted linear least squares cost function relating to the input signals. This is in contrast to other algorithms such as the least mean squares (LMS) that aim to reduce the mean square error. In the derivation of the RLS, the input signals are considered deterministic, while for the LMS and similar algorithm they are considered stochastic. Compared to most of its competitors, the RLS exhibits extremely fast convergence. However, this benefit comes at the cost of high computational complexity.
The Galton Watson process is a branching stochastic process arising from Francis Galton's statistical investigation of the extinction of family names. The process models family names as patrilineal (passed from father to son), while offspring are randomly either male or female, and names become extinct if the family name line dies out (holders of the family name die without male descendants). This is an accurate description of Y chromosome transmission in genetics, and the model is thus useful for understanding human Y-chromosome DNA haplogroups, and is also of use in understanding other processes (as described below); but its application to actual extinction of family names is fraught. In practice, family names change for many other reasons, and dying out of name line is only one factor, as discussed in examples, below; the Galton Watson process is thus of limited applicability in understanding actual family name distributions. There was concern amongst the Victorians that aristocratic surnames were becoming extinct. Galton originally posed the question regarding the probability of such an event in the Educational Times of 1873, and the Reverend Henry William Watson replied with a solution. Together, they then wrote an 1874 paper entitled On the probability of extinction of families. Galton and Watson appear to have derived their process independently of the earlier work by I. J. Bienayme ; see Heyde and Seneta 1977. For a detailed history see Kendall (1966 and 1975).
In computer science, the earth mover's distance (EMD) is a measure of the distance between two probability distributions over a region D. In mathematics, this is known as the Wasserstein metric. Informally, if the distributions are interpreted as two different ways of piling up a certain amount of dirt over the region D, the EMD is the minimum cost of turning one pile into the other; where the cost is assumed to be amount of dirt moved times the distance by which it is moved. The above definition is valid only if the two distributions have the same integral (informally, if the two piles have the same amount of dirt), as in normalized histograms or probability density functions. In that case, the EMD is equivalent to the 1st Mallows distance or 1st Wasserstein distance between the two distributions.
In time series analysis, the Box Jenkins method, named after the statisticians George Box and Gwilym Jenkins, applies autoregressive moving average ARMA or ARIMA models to find the best fit of a time-series model to past values of a time series.
In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size , the jackknife estimate is found by aggregating the estimates of each  estimate in the sample. The jackknife technique was developed by Maurice Quenouille (1949, 1956). John Tukey (1958) expanded on the technique and proposed the name "jackknife" since, like a Boy Scout's jackknife, it is a "rough and ready" tool that can solve a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool. The jackknife is a linear approximation of the bootstrap.
In probability and statistics, an elliptical distribution is any member of a broad family of probability distributions that generalize the multivariate normal distribution. Intuitively, in the simplified two and three dimensional case, the joint distribution forms an ellipse and an ellipsoid, respectively, in iso-density plots.
In mathematics and statistics, an asymptotic distribution is a distribution that is in a sense the "limiting" distribution of a sequence of distributions. One of the main uses of the idea of an asymptotic distribution is in providing approximations to the cumulative distribution functions of statistical estimators.
The Kaplan Meier estimator, also known as the product limit estimator, is a non-parametric statistic used to estimate the survival function from lifetime data. In medical research, it is often used to measure the fraction of patients living for a certain amount of time after treatment. In other fields, Kaplan Meier estimators may be used to measure the length of time people remain unemployed after a job loss, the time-to-failure of machine parts, or how long fleshy fruits remain on plants before they are removed by frugivores. The estimator is named after Edward L. Kaplan and Paul Meier, who each submitted similar manuscripts to the Journal of the American Statistical Association. The journal editor, John Tukey, convinced them to combine their work into one paper, which has been cited about 34,000 times since its publication.
Observational error (or measurement error) is the difference between a measured value of quantity and its true value. In statistics, an error is not a "mistake". Variability is an inherent part of things being measured and of the measurement process. Measurement errors can be divided into two components: random error and systematic error. Random errors are errors in measurement that lead to measurable values being inconsistent when repeated measures of a constant attribute or quantity are taken. Systematic errors are errors that are not determined by chance but are introduced by an inaccuracy (as of observation or measurement) inherent in the system. Systematic error may also refer to an error having a nonzero mean, so that its effect is not reduced when observations are averaged.
The Wald test is a parametric statistical test named after the Hungarian statistician Abraham Wald. Whenever a relationship within or between data items can be expressed as a statistical model with parameters to be estimated from a sample, the Wald test can be used to test the true value of the parameter based on the sample estimate. Suppose an economist, who has data on social class and shoe size, wonders whether social class is associated with shoe size. Say  is the average increase in shoe size for upper-class people compared to middle-class people: then the Wald test can be used to test whether  is 0 (in which case social class has no association with shoe size) or non-zero (shoe size varies between social classes). Here, , the hypothetical difference in shoe sizes between upper and middle-class people in the whole population, is a parameter. An estimate of  might be the difference in shoe size between upper and middle-class people in the sample. In the Wald test, the economist uses the estimate and an estimate of variability (see below) to draw conclusions about the unobserved true . Or, for a medical example, suppose smoking multiplies the risk of lung cancer by some number R: then the Wald test can be used to test whether R = 1 (i.e. there is no effect of smoking) or is greater (or less) than 1 (i.e. smoking alters risk). A Wald test can be used in a great variety of different models including models for dichotomous variables and models for continuous variables.
Particle filters or Sequential Monte Carlo (SMC) methods are a set of genetic-type particle Monte Carlo methodologies to solve the filtering problem. The term "particle filters" was first coined in 1996 by Del Moral in reference to mean field interacting particle methods used in fluid mechanics since the beginning of the 1960s. The terminology "sequential Monte Carlo" was proposed by Liu and Chen in 1998. From the statistical and probabilistic point of view, particle filters can be interpreted as mean field particle interpretations of Feynman-Kac probability measures. These particle integration techniques were developed in molecular chemistry and computational physics by Theodore E. Harris and Herman Kahn in 1951, Marshall. N. Rosenbluth and Arianna. W. Rosenbluth in 1955 and more recently by Jack H. Hetherington in 1984. In computational physics, these Feynman-Kac type path particle integration methods are also used in Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods. Feynman-Kac interacting particle methods are also strongly related to mutation-selection genetic algorithms currently used in evolutionary computing to solve complex optimization problems. The particle filter methodology is used to solve Hidden Markov Chain (HMM) and nonlinear filtering problems arising in signal processing and Bayesian statistical inference. The filtering problem consists in estimating the internal states in dynamical systems when partial observations are made, and random perturbations are present in the sensors as well as in the dynamical system. The objective is to compute the conditional probability (a.k.a. posterior distributions) of the states of some Markov process, given some noisy and partial observations. With the notable exception of linear-Gaussian signal-observation models (Kalman filter) or wider classes of models (Benes filter) Mireille Chaleyat-Maurel and Dominique Michel proved in 1984 that the sequence of posterior distributions of the random states of the signal given the observations (a.k.a. optimal filter) have no finitely recursive recursion. Various numerical methods based on fixed grid approximations, Markov Chain Monte Carlo techniques (MCMC), conventional linearization, extended Kalman filters, or determining the best linear system (in expect cost-error sense) have never really coped with large scale systems, unstable processes or when the nonlinearities are not sufficiently smooth. Particle filtering methodology uses a genetic type mutation-selection sampling approach, with a set of particles (also called individuals, or samples) to represent the posterior distribution of some stochastic process given some noisy and/or partial observations. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. Particle filter techniques provide a well-established methodology for generating samples from the required distribution without requiring assumptions about the state-space model or the state distributions. However, these methods do not perform well when applied to very high-dimensional systems. Particle filters implement the prediction-updating transitions of the filtering equation directly by using a genetic type mutation-selection particle algorithm. The samples from the distribution are represented by a set of particles; each particle has a likelihood weight assigned to it that represents the probability of that particle being sampled from the probability density function. Weight disparity leading to weight collapse is a common issue encountered in these filtering algorithms; however it can be mitigated by including a resampling step before the weights become too uneven. Several adaptive resampling criteria can be used, including the variance of the weights and the relative entropy w.r.t. the uniform distribution. In the resampling step, the particles with negligible weights are replaced by new particles in the proximity of the particles with higher weights. Particle filters and Feynman-Kac particle methodologies find application in signal and image processing, Bayesian inference, machine learning, risk analysis and rare event sampling, engineering and robotics, artificial intelligence, bioinformatics, phylogenetics, computational science, Economics and mathematical finance, molecular chemistry, computational physics, pharmacokinetic and other fields.
Modeling photon propagation with Monte Carlo methods is a flexible yet rigorous approach to simulate photon transport. In the method, local rules of photon transport are expressed as probability distributions which describe the step size of photon movement between sites of photon-tissue interaction and the angles of deflection in a photon's trajectory when a scattering event occurs. This is equivalent to modeling photon transport analytically by the radiative transfer equation (RTE), which describes the motion of photons using a differential equation. However, closed-form solutions of the RTE are often not possible; for some geometries, the diffusion approximation can be used to simplify the RTE, although this, in turn, introduces many inaccuracies, especially near sources and boundaries. In contrast, Monte Carlo simulations can be made arbitrarily accurate by increasing the number of photons traced. For example, see the movie, where a Monte Carlo simulation of a pencil beam incident on a semi-infinite medium models both the initial ballistic photon flow and the later diffuse propagation. The Monte Carlo method is necessarily statistical and therefore requires significant computation time to achieve precision. In addition Monte Carlo simulations can keep track of multiple physical quantities simultaneously, with any desired spatial and temporal resolution. This flexibility makes Monte Carlo modeling a powerful tool. Thus, while computationally inefficient, Monte Carlo methods are often considered the standard for simulated measurements of photon transport for many biomedical applications.
A reliability block diagram (RBD) is a diagrammatic method for showing how component reliability contributes to the success or failure of a complex system. RBD is also known as a dependence diagram (DD).  A RBD or DD is drawn as a series of blocks connected in parallel or series configuration. Each block represents a component of the system with a failure rate. Parallel paths are redundant, meaning that all of the parallel paths must fail for the parallel network to fail. By contrast, any failure along a series path causes the entire series path to fail. An RBD may be drawn using switches in place of blocks, where a closed switch represents a working component and an open switch represents a failed component. If a path may be found through the network of switches from beginning to end, the system still works. An RBD may be converted to a success tree by replacing series paths with AND gates and parallel paths with OR gates. A success tree may then be converted to a fault tree by applying de Morgan's theorem. In order to evaluate RBD, closed form solution are available in the case of statistical independence among blocks or components. In the case the statistical independence assumption is not satisfied, specific formalisms and solution tools, such as dynamic RBD, have to be considered.
The Davies Bouldin index (DBI) (introduced by David L. Davies and Donald W. Bouldin in 1979) is a metric for evaluating clustering algorithms. This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. This has a drawback that a good value reported by this method does not imply the best information retrieval.
Pareto analysis is a formal technique useful where many possible courses of action are competing for attention. In essence, the problem-solver estimates the benefit delivered by each action, then selects a number of the most effective actions that deliver a total benefit reasonably close to the maximal possible one. Pareto analysis is a creative way of looking at causes of problems because it helps stimulate thinking and organize thoughts. However, it can be limited by its exclusion of possibly important problems which may be small initially, but which grow with time. It should be combined with other analytical tools such as failure mode and effects analysis and fault tree analysis for example. This technique helps to identify the top portion of causes that need to be addressed to resolve the majority of problems. Once the predominant causes are identified, then tools like the Ishikawa diagram or Fish-bone Analysis can be used to identify the root causes of the problems. While it is common to refer to pareto as "80/20" rule, under the assumption that, in all situations, 20% of causes determine 80% of problems, this ratio is merely a convenient rule of thumb and is not nor should it be considered immutable law of nature. The application of the Pareto analysis in risk management allows management to focus on those risks that have the most impact on the project.
In mathematics and statistics, deviation is a measure of difference between the observed value of a variable and some other value, often that variable's mean. The sign of the deviation (positive or negative), reports the direction of that difference (the deviation is positive when the observed value exceeds the reference value). The magnitude of the value indicates the size of the difference.
Methods engineering is a subspecialty of industrial engineering and manufacturing engineering concerned with human integration in industrial production processes.
Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric. PCA is sensitive to the relative scaling of the original variables. PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed (and named) by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Kosambi-Karhunen Loe ve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of ), Eckart Young theorem (Harman, 1960), or Schmidt Mirsky theorem in psychometrics, empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics. PCA is mostly used as a tool in exploratory data analysis and for making predictive models. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute. The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score). PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection or "shadow" of this object when viewed from its (in some sense; see below) most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced. PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.
Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. The application of multivariate statistics is multivariate analysis. Multivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical implementation of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the actual problem being studied. In addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both  how these can be used to represent the distributions of observed data; how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis.  Certain types of problem involving multivariate data, for example simple linear regression and multiple regression, are not usually considered as special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.
In probability theory, the central limit theorem states that, under certain circumstances, the probability distribution of the scaled mean of a random sample converges to a normal distribution as the sample size increases to infinity. Under stronger assumptions, the Berry Esseen theorem, or Berry Esseen inequality, gives a more quantitative result, because it also specifies the rate at which this convergence takes place by giving a bound on the maximal error of approximation between the normal distribution and the true distribution of the scaled sample mean. The approximation is measured by the Kolmogorov Smirnov distance. In the case of independent samples, the convergence rate is n 1/2, where n is the sample size, and the constant is estimated in terms of the third absolute normalized moments.
People v. Collins was a 1968 American robbery trial noted for its misuse of probability and as an example of the prosecutor's fallacy.
In statistics, quality assurance, and survey methodology, sampling is concerned with the selection of a subset of individuals from within a statistical population to estimate characteristics of the whole population. Each observation measures one or more properties (such as weight, location, color) of observable bodies distinguished as independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. The sampling process comprises several stages: Defining the population of concern Specifying a sampling frame, a set of items or events possible to measure Specifying a sampling method for selecting items or events from the frame Determining the sample size Implementing the sampling plan Sampling and data collecting Data which can be selected
Given a population whose members can be potentially separated into a number of different sets or classes, a classification rule is a procedure in which the elements of the population set are each assigned to one of the classes. A perfect test is such that every element in the population is assigned to the class it really belongs. An imperfect test is such that some errors appear, and then statistical analysis must be applied to analyse the classification. A special kind of classification rule are binary classifications.
NCSS is a computer program for statistical/data analysis created in 1981. NCSS LLC is the name of the company that produces NCSS. NCSS LLC also produces PASS (Power Analysis and Sample Size). NCSS LLC specializes in providing statistical analysis software to researchers, businesses, and academic institutions. NCSS (originally, Number Cruncher Statistical System) includes over 230 documented statistical and plot procedures. NCSS imports and exports all major spreadsheet, database, and statistical file formats.
In statistics, a Q Q plot ("Q" stands for quantile) is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. First, the set of intervals for the quantiles is chosen. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the (number of the) interval for the quantile. If the two distributions being compared are similar, the points in the Q Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q Q plot will approximately lie on a line, but not necessarily on the line y = x. Q Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Q Q plots can be used to compare collections of data, or theoretical distributions. The use of Q Q plots to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions. A Q Q plot is generally a more powerful approach to do this than the common technique of comparing histograms of the two samples, but requires more skill to interpret. Q Q plots are commonly used to compare a data set to a theoretical model. This can provide an assessment of "goodness of fit" that is graphical, rather than reducing to a numerical summary. Q Q plots are also used to compare two theoretical distributions to each other. Since Q Q plots compare distributions, there is no need for the values to be observed as pairs, as in a scatter plot, or even for the numbers of values in the two groups being compared to be equal. The term "probability plot" sometimes refers specifically to a Q Q plot, sometimes to a more general class of plots, and sometimes to the less commonly used P P plot. The probability plot correlation coefficient is a quantity derived from the idea of Q Q plots, which measures the agreement of a fitted distribution with observed data and which is sometimes used as a means of fitting a distribution to data.
In statistical quality control, the CUSUM (or cumulative sum control chart) is a sequential analysis technique developed by E. S. Page of the University of Cambridge. It is typically used for monitoring change detection. CUSUM was announced in Biometrika, in 1954, a few years after the publication of Wald's SPRT algorithm. Page referred to a "quality number" , by which he meant a parameter of the probability distribution; for example, the mean. He devised CUSUM as a method to determine changes in it, and proposed a criterion for deciding when to take corrective action. When the CUSUM method is applied to changes in mean, it can be used for step detection of a time series. A few years later, George Alfred Barnard developed a visualization method, the V-mask chart, to detect both increases and decreases in .
In statistics, a shrinkage estimator is an estimator that, either explicitly or implicitly, incorporates the effects of shrinkage. In loose terms this means that a naive or raw estimate is improved by combining it with other information. The term relates to the notion that the improved estimate is made closer to the value supplied by the 'other information' than the raw estimate. In this sense, shrinkage is used to regularize ill-posed inference problems.
Uncomfortable science, as identified by statistician John Tukey, is situations in which there is a need to draw an inference from a limited sample of data, where further samples influenced by the same cause system will not be available. More specifically, it involves the analysis of a finite natural phenomenon for which it is difficult to overcome the problem of using a common sample of data for both exploratory data analysis and confirmatory data analysis. This leads to the danger of systematic bias through testing hypotheses suggested by the data. A typical example is Bode's law, which provides a simple numerical rule for the distances of the planets in the solar system from the Sun. Once the rule has been derived, through the trial and error matching of various rules with the observed data (exploratory data analysis), there are not enough planets remaining for a rigorous and independent test of the hypothesis (confirmatory data analysis). We have exhausted the natural phenomena. The agreement between data and the numerical rule should be no surprise, as we have deliberately chosen the rule to match the data. If we are concerned about what Bode's law tells us about the cause system of planetary distribution then we demand confirmation that will not be available until better information about other planetary systems becomes available.
An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation. It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following maximum expected utility criterion) can be modeled and solved. ID was first developed in mid-1970s within the decision analysis community with an intuitive semantic that is easy to understand. It is now adopted widely and becoming an alternative to decision tree which typically suffers from exponential growth in number of branches with each variable modeled. ID is directly applicable in team decision analysis, since it allows incomplete sharing of information among team members to be modeled and solved explicitly. Extension of ID also find its use in game theory as an alternative representation of game tree.
The Statistics Online Computational Resource (SOCR) is an online multi-institutional research and education organization. SOCR designs, validates and broadly shares a suite of online tools for statistical computing, and interactive materials for hands-on learning and teaching concepts in data science, statistical analysis and probability theory. The SOCR resources are platform agnostic based on HTML, XML and Java, and all materials, tools and services are freely available over the Internet. The core SOCR components include interactive distribution calculators, statistical analysis modules, tools for data modeling, graphics visualizaiton, instructional resources, learning activities and other resources . All SOCR resources are LGPL/CC-BY licensed, peer-reviewed, integrated internally and interoperate with independent digital libraries developed by other professional societies and scientific organizations like NSDL, Open Educational Resources, Mathematical Association of America, California Digital Library, LONI Pipeline, etc.
R v Adams [1996] 2 Cr App R 467, [1996] Crim LR 898, CA and R v Adams [1998] 1 Cr App R 377, The Times, 3 November 1997, CA, are rulings that ousted explicit Bayesian statistics from the reasoning admissible before a jury in DNA cases.
The Z-factor is a measure of statistical effect size. It has been proposed for use in high-throughput screening (where it is also known as Z-prime, and commonly written as Z') to judge whether the response in a particular assay is large enough to warrant further attention.
In statistics, for example in statistical quality control, a statistical assembly is a collection of parts or components which makes up a statistical unit. Thus a statistical unit, which would be the prime item of concern, is made of discrete components like organs or machine parts. The reliability of the statistical unit is, in part, determined by the reliability of the components in the statistical assembly, and by their interactions. Much of the observation of a statistical assembly requires special preparation of the unit, which demands that the intervention must not prejudice the observations. A simple version of this kind of research uses the stimulus-response model. In other contexts, statistical assembly refers to the process of constructing a manufactured item which must be carefully specified to contain given amounts of nonuniformity within it.
In statistics, a confounding variable (also confounding factor, a confound, or confounder) is an extraneous variable in a statistical model that correlates (directly or inversely) with both the dependent variable and the independent variable. A spurious relationship is a perceived relationship between an independent variable and a dependent variable that has been estimated incorrectly because the estimate fails to account for a confounding factor. The incorrect estimation suffers from omitted-variable bias. While specific definitions may vary, in essence a confounding variable fits the following four criteria, here given in a hypothetical situation with variable of interest "V", confounding variable "C" and outcome of interest "O": C is associated (inversely or directly) with O C is associated with O, independent of V C is associated (inversely or directly) with V C is not in the causal pathway of V to O (C is not a direct consequence of V, not a way by which V produces O) The preceding correlation-based definition, however, is metaphorical at best   a growing number of analysts agree that confounding is a causal concept, and as such, cannot be described in terms of correlations nor associations  (see causal definition).  
In statistics, a moving average (rolling average or running average) is a calculation to analyze data points by creating series of averages of different subsets of the full data set. It is also called a moving mean (MM) or rolling mean and is a type of finite impulse response filter. Variations include: simple, and cumulative, or weighted forms (described below). Given a series of numbers and a fixed subset size, the first element of the moving average is obtained by taking the average of the initial fixed subset of the number series. Then the subset is modified by "shifting forward"; that is, excluding the first number of the series and including the next number following the original subset in the series. This creates a new subset of numbers, which is averaged. This process is repeated over the entire data series. The plot line connecting all the (fixed) averages is the moving average. A moving average is a set of numbers, each of which is the average of the corresponding subset of a larger set of datum points. A moving average may also use unequal weights for each datum value in the subset to emphasize particular values in the subset. A moving average is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles. The threshold between short-term and long-term depends on the application, and the parameters of the moving average will be set accordingly. For example, it is often used in technical analysis of financial data, like stock prices, returns or trading volumes. It is also used in economics to examine gross domestic product, employment or other macroeconomic time series. Mathematically, a moving average is a type of convolution and so it can be viewed as an example of a low-pass filter used in signal processing. When used with non-time series data, a moving average filters higher frequency components without any specific connection to time, although typically some kind of ordering is implied. Viewed simplistically it can be regarded as smoothing the data.
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.
Policy capturing or "the PC technique" is a statistical method used in social psychology to quantify the relationship between a person's judgement and the information that was used to make that judgement. Policy capturing assessments rely upon regression analysis models. Policy capturing is frequently used by businesses to assess employee performance. Policy capturing is a technique that is used to examine how individuals reach decisions. Policy capturing is regarded as a form of judgment analysis and had been applied to a variety of settings and contexts (see Cooksey, 1996). A typical example was reported by Sherer, Schwab and Heneman (1987), in their study of how supervisors, in the setting of a private hospital, reach decisions about salary raises. Participants of this study, called judges, received information about a set of employees. The employees differed on five key factors: performance level was average or superior, performance was consistent or inconsistent, current salary was low, medium, or high, and the individuals either had or had not been offered another job from a different organization. After reading information about each employee, participants then decided whether the percentage and absolute increase in salary they would recommend. Which of these five factors shaped the decisions varied appreciably across the participants. Hitt and Barr reported another excellent example of policy capturing. This study assessed which factors determine evaluations of job applicants and corresponding salaries. The participants or judges-66 managers who often need to reach similar decisions in their work lives-read the applications of these applicants and watched a video presentation that each candidate had prepared. Several variables differed across applicants: the applicants, for example, had accumulated either 10 or 15 years of experience, were 35 or 35 years of age, were male or female, were African or Caucasian, had completed a BS or MBA, and were applying to be a regional sales manager or vice president of sales. Subsequent analysis showed that factors unrelated to experience, such as age and sex, affected decisions. Furthermore, the relevance of each factor interacted with one another.
In statistics, L-moments are a sequence of statistics used to summarize the shape of a probability distribution. They are linear combinations of order statistics (L-statistics) analogous to conventional moments, and can be used to calculate quantities analogous to standard deviation, skewness and kurtosis, termed the L-scale, L-skewness and L-kurtosis respectively (the L-mean is identical to the conventional mean). Standardised L-moments are called L-moment ratios and are analogous to standardized moments. Just as for conventional moments, a theoretical distribution has a set of population L-moments. Sample L-moments can be defined for a sample from the population, and can be used as estimators of the population L-moments.
In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.
In the theory of cluster analysis, the nearest-neighbor chain algorithm is a method that can be used to perform several types of agglomerative hierarchical clustering, using an amount of memory that is linear in the number of points to be clustered and an amount of time linear in the number of distinct distances between pairs of points. The main idea of the algorithm is to find pairs of clusters to merge by following paths in the nearest neighbor graph of the clusters until the paths terminate in pairs of mutual nearest neighbors. The algorithm was developed and implemented in 1982 by J. P. Benze cri and J. Juan, based on earlier methods that constructed hierarchical clusterings using mutual nearest neighbor pairs without taking advantage of nearest neighbor chains.
Nelson rules are a method in process control of determining if some measured variable is out of control (unpredictable versus consistent). Rules, for detecting "out-of-control" or non-random conditions were first postulated by Walter A. Shewhart  in the 1920s. The Nelson rules were first published in the October 1984 issue of the Journal of Quality Technology in an article by Lloyd S Nelson. The rules are applied to a control chart on which the magnitude of some variable is plotted against time. The rules are based on the mean value and the standard deviation of the samples. The above eight rules apply to a chart of a variable value. A second chart, the moving range chart, can also be used but only with rules 1, 2, 3 and 4. Such a chart plots a graph of the maximum value - minimum value of N adjacent points against the time sample of the range. An example moving range: if N = 3 and values are 1, 3, 5, 3, 3, 2, 4, 5 then the sets of adjacent points are (1,3,5) (3,5,3) (5,3,3) (3,3,2) (3,2,4) (2,4,5) resulting in moving range values of (5-1) (5-3) (5-3) (3-2) (4-2) (5-2) = 4, 2, 2, 1, 2, 3. Applying these rules indicates when a potential "out of control" situation has arisen. However there will always be some false alerts and the more rules applied the more will occur. For some processes, it may be beneficial to omit one or more rules. Equally there may be some missing alerts where some specific "out of control" situation is not detected. Empirically, the detection accuracy is good.
In statistics, controlling for a variable is the attempt to reduce the effect of confounding variables on an observational study. It means that when looking at the effect of one variable, all other variable predictors are held constant.
In the theory of stochastic processes in discrete time, a part of the mathematical theory of probability, the Doob decomposition theorem gives a unique decomposition of every adapted and integrable stochastic process as the sum of a martingale and a predictable process (or "drift") starting at zero. The theorem was proved by and is named for Joseph L. Doob. The analogous theorem in the continuous-time case is the Doob Meyer decomposition theorem.
A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value (or score) below which 20% of the observations may be found. The term percentile and the related term percentile rank are often used in the reporting of scores from norm-referenced tests. For example, if a score is at the 86th percentile, where 86 is the percentile rank, it is equal to the value below which 86% of the observations may be found (carefully contrast with in the 86th percentile, which means the score is at or below the value of which 86% of the observations may be found - every score is in the 100th percentile). The 25th percentile is also known as the first quartile (Q1), the 50th percentile as the median or second quartile (Q2), and the 75th percentile as the third quartile (Q3). In general, percentiles and quartiles are specific types of quantiles.  
In statistics, the t-statistic is a ratio of the departure of an estimated parameter from its notional value and its standard error. It is used in hypothesis testing, for example in the Student s t-test, in the augmented Dickey Fuller test, and in bootstrapping.
The bean machine, also known as the quincunx or Galton box, is a device invented by Sir Francis Galton to demonstrate the central limit theorem, in particular that the normal distribution is approximate to the binomial distribution. Among its applications, it afforded insight into regression to the mean or "regression to mediocrity". The machine consists of a vertical board with interleaved rows of pins. Balls are dropped from the top, and bounce left and right as they hit the pins. Eventually, they are collected into one-ball-wide bins at the bottom. The height of ball columns in the bins approximates a bell curve. Overlaying Pascal's triangle onto the pins shows the number of different paths that can be taken to get to each bin. A large-scale working model of this device can be seen at the Museum of Science, Boston in the Mathematica exhibit.
In combinatorics, Bertrand's ballot problem is the question: "In an election where candidate A receives p votes and candidate B receives q votes with p > q, what is the probability that A will be strictly ahead of B throughout the count " The answer is  The result was first published by W. A. Whitworth in 1878, but is named after Joseph Louis Franc ois Bertrand who rediscovered it in 1887. In Bertrand's original paper, he sketches a proof based on a general formula for the number of favourable sequences using a recursion relation. He remarks that it seems probable that such a simple result could be proved by a more direct method. Such a proof was given by De sire  Andre , based on the observation that the unfavourable sequences can be divided into two equally probable cases, one of which (the case where B receives the first vote) is easily computed; he proves the equality by an explicit bijection. A variation of his method is popularly known as Andre 's reflection method, although Andre  did not use any reflections.
Several methods for measuring crime exist. Public surveys are sometimes conducted to estimate the amount of crime not reported to police. Such surveys are usually more reliable for assessing trends. However, they also have their limitations and generally don't procure statistics useful for local crime prevention, often ignore offenses against children and do not count offenders brought before the criminal justice system. Law enforcement agencies in some countries offer compilations of statistics for various types of crime. Two major methods for collecting crime data are law enforcement reports, which only reflect crimes that are reported, recorded, and not subsequently canceled; and victim studys (victimization statistical surveys), which rely on individual memory and honesty. For less frequent crimes such as intentional homicide and armed robbery, reported incidences are generally more reliable, but suffer from under-recording; for example, no criming in the United Kingdom sees over one third of reported violent crimes being not recorded by the police. Because laws and practices vary between jurisdictions, comparing crime statistics between and even within countries can be difficult: typically only violent deaths (homicide or manslaughter) can reliably be compared, due to consistent and high reporting and relative clear definition. The U.S. has two major data collection programs, the Uniform Crime Reports from the FBI and the National Crime Victimization Survey from the Bureau of Justice Statistics. However, the U.S. has no comprehensive infrastructure to monitor crime trends and report the information to related parties such as law enforcement. Research using a series of victim surveys in 18 countries of the European Union, funded by the European Commission, has reported (2005) that the level of crime in Europe has fallen back to the levels of 1990, and notes that levels of common crime have shown declining trends in the U.S., Canada, Australia and other industrialized countries as well. The European researchers say a general consensus identifies demographic change as the leading cause for this international trend. Although homicide and robbery rates rose in the U.S. in the 1980s, by the end of the century they had declined by 40%. However, the European research suggests that "increased use of crime prevention measures may indeed be the common factor behind the near universal decrease in overall levels of crime in the Western world", since decreases have been most pronounced in property crime and less so, if at all, in contact crimes.
A flood risk assessment (FRA) is an assessment of the risk of flooding from all flooding mechanisms, the identification of flood mitigation measures and should provide advice on actions to be taken before and during a flood. The sources of water which produce floods include: Groundwater (saturated groundwater) Vadose (water flowing the ground in an unsaturated state) Surface water Artificial water (burst water mains, canals or reservoirs) Rivers, streams or watercourses Sewers and drains For each of the sources of water, different hydraulic intensities occur. Floods can occur because of a combination of sources of flooding, such as high groundwater and an inadequate surface water drainage system. The topography, hydrogeology and physical attributes of the existing or proposed development need to be considered. A flood risk assessment should be an evaluation of the flood risk and the consequences and impact and vulnerability. Non-professional flood risk assessments can be produced by members of the public, Architects, environment assessors, or others who are not specifically professionally qualified in this field. However, it is a complex evaluation and such assessments they can be rejected by Authorities as inadequate, or could be considered as negligent in the event of a flooding event, damage and a claim to insurers being made. In the UK, the writing of professional flood risk assessments is undertaken by Civil Engineering Consultants. They will have membership of the Institution of Civil Engineers and are bound by their rules of professional conduct. A key requirement is to ensure such professional flood risk assessments are independent to all parties by carrying out their professional duties with complete objectivity and impartiality. Their professional advice should be supported by professional indemnity insurance for such specific professional advice ultimately held with a Lloyds of London underwriter. Professional flood risk assessments can cover single buildings, or whole regions. They can part of a due-dilligence process for existing householders or businesses, or can be required in England and Wales to provide independent evidence to a planning application on the flood risk.
The generalized normal distribution or generalized Gaussian distribution (GGD) is either of two families of parametric continuous probability distributions on the real line. Both families add a shape parameter to the normal distribution. To distinguish the two families, they are referred to below as "version 1" and "version 2". However this is not a standard nomenclature.
The q-exponential distribution is a probability distribution arising from the maximization of the Tsallis entropy under appropriate constraints, including constraining the domain to be positive. It is one example of a Tsallis distribution. The q-exponential is a generalization of the exponential distribution in the same way that Tsallis entropy is a generalization of standard Boltzmann Gibbs entropy or Shannon entropy. The exponential distribution is recovered as . Originally proposed by the statisticians George Box and David Cox in 1964, and known as the reverse Box Cox transformation for , a particular case of power transform in statistics.
Data fusion is the process of integration of multiple data and knowledge representing the same real-world object into a consistent, accurate, and useful representation.  Data fusion processes are often categorized as low, intermediate or high, depending on the processing stage at which fusion takes place. Low level data fusion combines several sources of raw data to produce new raw data. The expectation is that fused data is more informative and synthetic than the original inputs. For example, sensor fusion is also known as (multi-sensor) data fusion and is a subset of information fusion.
In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be Markov random field if it satisfies Markov properties. A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite. When the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model. In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision.
EpiData refers to a group of applications used in combination for creating documented data structures and analysis of quantitative data. The EpiData Association, which created the software, was created in 1999 and is based in Denmark. EpiData was developed in Pascal and uses open standards such as HTML where possible. EpiData is widely used by organizations and individuals to create and analyze large amounts of data. The World Health Organization (WHO) uses EpiData in its STEPS method of collecting epidemiological, medical, and public health data, for biostatistics, and for other quantitative-based projects. Epicentre, the research wing of Me decins Sans Frontie res, uses EpiData to manage data from its international research studies and field epidemiology studies. E.g.: Piola P, Fogg C et al.: Supervised versus unsupervised intake of six-dose artemether-lumefantrine for treatment of acute, uncomplicated Plasmodium falciparum malaria in Mbarara, Uganda: a randomised trial. Lancet. 2005 Apr 23-29;365(9469):1467-73 'PMID 15850630'. Other examples: 'PMID 16765397', 'PMID 15569777' or 'PMID 17160135'. EpiData has two parts: Epidata Entry   used for simple or programmed data entry and data documentation. It handles simple forms or related systems EpiData Analysis   performs basic statistical analysis, graphs, and comprehensive data management, such as recoding data, label values and variables, and basic statistics. This application can create control charts, such as pareto charts or p-charts, and many other methods to visualize and describe statistical data. The software is free; development is funded by governmental and non-governmental organizations like WHO.
Kendall's W (also known as Kendall's coefficient of concordance) is a non-parametric statistic. It is a normalization of the statistic of the Friedman test, and can be used for assessing agreement among raters. Kendall's W ranges from 0 (no agreement) to 1 (complete agreement). Suppose, for instance, that a number of people have been asked to rank a list of political concerns, from most important to least important. Kendall's W can be calculated from these data. If the test statistic W is 1, then all the survey respondents have been unanimous, and each respondent has assigned the same order to the list of concerns. If W is 0, then there is no overall trend of agreement among the respondents, and their responses may be regarded as essentially random. Intermediate values of W indicate a greater or lesser degree of unanimity among the various responses. While tests using the standard Pearson correlation coefficient assume normally distributed values and compare two sequences of outcomes at a time, Kendall's W makes no assumptions regarding the nature of the probability distribution and can handle any number of distinct outcomes. W is linearly related to the mean value of the Spearman's rank correlation coefficients between all pairs of the rankings over which it is calculated.
Rothamsted Research, previously known as the Rothamsted Experimental Station and then the Institute of Arable Crops Research, is one of the oldest agricultural research institutions in the world, having been founded in 1843. It is located at Harpenden in the English county of Hertfordshire. One of the station's best known and longest running experiments is the Park Grass Experiment, a biological study that started in 1856 and has been continuously monitored ever since.
In probability theory and in particular in information theory, total correlation (Watanabe 1960) is one of several generalizations of the mutual information. It is also known as the multivariate constraint (Garner 1962) or multiinformation (Studeny  & Vejnarova  1999). It quantifies the redundancy or dependency among a set of n random variables.
In statistics, the exponentiated Weibull family of probability distributions was introduced by Mudholkar and Srivastava (1993) as an extension of the Weibull family obtained by adding a second shape parameter. The cumulative distribution function for the exponentiated Weibull distribution is  for x > 0, and F(x; k;  ;  ) = 0 for x < 0. Here k > 0 is the first shape parameter,   > 0 is the second shape parameter and   > 0 is the scale parameter of the distribution. The density is  There are two important special cases:   = 1 gives the Weibull distribution; k = 1 gives the exponentiated exponential distribution.
A crossover study, also referred to as a crossover trial, is a longitudinal study in which subjects receive a sequence of different treatments (or exposures). While crossover studies can be observational studies, many important crossover studies are controlled experiments, which are discussed in this article. Crossover designs are common for experiments in many scientific disciplines, for example psychology, education, pharmaceutical science, and medicine. Randomized, controlled crossover experiments are especially important in health care. In a randomized clinical trial, the subjects are randomly assigned to different arms of the study which receive different treatments. When the randomized clinical trial is a repeated measures design, the same measures are collected multiple times for each subject. A crossover clinical trial is a repeated measures design in which each patient is randomly assigned to a sequence of treatments, including at least two treatments (of which one "treatment" may be a standard treatment or a placebo). Nearly all crossover designs have "balance", which means that all subjects should receive the same number of treatments and that all subjects participate for the same number of periods. In most crossover trials, in fact, each subject receives all treatments. Statisticians suggest that designs have four periods, a design which allows studies to be truncated to three periods while still enjoying greater efficiency than the two-period design. However, the two-period design is often taught in non-statistical textbooks, partly because of its simplicity.
The Gallagher Index (or least squares index) is used to measure the disproportionality of an electoral outcome; that is, the difference between the percentage of votes received, and the percentage of seats a party gets in the resulting legislature. This is especially useful for comparing proportionality across electoral systems. The index involves taking the square root of half the sum of the squares of the difference between percent of vote and percent of seats for each of the political parties.  The index weighs the deviations by their own value, creating a responsive index, ranging from 0 to 100. The lower the index value the lower the disproportionality and vice versa. Michael Gallagher, who created the index, included 'other' parties as a whole category, and Arend Lijphart modified it, excluding those parties. Unlike the well-known Loosemore Hanby index, the Gallagher index is less sensitive to small discrepancies.
The Pearson distribution is a family of continuous probability distributions. It was first published by Karl Pearson in 1895 and subsequently extended by him in 1901 and 1916 in a series of articles on biostatistics.
Not to be confused with multi-label classification. In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification). While some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies. Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.
A paid or incentivized survey is a type of statistical survey where the participants/members are rewarded through an incentive program, generally entry into a sweepstakes program or a small cash reward, for completing one or more surveys.
For homogeneity of variance see homoscedasticity. In statistics, homogeneity and its opposite, heterogeneity, arise in describing the properties of a dataset, or several datasets. They relate to the validity of the often convenient assumption that the statistical properties of any one part of an overall dataset are the same as any other part. In meta-analysis, which combines the data from several studies, homogeneity measures the differences or similarities between the several studies (see also Study heterogeneity). Homogeneity can be studied to several degrees of complexity. For example, considerations of homoscedasticity examine how much the variability of data-values changes throughout a dataset. However, questions of homogeneity apply to all aspects of the statistical distributions, including the location parameter. Thus, a more detailed study would examine changes to the whole of the marginal distribution. An intermediate-level study might move from looking at the variability to studying changes in the skewness. In addition to these, questions of homogeneity apply also to the joint distributions. The concept of homogeneity can be applied in many different ways and, for certain types of statistical analysis, it is used to look for further properties that might need to be treated as varying within a dataset once some initial types of non-homogeneity have been dealt with.
In statistics, a generalized additive model (GAM) is a generalized linear model in which the linear predictor depends linearly on unknown smooth functions of some predictor variables, and interest focuses on inference about these smooth functions. GAMs were originally developed by Trevor Hastie and Robert Tibshirani to blend properties of generalized linear models with additive models. The model relates a univariate response variable, Y, to some predictor variables, xi. An exponential family distribution is specified for Y (for example normal, binomial or Poisson distributions) along with a link function g (for example the identity or log functions) relating the expected value of Y to the predictor variables via a structure such as  The functions fi(xi) may be functions with a specified parametric form (for example a polynomial, or a coefficient depending on the levels of a factor variable) or may be specified non-parametrically, or semi-parametrically, simply as 'smooth functions', to be estimated by non-parametric means. So a typical GAM might use a scatterplot smoothing function, such as a locally weighted mean, for f1(x1), and then use a factor model for f2(x2). This flexibility to allow non-parametric fits with relaxed assumptions on the actual relationship between response and predictor, provides the potential for better fits to data than purely parametric models, but arguably with some loss of interpretability.
In the theory of stochastic processes, the filtering problem is a mathematical model for a number of filtering problems in signal processing and the like. The general idea is to form some kind of "best estimate" for the true value of some system, given only some (potentially noisy) observations of that system. The problem of optimal non-linear filtering (even for the non-stationary case) was solved by Ruslan L. Stratonovich (1959, 1960), see also Harold J. Kushner's work  and Moshe Zakai's, who introduced a simplified dynamics for the unnormalized conditional law of the filter known as Zakai equation. The solution, however, is infinite-dimensional in the general case. Certain approximations and special cases are well-understood: for example, the linear filters are optimal for Gaussian random variables, and are known as the Wiener filter and the Kalman-Bucy filter. More generally, as the solution is infinite dimensional, it requires finite dimensional approximations to be implemented in a computer with finite memory. A finite dimensional approximated nonlinear filter may be more based on heuristics, such as the Extended Kalman Filter or the Assumed Density Filters, or more methodologically oriented such as for example the Projection Filters, some sub-families of which are shown to coincide with the Assumed Density Filters. In general, if the separation principle applies, then filtering also arises as part of the solution of an optimal control problem. For example, the Kalman filter is the estimation part of the optimal control solution to the Linear-quadratic-Gaussian control problem.
The conditional change model in statistics is the analytic procedure in which change scores are regressed on baseline values, together with the explanatory variables of interest (often including indicators of treatment groups). The method has some substantial advantages over the usual two-sample t-test recommended in textbooks.
An intention-to-treat (ITT) analysis of the results of an experiment is based on the initial treatment assignment and not on the treatment eventually received. ITT analysis is intended to avoid various misleading artifacts that can arise in intervention research such as non-random attrition of participants from the study or crossover. ITT is also simpler than other forms of study design and analysis because it does not require observation of compliance status for units assigned to different treatments or incorporation of compliance into the analysis. Although ITT analysis is widely employed in published clinical trials, it can be incorrectly described and there are some issues with its application. Furthermore, there is no consensus on how to carry out an ITT analysis in the presence of missing outcome data.
In statistics, resampling is any of a variety of methods for doing one of the following: Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping) Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests) Validating models by using random subsets (bootstrapping, cross validation) Common resampling techniques include bootstrapping, jackknifing and permutation tests.
In multivariate statistics, exploratory factor analysis (EFA) is a statistical method used to uncover the underlying structure of a relatively large set of variables. EFA is a technique within factor analysis whose overarching goal is to identify the underlying relationships between measured variables. It is commonly used by researchers when developing a scale (a scale is a collection of questions used to measure a particular research topic) and serves to identify a set of latent constructs underlying a battery of measured variables. It should be used when the researcher has no a priori hypothesis about factors or patterns of measured variables. Measured variables are any one of several attributes of people that may be observed and measured. An example of a measured variable would be the physical height of a human being. Researchers must carefully consider the number of measured variables to include in the analysis. EFA procedures are more accurate when each factor is represented by multiple measured variables in the analysis. EFA is based on the common factor model. Within the common factor model, a function of common factors, unique factors, and errors of measurements expresses measured variables. Common factors influence two or more measured variables, while each unique factor influences only one measured variable and does not explain correlations among measured variables. EFA assumes that any indicator/measured variable may be associated with any factor. When developing a scale, researchers should use EFA first before moving on to confirmatory factor analysis (CFA). EFA requires the researcher to make a number of important decisions about how to conduct the analysis because there is no one set method.
In statistics, a receiver operating characteristic (ROC), or ROC curve, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, or recall in machine learning. The false-positive rate is also known as the fall-out and can be calculated as (1 - specificity). The ROC curve is thus the sensitivity as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from  to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability in x-axis. ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making. The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, and other areas for many decades and is increasingly used in machine learning and data mining research. The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes.
Generalizability theory, or G Theory, is a statistical framework for conceptualizing, investigating, and designing reliable observations. It is used to determine the reliability (i.e., reproducibility) of measurements under specific conditions. It is particularly useful for assessing the reliability of performance assessments. It was originally introduced in Cronbach, L.J., Nageswari, R., & Gleser, G.C. (1963).
In statistical classification, the Bayes error rate is the lowest possible error rate for any classifier of a random outcome (into, for example, one of two categories) and is analogous to the irreducible error. A number of approaches to the estimation of the Bayes error rate exist. One method seeks to obtain analytical bounds which are inherently dependent on distribution parameters, and hence difficult to estimate. Another approach focuses on class densities, while yet another method combines and compares various classifiers. The Bayes error rate finds important use in the study of patterns and machine learning techniques.
In mathematical modeling, a guess value is more commonly called a starting value or initial value. These are necessary for most optimization problems which use search algorithms, because those algorithms are mainly deterministic and iterative, and they need to start somewhere. One common type of application is nonlinear regression.
Empirical Bayes methods are procedures for statistical inference in which the prior distribution is estimated from the data. This approach stands in contrast to standard Bayesian methods, for which the prior distribution is fixed before any data are observed. Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out. Empirical Bayes, also known as maximum marginal likelihood, represents one approach for setting hyperparameters.
In statistics, local asymptotic normality is a property of a sequence of statistical models, which allows this sequence to be asymptotically approximated by a normal location model, after a rescaling of the parameter. An important example when the local asymptotic normality holds is in the case of iid sampling from a regular parametric model. The notion of local asymptotic normality was introduced by Le Cam (1960).
In time series analysis, the cross-spectrum is used as part of a frequency domain analysis of the cross-correlation or cross-covariance between two time series.
In statistics and probability theory, the nonparametric skew is a statistic occasionally used with random variables that take real values. It is a measure of the skewness of a random variable's distribution that is, the distribution's tendency to "lean" to one side or the other of the mean. Its calculation does not require any knowledge of the form of the underlying distribution hence the name nonparametric. It has some desirable properties: it is zero for any symmetric distribution; it is unaffected by a scale shift; and it reveals either left- or right-skewness equally well. Although its use has been mentioned in older textbooks it appears to have gone out of fashion. In statistical samples it has been shown to be less powerful than the usual measures of skewness in detecting departures of the population from normality.
The multivariate stable distribution is a multivariate probability distribution that is a multivariate generalisation of the univariate stable distribution. The multivariate stable distribution defines linear relations between stable distribution marginals. In the same way as for the univariate case, the distribution is defined in terms of its characteristic function. The multivariate stable distribution can also be thought as an extension of the multivariate normal distribution. It has parameter,  , which is defined over the range 0 <     2, and where the case   = 2 is equivalent to the multivariate normal distribution. It has an additional skew parameter that allows for non-symmetric distributions, where the multivariate normal distribution is symmetric.
Probability distribution fitting or simply distribution fitting is the fitting of a probability distribution to a series of data concerning the repeated measurement of a variable phenomenon. The aim of distribution fitting is to predict the probability or to forecast the frequency of occurrence of the magnitude of the phenomenon in a certain interval. There are many probability distributions (see list of probability distributions) of which some can be fitted more closely to the observed frequency of the data than others, depending on the characteristics of the phenomenon and of the distribution. The distribution giving a close fit is supposed to lead to good predictions. In distribution fitting, therefore, one needs to select a distribution that suits the data well.
In statistics, best linear unbiased prediction (BLUP) is used in linear mixed models for the estimation of random effects. BLUP was derived by Charles Roy Henderson in 1950 but the term "best linear unbiased predictor" (or "prediction") seems not to have been used until 1962. "Best linear unbiased predictions" (BLUPs) of random effects are similar to best linear unbiased estimates (BLUEs) (see Gauss Markov theorem) of fixed effects. The distinction arises because it is conventional to talk about estimating fixed effects but predicting random effects, but the two terms are otherwise equivalent. (This is a bit strange since the random effects have already been "realized"; they already exist. The use of the term "prediction" may be because in the field of animal breeding in which Henderson worked, the random effects were usually genetic merit, which could be used to predict the quality of offspring (Robinson page 28)). However, the equations for the "fixed" effects and for the random effects are different. In practice, it is often the case that the parameters associated with the random effect(s) term(s) are unknown; these parameters are the variances of the random effects and residuals. Typically the parameters are estimated and plugged into the predictor, leading to the Empirical Best Linear Unbiased Predictor (EBLUP). Notice that by simply plugging in the estimated parameter into the predictor, additional variability is unaccounted for, leading to overly optimistic prediction variances for the EBLUP. Best linear unbiased predictions are similar to empirical Bayes estimates of random effects in linear mixed models, except that in the latter case, where weights depend on unknown values of components of variance, these unknown variances are replaced by sample-based estimates.
In statistics, a power transform is a family of functions that are applied to create a monotonic transformation of data using power functions. This is a useful data transformation technique used to stabilize variance, make the data more normal distribution-like, improve the validity of measures of association such as the Pearson correlation between variables and for other data stabilization procedures.
In statistics, kernel Fisher discriminant analysis (KFD), also known as generalized discriminant analysis and kernel discriminant analysis, is a kernelized version of linear discriminant analysis (LDA). It is named after Ronald Fisher. Using the kernel trick, LDA is implicitly performed in a new feature space, which allows non-linear mappings to be learned.
In regression analysis specification is the process of developing a regression model. This process consists of selecting an appropriate functional form for the model and choosing which variables to include. For instance, one may specify the functional relationship  between personal income  and human capital in terms of schooling  and on-the-job experience  as:  where  is the unexplained error term that is supposed to be independent and identically distributed. If assumptions of the regression model are correct, the least squares estimates of the parameters  and  will be efficient and unbiased. Hence specification diagnostics usually involve testing the first to fourth moment of the residuals.
In statistics, the Vuong closeness test is likelihood-ratio-based test for model selection using the Kullback-Leibler information criterion. This statistic makes probabilistic statements about two models. They can be nested, non-nested or overlapping. The statistic tests the null hypothesis that the two models are equally close to the true data generating process, against the alternative that one model is closer. It cannot make any decision whether the "closer" model is the true model. With non-nested models and iid exogenous variables, model 1 (2) is preferred with significance level  , if the z statistic  with  exceeds the positive (falls below the negative) (1    )-quantile of the standard normal distribution. Here K1 and K2 are the numbers of parameters in models 1 and 2 respectively. The numerator is the difference between the maximum likelihoods of the two models, corrected for the number of coefficients analogous to the BIC, the term in the denominator of the expression for Z, , is defined by setting  equal to either the mean of the squares of the pointwise log-likelihood ratios , or to the sample variance of these values, where  For nested or overlapping models the statistic  has to be compared to critical values from a weighted sum of chi squared distributions. This can be approximated by a gamma distribution:  with  and   is a vector of eigenvalues of a matrix of conditional expectations. The computation is quite difficult, so that in the overlapping and nested case many authors only derive statements from a subjective evaluation of the Z statistic (is it subjectively "big enough" to accept my hypothesis ). Vuong's test for non-nested models has sometimes been used to determine whether zero-inflation is present in data. As a given model and its zero-inflated counterpart are not non-nested, this is an erroneous use of the test
Statistical semantics is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access". How can we figure out what words mean, simply by looking at patterns of words in huge collections of text  What are the limits to this approach to understanding words 
Linguistic demography is the statistical study of languages among all populations. Estimating the number of speakers of a given language is not straightforward, and various estimates may diverge considerably. This is first of all due to the question of defining "language" vs. "dialect". Identification of varieties as a single language or as distinct languages is often based on ethnic, cultural, or political considerations rather than mutual intelligibility. The second difficulty is multilingualism, complicating the definition of "native language". Finally, in many countries, insufficient census data add to the difficulties. Demolinguistics is a branch of Sociology of language observing linguistic trends as affected by population distribution and redistribution and by the status of societies.  
In statistics, pooled variance (also known as combined, composite, or overall variance) is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same. If the populations are indexed , then the pooled variance  can be estimated by the weighted average of the sample variances   where  is the sample size of population  and  is . Use of  weighting factors instead of  comes from Bessel's correction. Under the assumption of equal population variances, the pooled sample variance provides a higher precision estimate of variance than the individual sample variances. This higher precision can lead to increased statistical power when used in statistical tests that compare the populations, such as the t-test. The square-root of a pooled variance estimator is known as a pooled standard deviation (also known as combined, composite, or overall standard deviation).
In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space. The overall approach works in jointly input-output space and an initial version was proposed by Neil Gershenfeld.
A structural break is a concept in econometrics. A structural break appears when we see an unexpected shift in a (macroeconomic) time series. This can lead to huge forecasting errors and unreliability of the model in general. This issue was popularised by David Hendry.
The Rasch model, named after Georg Rasch, is a psychometric model for analyzing categorical data, such as answers to questions on a reading assessment or questionnaire responses, as a function of the trade-off between (a) the respondent's abilities, attitudes or personality traits and (b) the item difficulty. For example, they may be used to estimate a student's reading ability, or the extremity of a person's attitude to capital punishment from responses on a questionnaire. In addition to psychometrics and educational research, the Rasch model and its extensions are used in other areas, including the health profession and market research because of their general applicability. The mathematical theory underlying Rasch models is a special case of item response theory and, more generally, a special case of a generalized linear model. However, there are important differences in the interpretation of the model parameters and its philosophical implications that separate proponents of the Rasch model from the item response modeling tradition. A central aspect of this divide relates to the role of specific objectivity, a defining property of the Rasch model according to Georg Rasch, as a requirement for successful measurement.
In mathematics, mixing is an abstract concept originating from physics: the attempt to describe the irreversible thermodynamic process of mixing in the everyday world: mixing paint, mixing drinks, etc. The concept appears in ergodic theory the study of stochastic processes and measure-preserving dynamical systems. Several different definitions for mixing exist, including strong mixing, weak mixing and topological mixing, with the last not requiring a measure to be defined. Some of the different definitions of mixing can be arranged in a hierarchical order; thus, strong mixing implies weak mixing. Furthermore, weak mixing (and thus also strong mixing) implies ergodicity: that is, every system that is weakly mixing is also ergodic (and so one says that mixing is a "stronger" notion than ergodicity).
In colloquial language, an average is the sum of a list of numbers divided by the number of numbers in the list. In mathematics and statistics, this would be called the arithmetic mean. In statistics, mean, median, and mode are all known as measures of central tendency.
In experimental methodology, a round robin test is an interlaboratory test (measurement, analysis, or experiment) performed independently several times. This can involve multiple independent scientists performing the test with the use of the same method in different equipment, or a variety of methods and equipment. In reality it is often a combination of the two, for example if a sample is analysed, or one (or more) of its properties is measured by different laboratories using different methods, or even just by different units of equipment of identical construction. A round robin program is a Measurement Systems Analysis technique which uses Analysis of Variance (ANOVA) random effects model to assess a measurement system.
In statistics, a simple random sample is a subset of individuals (a sample) chosen from a larger set (a population). Each individual is chosen randomly and entirely by chance, such that each individual has the same probability of being chosen at any stage during the sampling process, and each subset of k individuals has the same probability of being chosen for the sample as any other subset of k individuals. This process and technique is known as simple random sampling, and should not be confused with systematic random sampling. A simple random sample is an unbiased surveying technique. Simple random sampling is a basic type of sampling, since it can be a component of other more complex sampling methods. The principle of simple random sampling is that every object has the same probability of being chosen. For example, suppose N college students want to get a ticket for a basketball game, but there are only X < N tickets for them, so they decide to have a fair way to see who gets to go. Then, everybody is given a number in the range from 0 to N-1, and random numbers are generated, either electronically or from a table of random numbers. Numbers outside the range from 0 to N-1 are ignored, as are any numbers previously selected. The first X numbers would identify the lucky ticket winners. In small populations and often in large ones, such sampling is typically done "without replacement", i.e., one deliberately avoids choosing any member of the population more than once. Although simple random sampling can be conducted with replacement instead, this is less common and would normally be described more fully as simple random sampling with replacement. Sampling done without replacement is no longer independent, but still satisfies exchangeability, hence many results still hold. Further, for a small sample from a large population, sampling without replacement is approximately the same as sampling with replacement, since the odds of choosing the same individual twice is low. An unbiased random selection of individuals is important so that if a large number of samples were drawn, the average sample would accurately represent the population. However, this does not guarantee that a particular sample is a perfect representation of the population. Simple random sampling merely allows one to draw externally valid conclusions about the entire population based on the sample. Conceptually, simple random sampling is the simplest of the probability sampling techniques. It requires a complete sampling frame, which may not be available or feasible to construct for large populations. Even if a complete frame is available, more efficient approaches may be possible if other useful information is available about the units in the population. Advantages are that it is free of classification error, and it requires minimum advance knowledge of the population other than the frame. Its simplicity also makes it relatively easy to interpret data collected in this manner. For these reasons, simple random sampling best suits situations where not much information is available about the population and data collection can be efficiently conducted on randomly distributed items, or where the cost of sampling is small enough to make efficiency less important than simplicity. If these conditions do not hold, stratified sampling or cluster sampling may be a better choice.
In mathematics, a ca dla g (French "continue a  droite, limite a  gauche"), RCLL ( right continuous with left limits ), or corlol ("continuous on (the) right, limit on (the) left") function is a function defined on the real numbers (or a subset of them) that is everywhere right-continuous and has left limits everywhere. Ca dla g functions are important in the study of stochastic processes that admit (or even require) jumps, unlike Brownian motion, which has continuous sample paths. The collection of ca dla g functions on a given domain is known as Skorokhod space. Two related terms are ca gla d, standing for "continue a  gauche, limite a  droite", the left-right reversal of ca dla g, and ca lla l for "continue a  l'un, limite a  l autre" (continuous on one side, limit on the other side), for a function which is interchangeably either ca dla g or ca gla d at each point of the domain. A more mellifluous English term is ricowil or, more whimsically, ricowilli, both terms standing for "right continuous with left limits".
In mathematics, the Freidlin Wentzell theorem is a result in the large deviations theory of stochastic processes. Roughly speaking, the Freidlin Wentzell theorem gives an estimate for the probability that a (scaled-down) sample path of an Ito  diffusion will stray far from the mean path. This statement is made precise using rate functions. The Freidlin Wentzell theorem generalizes Schilder's theorem for standard Brownian motion.
A numeric sequence is said to be statistically random when it contains no recognizable patterns or regularities; sequences such as the results of an ideal dice roll, or the digits of   exhibit statistical randomness. Statistical randomness does not necessarily imply "true" randomness, i.e., objective unpredictability. Pseudorandomness is sufficient for many uses, such as statistics, hence the name statistical randomness. Global randomness and local randomness are different. Most philosophical conceptions of randomness are global because they are based on the idea that "in the long run" a sequence looks truly random, even if certain sub-sequences would not look random. In a "truly" random sequence of numbers of sufficient length, for example, it is probable there would be long sequences of nothing but repeating numbers, though on the whole the sequence might be random. Local randomness refers to the idea that there can be minimum sequence lengths in which random distributions are approximated. Long stretches of the same numbers, even those generated by "truly" random processes, would diminish the "local randomness" of a sample (it might only be locally random for sequences of 10,000 numbers; taking sequences of less than 1,000 might not appear random at all, for example). A sequence exhibiting a pattern is not thereby proved not statistically random. According to principles of Ramsey theory, sufficiently large objects must necessarily contain a given substructure ("complete disorder is impossible"). Chaos theorists disagree with Ramsey Theory. Legislation concerning gambling imposes certain standards of statistical randomness to slot machines.
The False discovery rate (FDR) is one way of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons. FDR-controlling procedures are designed to control the expected proportion of rejected null hypotheses that were incorrect rejections ("false discoveries"). FDR-controlling procedures provide less stringent control of Type I errors compared to familywise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of at least one Type I error. Thus, FDR-controlling procedures have greater power, at the cost of increased rates of Type I errors.
In statistics, the so-called 68 95 99.7 rule is a shorthand used to remember the percentage of values that lie within a band around the mean in a normal distribution with a width of one, two and three standard deviations, respectively; more accurately, 68.27%, 95.45% and 99.73% of the values lie within one, two and three standard deviations of the mean, respectively. In mathematical notation, these facts can be expressed as follows, where x is an observation from a normally distributed random variable,   is the mean of the distribution, and   is its standard deviation:  In the empirical sciences the so-called three-sigma rule of thumb expresses a conventional heuristic that "nearly all" values are taken to lie within three standard deviations of the mean, i.e. that it is empirically useful to treat 99.7% probability as "near certainty". The usefulness of this heuristic of course depends significantly on the question under consideration, and there are other conventions, e.g. in the social sciences a result may be considered "significant" if its confidence level is of the order of a two-sigma effect (95%), while in particle physics, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a "discovery". The "three sigma rule of thumb" is related to a result also known as the three-sigma rule, which states that even for non-normally distributed variables, at least 98% of cases should fall within properly-calculated three-sigma intervals.
Alignments of random points in the plane can be demonstrated by statistics to be remarkably and counter-intuitively easy to find when a large number of random points are marked on a bounded flat surface. This has been put forward as a demonstration that ley lines and other similar mysterious alignments believed by some to be phenomena of deep significance might exist solely due to chance alone, as opposed to the supernatural or anthropological explanations put forward by their proponents. The topic has also been studied in the fields of computer vision and astronomy. A number of studies have examined the mathematics of alignment of random points on the plane. In all of these, the width of the line - the allowed displacement of the positions of the points from a perfect straight line - is important. It allows the fact that real-world features are not mathematical points, and that their positions need not line up exactly for them to be considered in alignment. Alfred Watkins, in his classic work on ley lines The Old Straight Track, used the width of a pencil line on a map as the threshold for the tolerance of what might be regarded as an alignment. For example, using a 1 mm pencil line to draw alignments on an 1:50,000 Ordnance Survey map, the corresponding width on the ground would be 50 m.
In probability theory and directional statistics, a wrapped Cauchy distribution is a wrapped probability distribution that results from the "wrapping" of the Cauchy distribution around the unit circle. The Cauchy distribution is sometimes known as a Lorentzian distribution, and the wrapped Cauchy distribution may sometimes be referred to as a wrapped Lorentzian distribution. The wrapped Cauchy distribution is often found in the field of spectroscopy where it is used to analyze diffraction patterns (e.g. see Fabry Pe rot interferometer)
In statistical theory, Chauvenet's criterion (named for William Chauvenet) is a means of assessing whether one piece of experimental data   an outlier   from a set of observations, is likely to be spurious.
In epidemiology, Mendelian randomization is a method of using measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in non-experimental studies. The design was first described by Gray and Wheatley (1991) as a method for obtaining unbiased estimates of the effects of a putative causal variable without conducting a traditional randomised trial. These authors also coined the term Mendelian randomization. The design has a powerful control for reverse causation and confounding which otherwise bedevil epidemiological studies.
A winsorized mean is a winsorized statistical measure of central tendency, much like the mean and median, and even more similar to the truncated mean. It involves the calculation of the mean after replacing given parts of a probability distribution or sample at the high and low end with the most extreme remaining values, typically doing so for an equal amount of both extremes; often 10 to 25 percent of the ends are replaced.
In probability theory, the Lindley equation, Lindley recursion or Lindley processes is a discrete-time stochastic process An where n takes integer values and  An + 1 = max(0, An + Bn).  Processes of this form can be used to describe the waiting time of customers in a queue or evolution of a queue length over time. The idea was first proposed in the discussion following Kendall's 1951 paper.
A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing. A hypothesis test is typically specified in terms of a test statistic, considered as a numerical summary of a data-set that reduces the data to one value that can be used to perform the hypothesis test. In general, a test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis, where such an alternative is prescribed, or that would characterize the null hypothesis if there is no explicitly stated alternative hypothesis. An important property of a test statistic is that its sampling distribution under the null hypothesis must be calculable, either exactly or approximately, which allows p-values to be calculated. A test statistic shares some of the same qualities of a descriptive statistic, and many statistics can be used as both test statistics and descriptive statistics. However, a test statistic is specifically intended for use in statistical testing, whereas the main quality of a descriptive statistic is that it is easily interpretable. Some informative descriptive statistics, such as the sample range, do not make good test statistics since it is difficult to determine their sampling distribution.
A rank abundance curve or Whittaker plot is a chart used by ecologists to display relative species abundance, a component of biodiversity. It can also be used to visualize species richness and species evenness. It overcomes the shortcomings of biodiversity indices that cannot display the relative role different variables played in their calculation.  The curve is a 2D chart with relative abundance on the Y-axis and the abundance rank on the X-axis. X-axis: The abundance rank. The most abundant species is given rank 1, the second most abundant is 2 and so on. Y-axis: The relative abundance. Usually measured on a log scale, this is a measure of a species abundance (e.g., the number of individuals) relative to the abundance of other species.
The stimulus response model is a characterization of a statistical unit (such as a neuron) as a black box model, predicting a quantitative response to a quantitative stimulus, for example one administered by a researcher.
In probability theory, the inverse Gaussian distribution (also known as the Wald distribution) is a two-parameter family of continuous probability distributions with support on (0, ). Its probability density function is given by  for x > 0, where  is the mean and  is the shape parameter. As   tends to infinity, the inverse Gaussian distribution becomes more like a normal (Gaussian) distribution. The inverse Gaussian distribution has several properties analogous to a Gaussian distribution. The name can be misleading: it is an "inverse" only in that, while the Gaussian describes a Brownian Motion's level at a fixed time, the inverse Gaussian describes the distribution of the time a Brownian Motion with positive drift takes to reach a fixed positive level. Its cumulant generating function (logarithm of the characteristic function) is the inverse of the cumulant generating function of a Gaussian random variable. To indicate that a random variable X is inverse Gaussian-distributed with mean   and shape parameter   we write
Balanced repeated replication is a statistical technique for estimating the sampling variability of a statistic obtained by stratified sampling.
In probability theory, a transition rate matrix (also known as an intensity matrix or infinitesimal generator matrix) is an array of numbers describing the rate a continuous time Markov chain moves between states. In a transition rate matrix Q (sometimes written A) element qij (for i =  j) denotes the rate departing from i and arriving in state j. Diagonal elements qii are defined such that  and therefore the rows of the matrix sum to zero.
A discrete frequency domain is a frequency domain that is discrete rather than continuous. For example, the discrete Fourier transform maps a function having a discrete time domain into one having a discrete frequency domain. The discrete-time Fourier transform, on the other hand, maps functions with discrete time (discrete-time signals) to functions that have a continuous frequency domain.
SPC XL is a statistical add-in for Microsoft Excel. SPC XL is a replacement for SPC KISS which was released in 1993 making it one of the oldest statistical addons to Excel. SPC XL provides statistical analysis including Control chart, Process capability, Histogram, Pareto chart, and ANOVA Gage R&R. SPC XL is compatible with Microsoft Excel 2000, 2002 & 2003, and Excel 2007.
A diversity index is a quantitative measure that reflects how many different types (such as species) there are in a dataset, and simultaneously takes into account how evenly the basic entities (such as individuals) are distributed among those types. The value of a diversity index increases both when the number of types increases and when evenness increases. For a given number of types, the value of a diversity index is maximized when all types are equally abundant. When diversity indices are used in ecology, the types of interest are usually species, but they can also be other categories, such as genera, families, functional types or haplotypes. The entities of interest are usually individual plants or animals, and the measure of abundance can be, for example, number of individuals, biomass or coverage. In demography, the entities of interest can be people, and the types of interest various demographic groups. In information science, the entities can be characters and the types the different letters of the alphabet. The most commonly used diversity indices are simple transformations of the effective number of types (also known as 'true diversity'), but each diversity index can also be interpreted in its own right as a measure corresponding to some real phenomenon (but a different one for each diversity index).
In statistics, the midhinge is the average of the first and third quartiles and is thus a measure of location. Equivalently, it is the 25% trimmed mid-range or 25% midsummary; it is an L-estimator. The midhinge is complemented by the H-spread, or interquartile range, which is the difference of the third and first quartiles and which is a measure of statistical dispersion, in sense that if one knows the midhinge and the interquartile range, one can find the first and third quartiles. The use of the term "hinge" for the lower or upper quartiles derives from John Tukey's work on exploratory data analysis, and "midhinge" is a fairly modern term dating from around that time. The midhinge is slightly simpler to calculate than the trimean, which originated in the same context and equals the average of the median and the midhinge.
Tampering in the context of a controlled process is adjusting the process on the basis of outcomes which are within the expected range of variability. The net result is to re-align the process so that an increased proportion of the output is out of specification. The term was introduced in this context by W. Edwards Deming, and he was a strong proponent of using control charts to avoid tampering.
In probability theory, the "standard" Cauchy distribution is the probability distribution whose probability density function (pdf) is  for x real. This has median 0, and first and third quartiles respectively  1 and +1. Generally, a Cauchy distribution is any probability distribution belonging to the same location-scale family as this one. Thus, if X has a standard Cauchy distribution and   is any real number and   > 0, then Y =   +  X has a Cauchy distribution whose median is   and whose first and third quartiles are respectively       and   +  . McCullagh's parametrization, introduced by Peter McCullagh, professor of statistics at the University of Chicago uses the two parameters of the non-standardised distribution to form a single complex-valued parameter, specifically, the complex number   =   + i , where i is the imaginary unit. It also extends the usual range of scale parameter to include   < 0. Although the parameter is notionally expressed using a complex number, the density is still a density over the real line. In particular the density can be written using the real-valued parameters   and  , which can each take positive or negative values, as  where the distribution is regarded as degenerate if   = 0. An alternative form for the density can be written using the complex parameter   =   + i  as  where . To the question "Why introduce complex numbers when only real-valued random variables are involved ", McCullagh wrote: In other words, if the random variable Y has a Cauchy distribution with complex parameter  , then the random variable Y * defined above has a Cauchy distribution with parameter (a  + b)/(c  + d). McCullagh also wrote, "The distribution of the first exit point from the upper half-plane of a Brownian particle starting at   is the Cauchy density on the real line with parameter  ." In addition, McCullagh shows that the complex-valued parameterisation allows a simple relationship to be made between the Cauchy and the "circular Cauchy distribution".
In statistics, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population are less likely to be included than others. It results in a biased sample, a non-random sample of a population (or non-human factors) in which all individuals, or instances, were not equally likely to have been selected. If this is not accounted for, results can be erroneously attributed to the phenomenon under study rather than to the method of sampling. Medical sources sometimes refer to sampling bias as ascertainment bias. Ascertainment bias has basically the same definition, but is still sometimes classified as a separate type of bias.
In combinatorial mathematics, group testing refers to any procedure which breaks up the task of locating elements of a set which have certain properties into tests on subsets ("groups") rather than on individual elements. A familiar example of this type of technique is the false coin problem of recreational mathematics. In this problem there are n coins and one of them is false, weighing less than a real coin. The objective is to find the false coin, using a balance scale, in the fewest number of weighings. By repeatedly dividing the coins in half and comparing the two halves, the false coin can be found quickly as it is always in the lighter half. Schemes for carrying out such group testing can be simple or complex and the tests involved at each stage may be different. Schemes in which the tests for the next stage depend on the results of the previous stages are called adaptive procedures, while schemes designed so that all the tests are known beforehand are called non-adaptive procedures. The structure of the scheme of the tests involved in a non-adaptive procedure is known as a pooling design.
In probability theory and statistics, a normal variance-mean mixture with mixing probability density  is the continuous probability distribution of a random variable  of the form  where ,  and  are real numbers, and random variables  and  are independent,  is normally distributed with mean zero and variance one, and  is continuously distributed on the positive half-axis with probability density function . The conditional distribution of  given  is thus a normal distribution with mean  and variance . A normal variance-mean mixture can be thought of as the distribution of a certain quantity in an inhomogeneous population consisting of many different normal distributed subpopulations. It is the distribution of the position of a Wiener process (Brownian motion) with drift  and infinitesimal variance  observed at a random time point independent of the Wiener process and with probability density function . An important example of normal variance-mean mixtures is the generalised hyperbolic distribution in which the mixing distribution is the generalized inverse Gaussian distribution. The probability density function of a normal variance-mean mixture with mixing probability density  is  and its moment generating function is  where  is the moment generating function of the probability distribution with density function , i.e.
In statistics, the intraclass correlation (or the intraclass correlation coefficient, abbreviated ICC) is a descriptive statistic that can be used when quantitative measurements are made on units that are organized into groups. It describes how strongly units in the same group resemble each other. While it is viewed as a type of correlation, unlike most other correlation measures it operates on data structured as groups, rather than data structured as paired observations. The intraclass correlation is commonly used to quantify the degree to which individuals with a fixed degree of relatedness (e.g. full siblings) resemble each other in terms of a quantitative trait (see heritability). Another prominent application is the assessment of consistency or reproducibility of quantitative measurements made by different observers measuring the same quantity.
In queueing theory, a discipline within the mathematical theory of probability, quasireversibility (sometimes QR) is a property of some queues. The concept was first identified by Richard R. Muntz and further developed by Frank Kelly. Quasireversibility differs from reversibility in that a stronger condition is imposed on arrival rates and a weaker condition is applied on probability fluxes. For example, an M/M/1 queue with state-dependent arrival rates and state-dependent service times is reversible, but not quasireversible. A network of queues, such that each individual queue when considered in isolation is quasireversible, always has a product form stationary distribution. Quasireversibility had been conjectured to be a necessary condition for a product form solution in a queueing network, but this was shown not to be the case. Chao et al. exhibited a product form network where quasireversibility was not satisfied.
In probability theory and statistics, smoothness of a density function is a measure which determines how many times the density function can be differentiated, or equivalently the limiting behavior of distribution s characteristic function. Formally, we call the distribution of a random variable X ordinary smooth of order    if its characteristic function satisfies  for some positive constants d0, d1,  . The examples of such distributions are gamma, exponential, uniform, etc. The distribution is called supersmooth of order    if its characteristic function satisfies  for some positive constants d0, d1,  ,   and constants  0,  1. Such supersmooth distributions have derivatives of all orders. Examples: normal, Cauchy, mixture normal.
In economics the Pareto index, named after the Italian economist and sociologist Vilfredo Pareto, is a measure of the breadth of income or wealth distribution. It is one of the parameters specifying a Pareto distribution and embodies the Pareto principle. As applied to income, the Pareto principle is sometimes stated in popular expositions by saying 20% of the population has 80% of the income. In fact, Pareto's data on British income taxes in his Cours d'e conomie politique indicates that about 20% of the population had about 80% of the income. One of the simplest characterizations of the Pareto distribution, when used to model the distribution of incomes, says that the proportion of the population whose income exceeds any positive number x > xm is  where xm is a positive number, the minimum of the support of this probability distribution (the subscript m stands for minimum). The Pareto index is the parameter  . Since a proportion must be between 0 and 1, inclusive, the index   must be positive, but in order for the total income of the whole population to be finite,   must also be greater than 1. The larger the Pareto index, the smaller the proportion of very high-income people. Given a  rule, with , the Pareto index is given by:  If , this simplifies to  Alternatively, in terms of odds, X:Y  so X:1 yields  For example, the 80 20 (4:1) rule corresponds to   = log(5)/log(4)   1.16, 90 10 (9:1) corresponds to   = log(10)/log(9)   1.05, and 99 1 corresponds to   = log(100)/log(99)   1.002, whereas the 70 30 rule corresponds to   = log(0.3)/log(0.3/0.7)   1.42 and 2:1 (67 33) corresponds to   = log(3)/log(2)   1.585. Mathematically, the formula above entails that all incomes are at least the lower bound xm, which is positive. At this income the probability density suddenly jumps up from zero and then starts decreasing, which is clearly unrealistic. Economists therefore sometimes state that the Pareto law as stated here applies only to the upper tail of the distribution.
Statistical epidemiology is an emerging branch of the disciplines of epidemiology and biostatistics that aims to: Bring more statistical rigour to bear in the field of epidemiology Recognise the importance of applied statistics, especially with respect to the context in which statistical methods are appropriate and inappropriate Aid and improve our interpretation of observations
In statistics, Moran's I is a measure of spatial autocorrelation developed by Patrick Alfred Pierce Moran. Spatial autocorrelation is characterized by a correlation in a signal among nearby locations in space. Spatial autocorrelation is more complex than one-dimensional autocorrelation because spatial correlation is multi-dimensional (i.e. 2 or 3 dimensions of space) and multi-directional.
In probability theory and statistics, Goodman & Kruskal's lambda () is a measure of proportional reduction in error in cross tabulation analysis. For any sample with a nominal independent variable and dependent variable (or ones that can be treated nominally), it indicates the extent to which the modal categories and frequencies for each value of the independent variable differ from the overall modal category and frequency, i.e. for all values of the independent variable together.  can be calculated with the equation  where  is the overall non-modal frequency, and  is the sum of the non-modal frequencies for each value of the independent variable. Values for lambda range from zero (no association between independent and dependent variables) to one (perfect association).
In mathematics, specifically in the theory of Markovian stochastic processes in probability theory, the Chapman Kolmogorov equation is an identity relating the joint probability distributions of different sets of coordinates on a stochastic process. The equation was derived independently by both the British mathematician Sydney Chapman and the Russian mathematician Andrey Kolmogorov.
The secular variation of a time series is its long-term non-periodic variation (see Decomposition of time series). Whether something is perceived as a secular variation or not depends on the available timescale: a secular variation over a time scale of centuries may be part of a periodic variation over a time scale of millions of years. Natural quantities often have both periodic and secular variations. Secular variation is sometimes called secular trend or secular drift when the emphasis is on a linear long-term trend. The term secular variation is used wherever time series are applicable in economics, operations research, biological anthropology, astronomy (particularly celestial mechanics) such as VSOP (planets) etc.
The theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics. The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures. Apart from philosophical considerations about how to make statistical inferences and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.
A spherical design, part of combinatorial design theory in mathematics, is a finite set of N points on the d-dimensional unit n-sphere Sd such that the average value of any polynomial f of degree t or less on the set equals the average value of f on the whole sphere (that is, the integral of f over Sd divided by the area or measure of Sd). Such a set is often called a spherical t-design to indicate the value of t, which is a fundamental parameter. Spherical designs can be of value in approximation theory, in statistics for experimental design (being usable to construct rotatable designs), in combinatorics, and in geometry. The main problem is to find examples, given d and t, that are not too large. However, such examples may be hard to come by. Spherical t-designs have also recently been appropriated in quantum mechanics in the form of quantum t-designs with various applications to quantum information theory, quantum computing and POVMs. The concept of a spherical design is due to Delsarte, Goethals, and Seidel (1977). The existence and structure of spherical designs with d = 1 (that is, in a circle) was studied in depth by Hong (1982).
In probability theory and statistics, the multivariate normal distribution or multivariate Gaussian distribution, is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions. One possible definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.
In statistics, Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.  Random walk Monte Carlo methods make up a large subclass of MCMC methods.
In actuarial science and applied probability ruin theory (sometimes risk theory collective risk theory) uses mathematical models to describe an insurer's vulnerability to insolvency/ruin. In such models key quantities of interest are the probability of ruin, distribution of surplus immediately prior to ruin and deficit at time of ruin.
In probability theory and statistics, the index of dispersion, dispersion index, coefficient of dispersion, relative variance, or variance-to-mean ratio (VMR), like the coefficient of variation, is a normalized measure of the dispersion of a probability distribution: it is a measure used to quantify whether a set of observed occurrences are clustered or dispersed compared to a standard statistical model. It is defined as the ratio of the variance  to the mean ,  It is also known as the Fano factor, though this term is sometimes reserved for windowed data (the mean and variance are computed over a subpopulation), where the index of dispersion is used in the special case where the window is infinite. Windowing data is frequently done: the VMR is frequently computed over various intervals in time or small regions in space, which may be called "windows", and the resulting statistic called the Fano factor. It is only defined when the mean  is non-zero, and is generally only used for positive statistics, such as count data or time between events, or where the underlying distribution is assumed to be the exponential distribution or Poisson distribution.
Control limits, also known as natural process limits, are horizontal lines drawn on a statistical process control chart, usually at a distance of  3 standard deviations of the plotted statistic from the statistic's mean. Control limits should not be confused with tolerance limits or specifications, which are completely independent of the distribution of the plotted sample statistic. Control limits describe what a process is capable of producing (sometimes referred to as the  voice of the process ), while tolerances and specifications describe how the product should perform to meet the customer's expectations (referred to as the  voice of the customer ).
In probability theory, Kolmogorov's criterion, named after Andrey Kolmogorov, is a theorem giving a necessary and sufficient condition for a Markov chain or continuous-time Markov chain to be stochastically identical to its time-reversed version.
Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association. Bivariate analysis can help determine to what extent it becomes easier to know and predict a value for one variable (possibly a dependent variable) if we know the value of the other variable (possibly the independent variable) (see also correlation and simple linear regression). Bivariate analysis can be contrasted with univariate analysis in which only one variable is analysed. Like univariate analysis, bivariate analysis can be descriptive or inferential. It is the analysis of the relationship between the two variables. Bivariate analysis is a simple (two variable) special case of multivariate analysis (where multiple relations between multiple variables are examined simultaneously). There are 2 types, one is inferential and the other is descriptive.
A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if  parent nodes represent  Boolean variables then the probability function could be represented by a table of  entries, one entry for each of the  possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks. Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
In statistics, a pivotal quantity or pivot is a function of observations and unobservable parameters whose probability distribution does not depend on the unknown parameters  (also referred to as nuisance parameters). Note that a pivot quantity need not be a statistic the function and its value can depend on the parameters of the model, but its distribution must not. If it is a statistic, then it is known as an ancillary statistic. More formally, let  be a random sample from a distribution that depends on a parameter (or vector of parameters) . Let  be a random variable whose distribution is the same for all . Then  is called a pivotal quantity (or simply a pivot). Pivotal quantities are commonly used for normalization to allow data from different data sets to be compared. It is relatively easy to construct pivots for location and scale parameters: for the former we form differences so that location cancels, for the latter ratios so that scale cancels. Pivotal quantities are fundamental to the construction of test statistics, as they allow the statistic to not depend on parameters   for example, Student's t-statistic is for a normal distribution with unknown variance (and mean). They also provide one method of constructing confidence intervals, and the use of pivotal quantities improves performance of the bootstrap. In the form of ancillary statistics, they can be used to construct frequentist prediction intervals (predictive confidence intervals).
In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics. An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.
Equiprobability is a philosophical concept in probability theory that allows one to assign equal probabilities to outcomes when they are judged to be equipossible or to be "equally likely" in some sense. The best-known formulation of the rule is Laplace's principle of indifference (or principle of insufficient reason), which states that, when "we have no other information than" that exactly N mutually exclusive events can occur, we are justified in assigning each the probability 1/N. This subjective assignment of probabilities is especially justified for situations such as rolling dice and lotteries since these experiments carry a symmetry structure, and one's state of knowledge must clearly be invariant under this symmetry. A similar argument could lead to the seemingly absurd conclusion that the sun is as likely to rise as to not rise tomorrow morning. However, the conclusion that the sun is equally likely to rise as it is to not rise is only absurd when additional information is known, such as the laws of gravity and the sun's history. Similar applications of the concept are effectively instances of circular reasoning, with "equally likely" events being assigned equal probabilities, which means in turn that they are equally likely. Despite this, the notion remains useful in probabilistic and statistical modeling. In Bayesian probability, one needs to establish prior probabilities for the various hypotheses before applying Bayes' theorem. One procedure is to assume that these prior probabilities have some symmetry which is typical of the experiment, and then assign a prior which is proportional to the Haar measure for the symmetry group: this generalization of equiprobability is known as the principle of transformation groups and leads to misuse of equiprobability as a model for incertitude.
In statistics, the standard score is the signed number of standard deviations an observation or datum is above the mean. A positive standard score indicates a datum above the mean, while a negative standard score indicates a datum below the mean. It is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This conversion process is called standardizing or normalizing (however, "normalizing" can refer to many types of ratios; see normalization (statistics) for more). Standard scores are also called z-values, z-scores, normal scores, and standardized variables; the use of "Z" is because the normal distribution is also known as the "Z distribution". They are most frequently used to compare a sample to a standard normal deviate, though they can be defined without assumptions of normality. The z-score is only defined if one knows the population parameters; if one only has a sample set, then the analogous computation with sample mean and sample standard deviation yields the Student's t-statistic.
In statistics, a linear probability model is a special case of a binomial regression model. Here the dependent variable for each observation takes values which are either 0 or 1. The probability of observing a 0 or 1 in any one case is treated as depending on one or more explanatory variables. For the "linear probability model", this relationship is a particularly simple one, and allows the model to be fitted by simple linear regression. The model assumes that, for a binary outcome (Bernoulli trial), , and its associated vector of explanatory variables, ,  For this model,  and hence the vector of parameters   can be estimated using least squares. This method of fitting would be inefficient. This method of fitting can be improved by adopting an iterative scheme based on weighted least squares, in which the model from the previous iteration is used to supply estimates of the conditional variances, , which would vary between observations. This approach can be related to fitting the model by maximum likelihood. A drawback of this model is that, unless restrictions are placed on , the estimated coefficients can imply probabilities outside the unit interval . For this reason, models such as the logit model or the probit model are more commonly used.
In the study of survey and census data, microdata is information at the level of individual respondents. For instance, a national census might collect age, home address, educational level, employment status, and many other variables, recorded separately for every person who responds; this is microdata.
In statistics, errors-in-variables models or measurement error models are regression models that account for measurement errors in the independent variables. In contrast, standard regression models assume that those regressors have been measured exactly, or observed without error; as such, those models account only for errors in the dependent variables, or responses. In the case when some regressors have been measured with errors, estimation based on the standard assumption leads to inconsistent estimates, meaning that the parameter estimates do not tend to the true values even in very large samples. For simple linear regression the effect is an underestimate of the coefficient, known as the attenuation bias. In non-linear models the direction of the bias is likely to be more complicated.
A statistic (singular) is a single measure of some attribute of a sample (e.g., its arithmetic mean value). It is calculated by applying a function (statistical algorithm) to the values of the items of the sample, which are known together as a set of data. More formally, statistical theory defines a statistic as a function of a sample where the function itself is independent of the sample's distribution; that is, the function can be stated before realization of the data. The term statistic is used both for the function and for the value of the function on a given sample. A statistic is distinct from a statistical parameter, which is not computable because often the population is much too large to examine and measure all its items. However, a statistic, when used to estimate a population parameter, is called an estimator. For instance, the sample mean is a statistic that estimates the population mean, which is a parameter. When a statistic (a function) is being used for a specific purpose, it may be referred to by a name indicating its purpose: in descriptive statistics, a descriptive statistic is used to describe the data; in estimation theory, an estimator is used to estimate a parameter of the distribution (population); in statistical hypothesis testing, a test statistic is used to test a hypothesis. However, a single statistic can be used for multiple purposes   for example the sample mean can be used to describe a data set, to estimate the population mean, or to test a hypothesis.
In statistics, a design matrix is a matrix of values of explanatory variables of a set of objects, often denoted by X. Each row represents an individual object, with the successive columns corresponding to the variables and their specific values for that object. The design matrix is used in certain statistical models, e.g., the general linear model. It can contain indicator variables (ones and zeros) that indicate group membership in an ANOVA, or it can contain values of continuous variables. The design matrix contains data on the independent variables (also called explanatory variables) in statistical models which attempt to explain observed data on a response variable (often called a dependent variable) in terms of the explanatory variables. The theory relating to such models makes substantial use of matrix manipulations involving the design matrix: see for example linear regression. A notable feature of the concept of a design matrix is that it is able to represent a number of different experimental designs and statistical models, e.g., ANOVA, ANCOVA, and linear regression.  
In probability theory and statistics, the triangular distribution is a continuous probability distribution with lower limit a, upper limit b and mode c, where a < b and a   c   b.
In information theory, systems are modeled by a transmitter, channel, and receiver. The transmitter produces messages that are sent through the channel. The channel modifies the message in some way. The receiver attempts to infer which message was sent. In this context, entropy (more specifically, Shannon entropy) is the expected value (average) of the information contained in each message. 'Messages' can be modeled by any flow of information. In a more technical sense, there are reasons (explained below) to define information as the negative of the logarithm of the probability distribution. The probability distribution of the events, coupled with the information amount of every event, forms a random variable whose expected value is the average amount of information, or entropy, generated by this distribution. Units of entropy are the shannon, nat, or hartley, depending on the base of the logarithm used to define it, though the shannon is commonly referred to as a bit. The logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a coin toss is 1 shannon, whereas of m tosses it is m shannons. Generally, you need log2(n) bits to represent a variable that can take one of n values if n is a power of 2. If these values are equally probable, the entropy (in shannons) is equal to the number of bits. Equality between number of bits and shannons holds only while all outcomes are equally probable. If one of the events is more probable than others, observation of that event is less informative. Conversely, rarer events provide more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is less than log2(n). Entropy is zero when one outcome is certain. Shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known. The meaning of the events observed (the meaning of messages) does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves. Generally, entropy refers to disorder or uncertainty. Shannon entropy was introduced by Claude E. Shannon in his 1948 paper "A Mathematical Theory of Communication". Shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source. Re nyi entropy generalizes Shannon entropy.
A latent variable model is a statistical model that relates a set of variables (so-called manifest variables) to a set of latent variables. It is assumed that the responses on the indicators or manifest variables are the result of an individual's position on the latent variable(s), and that the manifest variables have nothing in common after controlling for the latent variable (local independence). Different types of the latent variable model can be grouped according to whether the manifest and latent variables are categorical or continuous:   The Rasch model represents the simplest form of item response theory. Mixture models are central to latent profile analysis. In factor analysis and latent trait analysis the latent variables are treated as continuous normally distributed variables, and in latent profile analysis and latent class analysis as from a multinomial distribution. The manifest variables in factor analysis and latent profile analysis are continuous and in most cases, their conditional distribution given the latent variables is assumed to be normal. In latent trait analysis and latent class analysis, the manifest variables are discrete. These variables could be dichotomous, ordinal or nominal variables. Their conditional distributions are assumed to be binomial or multinomial. Because the distribution of a continuous latent variable can be approximated by a discrete distribution, the distinction between continuous and discrete variables turns out not to be fundamental at all. Therefore there may be a psychometrical latent variable, but not a psychological psychometric variable.  
In statistics, importance sampling is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. It is related to umbrella sampling in computational physics. Depending on the application, the term may refer to the process of sampling from this alternative distribution, the process of inference, or both.
In statistics and econometrics, the multivariate probit model is a generalization of the probit model used to estimate several correlated binary outcomes jointly. For example, if it is believed that the decisions of sending at least one child to public school and that of voting in favor of a school budget are correlated (both decisions are binary), then the multivariate probit model would be appropriate for jointly predicting these two choices on an individual-specific basis.
In statistics, hypotheses suggested by a given dataset, when tested with the same dataset that suggested them, are likely to be accepted even when they are not true. This is because circular reasoning (double dipping) would be involved: something seems true in the limited data set, therefore we hypothesize that it is true in general, therefore we (wrongly) test it on the same limited data set, which seems to confirm that it is true. Generating hypotheses based on data already observed, in the absence of testing them on new data, is referred to as post hoc theorizing (from Latin post hoc, "after this"). The correct procedure is to test any hypothesis on a data set that was not used to generate the hypothesis.  
An acronym for "Standing Committee of Regional and Urban Statistics", SCORUS is a sub-committee of the International Association for Official Statistics (IAOS) which is a section of the International Statistical Institute. The sub-committee has specific responsibility for regional and urban statistics and research. Its members form a dedicated international network of persons interested in regional and urban statistical issues.
In probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable's mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterised. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from the zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location. Sets of central moments can be defined for both univariate and multivariate distributions.
The History of statistics can be said to start around 1749 although, over time, there have been changes to the interpretation of the word statistics. In early times, the meaning was restricted to information about states. This was later extended to include all collections of information of all types, and later still it was extended to include the analysis and interpretation of such data. In modern terms, "statistics" means both sets of collected information, as in national accounts and temperature records, and analytical work which requires statistical inference. Statistical activities are often associated with models expressed using probabilities, and require probability theory for them to be put on a firm theoretical basis: see History of probability. A number of statistical concepts have had an important impact on a wide range of sciences. These include the design of experiments and approaches to statistical inference such as Bayesian inference, each of which can be considered to have their own sequence in the development of the ideas underlying modern statistics.
In statistics, the Bingham distribution, named after Christopher Bingham, is an antipodally symmetric probability distribution on the n-sphere. It is widely used in paleomagnetic data analysis, and has been reported as being of use in the field of computer vision. Its probability density function is given by  which may also be written  where x is an axis, M is an orthogonal orientation matrix, Z is a diagonal concentration matrix,  is a confluent hypergeometric function of matrix argument.  
In statistics, the Bonferroni correction is a method used to counteract the problem of multiple comparisons. It is named after Italian mathematician Carlo Emilio Bonferroni for its use of Bonferroni inequalities, but modern usage is often credited to Olive Jean Dunn, who described the procedure in a pair of articles written in 1959 and 1961.
In the design of experiments, optimal designs are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith. In the design of experiments for estimating statistical models, optimal designs allow parameters to be estimated without bias and with minimum-variance. A non-optimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design. In practical terms, optimal experiments can reduce the costs of experimentation. The optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion, which is related to the variance-matrix of the estimator. Specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments. Optimal designs are also called optimum designs.
Population viability analysis (PVA) is a species-specific method of risk assessment frequently used in conservation biology. It is traditionally defined as the process that determines the probability that a population will go extinct within a given number of years. More recently, PVA has been described as a marriage of ecology and statistics that brings together species characteristics and environmental variability to forecast population health and extinction risk. Each PVA is individually developed for a target population or species, and consequently, each PVA is unique. The larger goal in mind when conducting a PVA is to ensure that the population of a species is self-sustaining over the long term.
Binary or binomial classification is the task of classifying the elements of a given set into two groups on the basis of a classification rule. Some typical binary classification tasks are: medical testing to determine if a patient has certain disease or not   the classification property is the presence of the disease; A "pass or fail" test method or quality control in factories; i.e. deciding if a specification has or has not been met: a Go/no go classification. An item may have a qualitative property; it does or does not have a specified characteristic information retrieval, namely deciding whether a page or an article should be in the result set of a search or not   the classification property is the relevance of the article, or the usefulness to the user. An important point is that in many practical binary classification problems, the two groups are not symmetric   rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present). Sometimes, classification tasks are trivial. Given 100 balls, some of them red and some blue, a human with normal color vision can easily separate them into red ones and blue ones. However, some tasks, like those in practical medicine, and those interesting from the computer science point of view, are far from trivial, and may produce faulty results if executed imprecisely.
In statistics, the closed testing procedure is a general method for performing more than one hypothesis test simultaneously.
Lead time is the length of time between the detection of a disease (usually based on new, experimental criteria) and its usual clinical presentation and diagnosis (based on traditional criteria). OR It is the time between early diagnosis with screening, and when diagnosis would have been made without screening. Lead time bias is that factor when comparing, in detection of disease, a traditional test with a new or experimental test but the outcome of the disease is unchanged; the new or experimental test merely identifies the disease earlier than the previous and thus gives the impression that survival is prolonged. It is an important factor when evaluating the effectiveness of a specific test.
Rounding a numerical value means replacing it by another value that is approximately equal but has a shorter, simpler, or more explicit representation; for example, replacing  23.4476 with  23.45, or the fraction 312/937 with 1/3, or the expression  2 with 1.414. Rounding is often done to obtain a value that is easier to report and communicate than the original. Rounding can also be important to avoid misleadingly precise reporting of a computed number, measurement or estimate; for example, a quantity that was computed as 123,456 but is known to be accurate only to within a few hundred units is better stated as "about 123,500". On the other hand, rounding of exact numbers will introduce some round-off error in the reported result. Rounding is almost unavoidable when reporting many computations   especially when dividing two numbers in integer or fixed-point arithmetic; when computing mathematical functions such as square roots, logarithms, and sines; or when using a floating point representation with a fixed number of significant digits. In a sequence of calculations, these rounding errors generally accumulate, and in certain ill-conditioned cases they may make the result meaningless. Accurate rounding of transcendental mathematical functions is difficult because the number of extra digits that need to be calculated to resolve whether to round up or down cannot be known in advance. This problem is known as "the table-maker's dilemma". Rounding has many similarities to the quantization that occurs when physical quantities must be encoded by numbers or digital signals. A wavy equals sign ( ) is sometimes used to indicate rounding of exact numbers. For example: 9.98   10.
The positive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results. The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test; they depend also on the prevalence. The PPV can be derived using Bayes' theorem. Although sometimes used synonymously, a positive predictive value generally refers to what is established by control groups, while a post-test probability refers to a probability for an individual. Still, if the individual's pre-test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal. In information retrieval, the PPV statistic is often called the precision.
The Hurst exponent is used as a measure of long-term memory of time series. It relates to the autocorrelations of the time series, and the rate at which these decrease as the lag between pairs of values increases. Studies involving the Hurst exponent were originally developed in hydrology for the practical matter of determining optimum dam sizing for the Nile river's volatile rain and drought conditions that had been observed over a long period of time. The name "Hurst exponent", or "Hurst coefficient", derives from Harold Edwin Hurst (1880 1978), who was the lead researcher in these studies; the use of the standard notation H for the coefficient relates to his name also. In fractal geometry, the generalized Hurst exponent has been denoted by H or Hq in honor of both Harold Edwin Hurst and Ludwig Otto Ho lder (1859 1937) by Benoi t Mandelbrot (1924 2010). H is directly related to fractal dimension, D, and is a measure of a data series' "mild" or "wild" randomness. The Hurst exponent is referred to as the "index of dependence" or "index of long-range dependence". It quantifies the relative tendency of a time series either to regress strongly to the mean or to cluster in a direction. A value H in the range 0.5 1 indicates a time series with long-term positive autocorrelation, meaning both that a high value in the series will probably be followed by another high value and that the values a long time into the future will also tend to be high. A value in the range 0   0.5 indicates a time series with long-term switching between high and low values in adjacent pairs, meaning that a single high value will probably be followed by a low value and that the value after that will tend to be high, with this tendency to switch between high and low values lasting a long time into the future. A value of H=0.5 can indicate a completely uncorrelated series, but in fact it is the value applicable to series for which the autocorrelations at small time lags can be positive or negative but where the absolute values of the autocorrelations decay exponentially quickly to zero. This in contrast to the typically power law decay for the 0.5 < H < 1 and 0 < H < 0.5 cases.
In statistics, Cochran's theorem, devised by William G. Cochran, is a theorem used to justify results relating to the probability distributions of statistics that are used in the analysis of variance.
In probability theory, an  -divergence is a function Df (P  || Q) that measures the difference between two probability distributions P and Q. It helps the intuition to think of the divergence as an average, weighted by the function f, of the odds ratio given by P and Q. These divergences were introduced and studied independently by Csisza r (1963), Morimoto (1963) and Ali & Silvey (1966) and are sometimes known as Csisza r  -divergences, Csisza r-Morimoto divergences or Ali-Silvey distances.
In credibility theory, a branch of study in actuarial science, the Bu hlmann model is a random effects model (or "variance components model" or hierarchical linear model) used in to determine the appropriate premium for a group of insurance contracts. The model is named after Hans Bu hlmann who first published a description in 1967.
In psychology and social research, unmatched count, or item count, is a technique to improve through anonymity the number of true answers to possibly embarrassing or self-incriminating questions. It is very simple to use but yields only the number of people bearing the property of interest. It was introduced by Raghavarao and Federer in 1979
The Lomax distribution, conditionally also called the Pareto Type II distribution, is a heavy-tail probability distribution often used in business, economics, and actuarial modeling. It is named after K. S. Lomax. It is essentially a Pareto distribution that has been shifted so that its support begins at zero.  
The McDonald Kreitman test is a statistical test often used by evolution and population biologists to detect and measure the amount of adaptive evolution within a species by determining whether adaptive evolution has occurred, and the proportion of substitutions that resulted from positive selection (also known as directional selection). To do this, the McDonald Kreitman test compares the amount of variation within a species (polymorphism) to the divergence between species (substitutions) at two types of sites, neutral and nonneutral. A substitution refers to a nucleotide that is fixed within one species, but a different nucleotide is fixed within a second species at the same base pair of homologous DNA sequences. A site is nonneutral if it is either advantageous or deleterious. The two types of sites can be either synonymous or nonsynonymous within a protein-coding region. In a protein-coding sequence of DNA, a site is synonymous if a point mutation at that site would not change the amino acid, also known as a silent mutation. Because the mutation did not result in a change in the amino acid that was originally coded for by the protein-coding sequence, the phenotype, or the observable trait, of the organism is generally unchanged by the silent mutation. A site in a protein-coding sequence of DNA is nonsynonymous if a point mutation at that site results in a change in the amino acid, resulting in a change in the organism's phenotype. Typically, silent mutations in protein-coding regions are used as the "control" in the McDonald Kreitman test. In 1991, John H. McDonald and Martin Kreitman derived the McDonald Kreitman test while performing an experiment with Drosophila (fruit flies) and their differences in amino acid sequence of the alcohol dehydrogenase gene. McDonald and Kreitman proposed this method to estimate the proportion of substitutions that are fixed by positive selection rather than by genetic drift. In order to set up the McDonald Kreitman test, we must first set up a two-way contingency table of our data on the species being investigated as shown below: Ds: the number of synonymous substitutions per gene Dn: the number of non-synonymous substitutions per gene Ps: the number of synonymous polymorphisms per gene Pn: the number of non-synonymous polymorphisms per gene To quantify the values for Ds, Dn, Ps, and Pn, you count the number of differences in the protein-coding region for each type of variable in the contingency table. The null hypothesis of the McDonald Kreitman test is that the ratio of nonsynonymous to synonymous variation within a species is going to equal the ratio of nonsynonymous to synonymous variation between species (i.e. Dn/Ds = Pn/Ps). When positive or negative selection (natural selection) influences nonsynonymous variation, the ratios will no longer equal. The ratio of nonsynonymous to synonymous variation between species is going to be lower than the ratio of nonsynonymous to synonymous variation within species (i.e. Dn/Ds < Pn/Ps) when negative selection is at work, and deleterious mutations strongly affect polymorphism. The ratio of nonsynonymous to synonymous variation within species is lower than the ratio of nonsynonymous to synonymous variation between species (i.e. Dn/Ds > Pn/Ps) when we observe positive selection. Since mutations under positive selection spread through a population rapidly, they don't contribute to polymorphism but do have an effect on divergence. Using an equation derived by Smith and Eyre-Walker, we can estimate the proportion of base substitutions fixed by natural selection,  , using the following formula:  Alpha represents the proportion of substitutions driven by positive selection. Alpha can be equal to any number between -  and 1. Negative values of alpha are produced by sampling error or violations of the model, such as the segregation of slightly deleterious amino acid mutations. Similar to above, our null hypothesis here is that  =0, and we expect Dn/Ds to equal Pn/Ps.
A variety of methods are used in econometrics to estimate models consisting of a single equation. The oldest and still the most commonly used is the ordinary least squares method used to estimate linear regressions. A variety of methods are available to estimate non-linear models. A particularly important class of non-linear models are those used to estimate relationships where the dependent variable is discrete, truncated or censored. These include logit, probit and Tobit models. Single equation methods may be applied to time-series, cross section or panel data.
In probability theory, a martingale is a model of a fair game where knowledge of past events never helps predict the mean of the future winnings. In particular, a martingale is a sequence of random variables (i.e., a stochastic process) for which, at a particular time in the realized sequence, the expectation of the next value in the sequence is equal to the present observed value even given knowledge of all prior observed values. To contrast, in a process that is not a martingale, it may still be the case that the expected value of the process at one time is equal to the expected value of the process at the next time. However, knowledge of the prior outcomes (e.g., all prior cards drawn from a card deck) may be able to reduce the uncertainty of future outcomes. Thus, the expected value of the next outcome given knowledge of the present and all prior outcomes may be higher than the current outcome if a winning strategy is used. Martingales exclude the possibility of winning strategies based on game history, and thus they are a model of fair games.
Uncertainty is the situation which involves imperfect and / or unknown information. In other words, it is a term used in subtly different ways in a number of fields, including insurance, philosophy, physics, statistics, economics, finance, psychology, sociology, engineering, metrology, and information science. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable and/or stochastic environments, as well as due to ignorance and/or indolence.
Laboratory quality control is designed to detect, reduce, and correct deficiencies in a laboratory's internal analytical process prior to the release of patient results, in order to improve the quality of the results reported by the laboratory. Quality control is a measure of precision, or how well the measurement system reproduces the same result over time and under varying operating conditions. Laboratory quality control material is usually run at the beginning of each shift, after an instrument is serviced, when reagent lots are changed, after calibration, and whenever patient results seem inappropriate. Quality control material should approximate the same matrix as patient specimens, taking into account properties such as viscosity, turbidity, composition, and color. It should be simple to use, with minimal vial to vial variability, because variability could be misinterpreted as systematic error in the method or instrument. It should be stable for long periods of time, and available in large enough quantities for a single batch to last at least one year. Liquid controls are more convenient than lyophilized controls because they do not have to be reconstituted minimizing pipetting error.
In probability theory and statistics, Fisher's noncentral hypergeometric distribution is a generalization of the hypergeometric distribution where sampling probabilities are modified by weight factors. It can also be defined as the conditional distribution of two or more binomially distributed variables dependent upon their fixed sum. The distribution may be illustrated by the following urn model. Assume, for example, that an urn contains m1 red balls and m2 white balls, totalling N = m1 + m2 balls. Each red ball has the weight  1 and each white ball has the weight  2. We will say that the odds ratio is   =  1 /  2. Now we are taking balls randomly in such a way that the probability of taking a particular ball is proportional to its weight, but independent of what happens to the other balls. The number of balls taken of a particular color follows the binomial distribution. If the total number n of balls taken is known then the conditional distribution of the number of taken red balls for given n is Fisher's noncentral hypergeometric distribution. To generate this distribution experimentally, we have to repeat the experiment until it happens to give n balls. If we want to fix the value of n prior to the experiment then we have to take the balls one by one until we have n balls. The balls are therefore no longer independent. This gives a slightly different distribution known as Wallenius' noncentral hypergeometric distribution. It is far from obvious why these two distributions are different. See the entry for noncentral hypergeometric distributions for an explanation of the difference between these two distributions and a discussion of which distribution to use in various situations. The two distributions are both equal to the (central) hypergeometric distribution when the odds ratio is 1. Unfortunately, both distributions are known in the literature as "the" noncentral hypergeometric distribution. It is important to be specific about which distribution is meant when using this name. Fisher's noncentral hypergeometric distribution was first given the name extended hypergeometric distribution (Harkness, 1965), and some authors still use this name today.
In statistics, dependence is any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationships involving dependence, though in common usage it most often refers to the extent to which two variables have a linear relationship with each other. Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, and the correlation between the demand for a product and its price. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling; however, statistical dependence is not sufficient to demonstrate the presence of such a causal relationship (i.e., correlation does not imply causation). Formally, dependence refers to any situation in which random variables do not satisfy a mathematical condition of probabilistic independence. In loose usage, correlation can refer to any departure of two or more random variables from independence, but technically it refers to any of several more specialized types of relationship between mean values. There are several correlation coefficients, often denoted   or r, measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may exist even if one is a nonlinear function of the other). Other correlation coefficients have been developed to be more robust than the Pearson correlation   that is, more sensitive to nonlinear relationships. Mutual information can also be applied to measure dependence between two variables.
In probability theory, especially as that field is used in statistics, a group family of probability distributions is a family obtained by subjecting a random variable with a fixed distribution to a suitable family of transformations such as a location-scale family, or otherwise a family of probability distributions acted upon by a group. Consideration of a particular family of distributions as a group family can, in statistical theory, lead to the identification of an ancillary statistic.
In statistics, canonical analysis (from Ancient Greek:       bar, measuring rod, ruler) belongs to the family of regression methods for data analysis. Regression analysis quantifies a relationship between a predictor variable and a criterion variable by the coefficient of correlation r, coefficient of determination r2, and the standard regression coefficient  . Multiple regression analysis expresses a relationship between a set of predictor variables and a single criterion variable by the multiple correlation R, multiple coefficient of determination R2, and a set of standard partial regression weights  1,  2, etc. Canonical variate analysis captures a relationship between a set of predictor variables and a set of criterion variables by the canonical correlations  1,  2, ..., and by the sets of canonical weights C and D.
Fides (latin: trust) is a guide allowing estimated reliability calculation for electronic components and systems. The reliability prediction is generally expressed in FIT (number of failures for 109 hours) or MTBF (Mean Time Between Failures). This guide provides reliability data for RAMS (Reliability, Availability, Maintainability, Safety) studies.  
The power spectrum  of a time series  describes the distribution of power into frequency components composing that signal. According to Fourier analysis any physical signal can be decomposed into a number of discrete frequencies, or a spectrum of frequencies over a continuous range. The statistical average of a certain signal or sort of signal (including noise) as analyzed in terms of its frequency content, is called its spectrum. When the energy of the signal is concentrated around a finite time interval, especially if its total energy is finite, one may compute the energy spectral density. More commonly used is the power spectral density (or simply power spectrum), which applies to signals existing over all time, or over a time period large enough (especially in relation to the duration of a measurement) that it could as well have been over an infinite time interval. The power spectral density (PSD) then refers to the spectral energy distribution that would be found per unit time, since the total energy of such a signal over all time would generally be infinite. Summation or integration of the spectral components yields the total power (for a physical process) or variance (in a statistical process), identical to what would be obtained by integrating  over the time domain, as dictated by Parseval's theorem. The spectrum of a physical process  often contains essential information about the nature of . For instance, the pitch and timbre of a musical instrument are immediately determined from a spectral analysis. The color of a light source is determined by the spectrum of the electromagnetic wave's electric field  as it fluctuates at an extremely high frequency. Obtaining a spectrum from time series' such as these involves the Fourier transform, and generalizations based on Fourier analysis. In many cases the time domain is not specifically employed in practice, such as when a dispersive prism is used to obtain a spectrum of light in a spectrograph, or when a sound is perceived through its effect on the auditory receptors of the inner ear, each of which is sensitive to a particular frequency. However this article concentrates on situations in which the time series is known (at least in a statistical sense) or directly measured (such as by a microphone sampled by a computer). The power spectrum is important in statistical signal processing and in the statistical study of stochastic processes, as well as in many other branches of physics and engineering. Typically the process is a function of time but one can similarly discuss data in the spatial domain being decomposed in terms of spatial frequency.
Galton s problem, named after Sir Francis Galton, is the problem of drawing inferences from cross-cultural data, due to the statistical phenomenon now called autocorrelation. The problem is now recognized as a general one that applies to all nonexperimental studies and to experimental design as well. It is most simply described as the problem of external dependencies in making statistical estimates when the elements sampled are not statistically independent. Asking two people in the same household whether they watch TV, for example, does not give you statistically independent answers. The sample size, n, for independent observations in this case is one, not two. Once proper adjustments are made that deal with external dependencies, then the axioms of probability theory concerning statistical independence will apply. These axioms are important for deriving measures of variance, for example, or tests of statistical significance.
In statistics, a collection of random variables is heteroscedastic (or 'heteroskedastic'; from Ancient Greek hetero  different  and skedasis  dispersion ) if there are sub-populations that have different variabilities from others. Here "variability" could be quantified by the variance or any other measure of statistical dispersion. Thus heteroscedasticity is the absence of homoscedasticity. The existence of heteroscedasticity is a major concern in the application of regression analysis, including the analysis of variance, as it can invalidate statistical tests of significance that assume that the modelling errors are uncorrelated and uniform hence that their variances do not vary with the effects being modeled. For instance, while the ordinary least squares estimator is still unbiased in the presence of heteroscedasticity, it is inefficient because the true variance and covariance are underestimated. Similarly, in testing for differences between sub-populations using a location test, some standard tests assume that variances within groups are equal. Because heteroscedasticity concerns expectations of the second moment of the errors, its presence is referred to as misspecification of the second order.
A hidden Markov random field is a generalization of a hidden Markov model. Instead of having an underlying Markov chain, hidden Markov random fields have an underlying Markov random field. Suppose that we observe a random variable , where . Hidden Markov random fields assume that the probabilistic nature of  is determined by the unobservable Markov random field , . That is, given the neighbors  of ,  is independent of all other  (Markov property). The main difference with a hidden Markov model is that neighborhood is not defined in 1 dimension but within a network, i.e.  is allowed to have more than the two neighbors that it would have in a Markov chain. The model is formulated in such a way that given ,  are independent (conditional independence of the observable variables given the Markov random field).
Barnes interpolation, named after Stanley L. Barnes, is the interpolation of unstructured data points from a set of measurements of an unknown function in two dimensions into an analytic function of two variables. An example of a situation where the Barnes scheme is important is in weather forecasting where measurements are made wherever monitoring stations may be located, the positions of which are constrained by topography. Such interpolation is essential in data visualisation, e.g. in the construction of contour plots or other representations of analytic surfaces.
In statistics, a sampling frame is the source material or device from which a sample is drawn. It is a list of all those within a population who can be sampled, and may include individuals, households or institutions. Importance of the sampling frame is stressed by Jessen and Salant and Dillman.  In many practical situations the frame is a matter of choice to the survey planner, and sometimes a critical one. [...] Some very worthwhile investigations are not undertaken at all because of the lack of an apparent frame; others, because of faulty frames, have ended in a disaster or in cloud of doubt.
A testimator is an estimator whose value depends on the result of a test for statistical significance. In the simplest case the value of the final estimator is that of the basic estimator if the test result is significant, and otherwise the value is zero. However more general testimators are possible.
In statistics, signal processing, and econometrics, an unevenly (or unequally or irregularly) spaced time series is a sequence of observation time and value pairs (tn, Xn) with strictly increasing observation times. As opposed to equally spaced time series, the spacing of observation times is not constant. Unevenly spaced time series naturally occur in many industrial and scientific domains: Natural disasters such as earthquakes, floods, or volcanic eruptions typically occur at irregular time intervals. In observational astronomy, measurements such as spectra of celestial objects are taken at times determined by weather conditions, availability of observation time slots, and suitable planetary configurations. In clinical trials (or more generally, longitudinal studies), a patient's state of health may be observed only at irregular time intervals, and different patients are usually observed at different points in time. There are many more examples in climatology, ecology, high-frequency finance, geology, and signal processing.
OxMetrics is an econometric software including the Ox programming language for econometrics and statistics, developed by Jurgen Doornik and David Hendry. OxMetrics originates from PcGive, one of the first econometric software for personal computers, initiated by David Hendry in the 1980s at the London School of Economics. OxMetrics builds on the Ox programming language of Jurgen Doornik developed at University of Oxford. Renfro (2004) describes the history of econometric software packages. OxMetrics is a family of software packages for the econometric and financial analysis of time series, forecasting, econometric model selection and for the statistical analysis of cross-sectional data and panel data. The main modules apart from PcGive for dynamic econometric models (ARDL, VAR, GARCH, Switching, Autometrics), panel data models (DPD), Limited dependent models, are STAMP for structural time series modelling, "SsfPack" for State space methods and "G@RCH" for financial volatility modelling. Hendry & Nielsen (2007) present many empirical examples in PcGive for OxMetrics in their econometrics textbook. Durbin & Koopman (2012) give modern examples in their time series analysis textbook.
In a thought experiment proposed by the Italian probabilist Bruno de Finetti in order to justify Bayesian probability, an array of wagers is coherent precisely if it does not expose the wagerer to certain loss regardless of the outcomes of events on which he is wagering, even if his opponent makes the most judicious choices.
Exponential dispersion models are statistical models in which the probability distribution is of a special form. This class of models represents a generalisation of the exponential family of models which themselves play an important role in statistical theory because they have a special structure which enables deductions to be made about appropriate statistical inference.
In industrial statistics, the X-bar chart is a type of Shewhart control chart that is used to monitor the arithmetic means of successive samples of constant size, n. This type of control chart is used for characteristics that can be measured on a continuous scale, such as weight, temperature, thickness etc. For example, one might take a sample of 5 shafts from production every hour, measure the diameter of each, and then plot, for each sample, the average of the five diameter values on the chart. For the purposes of control limit calculation, the sample means are assumed to be normally distributed, an assumption justified by the Central Limit Theorem. The X-bar chart is always used in conjunction with a variation chart such as the  and R chart or  and s chart. The R-chart shows sample ranges (difference between the largest and the smallest values in the sample), while the s-chart shows the samples' standard deviation. The R-chart was preferred in times when calculations were performed manually, as the range is far easier to calculate than the standard deviation; with the advent of computers, ease of calculation ceased to be an issue, and the s-chart is preferred these days, as it is statistically more meaningful and efficient. Depending on the type of variation chart used, the average sample range or the average sample standard deviation is used to derive the X-bar chart's control limits.
In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function that describes the relative likelihood for this random variable to take on a given value. The probability of the random variable falling within a particular range of values is given by the integral of this variable s density over that range that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to one. The terms "probability distribution function" and "probability function" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, "probability distribution function" may be used when the probability distribution is defined as a function over general sets of values, or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. Further confusion of terminology exists because density function has also been used for what is here called the "probability mass function" (PMF). In general though, the PMF is used in the context of discrete random variables (random variables that take values on a discrete set), while PDF is used in the context of continuous random variables.
SUDAAN is a statistical software package for the analysis of correlated data, including correlated data encountered in complex sample surveys. SUDAAN originated in 1972 at RTI International (the trade name of Research Triangle Institute).
An error correction model belongs to a category of multiple time series models most commonly used for data where the underlying variables have a long-run stochastic trend, also known as cointegration. ECMs are a theoretically-driven approach useful for estimating both short-term and long-term effects of one time series on another. The term error-correction relates to the fact that last-periods deviation from a long-run equilibrium, the error, influences its short-run dynamics. Thus ECMs directly estimate the speed at which a dependent variable returns to equilibrium after a change in other variables.
In statistics and econometrics, particularly in regression analysis, a dummy variable (also known as an indicator variable, design variable, Boolean indicator, categorical variable, binary variable, or qualitative variable) is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome. Dummy variables are used as devices to sort data into mutually exclusive categories (such as smoker/non-smoker, etc.). For example, in econometric time series analysis, dummy variables may be used to indicate the occurrence of wars or major strikes. A dummy variable can thus be thought of as a truth value represented as a numerical value 0 or 1 (as is sometimes done in computer programming). Dummy variables are "proxy" variables or numeric stand-ins for qualitative facts in a regression model. In regression analysis, the dependent variables may be influenced not only by quantitative variables (income, output, prices, etc.), but also by qualitative variables (gender, religion, geographic region, etc.). A dummy independent variable (also called a dummy explanatory variable) which for some observation has a value of 0 will cause that variable's coefficient to have no role in influencing the dependent variable, while when the dummy takes on a value 1 its coefficient acts to alter the intercept. For example, suppose Gender is one of the qualitative variables relevant to a regression. Then, female and male would be the categories included under the Gender variable. If female is arbitrarily assigned the value of 1, then male would get the value 0. Then the intercept (the value of the dependent variable if all other explanatory variables hypothetically took on the value zero) would be the constant term for males but would be the constant term plus the coefficient of the gender dummy in the case of females. Dummy variables are used frequently in time series analysis with regime switching, seasonal analysis and qualitative data applications. Dummy variables are involved in studies for economic forecasting, bio-medical studies, credit scoring, response modelling, etc. Dummy variables may be incorporated in traditional regression methods or newly developed modeling paradigms.
In econometrics and other applications of multivariate time series analysis, a variance decomposition or forecast error variance decomposition (FEVD) is used to aid in the interpretation of a vector autoregression (VAR) model once it has been fitted. The variance decomposition indicates the amount of information each variable contributes to the other variables in the autoregression. It determines how much of the forecast error variance of each of the variables can be explained by exogenous shocks to the other variables.
In probability and statistics, given a stochastic process , the autocovariance is a function that gives the covariance of the process with itself at pairs of time points. With the usual notation E  for the expectation operator, if the process has the mean function , then the autocovariance is given by  Autocovariance is related to the more commonly used autocorrelation of the process in question. In the case of a random vector , the autocovariance would be a square n by n matrix  with entries  This is commonly known as the covariance matrix or matrix of covariances of the given random vector.
Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding the precise global optimum is less important than finding an acceptable local optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as brute-force search or gradient descent. Simulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution. The method was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, and by Vlado C erny  in 1985. The method is an adaptation of the Metropolis Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953.
A mathematical or physical process is time-reversible if the dynamics of the process remain well-defined when the sequence of time-states is reversed. A deterministic process is time-reversible if the time-reversed process satisfies the same dynamic equations as the original process; in other words, the equations are invariant or symmetrical under a change in the sign of time. A stochastic process is reversible if the statistical properties of the process are the same as the statistical properties for time-reversed data from the same process.
The Mahalanobis distance is a measure of the distance between a point P and a distribution D, introduced by P. C. Mahalanobis in 1936. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean: along each principal component axis, it measures the number of standard deviations from P to the mean of D. If each of these axes is rescaled to have unit variance, then Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. Mahalanobis distance is thus unitless and scale-invariant, and takes into account the correlations of the data set.
Change detection for GIS (geographical information systems) is a process that measures how the attributes of a particular area have changed between two or more time periods. Change detection often involves comparing aerial photographs or satellite imagery of the area taken at different times. Change detection has been widely used to assess shifting cultivation, deforestation, urban growth, impact of natural disasters like Tsunamis, earthquakes and use/land cover changes etc..
A fast Fourier transform (FFT) algorithm computes the discrete Fourier transform (DFT) of a sequence, or its inverse. Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. An FFT rapidly computes such transformations by factorizing the DFT matrix into a product of sparse (mostly zero) factors. As a result, it manages to reduce the complexity of computing the DFT from , which arises if one simply applies the definition of DFT, to , where  is the data size. Fast Fourier transforms are widely used for many applications in engineering, science, and mathematics. The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805. In 1994 Gilbert Strang described the FFT as "the most important numerical algorithm of our lifetime" and it was included in Top 10 Algorithms of 20th Century by the IEEE journal Computing in Science & Engineering.
In statistics, latent variables (from Latin: present participle of lateo ( lie hidden ), as opposed to observable variables), are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured). Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, economics, medicine, physics, machine learning/artificial intelligence, bioinformatics, natural language processing, econometrics, management and the social sciences. Sometimes latent variables correspond to aspects of physical reality, which could in principle be measured, but may not be for practical reasons. In this situation, the term hidden variables is commonly used (reflecting the fact that the variables are "really there", but hidden). Other times, latent variables correspond to abstract concepts, like categories, behavioral or mental states, or data structures. The terms hypothetical variables or hypothetical constructs may be used in these situations. One advantage of using latent variables is that it reduces the dimensionality of data. A large number of observable variables can be aggregated in a model to represent an underlying concept, making it easier to understand the data. In this sense, they serve a function similar to that of scientific theories. At the same time, latent variables link observable ("sub-symbolic") data in the real world to symbolic data in the modeled world. Latent variables, as created by factor analytic methods, generally represent "shared" variance, or the degree to which variables "move" together. Variables that have no correlation cannot result in a latent construct based on the common factor model.
In probability theory, Slutsky s theorem extends some properties of algebraic operations on convergent sequences of real numbers to sequences of random variables. The theorem was named after Eugen Slutsky. Slutsky s theorem is also attributed to Harald Crame r.
Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability".
Human subject research is systematic, scientific investigation that can be either interventional (a "trial") or observational (no "test article") and involves human beings as research subjects. Human subject research can be either medical (clinical) research or non-medical (e.g., social science) research. Systematic investigation incorporates both the collection and analysis of data in order to answer a specific question. Medical human subject research often involves analysis of biological specimens, epidemiological and behavioral studies and medical chart review studies. (A specific, and especially heavily regulated, type of medical human subject research is the "clinical trial", in which drugs, vaccines and medical devices are evaluated.) Human subject research in the social sciences often involves surveys, questionnaires, interviews, and focus groups. Human subject research is used in various fields, including research into basic biology, clinical medicine, nursing, psychology, sociology, political science, and anthropology. As research has become formalized, the academic community has developed formal definitions of "human subject research", largely in response to abuses of human subjects.
In statistics, a contingency table is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables. They are heavily used in survey research, business intelligence, engineering and scientific research. They provide a basic picture of the interrelation between two variables and can help find interactions between them. The term contingency table was first used by Karl Pearson in "On the Theory of Contingency and Its Relation to Association and Normal Correlation", part of the Drapers' Company Research Memoirs Biometric Series I published in 1904. A crucial problem of multivariate statistics is finding (direct-)dependence structure underlying the variables contained in high-dimensional contingency tables. If some of the conditional independences are revealed, then even the storage of the data can be done in a smarter way (see Lauritzen (2002)). In order to do this one can use information theory concepts, which gain the information only from the distribution of probability, which can be expressed easily from the contingency table by the relative frequencies.
A product distribution is a probability distribution constructed as the distribution of the product of random variables having two other known distributions. Given two statistically independent random variables X and Y, the distribution of the random variable Z that is formed as the product  is a product distribution.
A Bayesian network, Bayes network, belief network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Formally, Bayesian networks are DAGs whose nodes represent random variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (there is no path from one of the variables to the other in the bayesian network) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if  parent nodes represent  Boolean variables then the probability function could be represented by a table of  entries, one entry for each of the  possible combinations of its parents being true or false. Similar ideas may be applied to undirected, and possibly cyclic, graphs; such are called Markov networks. Efficient algorithms exist that perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
The geometric median of a discrete set of sample points in a Euclidean space is the point minimizing the sum of distances to the sample points. This generalizes the median, which has the property of minimizing the sum of distances for one-dimensional data, and provides a central tendency in higher dimensions. It is also known as the 1-median, spatial median, Euclidean minisum point, or Torricelli point. The geometric median is an important estimator of location in statistics, where it is also known as the L1 estimator. It is also a standard problem in facility location, where it models the problem of locating a facility to minimize the cost of transportation. The special case of the problem for three points in the plane (that is, m = 3 and n = 2 in the definition below) is sometimes also known as Fermat's problem; it arises in the construction of minimal Steiner trees, and was originally posed as a problem by Pierre de Fermat and solved by Evangelista Torricelli. Its solution is now known as the Fermat point of the triangle formed by the three sample points. The geometric median may in turn be generalized to the problem of minimizing the sum of weighted distances, known as the Weber problem after Alfred Weber's discussion of the problem in his 1909 book on facility location. Some sources instead call Weber's problem the Fermat Weber problem, but others use this name for the unweighted geometric median problem. Wesolowsky (1993) provides a survey of the geometric median problem. See Fekete, Mitchell & Beurer (2005) for generalizations of the problem to non-discrete point sets.
In natural language processing, Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003. Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000. Both papers have been highly influential, with 13320 and 15857 citations respectively in January 2016.
In statistics, a semiparametric model is a model that has parametric and nonparametric components. A model is a collection of distributions:  indexed by a parameter . A parametric model is one in which the indexing parameter is a finite-dimensional vector (in -dimensional Euclidean space for some integer ); i.e. the set of possible values for  is a subset of , or . In this case we say that  is finite-dimensional. In nonparametric models, the set of possible values of the parameter  is a subset of some space, not necessarily finite-dimensional. For example, we might consider the set of all distributions with mean 0. Such spaces are vector spaces with topological structure, but may not be finite-dimensional as vector spaces. Thus,  for some possibly infinite-dimensional space . In semiparametric models, the parameter has both a finite-dimensional component and an infinite-dimensional component (often a real-valued function defined on the real line). Thus the parameter space  in a semiparametric model satisfies , where  is an infinite-dimensional space. It may appear at first that semiparametric models include nonparametric models, since they have an infinite-dimensional as well as a finite-dimensional component. However, a semiparametric model is considered to be "smaller" than a completely nonparametric model because we are often interested only in the finite-dimensional component of . That is, we are not interested in estimating the infinite-dimensional component. In nonparametric models, by contrast, the primary interest is in estimating the infinite-dimensional parameter. Thus the estimation task is statistically harder in nonparametric models. These models often use smoothing or kernels.
Response bias is a general term for a wide range of cognitive biases that influence the responses of participants away from an accurate or truthful response. These biases are most prevalent in the types of studies and research that involve participant self-report, such as structured interviews or surveys. Response biases can have a large impact on the validity of questionnaires or surveys. Response bias can be induced or caused by a number of factors, all relating to the idea that human subjects do not respond passively to stimuli, but rather actively integrate multiple sources of information to generate a response in a given situation. Because of this, almost any aspect of an experimental condition may potentially bias a respondent. Examples include the phrasing of questions in surveys, the demeanor of the researcher, the way the experiment is conducted, or the desires of the participant to be a good experimental subject and to provide socially desirable responses may affect the response in some way. All of these "artifacts" of survey and self-report research may have the potential to damage the validity of a measure or study. Compounding this issue is that surveys affected by response bias still often have high reliability, which can lure researchers into a false sense of security about the conclusions they draw. Because of response bias, it is possible that some study results are due to a systematic response bias rather than the hypothesized effect, which can have a profound effect on psychological and other types of research using questionnaires or surveys. It is therefore important for researchers to be aware of response bias and the effect it can have on their research so that they can attempt to prevent it from impacting their findings in a negative manner.
Frequentist probability or frequentism is a standard interpretation of probability; it defines an event's probability as the limit of its relative frequency in a large number of trials. This interpretation supports the statistical needs of experimental scientists and pollsters; probabilities can be found (in principle) by a repeatable objective process (and are thus ideally devoid of opinion). It does not support all needs; gamblers typically require estimates of the odds without experiments. The development of the frequentist account was motivated by the problems and paradoxes of the previously dominant viewpoint, the classical interpretation. In the classical interpretation, probability was defined in terms of the principle of indifference, based on the natural symmetry of a problem, so, e.g. the probabilities of dice games arise from the natural symmetric 6-sidedness of the cube. This classical interpretation stumbled at any statistical problem that has no natural symmetry for reasoning.
In statistical quality control, the np-chart is a type of control chart used to monitor the number of nonconforming units in a sample. It is an adaptation of the p-chart and used in situations where personnel find it easier to interpret process performance in terms of concrete numbers of units rather than the somewhat more abstract proportion. The np-chart differs from the p-chart in only the three following aspects: The control limits are , where n is the sample size and  is the estimate of the long-term process mean established during control-chart setup. The number nonconforming (np), rather than the fraction nonconforming (p), is plotted against the control limits. The sample size, , is constant.
In statistics, maximum spacing estimation (MSE or MSP), or maximum product of spacing estimation (MPS), is a method for estimating the parameters of a univariate statistical model. The method requires maximization of the geometric mean of spacings in the data, which are the differences between the values of the cumulative distribution function at neighbouring data points. The concept underlying the method is based on the probability integral transform, in that a set of independent random samples derived from any random variable should on average be uniformly distributed with respect to the cumulative distribution function of the random variable. The MPS method chooses the parameter values that make the observed data as uniform as possible, according to a specific quantitative measure of uniformity. One of the most common methods for estimating the parameters of a distribution from data, the method of maximum likelihood (MLE), can break down in various cases, such as involving certain mixtures of continuous distributions. In these cases the method of maximum spacing estimation may be successful. Apart from its use in pure mathematics and statistics, the trial applications of the method have been reported using data from fields such as hydrology, econometrics, magnetic resonance imaging, and others.
X-12-ARIMA was the U.S. Census Bureau's software package for seasonal adjustment. X-12-ARIMA can be used together with many statistical packages, such as Gretl or EViews which provides a graphical user interface for X-12-ARIMA, and NumXL which avails X-12-ARIMA functionality in Microsoft Excel. Notable statistical agencies presently using X-12-ARIMA for seasonal adjustment include Statistics Canada and the U.S. Bureau of Labor Statistics. X-12-ARIMA was the successor to X-11-ARIMA; the current version is X-13ARIMA-SEATS. Both X-12-ARIMA and X-13-ARIMA-SEATS's source code can be found on the Census Bureau's website.
In statistical classification the Bayes classifier minimizes the probability of misclassification.
In statistics, there is a negative relationship or inverse relationship between two variables if higher values of one variable tend to be associated with lower values of the other. A negative relationship between two variables usually implies that the correlation between them is negative, or what is in some contexts equivalent  that the slope in a corresponding graph is negative. A negative correlation between variables is also called anticorrelation or inverse correlation. An example would be a negative cross-sectional relationship between illness and vaccination, if it is observed that where the incidence of one is higher than average, the incidence of the other tends to be lower than average. Similarly, there would be a negative temporal relationship between illness and vaccination if it is observed in one location that times with a higher-than-average incidence of one tend to coincide with a lower-than-average incidence of the other.
In statistics, the pseudomedian is a measure of centrality for data-sets and populations. It agrees with the median for symmetric data-sets or populations. In mathematical statistics, the pseudomedian is also a location parameter for probability distributions.
In probability theory and statistics, the beta prime distribution (also known as inverted beta distribution or beta distribution of the second kind) is an absolutely continuous probability distribution defined for  with two parameters   and  , having the probability density function:  where B is a Beta function. The cumulative distribution function is  where I is the regularized incomplete beta function. The expectation value, variance, and other details of the distribution are given in the sidebox; for , the excess kurtosis is . While the related beta distribution is the conjugate prior distribution of the parameter of a Bernoulli distribution expressed as a probability, the beta prime distribution is the conjugate prior distribution of the parameter of a Bernoulli distribution expressed in odds. The distribution is a Pearson type VI distribution. The mode of a variate X distributed as  is . Its mean is  if  (if  the mean is infinite, in other words it has no well defined mean) and its variance is  if . For , the k-th moment  is given by  For  with , this simplifies to  The cdf can also be written as  where  is the Gauss's hypergeometric function 2F1 . Differential equation
Scoring algorithm, also known as Fisher's scoring, is a form of Newton's method used in statistics to solve maximum likelihood equations numerically, named after Ronald Fisher.
In probability theory and statistics, variance measures how far a set of numbers are spread out. A variance of zero indicates that all the values are identical. Variance is always non-negative: a small variance indicates that the data points tend to be very close to the mean (expected value) and hence to each other, while a high variance indicates that the data points are very spread out around the mean and from each other. An equivalent measure is the square root of the variance, called the standard deviation. The standard deviation has the same dimension as the data, and hence is comparable to deviations from the mean. As standard deviation is often represented with the symbol   (lowercase sigma), so variance is often represented with the symbol  2 (sigma squared). There are two distinct concepts that are both called "variance". One variance is a characteristic of a set of observations. The other is part of a theoretical probability distribution and is defined by an equation. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below. The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance. The variance is one of several descriptors of a probability distribution. In particular, the variance is one of the moments of a distribution. In that context, it forms part of a systematic approach to distinguishing between probability distributions. While other such approaches have been developed, those based on moments are advantageous in terms of mathematical and computational simplicity.
Sensitivity analysis is the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be apportioned to different sources of uncertainty in its inputs. A related practice is uncertainty analysis, which has a greater focus on uncertainty quantification and propagation of uncertainty. Ideally, uncertainty and sensitivity analysis should be run in tandem. The process of recalculating outcomes under alternative assumptions to determine the impact of variable under sensitivity analysis can be useful for a range of purposes, including Testing the robustness of the results of a model or system in the presence of uncertainty. Increased understanding of the relationships between input and output variables in a system or model. Uncertainty reduction: identifying model inputs that cause significant uncertainty in the output and should therefore be the focus of attention if the robustness is to be increased (perhaps by further research). Searching for errors in the model (by encountering unexpected relationships between inputs and outputs). Model simplification   fixing model inputs that have no effect on the output, or identifying and removing redundant parts of the model structure. Enhancing communication from modelers to decision makers (e.g. by making recommendations more credible, understandable, compelling or persuasive). Finding regions in the space of input factors for which the model output is either maximum or minimum or meets some optimum criterion (see optimization and Monte Carlo filtering). In case of calibrating models with large number of parameters, a primary sensitivity test can ease the calibration stage by focusing on the sensitive parameters. Not knowing the sensitivity of parameters can result in time being uselessly spent on non-sensitive ones. Taking an example from economics, in any budgeting process there are always variables that are uncertain. Future tax rates, interest rates, inflation rates, headcount, operating expenses and other variables may not be known with great precision. Sensitivity analysis answers the question, "if these deviate from expectations, what will the effect be (on the business, model, system, or whatever is being analyzed), and which variables are causing the largest deviations "
In statistics, a fixed effects model is a statistical model that represents the observed quantities in terms of explanatory variables that are treated as if the quantities were non-random. This is in contrast to random effects models and mixed models in which either all or some of the explanatory variables are treated as if they arise from random causes. Contrast this to the biostatistics definitions, as biostatisticians use "fixed" and "random" effects to respectively refer to the population-average and subject-specific effects (and where the latter are generally assumed to be unknown, latent variables). Often the same structure of model, which is usually a linear regression model, can be treated as any of the three types depending on the analyst's viewpoint, although there may be a natural choice in any given situation. In panel data analysis, the term fixed effects estimator (also known as the within estimator) is used to refer to an estimator for the coefficients in the regression model. If we assume fixed effects, we impose time independent effects for each entity that are possibly correlated with the regressors.
In statistics, the Page test for multiple comparisons between ordered correlated variables is the counterpart of Spearman's rank correlation coefficient which summarizes the association of continuous variables. It is also known as Page's trend test or Page's L test. It is a repeated measure trend test. The Page test is useful where: there are three or more conditions, a number of subjects (or other randomly sampled entities) are all observed in each of them, and we predict that the observations will have a particular order. For example, a number of subjects might each be given three trials at the same task, and we predict that performance will improve from trial to trial. A test of the significance of the trend between conditions in this situation was developed by Page (1963). More formally, the test considers the null hypothesis that, for n conditions, where mi is a measure of the central tendency of the ith condition,  against the alternative hypothesis that  It has more statistical power than the Friedman test against the alternative that there is a difference in trend. Friedman's test considers the alternative hypothesis that the central tendencies of the observations under the n conditions are different without specifying their order. Procedure for the Page test, with k subjects each exposed to n conditions: Arrange the n conditions in the order implied by the alternative hypothesis, and assign each of them a rank Yi. For each of the k subjects separately, rank the n observations from 1 to n. Add the ranks for each condition to give a total Xi. Multiply Xi by Yi and add all the products together; this sum is called L. To test whether there is a significant trend, values of L can be compared with those tabulated by Page (1963). Alternatively, the quantity  may be compared with values of the chi-squared distribution with one degree of freedom. This gives a two-tailed test. The approximation is reliable for more than 20 subjects with any number of conditions, for more than 12 subjects when there are 4 or more conditions, and for any number of subjects when there are 9 or more conditions. If a measure of the overall correlation between the conditions and the data is required, it can be calculated as    = 12L/k(n3   n)   3(n + 1)/(n   1)  if k = 1, this reduces to the familiar Spearman coefficient. The Page test is most often used with fairly small numbers of conditions and subjects. The minimum values of L for significance at the 0.05 level, one-tailed, with three conditions, are 56 for 4 subjects (the lowest number that is capable of giving a significant result at this level), 54 for 5 subjects, 91 for 7 subjects, 128 for 10 subjects, 190 for 15 subjects and 251 for 20 subjects. A corresponding extension of Kendall's tau was developed by Jonckheere (1954).
A chi-squared test, also referred to as  test (or chi-square test), is any statistical hypothesis test in which the sampling distribution of the test statistic is a chi-square distribution when the null hypothesis is true. Chi-squared tests are often constructed from a sum of squared errors, or through the sample variance. Test statistics that follow a chi-squared distribution arise from an assumption of independent normally distributed data, which is valid in many cases due to the central limit theorem. A chi-squared test can then be used to reject the null hypothesis that the data are independent. Also considered a chi-square test is a test in which this is asymptotically true, meaning that the sampling distribution (if the null hypothesis is true) can be made to approximate a chi-square distribution as closely as desired by making the sample size large enough. The chi-squared test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories. Does the number of individuals or objects that fall in each category differ significantly from the number you would expect  Is this difference between the expected and observed due to sampling variation, or is it a real difference 
In statistics, originally in geostatistics, Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process governed by prior covariances, as opposed to a piecewise-polynomial spline chosen to optimize smoothness of the fitted values. Under suitable assumptions on the priors, Kriging gives the best linear unbiased prediction of the intermediate values. Interpolating methods based on other criteria such as smoothness need not yield the most likely intermediate values. The method is widely used in the domain of spatial analysis and computer experiments. The technique is also known as Wiener Kolmogorov prediction, after Norbert Wiener and Andrey Kolmogorov.  The theoretical basis for the method was developed by the French mathematician Georges Matheron based on the Master's thesis of Danie G. Krige, the pioneering plotter of distance-weighted average gold grades at the Witwatersrand reef complex in South Africa. Krige sought to estimate the most likely distribution of gold based on samples from a few boreholes. The English verb is to krige and the most common noun is Kriging; both are often pronounced with a hard "g", following the pronunciation of the name "Krige".
In probability theory and statistics, coherence can have several different meanings.
In quantum field theory, the (real space) n-point correlation function is defined as the functional average (functional expectation value) of a product of  field operators at different positions  For time-dependent correlation functions, the time-ordering operator  is included. Correlation functions are also called simply correlators. Sometimes, the phrase Green's function is used not only for two-point functions, but for any correlators. The correlation function can be interpreted physically as the amplitude for propagation of a particle or excitation between y and x. In the free theory, it is simply the Feynman propagator(for n=2). [For more information see 'An Introduction to Quantum Field Theory' by Peskin & Schroeder, Section 4.2 : Perturbation Expansion of Correlation Functions]
BMDP is a statistical package developed in 1965 by Wilfrid Dixon at the University of California, Los Angeles. Based on the older BIMED program, developed in 1960 for biomedical applications, it used keyword parameters in the input instead of fixed-format cards, so the letter P was added to the letters BMD, although the name was later defined as being an backronym for Biomedical Package. BMDP was originally distributed for free. It is now offered by Statistical Solutions.
Placebo-controlled studies are a way of testing a medical therapy in which, in addition to a group of subjects that receives the treatment to be evaluated, a separate control group receives a sham "placebo" treatment which is specifically designed to have no real effect. Placebos are most commonly used in blinded trials, where subjects do not know whether they are receiving real or placebo treatment. Often, there is also a further "natural history" group that does not receive any treatment at all. The purpose of the placebo group is to account for the placebo effect, that is, effects from treatment that do not depend on the treatment itself. Such factors include knowing one is receiving a treatment, attention from health care professionals, and the expectations of a treatment's effectiveness by those running the research study. Without a placebo group to compare against, it is not possible to know whether the treatment itself had any effect. Patients frequently show improvement even when given a sham or "fake" treatment. Such intentionally inert placebo treatments can take many forms, such as a pill containing only sugar, a surgery where nothing efficacious is actually done (just an incision and sometimes some minor touching or handling of the underlying structures), or a medical device (such as an ultrasound machine) that is not actually turned on. Also, due to the body's natural healing ability and statistical effects such as regression to the mean, many patients will get better even when given no treatment at all. Thus, the relevant question when assessing a treatment is not "does the treatment work " but "does the treatment work better than a placebo treatment, or no treatment at all " As one early clinical trial researcher wrote, "the first object of a therapeutic trial is to discover whether the patients who receive the treatment under investigation are cured more rapidly, more completely or more frequently, than they would have been without it."p.195 More broadly, the aim of a clinical trial is to determine what treatments, delivered in what circumstances, to which patients, in what conditions, are the most effective. Therefore, the use of placebos is a standard control component of most clinical trials, which attempt to make some sort of quantitative assessment of the efficacy of medicinal drugs or treatments. Such a test or clinical trial is called a placebo-controlled study, and its control is of the negative type. A study whose control is a previously tested treatment, rather than no treatment, is called a positive-control study, because its control is of the positive type. Government regulatory agencies approve new drugs only after tests establish not only that patients respond to them, but also that their effect is greater than that of a placebo (by way of affecting more patients, by affecting responders more strongly, or both).
In statistics, Scheffe 's method, named after the American statistician Henry Scheffe , is a method for adjusting significance levels in a linear regression analysis to account for multiple comparisons. It is particularly useful in analysis of variance (a special case of regression analysis), and in constructing simultaneous confidence bands for regressions involving basis functions. Scheffe 's method is a single-step multiple comparison procedure which applies to the set of estimates of all possible contrasts among the factor level means, not just the pairwise differences considered by the Tukey Kramer method.
Winsorizing or winsorization is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers. It is named after the engineer-turned-biostatistician Charles P. Winsor (1895 1951). The effect is the same as clipping in signal processing. The distribution of many statistics can be heavily influenced by outliers. A typical strategy is to set all outliers to a specified percentile of the data; for example, a 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile. Winsorized estimators are usually more robust to outliers than their more standard forms, although there are alternatives, such as trimming, that will achieve a similar effect.
The Taguchi loss function is graphical depiction of loss developed by the Japanese business statistician newton describe a phenomenon affecting the value of products produced by a company. Praised by Dr. W. Edwards Deming (the business guru of the 1980s American quality movement), it made clear the concept that quality does not suddenly plummet when, for instance, a machinist exceeds a rigid blueprint tolerance. Instead 'loss' in value progressively increases as variation increases from the intended condition. This was considered a breakthrough in describing quality, and helped fuel the continuous improvement movement that since has become known as lean manufacturing. The concept of Taguchi's quality loss function was in contrast with the American concept of quality, popularly known as goal post philosophy, the concept given by American quality guru Phil Crosby. Goal post philosophy emphasizes that if a product feature doesn't meet the designed specifications it is termed as a product of poor quality (rejected), irrespective of amount of deviation from the target value (mean value of tolerance zone). This concept has similarity with the concept of scoring a 'goal' in the game of football or hockey, because a goal is counted 'one' irrespective of the location of strike of the ball in the 'goal post', whether it is in the center or towards the corner. This means that if the product dimension goes out of the tolerance limit the quality of the product drops suddenly. Through his concept of the quality loss function, Taguchi explained that from the customer's point of view this drop of quality is not sudden. The customer experiences a loss of quality the moment product specification deviates from the 'target value'. This 'loss' is depicted by a quality loss function and it follows a parabolic curve mathematically given by L = k(y m)2, where m is the theoretical 'target value' or 'mean value' and y is the actual size of the product, k is a constant and L is the loss. This means that if the difference between 'actual size' and 'target value' i.e. (y m) is large, loss would be more, irrespective of tolerance specifications. In Taguchi's view tolerance specifications are given by engineers and not by customers; what the customer experiences is 'loss'. This equation is true for a single product; if 'loss' is to be calculated for multiple products the loss function is given by L = k[S2 + (   m)2], where S2 is the 'variance of product size' and  is the average product size.
In statistical modeling (especially process modeling), polynomial functions and rational functions are sometimes used as an empirical technique for curve fitting.
Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. The algorithm for inducing Breiman's random forest was developed by Leo Breiman and Adele Cutler, and "Random Forests" is their trademark. The method combines Breiman's "bagging" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance. The selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.
Polynomial chaos (PC), also called Wiener chaos expansion, is a non-sampling-based method to determine evolution of uncertainty in dynamical system, when there is probabilistic uncertainty in the system parameters. PC was first introduced by Norbert Wiener where Hermite polynomials were used to model stochastic processes with Gaussian random variables. It can be thought of as an extension of Volterra's theory of nonlinear functionals for stochastic systems. According to Cameron and Martin such an expansion converges in the  sense for any arbitrary stochastic process with finite second moment. This applies to most physical systems.
In engineering, science, and statistics, replication is the repetition of an experimental condition so that the variability associated with the phenomenon can be estimated. ASTM, in standard E1847, defines replication as "the repetition of the set of all the treatment combinations to be compared in an experiment. Each of the repetitions is called a replicate." Replication is not the same as repeated measurements of the same item: they are dealt with differently in statistical experimental design and data analysis. For proper sampling, a process or batch of products should be in reasonable statistical control; inherent random variation is present but variation due to assignable (special) causes is not. Evaluation or testing of a single item does not allow for item-to-item variation and may not represent the batch or process. Replication is needed to account for this variation among items and treatments.
Gauss Markov stochastic processes (named after Carl Friedrich Gauss and Andrey Markov) are stochastic processes that satisfy the requirements for both Gaussian processes and Markov processes. The stationary Gauss Markov process is a very special case because it is unique, except for some trivial exceptions. Every Gauss Markov process X(t) possesses the three following properties: If h(t) is a non-zero scalar function of t, then Z(t) = h(t)X(t) is also a Gauss Markov process If f(t) is a non-decreasing scalar function of t, then Z(t) = X(f(t)) is also a Gauss Markov process There exists a non-zero scalar function h(t) and a non-decreasing scalar function f(t) such that X(t) = h(t)W(f(t)), where W(t) is the standard Wiener process. Property (3) means that every Gauss Markov process can be synthesized from the standard Wiener process (SWP).
The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a forecasting method in statistics, for example in trend estimation. It usually expresses accuracy as a percentage, and is defined by the formula:  where At is the actual value and Ft is the forecast value. The difference between At and Ft is divided by the Actual value At again. The absolute value in this calculation is summed for every forecasted point in time and divided by the number of fitted points n. Multiplying by 100 makes it a percentage error. Although the concept of MAPE sounds very simple and convincing, it has major drawbacks in practical application  It cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division by zero. For forecasts which are too low the percentage error cannot exceed 100%, but for forecasts which are too high there is no upper limit to the percentage error. When MAPE is used to compare the accuracy of prediction methods it is biased in that it will systematically select a method whose forecasts are too low. This little-known but serious issue can be overcome by using an accuracy measure based on the ratio of the predicted to actual value (called the Accuracy Ratio), this approach leads to superior statistical properties and leads to predictions which can be interpreted in terms of the geometric mean.
Particle filters or Sequential Monte Carlo (SMC) methods are a set of genetic-type particle Monte Carlo methodologies to solve the filtering problem. The term "particle filters" was first coined in 1996 by Del Moral in reference to mean field interacting particle methods used in fluid mechanics since the beginning of the 1960s. The terminology "sequential Monte Carlo" was proposed by Liu and Chen in 1998. From the statistical and probabilistic point of view, particle filters can be interpreted as mean field particle interpretations of Feynman-Kac probability measures. These particle integration techniques were developed in molecular chemistry and computational physics by Theodore E. Harris and Herman Kahn in 1951, Marshall. N. Rosenbluth and Arianna. W. Rosenbluth in 1955 and more recently by Jack H. Hetherington in 1984. In computational physics, these Feynman-Kac type path particle integration methods are also used in Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods. Feynman-Kac interacting particle methods are also strongly related to mutation-selection genetic algorithms currently used in evolutionary computing to solve complex optimization problems. The particle filter methodology is used to solve Hidden Markov Chain (HMM) and nonlinear filtering problems arising in signal processing and Bayesian statistical inference. The filtering problem consists in estimating the internal states in dynamical systems when partial observations are made, and random perturbations are present in the sensors as well as in the dynamical system. The objective is to compute the conditional probability (a.k.a. posterior distributions) of the states of some Markov process, given some noisy and partial observations. With the notable exception of linear-Gaussian signal-observation models (Kalman filter) or wider classes of models (Benes filter) Mireille Chaleyat-Maurel and Dominique Michel proved in 1984 that the sequence of posterior distributions of the random states of the signal given the observations (a.k.a. optimal filter) have no finitely recursive recursion. Various numerical methods based on fixed grid approximations, Markov Chain Monte Carlo techniques (MCMC), conventional linearization, extended Kalman filters, or determining the best linear system (in expect cost-error sense) have never really coped with large scale systems, unstable processes or when the nonlinearities are not sufficiently smooth. Particle filtering methodology uses a genetic type mutation-selection sampling approach, with a set of particles (also called individuals, or samples) to represent the posterior distribution of some stochastic process given some noisy and/or partial observations. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. Particle filter techniques provide a well-established methodology for generating samples from the required distribution without requiring assumptions about the state-space model or the state distributions. However, these methods do not perform well when applied to very high-dimensional systems. Particle filters implement the prediction-updating transitions of the filtering equation directly by using a genetic type mutation-selection particle algorithm. The samples from the distribution are represented by a set of particles; each particle has a likelihood weight assigned to it that represents the probability of that particle being sampled from the probability density function. Weight disparity leading to weight collapse is a common issue encountered in these filtering algorithms; however it can be mitigated by including a resampling step before the weights become too uneven. Several adaptive resampling criteria can be used, including the variance of the weights and the relative entropy w.r.t. the uniform distribution. In the resampling step, the particles with negligible weights are replaced by new particles in the proximity of the particles with higher weights. Particle filters and Feynman-Kac particle methodologies find application in signal and image processing, Bayesian inference, machine learning, risk analysis and rare event sampling, engineering and robotics, artificial intelligence, bioinformatics, phylogenetics, computational science, Economics and mathematical finance, molecular chemistry, computational physics, pharmacokinetic and other fields.
Classical test theory is a body of related psychometric theory that predicts outcomes of psychological testing such as the difficulty of items or the ability of test-takers. Generally speaking, the aim of classical test theory is to understand and improve the reliability of psychological tests. Classical test theory may be regarded as roughly synonymous with true score theory. The term "classical" refers not only to the chronology of these models but also contrasts with the more recent psychometric theories, generally referred to collectively as item response theory, which sometimes bear the appellation "modern" as in "modern latent trait theory". Classical test theory as we know it today was codified by Novick (1966) and described in classic texts such as Lord & Novick (1968) and Allen & Yen (1979/2002). The description of classical test theory below follows these seminal publications.
Precision is a description of random errors, a measure of statistical variability. Accuracy has two definitions: more commonly, it is a description of systematic errors, a measure of statistical bias; alternatively, the ISO defines accuracy as describing both types of observational error above (preferring the term trueness for the common definition of accuracy).
In probability theory, the sample space of an experiment or random trial is the set of all possible outcomes or results of that experiment. A sample space is usually denoted using set notation, and the possible outcomes are listed as elements in the set. It is common to refer to a sample space by the labels S,  , or U (for "universal set"). For example, if the experiment is tossing a coin, the sample space is typically the set {head, tail}. For tossing two coins, the corresponding sample space would be {(head,head), (head,tail), (tail,head), (tail,tail)}. For tossing a single six-sided die, the typical sample space is {1, 2, 3, 4, 5, 6} (in which the result of interest is the number of pips facing up). A well-defined sample space is one of three basic elements in a probabilistic model (a probability space); the other two are a well-defined set of possible events (a sigma-algebra) and a probability assigned to each event (a probability measure function).
In statistics, compositional data are quantitative descriptions of the parts of some whole, conveying exclusively relative information. This definition, given by John Aitchison (1986) has several consequences: A compositional data point, or composition for short, can be represented by a positive real vector with as many parts as considered. Sometimes, if the total amount is fixed and known, one component of the vector can be omitted. As compositions only carry relative information, the only information is given by the ratios between components. Consequently, a composition multiplied by any positive constant contains the same information as the former. Therefore, proportional positive vectors are equivalent when considered as compositions. As usual in mathematics, equivalent classes are represented by some element of the class, called a representative. Thus, equivalent compositions can be represented by positive vectors whose components add to a given constant . The vector operation assigning the constant sum representative is called closure and is denoted by :  where D is the number of parts (components) and  denotes a row vector. Compositional data can be represented by constant sum real vectors with positive components, and this vectors span a simplex, defined as  This is the reason why  is considered to be the sample space of compositional data. The positive constant  is arbitrary. Frequent values for  are 1 (per unit), 100 (percent, %), 1000, 106 (ppm), 109 (ppb), ... In statistics, compositional data is frequently considered to be data in which each data point is an D-tuple of nonnegative numbers whose sum is 1. Typically each of the D components xi of each data point [x1, ..., xD] says what proportion (or "percentage") of a statistical unit falls into the ith category in a list of D categories. Very often ternary plots are used in analysis of compositional data to represent a three part composition. An alternative nomenclature for compositional analysis is simplicial analysis, motivated by the concept of simplicial sets. Remarks on the definition of the simplex: In mathematical frameworks, the superscript of , accounting for the number of parts, is often changed to D   1, describing the dimension. The components of the vector are assumed to be positive. However, in some definitions of the simplex, non-negative components are admitted. Here null components are avoided, because ratios between components of which some are zero are meaningless.
A randomized controlled trial (or randomized control trial; RCT) is a type of scientific (often medical) experiment which aims to reduce bias when testing a new treatment. The people participating in the trial are randomly allocated to either the group receiving the treatment under investigation or to a group receiving standard treatment (or placebo treatment) as the control. Randomization minimises selection bias and the different comparison groups allow the researchers to determine any effects of the treatment when compared with the no treatment (control) group, while other variables are kept constant. The RCT is often considered the gold standard for a clinical trial. RCTs are often used to test the efficacy or effectiveness of various types of medical intervention and may provide information about adverse effects, such as drug reactions. Random assignment of intervention is done after subjects have been assessed for eligibility and recruited, but before the intervention to be studied begins. Random allocation in real trials is complex, but conceptually the process is like tossing a coin. After randomization, the two (or more) groups of subjects are followed in exactly the same way and the only differences between them is the care they receive. For example, in terms of procedures, tests, outpatient visits, and follow-up calls, should be those intrinsic to the treatments being compared. The most important advantage of proper randomization is that it minimizes allocation bias, balancing both known and unknown prognostic factors, in the assignment of treatments. The terms "RCT" and randomized trial are sometimes used synonymously, but the methodologically sound practice is to reserve the "RCT" name only for trials that contain control groups, in which groups receiving the experimental treatment are compared with control groups receiving no treatment (a placebo-controlled study) or a previously tested treatment (a positive-control study). The term "randomized trials" omits mention of controls and can describe studies that compare multiple treatment groups with each other (in the absence of a control group). Similarly, although the "RCT" name is sometimes expanded as "randomized clinical trial" or "randomized comparative trial", the methodologically sound practice, to avoid ambiguity in the scientific literature, is to retain "control" in the definition of "RCT" and thus reserve that name only for trials that contain controls. Not all randomized clinical trials are randomized controlled trials (and some of them could never be, in cases where controls would be impractical or unethical to institute). The term randomized controlled clinical trials is a methodologically sound alternate expansion for "RCT" in RCTs that concern clinical research; however, RCTs are also employed in other research areas, including many of the social sciences.
Generalized expected utility is a decision-making metric based on any of a variety of theories that attempt to resolve some discrepancies between expected utility theory and empirical observations, concerning choice under risky (probabilistic) circumstances. The expected utility model developed by John von Neumann and Oskar Morgenstern dominated decision theory from its formulation in 1944 until the late 1970s, not only as a prescriptive, but also as a descriptive model, despite powerful criticism from Maurice Allais and Daniel Ellsberg who showed that, in certain choice problems, decisions were usually inconsistent with the axioms of expected utility theory. These problems are usually referred to as the Allais paradox and Ellsberg paradox. Beginning in 1979 with the publication of the prospect theory of Daniel Kahneman and Amos Tversky, a range of generalized expected utility models were developed with the aim of resolving the Allais and Ellsberg paradoxes, while maintaining many of the attractive properties of expected utility theory. Important examples were anticipated utility theory, later referred to as rank-dependent utility theory, weighted utility (Chew 1982), and expected uncertain utility theory. A general representation, using the concept of the local utility function was presented by Mark J. Machina. Since then, generalizations of expected utility theory have proliferated, but the probably most frequently used model is nowadays cumulative prospect theory, a rank-dependent development of prospect theory, introduced in 1992 by Daniel Kahneman and Amos Tversky. Given its motivations and approach, generalized expected utility theory may properly be regarded as a subfield of behavioral economics, but it is more frequently located within mainstream economic theory.
Multilinear principal component analysis (MPCA)    is a mathematical procedure that uses multiple orthogonal transformations to convert a set of multidimensional objects into another set of multidimensional objects of lower dimensions. There is one orthogonal (linear) transformation for each dimension (mode); hence multilinear. This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data as possible, subject to the constraint of mode-wise orthogonality. MPCA is a multilinear extension of principal component analysis (PCA). The major difference is that PCA needs to reshape a multidimensional object into a vector, while MPCA operates directly on multidimensional objects through mode-wise processing. For example, for 100x100 images, PCA operates on vectors of 10000x1 while MPCA operates on vectors of 100x1 in two modes. For the same amount of dimension reduction, PCA needs to estimate 49*(10000/(100*2)-1) times more parameters than MPCA. Thus, MPCA is more efficient and better conditioned in practice. MPCA is a basic algorithm for dimension reduction via multilinear subspace learning. In wider scope, it belongs to tensor-based computation. Its origin can be traced back to the Tucker decomposition in 1960s and it is closely related to higher-order singular value decomposition, (HOSVD) and to the best rank-(R1, R2, ..., RN ) approximation of higher-order tensors.
Spatial variability occurs when a quantity that is measured at different spatial locations exhibits values that differ across the locations. Spatial variability can be assessed using spatial descriptive statistics such as the range.
In statistics, an endogeneity problem occurs when an explanatory variable is correlated with the error term. Endogeneity can arise as a result of measurement error, autoregression with autocorrelated errors, simultaneity and omitted variables. Two common causes of endogeneity are: 1) an uncontrolled confounder causing both independent and dependent variables of a model; and 2) a loop of causality between the independent and dependent variables of a model. For example, in a simple supply and demand model, when predicting the quantity demanded in equilibrium, the price is endogenous because producers change their price in response to demand and consumers change their demand in response to price. In this case, the price variable is said to have total endogeneity once the demand and supply curves are known. In contrast, a change in consumer tastes or preferences would be an exogenous change on the demand curve.
An optimal decision is a decision that leads to a better outcome than all other available decision options. It is an important concept in decision theory. In order to compare the different decision outcomes, one commonly assigns a relative utility to each of them. If there is uncertainty in what the outcome will be, the optimal decision maximizes the expected utility (utility averaged over all possible outcomes of a decision). Sometimes, the equivalent problem of minimizing loss is considered, particularly in financial situations, where the utility is defined as economic gain. "Utility" is only an arbitrary term for quantifying the desirability of a particular decision outcome and not necessarily related to "usefulness." For example, it may well be the optimal decision for someone to buy a sports car rather than a station wagon, if the outcome in terms of another criterion (e.g., effect on personal image) is more desirable, even given the higher cost and lack of versatility of the sports car. The problem of finding the optimal decision is a mathematical optimization problem. In practice, few people verify that their decisions are optimal, but instead use heuristics to make decisions that are "good enough" that is, they engage in satisficing. A more formal approach may be used when the decision is important enough to motivate the time it takes to analyze it, or when it is too complex to solve with more simple intuitive approaches, such as with a large number of available decision options and a complex decision   outcome relationship.
Spatial descriptive statistics are used for a variety of purposes in geography, particularly in quantitative data analyses involving Geographic Information Systems (GIS).
The standard error (SE) is the standard deviation of the sampling distribution of a statistic, most commonly of the mean. The term may also be used to refer to an estimate of that standard deviation, derived from a particular sample used to compute the estimate. For example, the sample mean is the usual estimator of a population mean. However, different samples drawn from that same population would in general have different values of the sample mean, so there is a distribution of sampled means (with its own mean and variance). The standard error of the mean (SEM) (i.e., of using the sample mean as a method of estimating the population mean) is the standard deviation of those sample means over all possible samples (of a given size) drawn from the population. Secondly, the standard error of the mean can refer to an estimate of that standard deviation, computed from the sample of data being analyzed at the time. In regression analysis, the term "standard error" is also used in the phrase standard error of the regression to mean the ordinary least squares estimate of the standard deviation of the underlying errors.
The rank product is a biologically motivated test for the detection of differentially expressed genes in replicated microarray experiments. It is a simple non-parametric statistical method based on ranks of fold changes. In addition to its use in expression profiling, it can be used to combine ranked lists in various application domains, including proteomics, metabolomics, statistical meta-analysis, and general feature selection.
In probability theory and statistics, the cumulants  n of a probability distribution are a set of quantities that provide an alternative to the moments of the distribution. The moments determine the cumulants in the sense that any two probability distributions whose moments are identical will have identical cumulants as well, and similarly the cumulants determine the moments. In some cases theoretical treatments of problems in terms of cumulants are simpler than those using moments. Just as for moments, where joint moments are used for collections of random variables, it is possible to define joint cumulants.
In statistics, the Khmaladze transformation is a mathematical tool used in constructing convenient goodness of fit tests for hypothetical distribution functions. More precisely, suppose  are i.i.d., possibly multi-dimensional, random observations generated from an unknown probability distribution. A classical problem in statistics is to decide how well a given hypothetical distribution function , or a given hypothetical parametric family of distribution functions , fits the set of observations. The Khmaladze transformation allows us to construct goodness of fit tests with desirable properties. It is named after Estate V. Khmaladze. Consider the sequence of empirical distribution functions  based on a sequence of i.i.d random variables, , as n increases. Suppose  is the hypothetical distribution function of each . To test whether the choice of  is correct or not, statisticians use the normalized difference,  This , as a random process in , is called the empirical process. Various functionals of  are used as test statistics. The change of the variable ,  transforms to the so-called uniform empirical process . The latter is an empirical processes based on independent random variables , which are uniformly distributed on  if the s do indeed have distribution function . This fact was discovered and first utilized by Kolmogorov (1933), Wald and Wolfowitz (1936) and Smirnov (1937) and, especially after Doob (1949) and Anderson and Darling (1952), it led to the standard rule to choose test statistics based on . That is, test statistics  are defined (which possibly depend on the  being tested) in such a way that there exists another statistic  derived from the uniform empirical process, such that . Examples are  and  For all such functionals, their null distribution (under the hypothetical ) does not depend on , and can be calculated once and then used to test any . However, it is only rarely that one needs to test a simple hypothesis, when a fixed  as a hypothesis is given. Much more often, one needs to verify parametric hypotheses where the hypothetical , depends on some parameters , which the hypothesis does not specify and which have to be estimated from the sample  itself. Although the estimators , most commonly converge to true value of , it was discovered that the parametric, or estimated, empirical process  differs significantly from  and that the transformed process ,  has a distribution for which the limit distribution, as , is dependent on the parametric form of  and on the particular estimator  and, in general, within one parametric family, on the value of . From mid-1950s to the late-1980s, much work was done to clarify the situation and understand the nature of the process . In 1981, and then 1987 and 1993, Khmaladze suggested to replace the parametric empirical process  by its martingale part  only.  where  is the compensator of . Then the following properties of  were established: Although the form of , and therefore, of , depends on , as a function of both  and , the limit distribution of the time transformed process  is that of standard Brownian motion on , i.e., is again standard and independent of the choice of . The relationship between  and  and between their limits, is one to one, so that the statistical inference based on  or on  are equivalent, and in , nothing is lost compared to . The construction of innovation martingale  could be carried over to the case of vector-valued , giving rise to the definition of the so-called scanning martingales in . For a long time the transformation was, although known, still not used. Later, the work of researchers like Koenker, Stute, Bai, Koul, Koening, and others made it popular in econometrics and other fields of statistics.  
In probability theory and statistics, the Zipf Mandelbrot law is a discrete probability distribution. Also known as the Pareto-Zipf law, it is a power-law distribution on ranked data, named after the linguist George Kingsley Zipf who suggested a simpler distribution called Zipf's law, and the mathematician Benoit Mandelbrot, who subsequently generalized it. The probability mass function is given by:  where  is given by:  which may be thought of as a generalization of a harmonic number. In the formula,  is the rank of the data, and  and  are parameters of the distribution. In the limit as  approaches infinity, this becomes the Hurwitz zeta function . For finite  and  the Zipf Mandelbrot law becomes Zipf's law. For infinite  and  it becomes a Zeta distribution.
In applied statistics, the Morris method for global sensitivity analysis is a so-called one-step-at-a-time method (OAT), meaning that in each run only one input parameter is given a new value. It facilitates a global sensitivity analysis by making a number r of local changes at different points x(1   r) of the possible range of input values.
The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions (see Kolmogorov Smirnov test), or whether outcome frequencies follow a specified distribution (see Pearson's chi-squared test). In the analysis of variance, one of the components into which the variance is partitioned may be a lack-of-fit sum of squares.  
Information theory is a branch of applied mathematics, electrical engineering, and computer science involving the quantification, storage, and communication of information. Information theory was originally developed by Claude E. Shannon to find fundamental limits on signal processing and communication operations such as data compression. Since its inception in a landmark 1948 paper by Shannon entitled "A Mathematical Theory of Communication", it has been broadened to find applications in many other areas, including statistical inference, natural language processing, cryptography, neurobiology, the evolution and function of molecular codes, model selection in ecology, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, anomaly detection and other forms of data analysis. A key measure in information theory is "entropy". Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for Digital Subscriber Line (DSL)). The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields. Important sub-fields of information theory include source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.
In mathematics, the geometric mean is a type of mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the nth root of the product of n numbers, i.e., for a set of numbers x1, x2, ..., xn, the geometric mean is defined as  For instance, the geometric mean of two numbers, say 2 and 8, is just the square root of their product, that is, . As another example, the geometric mean of the three numbers 4, 1, and 1/32 is the cube root of their product (1/8), which is 1/2, that is, . A geometric mean is often used when comparing different items finding a single "figure of merit" for these items when each item has multiple properties that have different numeric ranges. For example, the geometric mean can give a meaningful "average" to compare two companies which are each rated at 0 to 5 for their environmental sustainability, and are rated at 0 to 100 for their financial viability. If an arithmetic mean were used instead of a geometric mean, the financial viability is given more weight because its numeric range is larger so a small percentage change in the financial rating (e.g. going from 80 to 90) makes a much larger difference in the arithmetic mean than a large percentage change in environmental sustainability (e.g. going from 2 to 5). The use of a geometric mean "normalizes" the ranges being averaged, so that no range dominates the weighting, and a given percentage change in any of the properties has the same effect on the geometric mean. So, a 20% change in environmental sustainability from 4 to 4.8 has the same effect on the geometric mean as a 20% change in financial viability from 60 to 72. The geometric mean can be understood in terms of geometry. The geometric mean of two numbers,  and , is the length of one side of a square whose area is equal to the area of a rectangle with sides of lengths  and . Similarly, the geometric mean of three numbers, , , and , is the length of one edge of a cube whose volume is the same as that of a cuboid with sides whose lengths are equal to the three given numbers. The geometric mean applies only to numbers of the same sign. It is also often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as data on the growth of the human population or interest rates of a financial investment. The geometric mean is also one of the three classical Pythagorean means, together with the aforementioned arithmetic mean and the harmonic mean. For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see Inequality of arithmetic and geometric means.)
In statistics, quasi-likelihood estimation is one way of allowing for overdispersion, that is, greater variability in the data than would be expected from the statistical model used. It is most often used with models for count data or grouped binary data, i.e. data that would otherwise be modelled using the Poisson or binomial distribution. The term quasi-likelihood function was introduced by Robert Wedderburn in 1974 to describe a function which has similar properties to the log-likelihood function, except that a quasi-likelihood function is not the log-likelihood corresponding to any actual probability distribution. Quasi-likelihood models can be fitted using a straightforward extension of the algorithms used to fit generalized linear models. Instead of specifying a probability distribution for the data, only a relationship between the mean and the variance is specified in the form of a variance function giving the variance as a function of the mean. Generally, this function is allowed to include a multiplicative factor known as the overdispersion parameter or scale parameter that is estimated from the data. Most commonly, the variance function is of a form such that fixing the overdispersion parameter at unity results in the variance-mean relationship of an actual probability distribution such as the binomial or Poisson. (For formulae, see the binomial data example and count data example under generalized linear models.)
Kernel density estimation is a nonparametric technique for density estimation i.e., estimation of probability density functions, which is one of the fundamental questions in statistics. It can be viewed as a generalisation of histogram density estimation with improved statistical properties. Apart from histograms, other types of density estimators include parametric, spline, wavelet and Fourier series. Kernel density estimators were first introduced in the scientific literature for univariate data in the 1950s and 1960s and subsequently have been widely adopted. It was soon recognised that analogous estimators for multivariate data would be an important addition to multivariate statistics. Based on research carried out in the 1990s and 2000s, multivariate kernel density estimation has reached a level of maturity comparable to its univariate counterparts.
In statistical mechanics, the mean squared displacement (MSD, also mean square displacement, average squared displacement, or mean square fluctuation) is a measure of the deviation over time between the position of a particle and some reference position. It is the most common measure of the spatial extent of random motion, and can be thought of as measuring the portion of the system "explored" by the random walker. It prominently appears in the Debye Waller factor (describing vibrations within the solid state) and in the Langevin equation (describing diffusion of a Brownian particle). The MSD is defined as  where T is the time over which one wants to average, and  is the reference position of the particle. Typically this reference position will be the time-averaged position of the same particle, i.e. .
In statistics, the matrix t-distribution (or matrix variate t-distribution) is the generalization of the multivariate t-distribution from vectors to matrices. The matrix t-distribution shares the same relationship with the multivariate t-distribution that the matrix normal distribution shares with the multivariate normal distribution. For example, the matrix t-distribution is the compound distribution that results from sampling from a matrix normal distribution having sampled the covariance matrix of the matrix normal from an inverse Wishart distribution. In a Bayesian analysis of a multivariate linear regression model based on the matrix normal distribution, the matrix t-distribution is the posterior predictive distribution.
In statistics, logistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. Logistic regression was developed by statistician David Cox in 1958 (although much work was done in the single independent variable case almost two decades earlier). The binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). As such it is not a classification method. It could be called a qualitative response/discrete choice model in the terminology of economics. Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Thus, it treats the same set of problems as probit regression using similar techniques, with the latter using a cumulative normal distribution curve instead. Equivalently, in the latent variable interpretations of these two methods, the first assumes a standard logistic distribution of errors and the second a standard normal distribution of errors. Logistic regression can be seen as a special case of generalized linear model and thus analogous to linear regression. The model of logistic regression, however, is based on quite different assumptions (about the relationship between dependent and independent variables) from those of linear regression. In particular the key differences of these two models can be seen in the following two features of logistic regression. First, the conditional distribution  is a Bernoulli distribution rather than a Gaussian distribution, because the dependent variable is binary. Second, the predicted values are probabilities and are therefore restricted to (0,1) through the logistic distribution function because logistic regression predicts the probability of particular outcomes. Logistic regression is an alternative to Fisher's 1936 method, linear discriminant analysis. If the assumptions of linear discriminant analysis hold, application of Bayes' rule to reverse the conditioning results in the logistic model, so if linear discriminant assumptions are true, logistic regression assumptions must hold. The converse is not true, so the logistic model has fewer assumptions than discriminant analysis and makes no assumption on the distribution of the independent variables.
In statistics and business, a long tail of some distributions of numbers is the portion of the distribution having a large number of occurrences far from the "head" or central part of the distribution. The distribution could involve popularities, random numbers of occurrences of events with various probabilities, etc. The term is often used loosely, with no definition or arbitrary definition, but precise definitions are possible. In statistics, the term long-tailed distribution has a narrow technical meaning, and is a subtype of heavy-tailed distribution; see that article for details. Intuitively, a distribution is (right) long-tailed if, for any fixed amount, when a quantity exceeds a high level, it almost certainly exceeds it by at least that amount: big quantities are probably even bigger. Note that statistically, there is no sense of the "long tail" of a distribution, but only the property of a distribution being long-tailed. In business, the term long tail is applied to rank-size distributions or rank-frequency distributions (primarily of popularity), which often form power laws and are thus long-tailed distributions in the statistical sense. This is used to describe the retailing strategy of selling a large number of unique items with relatively small quantities sold of each (the "long tail")   usually in addition to selling fewer popular items in large quantities (the "head"). Sometimes an intermediate category is also included, variously called the body, belly, torso, or middle. The specific cutoff of what part of a distribution is the "long tail" is often arbitrary, but in some cases may be specified objectively; see segmentation of rank-size distributions. The long tail concept has found some ground for application, research, and experimentation. It is a term used in online business, mass media, micro-finance (Grameen Bank, for example), user-driven innovation (Eric von Hippel), and social network mechanisms (e.g. crowdsourcing, crowdcasting, peer-to-peer), economic models, and marketing (viral marketing).
The eddy covariance (also known as eddy correlation and eddy flux) technique is a key atmospheric measurement technique to measure and calculate vertical turbulent fluxes within atmospheric boundary layers. The method analyzes high-frequency wind and scalar atmospheric data series, and yields values of fluxes of these properties. It is a statistical method used in meteorology and other applications (micrometeorology, oceanography, hydrology, agricultural sciences, industrial and regulatory applications, etc.) to determine exchange rates of trace gases over natural ecosystems and agricultural fields, and to quantify gas emissions rates from other land and water areas. It is frequently used to estimate momentum, heat, water vapour, carbon dioxide and methane fluxes      The technique is also used extensively for verification and tuning of global climate models, mesoscale and weather models, complex biogeochemical and ecological models, and remote sensing estimates from satellites and aircraft. The technique is mathematically complex, and requires significant care in setting up and processing data. To date, there is no uniform terminology or a single methodology for the Eddy Covariance technique, but much effort is being made by flux measurement networks (e.g., FluxNet, Ameriflux, ICOS, CarboEurope, Fluxnet Canada, OzFlux, NEON, and iLEAPS) to unify the various approaches.  The technique has additionally proven applicable under water to the benthic zone for measuring oxygen fluxes between seafloor and overlying water. In these environments, the technique is generally known as the eddy correlation technique, or just eddy correlation. Oxygen fluxes are extracted from raw measurements largely following the same principles as used in the atmosphere, and they are typically used as a proxy for carbon exchange, which is important for local and global carbon budgets. For most benthic ecosystems, eddy correlation is the most accurate technique for measuring in situ fluxes. The technique's development and its applications under water remains a fruitful area of research.
In probability theory, stochastic drift is the change of the average value of a stochastic (random) process. A related term is the drift rate, which is the rate at which the average changes. For example, a process that counts the number of heads in a series of  coin tosses has a drift rate of 1/2 per toss. This is in contrast to the random fluctuations about this average value. The stochastic mean of that coin-toss process is 1/2 and the drift rate of the stochastic mean is 0, assuming 1=heads and 0=tails.
In hydrology, generalized likelihood uncertainty estimation (GLUE) is a statistical method for quantifying the uncertainty of model predictions. The method has been introduced by Beven and Binley (1992). The basic idea of GLUE is that given our inability to represent exactly in a mathematical model how nature works, there will always be several different models that mimic equally well an observed natural process (such as river discharge). Such equally acceptable or behavioral models are therefore called equifinal. The methodology deals with models whose results are expressed as probability distributions of possible outcomes, often in the form of Monte Carlo simulations, and the problem can be viewed as assessing, and comparing between models, how good these representations of uncertainty are. There is an implicit understanding that the models being used are approximations to what might be obtained from a thorough Bayesian analysis of the problem if a fully adequate model of real-world hydrological processes were available.
In statistics, the Holm Bonferroni method is a method used to counteract the problem of multiple comparisons. It is intended to control the Familywise error rate and offers a simple test uniformly more powerful than the Bonferroni correction. It is one of the earliest usages of stepwise algorithms in simultaneous inference. It is named after Sture Holm, who invented the method in 1978, and Carlo Emilio Bonferroni.
In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures. In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience: by supplying a valid probability mass function or probability density function by supplying a valid cumulative distribution function or survival function by supplying a valid hazard function by supplying a valid characteristic function by supplying a rule for constructing a new random variable from other random variables whose joint probability distribution is known. A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector a set of two or more random variables taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.
A Le vy flight, named for French mathematician Paul Le vy, is a random walk in which the step-lengths have a probability distribution that is heavy-tailed. When defined as a walk in a space of dimension greater than one, the steps made are in isotropic random directions. The term "Le vy flight" was coined by Benoi t Mandelbrot, who used this for one specific definition of the distribution of step sizes. He used the term Cauchy flight for the case where the distribution of step sizes is a Cauchy distribution, and Rayleigh flight for when the distribution is a normal distribution (which is not an example of a heavy-tailed probability distribution). Later researchers have extended the use of the term "Le vy flight" to include cases where the random walk takes place on a discrete grid rather than on a continuous space. A Le vy flight is a random walk in which the steps are defined in terms of the step-lengths, which have a certain probability distribution, with the directions of the steps being isotropic and random. The particular case for which Mandelbrot used the term "Le vy flight" is defined by the survivor function (commonly known as the survival function) of the distribution of step-sizes, U, being  Here D is a parameter related to the fractal dimension and the distribution is a particular case of the Pareto distribution. Later researchers allow the distribution of step sizes to be any distribution for which the survival function has a power-like tail  for some k satisfying 1 < k < 3. (Here the notation O is the Big O notation.) Such distributions have an infinite variance. Typical examples are the symmetric stable distributions.
Pythagorean expectation is a formula invented by Bill James to estimate how many games a baseball team "should" have won based on the number of runs they scored and allowed. Comparing a team's actual and Pythagorean winning percentage can be used to evaluate how lucky that team was (by examining the variation between the two winning percentages). The name comes from the formula's resemblance to the Pythagorean theorem. The basic formula is:  where Win is the winning ratio generated by the formula. The expected number of wins would be the expected winning ratio multiplied by the number of games played.
A log-linear model is a mathematical model that takes the form of a function whose logarithm is a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form , in which the fi(X) are quantities that are functions of the variables X, in general a vector of values, while c and the wi stand for the model parameters. The term may specifically be used for: A log-linear plot or graph, which is a type of semi-log plot. Poisson regression for contingency tables, a type of generalized linear model. The specific applications of log-linear models are where the output quantity lies in the range 0 to  , for values of the independent variables X, or more immediately, the transformed quantities fi(X) in the range    to + . This may be contrasted to logistic models, similar to the logistic function, for which the output quantity lies in the range 0 to 1. Thus the contexts where these models are useful or realistic often depends on the range of the values being modelled.
Geospatial predictive modeling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution   there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.
In probability theory and statistics, two real-valued random variables, X,Y, are said to be uncorrelated if their covariance, E(XY)   E(X)E(Y), is zero. A set of two or more random variables is called uncorrelated if each pair of them are uncorrelated. If two variables are uncorrelated, there is no linear relationship between them. Uncorrelated random variables have a Pearson correlation coefficient of zero, except in the trivial case when either variable has zero variance (is a constant). In this case the correlation is undefined. In general, uncorrelatedness is not the same as orthogonality, except in the special case where either X or Y has an expected value of 0. In this case, the covariance is the expectation of the product, and X and Y are uncorrelated if and only if E(XY) = 0. If X and Y are independent, with finite second moments, then they are uncorrelated. However, not all uncorrelated variables are independent. For example, if X is a continuous random variable uniformly distributed on [ 1, 1] and Y = X2, then X and Y are uncorrelated even though X determines Y and a particular value of Y can be produced by only one or two values of X.
In statistical analysis, Freedman's paradox, named after David Freedman, describes a problem in model selection whereby predictor variables with no explanatory power can appear artificially important. Freedman demonstrated (through simulation and asymptotic calculation) that this is a common occurrence when the number of variables is similar to the number of data points. Recently, new information-theoretic estimators have been developed in an attempt to reduce this problem, in addition to the accompanying issue of model selection bias, whereby estimators of predictor variables that have a weak relationship with the response variable are biased.
In probability theory, the Schramm Loewner evolution with parameter  , also known as stochastic Loewner evolution (SLE ), is a family of random planar curves that have been proven to be the scaling limit of a variety of two-dimensional lattice models in statistical mechanics. Given a parameter   and a domain in the complex plane U, it gives a family of random curves in U, with   controlling how much the curve turns. There are two main variants of SLE, chordal SLE which gives a family of random curves from two fixed boundary points, and radial SLE, which gives a family of random curves from a fixed boundary point to a fixed interior point. These curves are defined to satisfy conformal invariance and a domain Markov property. It was discovered by Oded Schramm (2000) as a conjectured scaling limit of the planar uniform spanning tree (UST) and the planar loop-erased random walk (LERW) probabilistic processes, and developed by him together with Greg Lawler and Wendelin Werner in a series of joint papers. Besides UST and LERW, the Schramm Loewner evolution is conjectured or proven to describe the scaling limit of various stochastic processes in the plane, such as critical percolation, the critical Ising model, the double-dimer model, self-avoiding walks, and other critical statistical mechanics models that exhibit conformal invariance. The SLE curves are the scaling limits of interfaces and other non-self-intersecting random curves in these models. The main idea is that the conformal invariance and a certain Markov property inherent in such stochastic processes together make it possible to encode these planar curves into a one-dimensional Brownian motion running on the boundary of the domain (the driving function in Loewner's differential equation). This way, many important questions about the planar models can be translated into exercises in Ito  calculus. Indeed, several mathematically non-rigorous predictions made by physicists using conformal field theory have been proven using this strategy.
An experimental paradigm, in the behavioural sciences (e.g. psychology, biology, neurosciences), is an experimental setup (i.e. a way to conduct a certain type of experiment) that is defined by certain fine-tuned standards and often has a theoretical background. A paradigm in this technical sense, however, is not a way of thinking as it is in the epistemological meaning.
In frequentist statistics, the p-value is a function of the observed sample results (a test statistic) relative to a statistical model, which measures how extreme the observation is. Statistical hypothesis tests making use of p-values are commonly used in many fields of science and social sciences, such as economics, psychology, biology, criminal justice and criminology, and sociology. Their use has been a matter of considerable controversy, leading some Bayesian statisticians to claim that frequentist statistics is unprincipled.
In statistical signal processing, the goal of spectral density estimation (SDE) is to estimate the spectral density (also known as the power spectral density) of a random signal from a sequence of time samples of the signal. Intuitively speaking, the spectral density characterizes the frequency content of the signal. One purpose of estimating the spectral density is to detect any periodicities in the data, by observing peaks at the frequencies corresponding to these periodicities. SDE should be distinguished from the field of frequency estimation, which assumes that a signal is composed of a limited (usually small) number of generating frequencies plus noise and seeks to find the location and intensity of the generated frequencies. SDE makes no assumption on the number of components and seeks to estimate the whole generating spectrum.
Labour Force Surveys are statistical surveys conducted in a number of countries designed to capture data about the labour market. All European Union member states are required to conduct a Labour Force Survey annually. Labour Force Surveys are also carried out in some non-EU countries. They are used to calculate the International Labour Organization (ILO)-defined unemployment rate. The ILO agrees the definitions and concepts employed in Labour Force Surveys.
During sampling of granular materials (whether airborne, suspended in liquid, aerosol, or aggragated), correct sampling is defined in Gy's sampling theory as a sampling scenario in which all particles in a population have the same probability of ending up in the sample. The concentration of the property of interest in a sample can be a biased estimate for the concentration of the property of interest in the population from which the sample is drawn. Although generally non-zero, for correct sampling this bias is thought to be negligible.
In statistics and uncertainty analysis, the Welch Satterthwaite equation is used to calculate an approximation to the effective degrees of freedom of a linear combination of independent sample variances, also known as the pooled degrees of freedom, corresponding to the pooled variance. For n sample variances si2 (i = 1, ..., n), each respectively having  i degrees of freedom, often one computes the linear combination  where  is a real positive number, typically . In general, the probability distribution of  ' cannot be expressed analytically. However, its distribution can be approximated by another chi-squared distribution, whose effective degrees of freedom are given by the Welch Satterthwaite equation  There is no assumption that the underlying population variances  i2 are equal. This is known as the Behrens Fisher problem. The result can be used to perform approximate statistical inference tests. The simplest application of this equation is in performing Welch's t test.
Neighbourhood components analysis is a supervised learning method for classifying multivariate data into distinct classes according to a given distance metric over the data. Functionally, it serves the same purposes as the K-nearest neighbors algorithm, and makes direct use of a related concept termed stochastic nearest neighbours.  
In probability and statistics, the Kumaraswamy's double bounded distribution is a family of continuous probability distributions defined on the interval [0,1]. It is similar to the Beta distribution, but much simpler to use especially in simulation studies due to the simple closed form of both its probability density function and cumulative distribution function. This distribution was originally proposed by Poondi Kumaraswamy for variables that are lower and upper bounded.
A multiple of the median (MoM) is a measure of how far an individual test result deviates from the median. MoM is commonly used to report the results of medical screening tests, particularly where the results of the individual tests are highly variable. MoM was originally used as a method to normalize data from participating laboratories of Alpha-fetoprotein (AFP) so that individual test results could be compared. 35 years later, it is the established standard for reporting maternal serum screening results. An MoM for a test result for a patient can be determined by the following:  As an example, Alpha-fetoprotein (AFP) testing is used to screen for a neural tube defect (NTD) during the second trimester of pregnancy. If the median AFP result at 16 weeks of gestation is 30 ng/mL and a pregnant woman's AFP result at that same gestational age is 60 ng/mL, then her MoM is equal to 60/30 = 2.0. In other words, her AFP result is 2 times higher than "normal."
In applied statistics, (e.g., applied to the social sciences and psychometrics), common-method variance (CMV) is the spurious "variance that is attributable to the measurement method rather than to the constructs the measures are assumed to represent" or equivalently as "systematic error variance shared among variables measured with and introduced as a function of the same method and/or source". For example, an electronic survey method might influence results for those who might be unfamiliar with an electronic survey interface differently than for those who might be familiar. If measures are affected by CMV or common-method bias, the intercorrelations among them can be inflated or deflated depending upon several factors. Although it is sometimes assumed that CMV affects all variables, evidence suggests that whether or not the correlation between two variables is affected by CMV is a function of both the method and the particular constructs being measured.
In queueing theory, a discipline within the mathematical theory of probability, Little's result, theorem, lemma, law or formula is a theorem by John Little which states: The long-term average number of customers in a stable system L is equal to the long-term average effective arrival rate,  , multiplied by the (Palm )average time a customer spends in the system, W; or expressed algebraically: L =  W. Although it looks intuitively reasonable, it is quite a remarkable result, as the relationship is "not influenced by the arrival process distribution, the service distribution, the service order, or practically anything else." The result applies to any system, and particularly, it applies to systems within systems. So in a bank, the customer line might be one subsystem, and each of the tellers another subsystem, and Little's result could be applied to each one, as well as the whole thing. The only requirements are that the system is stable and non-preemptive; this rules out transition states such as initial startup or shutdown. In some cases it is possible to mathematically relate not only the average number in the system to the average wait but relate the entire probability distribution (and moments) of the number in the system to the wait.
Pseudoreplication, as originally defined, is a special case of inadequate specification of random factors where both random and fixed factors are present. The problem of inadequate specification arises when treatments are assigned to units that are subsampled and the treatment F-ratio in an analysis of variance (ANOVA) table is formed with respect to the residual mean square rather than with respect to the among unit mean square. The F-ratio relative to the within unit mean square is vulnerable to the confounding of treatment and unit effects, especially when experimental unit number is small (e.g. four tank units, two tanks treated, two not treated, several subsamples per tank). The problem is eliminated by forming the F-ratio relative to the correct mean square in the ANOVA table (tank by treatment MS in the example above), where this is possible. The problem is addressed by the use of mixed models. Hurlbert reported "pseudoreplication" in 48% of the studies he examined, that used inferential statistics. When time and resources limit the number of experimental units, and unit effects cannot be eliminated statistically by testing over the unit variance, it is important to use other sources of information to evaluate the degree to which an F-ratio is confounded by unit effects.
The Tracy Widom distribution, introduced by Craig Tracy and Harold Widom (1993, 1994), is the probability distribution of the normalized largest eigenvalue of a random Hermitian matrix. In practical terms, Tracy-Widom is the crossover function between the two phases of weakly versus strongly coupled components in a system. It also appears in the distribution of the length of the longest increasing subsequence of random permutations (Baik, Deift & Johansson 1999), in current fluctuations of the asymmetric simple exclusion process (ASEP) with step initial condition (Johansson 2000, Tracy & Widom 2009), and in simplified mathematical models of the behavior of the longest common subsequence problem on random inputs (Majumdar & Nechaev 2005). See (Takeuchi & Sano 2010, Takeuchi et al. 2011) for experimental testing (and verifying) that the interface fluctuations of a growing droplet (or substrate) are described by the TW distribution  (or ) as predicted by (Pra hofer & Spohn 2000). The distribution F1 is of particular interest in multivariate statistics (Johnstone 2007, 2008, 2009). For a discussion of the universality of F ,   = 1, 2, and 4, see Deift (2007). For an application of F1 to inferring population structure from genetic data see Patterson, Price & Reich (2006).  
The GEH Statistic is a formula used in traffic engineering, traffic forecasting, and traffic modelling to compare two sets of traffic volumes. The GEH formula gets its name from Geoffrey E. Havers, who invented it in the 1970s while working as a transport planner in London, England. Although its mathematical form is similar to a chi-squared test, is not a true statistical test. Rather, it is an empirical formula that has proven useful for a variety of traffic analysis purposes. The formula for the "GEH Statistic" is:  Where M is the hourly traffic volume from the traffic model (or new count) and C is the real-world hourly traffic count (or the old count) Using the GEH Statistic avoids some pitfalls that occur when using simple percentages to compare two sets of volumes. This is because the traffic volumes in real-world transportation systems vary over a wide range. For example, the mainline of a freeway/motorway might carry 5000 vehicles per hour, while one of the on-ramps leading to the freeway might carry only 50 vehicles per hour (in that situation it would not be possible to select a single percentage of variation that is acceptable for both volumes). The GEH statistic reduces this problem; because the GEH statistic is non-linear, a single acceptance threshold based on GEH can be used over a fairly wide range of traffic volumes. The use of GEH as an acceptance criterion for travel demand forecasting models is recognised in the UK Highways Agency's Design Manual for Roads and Bridges  the Wisconsin microsimulation modeling guidelines, the Transport for London Traffic Modelling Guidelines  and other references. For traffic modelling work in the "baseline" scenario, a GEH of less than 5.0 is considered a good match between the modelled and observed hourly volumes (flows of longer or shorter durations should be converted to hourly equivalents to use these thresholds). According to DMRB, 85% of the volumes in a traffic model should have a GEH less than 5.0. GEHs in the range of 5.0 to 10.0 may warrant investigation. If the GEH is greater than 10.0, there is a high probability that there is a problem with either the travel demand model or the data (this could be something as simple as a data entry error, or as complicated as a serious model calibration problem).  
A probability of precipitation (POP) is a description of the likelihood of precipitation that is often published with weather forecasts. Generally it refers to the probability that at least some minimum quantity of precipitation will occur within a specified forecast period and location.
In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation of the model parameters. Maximum-likelihood estimation remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian approaches and least squares fits to variance stabilized responses, have been developed.
In statistics, the multivariate t-distribution (or multivariate Student distribution) is a multivariate probability distribution. It is a generalization to random vectors of the Student's t-distribution, which is a distribution applicable to univariate random variables. While the case of a random matrix could be treated within this structure, the matrix t-distribution is distinct and makes particular use of the matrix structure.
In statistical quality control, the regression control chart allows for monitoring a change in a process where two or more variables are correlated. The change in a dependent variable can be detected and compensatory change in the independent variable can be recommended. Examples from the Post Office Department provide an application of such models. Regression control chart differs from a traditional control chart in four main aspects: It is designed to control a varying (rather than a constant) average. The control limit lines are parallel to the regression line rather than the horizontal line. The computations here are much more complex. It is appropriate for use in more complex situations.
Acceptance sampling uses statistical sampling to determine whether to accept or reject a production lot of material. It has been a common quality control technique used in industry. It is usually done as products leave the factory, or in some cases even within the factory. Most often a producer supplies a consumer a number of items and a decision to accept or reject the lot is made by determining the number of defective items in a sample from the lot. The lot is accepted if the number of defects falls below where the acceptance number or otherwise the lot is rejected. A wide variety of acceptance sampling plans are available.
In mathematics, random graph is the general term to refer to probability distributions over graphs. Random graphs may be described simply by a probability distribution, or by a random process which generates them. The theory of random graphs lies at the intersection between graph theory and probability theory. From a mathematical perspective, random graphs are used to answer questions about the properties of typical graphs. Its practical applications are found in all areas in which complex networks need to be modeled   a large number of random graph models are thus known, mirroring the diverse types of complex networks encountered in different areas. In a mathematical context, random graph refers almost exclusively to the Erdo s Re nyi random graph model. In other contexts, any graph model may be referred to as a random graph.
In statistics, Barnard's test is an exact test used in the analysis of contingency tables. It examines the association of two categorical variables and is a more powerful alternative than Fisher's exact test for 2 2 contingency tables. While first published in 1945 by George Alfred Barnard, the test did not gain popularity due to the computational difficulty of calculating the p-value. Nowadays, for small/moderate sample sizes (n<1000), computers can often implement Barnard's test in a few seconds.
In the theory of stochastic processes, a part of the mathematical theory of probability, the variance gamma process (VG), also known as Laplace motion, is a Le vy process determined by a random time change. The process has finite moments distinguishing it from many Le vy processes. There is no diffusion component in the VG process and it is thus a pure jump process. The increments are independent and follow a Variance-gamma distribution, which is a generalization of the Laplace distribution. There are several representations of the VG process that relate it to other processes. It can for example be written as a Brownian motion  with drift  subjected to a random time change which follows a gamma process  (equivalently one finds in literature the notation ):  An alternative way of stating this is that the variance gamma process is a Brownian motion subordinated to a Gamma subordinator. Since the VG process is of finite variation it can be written as the difference of two independent gamma processes:  where  Alternatively it can be approximated by a compound Poisson process that leads to a representation with explicitly given (independent) jumps and their locations. This last characterization gives an understanding of the structure of the sample path with location and sizes of jumps. On the early history of the variance-gamma process see Seneta (2000).
The following is a glossary of terms. It is not intended to be all-inclusive.  
The law of averages is a layman's term for a belief that the statistical distribution of outcomes among members of a small sample must reflect the distribution of outcomes across the population as a whole. As invoked in everyday life, the "law" usually reflects wishful thinking or a poor understanding of statistics rather than any mathematical principle. While there is a real theorem that a random variable will reflect its underlying probability over a very large sample, the law of averages typically assumes that unnatural short-term "balance" must occur. Typical applications of the law also generally assume no bias in the underlying probability distribution, which is frequently at odds with the empirical evidence.
A tolerance interval is a statistical interval within which, with some confidence level, a specified proportion of a sampled population falls. "More specifically, a 100 p%/100 (1  ) tolerance interval provides limits within which at least a certain proportion (p) of the population falls with a given level of confidence (1  )." "A (p, 1  ) tolerance interval (TI) based on a sample is constructed so that it would include at least a proportion p of the sampled population with confidence 1  ; such a TI is usually referred to as p-content   (1  ) coverage TI." "A (p, 1  ) upper tolerance limit (TL) is simply an 1   upper confidence limit for the 100 p percentile of the population." A tolerance interval can be seen as a statistical version of a probability interval. "In the parameters-known case, a 95% tolerance interval and a 95% prediction interval are the same." If we knew a population's exact parameters, we would be able to compute a range within which a certain proportion of the population falls. For example, if we know a population is normally distributed with mean  and standard deviation , then the interval  includes 95% of the population (1.96 is the z-score for 95% coverage of a normally distributed population). However, if we have only a sample from the population, we know only the sample mean  and sample standard deviation , which are only estimates of the true parameters. In that case,  will not necessarily include 95% of the population, due to variance in these estimates. A tolerance interval bounds this variance by introducing a confidence level , which is the confidence with which this interval actually includes the specified proportion of the population. For a normally distributed population, a z-score can be transformed into a "k factor" or tolerance factor for a given  via lookup tables or several approximation formulas. "As the degrees of freedom approach infinity, the prediction and tolerance intervals become equal."
A Moran process or Moran model is a simple stochastic process used in biology to describe finite populations. The process is named after Patrick Moran, who first proposed the model in 1958. It can be used to model variety-increasing processes such as mutation as well as variety-reducing effects such as genetic drift and natural selection. The process can describe the probabilistic dynamics in a finite population of constant size N in which two alleles A and B are competing for dominance. The two alleles are considered to be true replicators (i.e. entities that make copies of themselves). In each time step a random individual (which is of either type A or B) is chosen for reproduction and a random individual is chosen for death; thus ensuring that the population size remains constant. To model selection, one type has to have a higher fitness and is thus more likely to be chosen for reproduction. The same individual can be chosen for death and for reproduction in the same step.
In arithmetic, the range of a set of data is the difference between the largest and smallest values. However, in descriptive statistics, this concept of range has a more complex meaning. The range is the size of the smallest interval which contains all the data and provides an indication of statistical dispersion. It is measured in the same units as the data. Since it only depends on two of the observations, it is most useful in representing the dispersion of small data sets.  
In statistics, the Lilliefors test, named after Hubert Lilliefors, professor of statistics at George Washington University, is a normality test based on the Kolmogorov Smirnov test. It is used to test the null hypothesis that data come from a normally distributed population, when the null hypothesis does not specify which normal distribution; i.e., it does not specify the expected value and variance of the distribution.
In probability theory, an exponentially modified Gaussian (EMG) distribution (ExGaussian distribution) describes the sum of independent normal and exponential random variables. An exGaussian random variable Z may be expressed as Z = X + Y where X and Y are independent, X is Gaussian with mean   and variance  2 and Y is exponential of rate  . It has a characteristic positive skew from the exponential component. It may also be regarded as a weighted function of a shifted exponential with the weight being a function of the normal distribution.
The purpose of this introductory article is to discuss the experimental uncertainty analysis of a derived quantity, based on the uncertainties in the experimentally measured quantities that are used in some form of mathematical relationship ("model") to calculate that derived quantity. The model used to convert the measurements into the derived quantity is usually based on fundamental principles of a science or engineering discipline. The uncertainty has two components, namely, bias (related to accuracy) and the unavoidable random variation that occurs when making repeated measurements (related to precision). The measured quantities may have biases, and they certainly have random variation, so what needs to be addressed is how these are "propagated" into the uncertainty of the derived quantity. Uncertainty analysis is often called the "propagation of error." It will be seen that this is a difficult and in fact sometimes intractable problem when handled in detail. Fortunately, approximate solutions are available that provide very useful results, and these approximations will be discussed in the context of a practical experimental example.
In probability theory and statistics, the Dirichlet-multinomial distribution is a family of discrete multivariate probability distributions on a finite support of non-negative integers. It is also called the Dirichlet compound multinomial distribution (DCM) or multivariate Po lya distribution (after George Po lya). It is a compound probability distribution, where a probability vector p is drawn from a Dirichlet distribution with parameter vector , and an observation drawn from a multinomial distribution with probability vector p and number of trials N. The compounding corresponds to a Polya urn scheme. It is frequently encountered in Bayesian statistics, empirical Bayes methods and classical statistics as an overdispersed multinomial distribution. It reduces to the Categorical distribution as a special case when n = 1. It also approximates the multinomial distribution arbitrarily well for large  . The Dirichlet-multinomial is a multivariate extension of the Beta-binomial distribution, as the multinomial and Dirichlet distributions are multivariate versions of the binomial distribution and beta distributions, respectively.
The Doomsday argument (DA) is a probabilistic argument that claims to predict the number of future members of the human species given only an estimate of the total number of humans born so far. Simply put, it says that supposing that all humans are born in a random order, chances are that any one human is born roughly in the middle. It was first proposed in an explicit way by the astrophysicist Brandon Carter in 1983, from which it is sometimes called the Carter catastrophe; the argument was subsequently championed by the philosopher John A. Leslie and has since been independently discovered by J. Richard Gott and Holger Bech Nielsen. Similar principles of eschatology were proposed earlier by Heinz von Foerster, among others. A more general form was given earlier in the Lindy effect, in which for certain phenomena the future life expectancy is proportional to (though not necessarily equal to) the current age, and is based on decreasing mortality rate over time: old things endure. Denoting by N the total number of humans who were ever or will ever be born, the Copernican principle suggests that humans are equally likely (along with the other N   1 humans) to find themselves at any position n of the total population N, so humans assume that our fractional position f = n/N is uniformly distributed on the interval [0, 1] prior to learning our absolute position. f is uniformly distributed on (0, 1) even after learning of the absolute position n. That is, for example, there is a 95% chance that f is in the interval (0.05, 1), that is f > 0.05. In other words, we could assume that we could be 95% certain that we would be within the last 95% of all the humans ever to be born. If we know our absolute position n, this implies an upper bound for N obtained by rearranging n/N > 0.05 to give N < 20n. If Leslie's Figure is used, then 60 billion humans have been born so far, so it can be estimated that there is a 95% chance that the total number of humans N will be less than 20   60 billion = 1.2 trillion. Assuming that the world population stabilizes at 10 billion and a life expectancy of 80 years, it can be estimated that the remaining 1,140 billion humans will be born in 9,120 years. Depending on the projection of world population in the forthcoming centuries, estimates may vary, but the main point of the argument is that it is unlikely that more than 1.2 trillion humans will ever live on Earth. This problem is similar to the famous German tank problem.
In statistics, the Lehmann Scheffe  theorem is prominent statement, tying together the ideas of completeness, sufficiency, uniqueness, and best unbiased estimation. The theorem states that any estimator which is unbiased for a given unknown quantity and that depends on the data only through a complete, sufficient statistic is the unique best unbiased estimator of that quantity. The Lehmann Scheffe  theorem is named after Erich Leo Lehmann and Henry Scheffe , given their two early papers. If T is a complete sufficient statistic for   and E(g(T)) =  ( ) then g(T) is the uniformly minimum-variance unbiased estimator (UMVUE) of  ( ).
In clinical trials and other scientific studies, an interim analysis is an analysis of data that is conducted before data collection has been completed. Clinical trials are unusual in that enrollment of patients is a continual process staggered in time. This means that if a treatment is particularly beneficial or harmful compared to the concurrent placebo group while the study is on-going, the investigators are ethically obliged to assess that difference using the data at hand and to make a deliberate consideration of terminating the study earlier than planned.
Cartography (from Greek         kharte s, "map"; and          graphein, "write") is the study and practice of making maps. Combining science, aesthetics, and technique, cartography builds on the premise that reality can be modeled in ways that communicate spatial information effectively. The fundamental problems of traditional cartography are to: Set the map's agenda and select traits of the object to be mapped. This is the concern of map editing. Traits may be physical, such as roads or land masses, or may be abstract, such as toponyms or political boundaries. Represent the terrain of the mapped object on flat media. This is the concern of map projections. Eliminate characteristics of the mapped object that are not relevant to the map's purpose. This is the concern of generalization. Reduce the complexity of the characteristics that will be mapped. This is also the concern of generalization. Orchestrate the elements of the map to best convey its message to its audience. This is the concern of map design. Modern cartography constitutes many theoretical and practical foundations of geographic information systems.
An Aggregate pattern can refer to concepts in either statistics or computer programming. Both uses deal with considering a large case as composed of smaller, simpler, pieces.
In probability theory, the de Moivre Laplace theorem, which is a special case of the central limit theorem, states that the normal distribution may be used as an approximation to the binomial distribution under certain conditions. In particular, the theorem shows that the probability mass function of the random number of "successes" observed in a series of n independent Bernoulli trials, each having probability p of success (a binomial distribution with n trials), converges to the probability density function of the normal distribution with mean np and standard deviation  np(1-p), as n grows large, assuming p is not 0 or 1. The theorem appeared in the second edition of The Doctrine of Chances by Abraham de Moivre, published in 1738. Although de Moivre did not use the term "Bernoulli trials", he wrote about the probability distribution of the number of times "heads" appears when a coin is tossed 3600 times. This is one derivation of the particular Gaussian function used in the normal distribution.
Variable-order Bayesian network (VOBN) models provide an important extension of both the Bayesian network models and the variable-order Markov models. VOBN models are used in machine learning in general and have shown great potential in bioinformatics applications. These models extend the widely used position weight matrix (PWM) models, Markov models, and Bayesian network (BN) models. In contrast to the BN models, where each random variable depends on a fixed subset of random variables, in VOBN models these subsets may vary based on the specific realization of observed variables. The observed realizations are often called the context and, hence, VOBN models are also known as context-specific Bayesian networks. The flexibility in the definition of conditioning subsets of variables turns out to be a real advantage in classification and analysis applications, as the statistical dependencies between random variables in a sequence of variables (not necessarily adjacent) may be taken into account efficiently, and in a position-specific and context-specific manner.
An alpha beta filter (also called alpha-beta filter, f-g filter or g-h filter ) is a simplified form of observer for estimation, data smoothing and control applications. It is closely related to Kalman filters and to linear state observers used in control theory. Its principal advantage is that it does not require a detailed system model.
In probability theory and statistics, the normal-inverse-gamma distribution (or Gaussian-inverse-gamma distribution) is a four-parameter family of multivariate continuous probability distributions. It is the conjugate prior of a normal distribution with unknown mean and variance.
Data cleansing, data cleaning or data scrubbing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database. Used mainly in databases, the term refers to identifying incomplete, incorrect, inaccurate, irrelevant, etc. parts of the data and then replacing, modifying, or deleting this dirty data or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting. After cleansing, a data set will be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleansing differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at entry time, rather than on batches of data. The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code) or fuzzy (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross checking with a validated data set. Also data enhancement, where data is made more complete by adding related information, is a common data cleansing practice. For example, appending addresses with phone numbers related to that address. Data cleansing may also involve activities like, harmonization of data, and standardization of data. For example, harmonization of short codes (st, rd, etc.) to actual words (street, road, etcetera). Standardization of data is a means of changing a reference data set to a new standard, ex, use of standard codes.
Tikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, and with multiple independent discoveries, it is also variously known as the Tikhonov Miller method, the Phillips Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg Marquardt algorithm for non-linear least-squares problems. Suppose that for a known matrix  and vector , we wish to find a vector  such that . The standard approach is ordinary least squares linear regression. However if no  satisfies the equation or more than one  does -- that is the solution is not unique -- the problem is said not to be well posed. In such cases, ordinary least squares estimation leads to an overdetermined (over-fitted), or more often an underdetermined (under-fitted) system of equations. Most real-world phenomena have the effect of low-pass filters in the forward direction where  maps  to . Therefore in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of  that is in the null-space of , rather than allowing for a model to be used as a prior for . Ordinary least squares seeks to minimize the sum of squared residuals, which can be compactly written as  where  is the Euclidean norm. In order to give preference to a particular solution with desirable properties, a regularization term can be included in this minimization:  for some suitably chosen Tikhonov matrix, . In many cases, this matrix is chosen as a multiple of the identity matrix (), giving preference to solutions with smaller norms; this is known as L2 regularization. In other cases, lowpass operators (e.g., a difference operator or a weighted Fourier operator) may be used to enforce smoothness if the underlying vector is believed to be mostly continuous. This regularization improves the conditioning of the problem, thus enabling a direct numerical solution. An explicit solution, denoted by , is given by:  The effect of regularization may be varied via the scale of matrix . For  this reduces to the unregularized least squares solution provided that (ATA) 1 exists. L2 regularization is used in many contexts aside from linear regression, such as classification with logistic regression or support vector machines, and matrix factorization.
Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically. NMF finds applications in such fields as computer vision, document clustering, chemometrics, audio signal processing and recommender systems.  
Energy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine. The need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (see picture), the pressure on energy supply increases tremendously.  The data on energy and electricity come from three principal sources: Energy industry Other industries ("self-producers") Consumers The flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics.
In mathematics, probability, and statistics, a multivariate random variable or random vector is a list of mathematical variables each of whose value is unknown, either because the value has not yet occurred or because there is imperfect knowledge of its value. The individual variables in a random vector are grouped together because there may be correlations among them   often they represent different properties of an individual statistical unit. For example, while a given person has a specific age, height and weight, the representation of any person from within a group would be a random vector. Normally each element of a random vector is a real number. Random vectors are often used as the underlying implementation of various types of aggregate random variables, e.g. a random matrix, random tree, random sequence, stochastic process, etc. More formally, a multivariate random variable is a column vector  (or its transpose, which is a row vector) whose components are scalar-valued random variables on the same probability space , where  is the sample space,  is the sigma-algebra (the collection of all events), and  is the probability measure (a function returning each event's probability).
In mathematics, a mean of circular quantities is a mean which is sometimes better-suited for quantities like angles, daytimes, and fractional parts of real numbers. This is necessary since most of the usual means may not be appropriate on circular quantities. For example, the arithmetic mean of 0  and 360  is 180 , which is misleading because for most purposes 360  is the same thing as 0 . As another example, the "average time" between 11 PM and 1 AM is either midnight or noon, depending on whether the two times are part of a single night or part of a single calendar day. This is one of the simplest examples of statistics of non-Euclidean spaces.
In probability theory, the mixing time of a Markov chain is the time until the Markov chain is "close" to its steady state distribution. More precisely, a fundamental result about Markov chains is that a finite state irreducible aperiodic chain has a unique stationary distribution   and, regardless of the initial state, the time-t distribution of the chain converges to   as t tends to infinity. Mixing time refers to any of several variant formalizations of the idea: how large must t be until the time-t distribution is approximately     One variant, variation distance mixing time, is defined as the smallest t such that  for all subsets A of states and all initial states. This is the sense in which Dave Bayer and Persi Diaconis (1992) proved that the number of riffle shuffles needed to mix an ordinary 52 card deck is 7. Mathematical theory focuses on how mixing times change as a function of the size of the structure underlying the chain. For an n-card deck, the number of riffle shuffles needed grows as 1.5 log (n) / log (2). The most developed theory concerns randomized algorithms for #P-Complete algorithmic counting problems such as the number of graph colorings of a given n vertex graph. Such problems can, for sufficiently large number of colors, be answered using the Markov chain Monte Carlo method and showing that the mixing time grows only as n log (n) (Jerrum 1995). This example and the shuffling example possess the rapid mixing property, that the mixing time grows at most polynomially fast in log (number of states of the chain). Tools for proving rapid mixing include arguments based on conductance and the method of coupling. In broader uses of the Markov chain Monte Carlo method, rigorous justification of simulation results would require a theoretical bound on mixing time, and many interesting practical cases have resisted such theoretical analysis.
In mathematics, Varadhan's lemma is a result from large deviations theory named after S. R. Srinivasa Varadhan. The result gives information on the asymptotic distribution of a statistic  (Z ) of a family of random variables Z  as   becomes small in terms of a rate function for the variables.
An epitome, in data processing, is a condensed digital representation of the essential statistical properties of ordered datasets such as matrices that represent images, audio signals, videos or genetic sequences. Although much smaller than the data, the epitome contains many of its smaller overlapping parts with much less repetition and with some level of generalization. As such, it can be used in tasks such as data mining, machine learning and signal processing. The first use of epitomic analysis was with image textures for the purposes of image parsing. Epitomes have also been used in video processing to replace, remove or superresolve imagery. Epitomes are also being investigated as tools for vaccine design.
In statistics, a bimodal distribution is a continuous probability distribution with two different modes. These appear as distinct peaks (local maxima) in the probability density function, as shown in Figure 1. More generally, a multimodal distribution is a continuous probability distribution with two or more modes, as illustrated in Figure 3.
Population models are used in population ecology to model the dynamics of wildlife or human populations. Matrix population models are a specific type of population model that uses matrix algebra. Matrix algebra, in turn, is simply a form of algebraic shorthand for summarizing a larger number of often repetitious and tedious algebraic computations. All populations can be modeled  where: Nt+1 = abundance at time t+1 Nt = abundance at time t B = number of births within the population between Nt and Nt+1 D = number of deaths within the population between Nt and Nt+1 I = number of individuals immigrating into the population between Nt and Nt+1 E = number of individuals emigrating from the population between Nt and Nt+1 This equation is called a BIDE model (Birth, Immigration, Death, Emigration model). Although BIDE models are conceptually simple, reliable estimates of the 5 variables contained therein (N, B, D, I and E) are often difficult to obtain. Usually a researcher attempts to estimate current abundance, Nt, often using some form of mark and recapture technique. Estimates of B might be obtained via a ratio of immatures to adults soon after the breeding season, Ri. Number of deaths can be obtained by estimating annual survival probability, usually via mark and recapture methods, then multipling present abundance and survival rate. Often, immigration and emigration are ignored because they are so difficult to estimate. For added simplicity it may help to think of time t as the end of the breeding season in year t and to imagine that one is studying a species that has only one discrete breeding season per year. The BIDE model can then be expressed as:  where: Nt,a = number of adult females at time t Nt,i = number of immature females at time t Sa = annual survival of adult females from time t to time t+1 Si = annual survival of immature females from time t to time t+1 Ri = ratio of surviving young females at the end of the breeding season per breeding female In matrix notation this model can be expressed as:  Suppose that you are studying a species with a maximum lifespan of 4 years. The following is an age-based Leslie matrix for this species. Each row in the first and third matrices corresponds to animals within a given age range (0 1 years, 1 2 years and 2 3 years). In a Leslie matrix the top row of the middle matrix consists of age-specific fertilities: F1, F2 and F3. Note, that F1 = Si Ri in the matrix above. Since this species does not live to be 4 years old the matrix does not contain an S3 term.  These models can give rise to interesting cyclical or seemingly chaotic patterns in abundance over time when fertility rates are high. The terms Fi and Si can be constants or they can be functions of environment, such as habitat or population size. Randomness can also be incorporated into the environmental component.
A logistic function or logistic curve is a common "S" shape (sigmoid curve), with equation:  where e = the natural logarithm base (also known as Euler's number), x0 = the x-value of the sigmoid's midpoint, L = the curve's maximum value, and k = the steepness of the curve. For values of x in the range of real numbers from    to + , the S-curve shown on the right is obtained (with the graph of f approaching L as x approaches +  and approaching zero as x approaches   ). The function was named in 1844 1845 by Pierre Franc ois Verhulst, who studied it in relation to population growth. The initial stage of growth is approximately exponential; then, as saturation begins, the growth slows, and at maturity, growth stops. The logistic function finds applications in a range of fields, including artificial neural networks, biology (especially ecology), biomathematics, chemistry, demography, economics, geoscience, mathematical psychology, probability, sociology, political science, and statistics.
In survival analysis, the area compatibility factor, F, is used in indirect standardisation of population mortality rates.  where:  is the standardised central exposed to risk from age x to x + t for the standard population,  is the central exposed to risk from age x to x + t for the population under study and  is the mortality rate in the standard population for ages x to x + t. The expression can be thought of as the crude mortality rate for the standard population divided by what the crude mortality rate is for the region being studied, assuming the mortality rates are the same as for the standard population. F is then multiplied by the crude mortality rate to arrive at the indirectly standardised mortality rate.
In probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were "medical professionals", and 99% of the public were not "medical professionals", then the base rate of medical professionals is simply 1%. In the sciences, including medicine, the base rate is critical for comparison. It may at first seem impressive that 1000 people beat their winter cold while using 'Treatment X', until we look at the entire 'Treatment X' population and find that the base rate of success is actually only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. "1000 people... out of how many ") is available. Note that controls may likewise offer further information for comparison; maybe the control groups, who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that 'Treatment X' actually makes things worse, despite that initial proud claim about 1000 people. The normative method for integrating base rates (prior probabilities) and featural evidence (likelihoods) is given by Bayes' rule.
In probability theory, Hoeffding's inequality provides an upper bound on the probability that the sum of random variables deviates from its expected value. Hoeffding's inequality was proved by Wassily Hoeffding in 1963. Hoeffding's inequality is a special case of the Azuma Hoeffding inequality, and it is more general than the Bernstein inequality, proved by Sergei Bernstein in 1923. They are also special cases of McDiarmid's inequality.
XploRe is a commercial statistics software package, developed by the German software company MD*Tech. XploRe is not sold any more. The last version, 4.8, is available for download at no cost. The user interacts with the software via the XploRe programming language, which is derived from the C programming language. Single XploRe programs are called Quantlets.
Simultaneous equation models are a form of statistical model in the form of a set of linear simultaneous equations. They are often used in econometrics.
In probability and statistics a Markov renewal process is a random process that generalizes the notion of Markov jump processes. Other random processes like Markov chain, Poisson process, and renewal process can be derived as a special case of an MRP (Markov renewal process).
Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning. Pattern recognition systems are in many cases trained from labeled "training" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform "most likely" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.
Rossmo's formula is a geographic profiling formula to predict where a serial criminal lives. The formula was developed and patented by criminologist Kim Rossmo and integrated into a specialized crime analysis software product called Rigel. The Rigel product is developed by the software company Environmental Criminology Research Inc. (ECRI), which Rossmo co-founded.
The Cornish Fisher expansion is an asymptotic expansion used to approximate the quantiles of a probability distribution based on its cumulants.
In statistics, originally in geostatistics, Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process governed by prior covariances, as opposed to a piecewise-polynomial spline chosen to optimize smoothness of the fitted values. Under suitable assumptions on the priors, Kriging gives the best linear unbiased prediction of the intermediate values. Interpolating methods based on other criteria such as smoothness need not yield the most likely intermediate values. The method is widely used in the domain of spatial analysis and computer experiments. The technique is also known as Wiener Kolmogorov prediction, after Norbert Wiener and Andrey Kolmogorov.  The theoretical basis for the method was developed by the French mathematician Georges Matheron based on the Master's thesis of Danie G. Krige, the pioneering plotter of distance-weighted average gold grades at the Witwatersrand reef complex in South Africa. Krige sought to estimate the most likely distribution of gold based on samples from a few boreholes. The English verb is to krige and the most common noun is Kriging; both are often pronounced with a hard "g", following the pronunciation of the name "Krige".
Engineering tolerance is the permissible limit or limits of variation in: a physical dimension, a measured value or physical property of a material manufactured object, system, or service, other measured values (such as temperature, humidity, etc.). in engineering and safety, a physical distance or space (tolerance), as in a truck (lorry), train or boat under a bridge as well as a train in a tunnel (see structure gauge and loading gauge). in mechanical engineering the space between a bolt and a nut or a hole, etc.. Dimensions, properties, or conditions may have some variation without significantly affecting functioning of systems, machines, structures, etc. A variation beyond the tolerance (for example, a temperature that's too hot or too cold) is said to be non-compliant, rejected, or exceeding the tolerance. If the tolerance is too restrictive, the machine being incapable of functioning in most environments, it is said to be intolerant.
In mathematics, a random compact set is essentially a compact set-valued random variable. Random compact sets are useful in the study of attractors for random dynamical systems.
In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. In an F-test, the null distribution is an F-distribution.
In parasitology, the quantitative study of parasitism in a host population involves the use of statistics to draw meaningful conclusions from observations of the prevalence and intensity of parasitic infection.
A quasi-experiment is an empirical study used to estimate the causal impact of an intervention on its target population. Quasi-experimental research shares similarities with the traditional experimental design or randomized controlled trial, but they specifically lack the element of random assignment to treatment or control. Instead, quasi-experimental designs typically allow the researcher to control the assignment to the treatment condition, but using some criterion other than random assignment (e.g., an eligibility cutoff mark). In some cases, the researcher may have control over assignment to treatment. Quasi-experiments are subject to concerns regarding internal validity, because the treatment and control groups may not be comparable at baseline. With random assignment, study participants have the same chance of being assigned to the intervention group or the comparison group. As a result, differences between groups on both observed and unobserved characteristics would be due to chance, rather than to a systematic factor related to treatment (e.g., illness severity). Randomization itself does not guarantee that groups will be equivalent at baseline. Any change in characteristics post-intervention is likely attributable to the intervention. With quasi-experimental studies, it may not be possible to convincingly demonstrate a causal link between the treatment condition and observed outcomes. This is particularly true if there are confounding variables that cannot be controlled or accounted for.
Additive white Gaussian noise (AWGN) is a basic noise model used in Information theory to mimic the effect of many random processes that occur in nature. The modifiers denote specific characteristics: Additive because it is added to any noise that might be intrinsic to the information system. White refers to the idea that it has uniform power across the frequency band for the information system. It is an analogy to the color white which has uniform emissions at all frequencies in the visible spectrum. Gaussian because it has a normal distribution in the time domain with an average time domain value of zero. Wideband noise comes from many natural sources, such as the thermal vibrations of atoms in conductors (referred to as thermal noise or Johnson-Nyquist noise), shot noise, black body radiation from the earth and other warm objects, and from celestial sources such as the Sun. The central limit theorem of probability theory indicates that the summation of many random processes will tend to have distribution called Gaussian or Normal. AWGN is often used as a channel model in which the only impairment to communication is a linear addition of wideband or white noise with a constant spectral density (expressed as watts per hertz of bandwidth) and a Gaussian distribution of amplitude. The model does not account for fading, frequency selectivity, interference, nonlinearity or dispersion. However, it produces simple and tractable mathematical models which are useful for gaining insight into the underlying behavior of a system before these other phenomena are considered. The AWGN channel is a good model for many satellite and deep space communication links. It is not a good model for most terrestrial links because of multipath, terrain blocking, interference, etc. However, for terrestrial path modeling, AWGN is commonly used to simulate background noise of the channel under study, in addition to multipath, terrain blocking, interference, ground clutter and self interference that modern radio systems encounter in terrestrial operation.
Minitab is a statistics package developed at the Pennsylvania State University by researchers Barbara F. Ryan, Thomas A. Ryan, Jr., and Brian L. Joiner in 1972. It began as a light version of OMNITAB, a statistical analysis program by NIST; the documentation for OMNITAB was published 1986, and there has been no significant development since then. Minitab is distributed by Minitab Inc, a privately owned company headquartered in State College, Pennsylvania. Minitab Inc. also produces Quality Trainer and Quality Companion, which can be used in conjunction with Minitab: the first being an eLearning package that teaches statistical tools and concepts in the context of quality improvement, while the second is a tool for managing Six Sigma and Lean Manufacturing.
k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes. The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.
In statistics, a scan statistic or window statistic is a problem relating to the clustering of randomly positioned points. An example of a typical problem is the maximum size of a cluster of points on a line or the longest series of successes recorded by a moving window of fixed length. Joseph Naus first published on the problem in the 1960s, and has been called the "father of the scan statistic" in honour of his early contributions. The results can be applied in epidemiology, public health and astronomy to find unusual clusters of events. It was extended by Martin Kulldorff to multi-dimensional settings and varying window sizes in a 1997 paper, which is (as of 11 October 2015) the most cited article in its journal, Communications in Statistics   Theory and Methods.
In probability theory, an exponentially modified Gaussian (EMG) distribution (ExGaussian distribution) describes the sum of independent normal and exponential random variables. An exGaussian random variable Z may be expressed as Z = X + Y where X and Y are independent, X is Gaussian with mean   and variance  2 and Y is exponential of rate  . It has a characteristic positive skew from the exponential component. It may also be regarded as a weighted function of a shifted exponential with the weight being a function of the normal distribution.
In statistics the Crame r von Mises criterion is a criterion used for judging the goodness of fit of a cumulative distribution function  compared to a given empirical distribution function , or for comparing two empirical distributions. It is also used as a part of other algorithms, such as minimum distance estimation. It is defined as  In one-sample applications  is the theoretical distribution and  is the empirically observed distribution. Alternatively the two distributions can both be empirically estimated ones; this is called the two-sample case. The criterion is named after Harald Crame r and Richard Edler von Mises who first proposed it in 1928 1930. The generalization to two samples is due to Anderson. The Crame r von Mises test is an alternative to the Kolmogorov Smirnov test.
Minimisation is a method of adaptive stratified sampling that is used in clinical trials, as described by Pocock and Simon. The aim of minimisation is to minimise the imbalance between the number of patients in each treatment group over a number of factors. Normally patients would be allocated to a treatment group randomly and while this maintains a good overall balance, it can lead to imbalances within sub-groups. For example, if a majority of the patients who were receiving the active drug happened to be male, or smokers, the statistical usefulness of the study would be reduced. The traditional method to avoid this problem, known as blocked randomisation, is to stratify patients according to a number of factors (e.g. male and female, or smokers and non-smokers) and to use a separate randomisation list for each group. Each randomisation list would be created such that after every block of x patients, there would be an equal number in each treatment group. The problem with this method is that the number of lists increases exponentially with the number of stratification factors. Minimisation addresses this problem by calculating the imbalance within each factor should the patient be allocated to a particular treatment group. The various imbalances are added together to give the overall imbalance in the study. The treatment group that would minimise the imbalance can be chosen directly, or a random element may be added (perhaps allocating a higher chance to the groups that will minimise the imbalance, or perhaps only allocating a chance to groups that will minimise the imbalance). The imbalances can be weighted if necessary to give some factors more importance than others. Similarly a ratio can be applied to the number of patients in each treatment group. In use, minimisation often maintains a better balance than traditional blocked randomisation, and its advantage rapidly increases with the number of stratification factors.
A choropleth map (from Greek       ("area/region") +         ("multitude")) is a thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical variable being displayed on the map, such as population density or per-capita income. The choropleth map provides an easy way to visualize how a measurement varies across a geographic area or it shows the level of variability within a region.
In game theory, a Bayesian game is one in which information about characteristics of the other players (i.e. payoffs) is incomplete. Following John C. Harsanyi's framework, a Bayesian game can be modelled by introducing Nature as a player in a game. Nature assigns a random variable to each player which could take values of types for each player and associating probabilities or a probability density function with those types (in the course of the game, nature randomly chooses a type for each player according to the probability distribution across each player's type space). Harsanyi's approach to modeling a Bayesian game in such a way allows games of incomplete information to become games of imperfect information (in which the history of the game is not available to all players). The type of a player determines that player's payoff function. The probability associated with a type is the probability that the player, for whom the type is specified, is that type. In a Bayesian game, the incompleteness of information means that at least one player is unsure of the type (and so the payoff function) of another player. Such games are called Bayesian because of the probabilistic analysis inherent in the game. Players have initial beliefs about the type of each player (where a belief is a probability distribution over the possible types for a player) and can update their beliefs according to Bayes' rule as play takes place in the game, i.e. the belief a player holds about another player's type might change on the basis of the actions they have played. The lack of information held by players and modeling of beliefs mean that such games are also used to analyse imperfect information scenarios.
Association mapping (genetics), also known as "linkage disequilibrium mapping", is a method of mapping quantitative trait loci (QTLs) that takes advantage of historic linkage disequilibrium to link phenotypes (observable characteristics) to genotypes (the genetic constitution of organisms).  
A data set (or dataset) is a collection of data. Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. The term data set may also be used more loosely, to refer to the data in a collection of closely related tables, corresponding to a particular experiment or event. An example of this type is the data sets collected by space agencies performing experiments with instruments aboard space probes.
Correspondence analysis (CA) is a multivariate statistical technique proposed by Hirschfeld and later developed by Jean-Paul Benze cri. It is conceptually similar to principal component analysis, but applies to categorical rather than continuous data. In a similar manner to principal component analysis, it provides a means of displaying or summarising a set of data in two-dimensional graphical form. All data should be nonnegative and on the same scale for CA to be applicable, and the method treats rows and columns equivalently. It is traditionally applied to contingency tables   CA decomposes the chi-squared statistic associated with this table into orthogonal factors. Because CA is a descriptive technique, it can be applied to tables whether or not the  statistic is appropriate.
In statistics, inverse-variance weighting is a method of aggregating two or more random variables to minimize the variance of the weighted average. Each random variable is weighted in inverse proportion to its variance. Given a sequence of independent observations yi with variances  i2, the inverse-variance weighted average is given by  The inverse-variance weighted average has the least variance among all weighted averages, which can be calculated as  If the variances of the measurements are all equal, then the inverse-variance weighted average becomes the simple average. Inverse-variance weighting is typically used in statistical meta-analysis to combine the results from independent measurements.
In finance, volatility is the degree of variation of a trading price series over time as measured by the standard deviation of returns. Historic volatility is derived from time series of past market prices. An implied volatility is derived from the market price of a market traded derivative (in particular an option). The symbol   is used for volatility, and corresponds to standard deviation, which should not be confused with the similarly named variance, which is instead the square,  2.
The Rind et al. controversy was a debate in the scientific literature, public media, and government legislatures in the United States regarding a 1998 peer reviewed meta-analysis of the self-reported harm caused by child sexual abuse (CSA). The debate resulted in the unprecedented condemnation of the paper by both Houses of the United States Congress. The social science research community was concerned that the condemnation by government legislatures might have a chilling effect on the future publication of controversial research results. The study's lead author is psychologist Bruce Rind, and it expanded on a 1997 meta-analysis for which Rind is also lead author. The authors stated their goal was to determine whether CSA caused pervasive, significant psychological harm for both males and females, controversially concluding that the harm caused by child sexual abuse was not necessarily intense or pervasive, that the prevailing construct of CSA was not scientifically valid, as it failed empirical verification, and that the psychological damage caused by the abusive encounters depends on other factors such as the degree of coercion or force involved. The authors concluded that even though CSA may not result in lifelong, significant harm to all victims, this does not mean it is not morally wrong and indicated that their findings did not imply current moral and legal prohibitions against CSA should be changed. The Rind et al. study has been criticized by various scientists and researchers, notably Stephanie Dallam (2001; 2002), on the grounds that its methodology and conclusions are poorly designed and statistically flawed. Its definition of harm, for example, has been subject to debate because it only examined long-term psychological effects, and harm can result in a number of ways, including short-term or medical harm (for example, sexually transmitted infections or injuries), a likelihood of revictimization, and the amount of time the victim spent attending therapy for the abuse. Seven years after the publication of the Rind et al. study, however, Heather Marie Ulrich, with two colleagues, replicated it in The Scientific Review of Mental Health Practice and confirmed its main findings, but did not endorse its authors' conclusions. The Rind paper has been quoted by people and organizations advocating age of consent reform, pedophile or pederasty groups in support of their efforts to change attitudes towards pedophilia and to decriminalize sexual activity between adults and minors (children or adolescents), and by defense attorneys who have used the study to minimize harm in child sexual abuse cases.
In mathematics, the Ito  isometry, named after Kiyoshi Ito , is a crucial fact about Ito  stochastic integrals. One of its main applications is to enable the computation of variances for stochastic processes. Let  denote the canonical real-valued Wiener process defined up to time , and let  be a stochastic process that is adapted to the natural filtration  of the Wiener process. Then  where  denotes expectation with respect to classical Wiener measure . In other words, the Ito  stochastic integral, as a function, is an isometry of normed vector spaces with respect to the norms induced by the inner products  and
The S rensen Dice index, also known by other names (see Names, below), is a statistic used for comparing the similarity of two samples. It was independently developed by the botanists Thorvald S rensen and Lee Raymond Dice, who published in 1948 and 1945 respectively.  
The empirical probability, also known as relative frequency, or experimental probability, is the ratio of the number of outcomes in which a specified event occurs to the total number of trials, not in a theoretical sample space but in an actual experiment. In a more general sense, empirical probability estimates probabilities from experience and observation. Given an event, A, in a sample space, the relative frequency of A is the ratio of m/n. m being the number of outcomes favourable to the occurrence of A; n being the total number of outcomes of an experiment. In statistical terms, the empirical probability is an estimate or estimator of a probability. In simple cases, where the result of a trial only determines whether or not the specified event has occurred, modelling using a binomial distribution might be appropriate and then the empirical estimate is the maximum likelihood estimate. It is the Bayesian estimate for the same case if certain assumptions are made for the prior distribution of the probability. If a trial yields more information, the empirical probability can be improved on by adopting further assumptions in the form of a statistical model: if such a model is fitted, it can be used to derive an estimate of the probability of the specified event.
Renewal theory is the branch of probability theory that generalizes Poisson processes for arbitrary holding times. Applications include calculating the best strategy for replacing worn-out machinery in a factory and comparing the long-term benefits of different insurance policies.
In statistics, the standard score is the signed number of standard deviations an observation or datum is above the mean. A positive standard score indicates a datum above the mean, while a negative standard score indicates a datum below the mean. It is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This conversion process is called standardizing or normalizing (however, "normalizing" can refer to many types of ratios; see normalization (statistics) for more). Standard scores are also called z-values, z-scores, normal scores, and standardized variables; the use of "Z" is because the normal distribution is also known as the "Z distribution". They are most frequently used to compare a sample to a standard normal deviate, though they can be defined without assumptions of normality. The z-score is only defined if one knows the population parameters; if one only has a sample set, then the analogous computation with sample mean and sample standard deviation yields the Student's t-statistic.
Calibrated probability assessments are subjective probabilities assigned by individuals who have been trained to assess probabilities in a way that historically represents their uncertainty. In other words, when a calibrated person says they are "80% confident" in each of 100 predictions they made, they will get about 80% of them correct. Likewise, they will be right 90% of the time they say they are 90% certain, and so on. Calibration training improves subjective probabilities because most people are either "overconfident" or "under-confident" (usually the former). By practicing with a series of trivia questions, it is possible for subjects to fine-tune their ability to assess probabilities. For example, a subject may be asked: True or False: "A hockey puck fits in a golf hole" Confidence: Choose the probability that best represents your chance of getting this question right... 50% 60% 70% 80% 90% 100%  If a person has no idea whatsoever, they will say they are only 50% confident. If they are absolutely certain they are correct, they will say 100%. But most people will answer somewhere in between. If a calibrated person is asked a large number of such questions, they will get about as many correct as they expected. An uncalibrated person who is systematically overconfident may say they are 90% confident in a large number of questions where they only get 70% of them correct. On the other hand, an uncalibrated person who is systematically underconfident may say they are 50% confident in a large number of questions where they actually get 70% of them correct. Calibration training generally involves taking a battery of such tests. Feedback is provided between tests and the subjects refine their probabilities. Calibration training may also involve learning other techniques that help to compensate for consistent over- or under-confidence. Since subjects are better at placing odds when they pretend to bet money, subjects are taught how to convert calibration questions into a type of betting game which is shown to improve their subjective probabilities. Various collaborative methods have been developed, such as prediction market, so that subjective estimates from multiple individuals can be taken into account. Stochastic modeling methods such as the Monte Carlo method often use subjective estimates from "subject matter experts". However, since research shows that such experts are very likely to be statistically overconfident, the model will tend to underestimate uncertainty and risk. The Applied Information Economics method systematically uses calibration training as part of a decision modeling process.
Preference regression is a statistical technique used by marketers to determine consumers  preferred core benefits. It usually supplements product positioning techniques like multi dimensional scaling or factor analysis and is used to create ideal vectors on perceptual maps.
In probability theory and statistics, the negative multinomial distribution is a generalization of the negative binomial distribution (NB(r, p)) to more than two outcomes. Suppose we have an experiment that generates m+1 2 possible outcomes, {X0,...,Xm}, each occurring with non-negative probabilities {p0,...,pm} respectively. If sampling proceeded until n observations were made, then {X0,...,Xm} would have been multinomially distributed. However, if the experiment is stopped once X0 reaches the predetermined value k0, then the distribution of the m-tuple {X1,...,Xm} is negative multinomial. These variables are not multinomially distributed because their sum X1+...+Xm is not fixed, being a draw from a negative binomial distribution.
A Doob martingale (also known as a Levy martingale) is a mathematical construction of a stochastic process which approximates a given random variable and has the martingale property with respect to the given filtration. It may be thought of as the evolving sequence of best approximations to the random variable based on information accumulated up to a certain time. When analyzing sums, random walks, or other additive functions of independent random variables, one can often apply the central limit theorem, law of large numbers, Chernoff's inequality, Chebyshev's inequality or similar tools. When analyzing similar objects where the differences are not independent, the main tools are martingales and Azuma's inequality.
In statistics, the ordered logit model (also ordered logistic regression or proportional odds model), is a regression model for ordinal dependent variables, first considered by Peter McCullagh. For example, if one question on a survey is to be answered by a choice among "poor", "fair", "good", "very good", and "excellent", and the purpose of the analysis is to see how well that response can be predicted by the responses to other questions, some of which may be quantitative, then ordered logistic regression may be used. It can be thought of as an extension of the logistic regression model that applies to dichotomous dependent variables, allowing for more than two (ordered) response categories.
The Doob Meyer decomposition theorem is a theorem in stochastic calculus stating the conditions under which a submartingale may be decomposed in a unique way as the sum of a martingale and an increasing predictable process. It is named for Joseph L. Doob and Paul-Andre  Meyer.
In statistics and in statistical physics, the Metropolis Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.
Bayesian probability is one interpretation of the concept of probability. In contrast to interpreting probability as frequency or propensity of some phenomenon, Bayesian probability is a quantity that is assigned to represent a state of knowledge, or a state of belief. The Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, i.e., the propositions whose truth or falsity is uncertain. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability. Bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies some prior probability, which is then updated to a posterior probability in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation. The term "Bayesian" derives from the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of Bayesian inference. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability. Broadly speaking, there are two views on Bayesian probability that interpret the probability concept in different ways. According to the objectivist view, the rules of Bayesian statistics can be justified by requirements of rationality and consistency and interpreted as an extension of logic. According to the subjectivist view, probability quantifies a "personal belief".
In epidemiology and demography, age adjustment, also called age standardization, is a technique used to allow populations to be compared when the age profiles of the populations are quite different.
In statistics, a doubly stochastic model is a type of model that can arise in many contexts, but in particular in modelling time-series and stochastic processes. The basic idea for a doubly stochastic model is that an observed random variable is modelled in two stages. In one stage, the distribution of the observed outcome is represented in a fairly standard way using one or more parameters. At a second stage, some of these parameters (often only one) are treated as being themselves random variables. In a univariate context this is essentially the same as the well-known concept of compounded distributions. For the more general case of doubly stochastic models, there is the idea that many values in a time-series or stochastic model are simultaneously affected by the underlying parameters, either by using a single parameter affecting many outcome variates, or by treating the underlying parameter as a time-series or stochastic process in its own right. The basic idea here is essentially similar to that broadly used in latent variable models except that here the quantities playing the role of latent variables usually have an underlying dependence structure related to the time-series or spatial context. An example of a doubly stochastic model is the following. The observed values in a point process might be modelled as a Poisson process in which the rate (the relevant underlying parameter) is treated as being the exponential of a Gaussian process.  
In statistics, quality assurance, and survey methodology, sampling is concerned with the selection of a subset of individuals from within a statistical population to estimate characteristics of the whole population. Each observation measures one or more properties (such as weight, location, color) of observable bodies distinguished as independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. The sampling process comprises several stages: Defining the population of concern Specifying a sampling frame, a set of items or events possible to measure Specifying a sampling method for selecting items or events from the frame Determining the sample size Implementing the sampling plan Sampling and data collecting Data which can be selected
In combinatorial mathematics, a Steiner system (named after Jakob Steiner) is a type of block design, specifically a t-design with   = 1 and t   2. A Steiner system with parameters t, k, n, written S(t,k,n), is an n-element set S together with a set of k-element subsets of S (called blocks) with the property that each t-element subset of S is contained in exactly one block. In an alternate notation for block designs, an S(t,k,n) would be a t-(n,k,1) design. This definition is relatively modern, generalizing the classical definition of Steiner systems which in addition required that k = t + 1. An S(2,3,n) was (and still is) called a Steiner triple (or triad) system, while an S(3,4,n) was called a Steiner quadruple system, and so on. With the generalization of the definition, this naming system is no longer strictly adhered to. A long-standing problem in design theory is if any nontrivial (t < k < n) Steiner systems have t   6; also if infinitely many have t = 4 or 5. This was claimed to be solved in the affirmative by Peter Keevash.
The computer program Statistical Lab (Statistiklabor) is an explorative and interactive toolbox for statistical analysis and visualization of data. It supports educational applications of statistics in business sciences, economics, social sciences and humanities. The program is developed and constantly advanced by the Center for Digital Systems of the Free University of Berlin. Their website states that the source code is available to private users under the GPL. So if a commercial user wishes to obtain a copy, then they must do so indirectly, from a private user who already has a copy (any of their employees will do). Simple or complex statistical problems can be simulated, edited and solved individually with the Statistical Lab. It can be extended by using external libraries. Via these libraries, it can also be adapted to individual and local demands like specific target groups. The versatile graphical diagrams allow demonstrative visualization of underlying data. The Statistical Lab is the successor of Statistik interaktiv!. In contrast to the commercial SPSS the Statistical Lab is didactically driven. It is focused on providing facilities for users with little statistical experience. It combines data frames, contingency tables, random numbers, matrices in a user friendly virtual worksheet. This worksheet allows users to explore the possibilities of calculations, analysis, simulations and manipulation of data. For mathematical calculations, the Statistical Lab uses the Engine R, which is a free implementation of the language S Plus (originally developed by Bell Laboratories). The R-Project is constantly being improved by worldwide community of Developers.
Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution. In physics-related problems, Monte Carlo methods are quite useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative "soft" methods. In principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is parameterized, mathematicians often use a Markov Chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler. In other important problems we are interested in generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation). In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and Markov chain Monte Carlo methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.
Correction for attenuation is a statistical procedure, due to Spearman (1904), to "rid a correlation coefficient from the weakening effect of measurement error" (Jensen, 1998), a phenomenon also known as regression dilution. In measurement and statistics, it is also called disattenuation. The correlation between two sets of parameters or measurements is estimated in a manner that accounts for measurement error contained within the estimates of those parameters.
The method of least squares is a standard approach in regression analysis to the approximate solution of overdetermined systems, i.e., sets of equations in which there are more equations than unknowns. "Least squares" means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation. The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals, a residual being the difference between an observed value and the fitted value provided by a model. When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares. Least squares problems fall into two categories: linear or ordinary least squares and non-linear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The non-linear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases. Polynomial least squares describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve. When the observations come from an exponential family and mild conditions are satisfied, least-squares estimates and maximum-likelihood estimates are identical. The method of least squares can also be derived as a method of moments estimator. The following discussion is mostly presented in terms of linear functions but the use of least-squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit a generalized linear model. For the topic of approximating a function by a sum of others using an objective function based on squared distances, see least squares (function approximation). The least-squares method is usually credited to Carl Friedrich Gauss (1795), but it was first published by Adrien-Marie Legendre.
In mathematics, the Cauchy Schwarz inequality is a useful inequality encountered in many different settings, such as linear algebra, analysis, probability theory, vector algebra and other areas. It is considered to be one of the most important inequalities in all of mathematics. It has a number of generalizations, among them Ho lder's inequality. The inequality for sums was published by Augustin-Louis Cauchy (1821), while the corresponding inequality for integrals was first proved by Viktor Bunyakovsky (1859). The modern proof of the integral inequality was given by Hermann Amandus Schwarz (1888).
The Pareto distribution, named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto, is a power law probability distribution that is used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena.
Quantile regression is a type of regression analysis used in statistics and econometrics. Whereas the method of least squares results in estimates that approximate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating either the conditional median or other quantiles of the response variable.
Analysis of variance   simultaneous component analysis (ASCA or ANOVA SCA) is a method that partitions variation and enables interpretation of these partitions by SCA, a method that is similar to principal components analysis (PCA). This method is a multivariate or even megavariate extension of analysis of variance (ANOVA). The variation partitioning is similar to ANOVA. Each partition matches all variation induced by an effect or factor, usually a treatment regime or experimental condition. The calculated effect partitions are called effect estimates. Because even the effect estimates are multivariate, interpretation of these effects estimates is not intuitive. By applying SCA on the effect estimates one gets a simple interpretable result. In case of more than one effect this method estimates the effects in such a way that the different effects are not correlated.
Phase dispersion minimization (PDM) is a data analysis technique that searches for periodic components of a time series data set. It is useful for data sets with gaps, non-sinusoidal variations, poor time coverage or other problems that would make Fourier techniques unusable. It was first developed by Stellingwerf in 1978  and has been widely used for astronomical and other types of periodic data analyses. Source code is available for PDM analysis. The current version of this application is available for download.
In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on conditions that might be related to the event. For example, suppose one is interested in whether a person has cancer, and knows the person's age. If cancer is related to age, then, using Bayes' theorem, information about the person's age can be used to more accurately assess the probability that they have cancer. When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. In one of these interpretations, the theorem is used directly as part of a particular approach to statistical inference. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for evidence: this is Bayesian inference, which is fundamental to Bayesian statistics. However, Bayes' theorem has applications in a wide range of calculations involving probabilities, not just in Bayesian inference. Bayes' theorem is named after Rev. Thomas Bayes (/ be z/; 1701 1761), who first provided an equation that allows new evidence to update beliefs. It was further developed by Pierre-Simon Laplace, who first published the modern formulation in his 1812 The orie analytique des probabilite s. Sir Harold Jeffreys put Bayes' algorithm and Laplace's formulation on an axiomatic basis. Jeffreys wrote that Bayes' theorem "is to the theory of probability what the Pythagorean theorem is to geometry".
Zipf's law / z f/, an empirical law formulated using mathematical statistics, refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. The law is named after the American linguist George Kingsley Zipf (1902 1950), who popularized it and sought to explain it (Zipf 1935, 1949), though he did not claim to have originated it. The French stenographer Jean-Baptiste Estoup (1868 1950) appears to have noticed the regularity before Zipf. It was also noted in 1913 by German physicist Felix Auerbach (1856 1933).
In the statistical analysis of time series, autoregressive moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the auto-regression and the second for the moving average. The general ARMA model was described in the 1951 thesis of Peter Whittle, Hypothesis testing in time series analysis, and it was popularized in the 1971 book by George E. P. Box and Gwilym Jenkins. Given a time series of data Xt, the ARMA model is a tool for understanding and, perhaps, predicting future values in this series. The model consists of two parts, an autoregressive (AR) part and a moving average (MA) part. The model is usually then referred to as the ARMA(p,q) model where p is the order of the autoregressive part and q is the order of the moving average part (as defined below).
In any quantitative science, the terms relative change and relative difference are used to compare two quantities while taking into account the "sizes" of the things being compared. The comparison is expressed as a ratio and is a unitless number. By multiplying these ratios by 100 they can be expressed as percentages so the terms percentage change, percent(age) difference, or relative percentage difference are also commonly used. The distinction between "change" and "difference" depends on whether or not one of the quantities being compared is considered a standard or reference or starting value. When this occurs, the term relative change (with respect to the reference value) is used and otherwise the term relative difference is preferred. Relative difference is often used as a quantitative indicator of quality assurance and quality control for repeated measurements where the outcomes are expected to be the same. A special case of percent change (relative change expressed as a percentage) called percent error occurs in measuring situations where the reference value is the accepted or actual value (perhaps theoretically determined) and the value being compared to it is experimentally determined (by measurement).
In probability theory, Markov's inequality gives an upper bound for the probability that a non-negative function of a random variable is greater than or equal to some positive constant. It is named after the Russian mathematician Andrey Markov, although it appeared earlier in the work of Pafnuty Chebyshev (Markov's teacher), and many sources, especially in analysis, refer to it as Chebyshev's inequality (sometimes, calling it the first Chebyshev inequality, while referring to the Chebyshev's inequality as the second Chebyshev's inequality) or Bienayme 's inequality. Markov's inequality (and other similar inequalities) relate probabilities to expectations, and provide (frequently loose but still useful) bounds for the cumulative distribution function of a random variable. An example of an application of Markov's inequality is the fact that (assuming incomes are non-negative) no more than 1/5 of the population can have more than 5 times the average income.
In probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable X, or just distribution function of X, evaluated at x, is the probability that X will take a value less than or equal to x. In the case of a continuous distribution, it gives the area under the probability density function from minus infinity to x. Cumulative distribution functions are also used to specify the distribution of multivariate random variables.
In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is. Although topic models were first described and implemented in the context of natural language processing, they have applications in other fields such as bioinformatics.
In statistics, the dual term variability is preferred to the use of precision. Variability is the amount of imprecision. There can be differences in usage of the term for particular statistical models but, in common statistical usage, the precision is defined to be the reciprocal of the variance, while the precision matrix is the matrix inverse of the covariance matrix. One particular use of the precision matrix is in the context of Bayesian analysis of the multivariate normal distribution: for example, Bernardo & Smith prefer to parameterise the multivariate normal distribution in terms of the precision matrix rather than the covariance matrix because of certain simplifications that then arise.
In computer science, Luby transform codes (LT codes) are the first class of practical fountain codes that are near-optimal erasure correcting codes. They were invented by Michael Luby in 1998 and published in 2002. Like some other fountain codes, LT codes depend on sparse bipartite graphs to trade reception overhead for encoding and decoding speed. The distinguishing characteristic of LT codes is in employing a particularly simple algorithm based on the exclusive or operation () to encode and decode the message. LT codes are rateless because the encoding algorithm can in principle produce an infinite number of message packets (i.e., the percentage of packets that must be received to decode the message can be arbitrarily small). They are erasure correcting codes because they can be used to transmit digital data reliably on an erasure channel. The next generation beyond LT codes are raptor codes (see for example IETF RFC 5053 or IETF RFC 6330), which have linear time encoding and decoding. Raptor codes use two encoding stages for encoding, where the second stage is an LT encoding.
The most probable number method, otherwise known as the method of Poisson zeroes, is a method of getting quantitative data on concentrations of discrete items from positive/negative (incidence) data. There are many discrete entities that are easily detected but difficult to count. Any sort of amplification reaction or catalysis reaction obliterates easy quantification but allows presence to be detected very sensitively. Common examples include microorganism growth, enzyme action, or catalytic chemistry. The MPN method involves taking the original solution or sample, and subdividing it by orders of magnitude (frequently 10  or 2 ), and assessing presence/absence in multiple subdivisions. The degree of dilution at which absence begins to appear indicates that the items have been diluted so much that there are many subsamples in which none appear. A suite of replicates at any given concentration allow finer resolution, to use the number of positive and negative samples to estimate the original concentration within the appropriate order of magnitude. In microbiology, the cultures are incubated and assessed by eye, bypassing tedious colony counting or expensive and tedious microscopic counts. In molecular biology, a common application involves DNA templates diluted into polymerase chain reactions (PCR). Reactions only proceed when a template is present, allowing for a form of quantitative PCR, to assess the original concentration of template molecules. Another application involves diluting enzyme stocks into solution containing a chromogenic substrate, or diluting antigens into solutions for ELISA (Enzyme-Linked ImmunoSorbent Assay) or some other antibody cascade detection reaction, to measure the original concentration of the enzyme or antigen. The major weakness of MPN methods is the need for large numbers of replicates at the appropriate dilution to narrow the confidence intervals. However, it is a very important method for counts when the appropriate order of magnitude is unknown a priori and sampling is necessarily destructive.
In probability theory and statistics, the log-Laplace distribution is the probability distribution of a random variable whose logarithm has a Laplace distribution. If X has a Laplace distribution with parameters   and b, then Y = eX has a log-Laplace distribution. The distributional properties can be derived from the Laplace distribution.
Meta-regression is a tool used in meta-analysis to examine the impact of moderator variables on study effect size using regression-based techniques. Meta-regression is more effective at this task than are standard meta-analytic techniques.
In statistics, the uncertainty coefficient, also called proficiency, entropy coefficient or Theil's U, is a measure of nominal association. It was first introduced by Henri Theil and is based on the concept of information entropy.
In statistics and signal processing, random variables in a time series have serial dependence if the value at some time t in the series is statistically dependent on the value at another time s. A series is serially independent if there is no dependence between any pair. Similarly, a time series has serial correlation if the condition holds that some pair of values are correlated, rather than the condition of statistical dependence: see autocorrelation. If a time series {Xt} is stationary, then statistical dependence between the pair (Xt , Xs) would imply that there is statistical dependence between all pairs of values at the same lag s t.
In spectroscopy, the Voigt profile (named after Woldemar Voigt) is a line profile resulting from the convolution of two broadening mechanisms, one of which alone would produce a Gaussian profile (usually, as a result of the Doppler broadening), and the other would produce a Lorentzian profile. Voigt profiles are common in many branches of spectroscopy and diffraction. Due to the computational expense of the convolution operation, the Voigt profile is often approximated using a pseudo-Voigt profile. All normalized line profiles can be considered to be probability distributions. The Gaussian profile is equivalent to a Gaussian, or normal, distribution and a Lorentzian profile is equivalent to a Lorentz, or Cauchy, distribution. Without loss of generality, we can consider only centered profiles, which peak at zero. The Voigt profile is then a convolution of a Lorentz profile and a Gaussian profile:  where x is the shift from the line center,  is the centered Gaussian profile:  and  is the centered Lorentzian profile:  The defining integral can be evaluated as:  where Re[w(z)] is the real part of the Faddeeva function evaluated for
Multiscale decision-making, also referred to as multiscale decision theory (MSDT), is an approach in operations research that combines game theory, multi-agent influence diagrams, in particular dependency graphs, and Markov decision processes to solve multiscale challenges in sociotechnical systems. MSDT considers interdependencies within and between the following scales: system level, time and information. Multiscale decision theory builds upon decision theory and multiscale mathematics. Multiscale decision theory can model and analyze complex decision-making networks that exhibit multiscale phenomena. The theory's results can be used by mechanism designers and decision-makers in organizations and complex systems to improve system performance and decision quality. Multiscale decision theory has been applied to manufacturing enterprise enterprises, service systems, supply chain management, healthcare, systems engineering, among others. In healthcare, for example, MSDT has been used to identify multi-level incentives that can improve healthcare value (quality of outcomes per dollar spent). The Multiscale Decision Making Laboratory at Virginia Tech directed by Dr. Christian Wernz is working at the forefront of MSDT theory and applications. Multiscale decision theory is related to: Multiscale modeling Decision analysis Cooperative distributed problem solving Decentralized decision making
In statistics, stratified sampling is a method of sampling from a population. In statistical surveys, when subpopulations within an overall population vary, it is advantageous to sample each subpopulation (stratum) independently. Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should be mutually exclusive: every element in the population must be assigned to only one stratum. The strata should also be collectively exhaustive: no population element can be excluded. Then simple random sampling or systematic sampling is applied within each stratum. This often improves the representativeness of the sample by reducing sampling error. It can produce a weighted mean that has less variability than the arithmetic mean of a simple random sample of the population. In computational statistics, stratified sampling is a method of variance reduction when Monte Carlo methods are used to estimate population statistics from a known population.
In statistics, a random effect(s) model, also called a variance components model, is a kind of hierarchical linear model. It assumes that the dataset being analysed consists of a hierarchy of different populations whose differences relate to that hierarchy. In econometrics, random effects models are used in the analysis of hierarchical or panel data when one assumes no fixed effects (it allows for individual effects). The random effects model is a special case of the fixed effects model. Contrast this to the biostatistics definitions, as biostatisticians use "fixed" and "random" effects to respectively refer to the population-average and subject-specific effects (and where the latter are generally assumed to be unknown, latent variables).
The fast Kalman filter (FKF), devised by Antti Lange (1941- ), is an extension of the Helmert-Wolf blocking (HWB) method from geodesy to real-time applications of Kalman filtering (KF) such as satellite imaging of the Earth.
In statistics, the generalized Pareto distribution (GPD) is a family of continuous probability distributions. It is often used to model the tails of another distribution. It is specified by three parameters: location , scale , and shape . Sometimes it is specified by only scale and shape and sometimes only by its shape parameter. Some references give the shape parameter as .
The auxiliary particle filter is a particle filtering algorithm introduced by Pitt and Shephard in 1999 to improve some deficiencies of the sequential importance resampling (SIR) algorithm when dealing with tailed observation densities. Assume that the filtered posterior is described by the following M weighted samples:  Then, each step in the algorithm consists of first drawing a sample of the particle index  which will be propagated from  into the new step . These indexes are auxiliary variables only used as an intermediary step, hence the name of the algorithm. The indexes are drawn according to the likelihood of some reference point  which in some way is related to the transition model  (for example, the mean, a sample, etc.):  This is repeated for , and using these indexes we can now draw the conditional samples:  Finally, the weights are updated to account for the mismatch between the likelihood at the actual sample and the predicted point :  
Cumulative frequency analysis is the analysis of the frequency of occurrence of values of a phenomenon less than a reference value. The phenomenon may be time- or space-dependent. Cumulative frequency is also called frequency of non-exceedance. Cumulative frequency analysis is performed to obtain insight into how often a certain phenomenon (feature) is below a certain value. This may help in describing or explaining a situation in which the phenomenon is involved, or in planning interventions, for example in flood protection.  This statistical technique can be used to see how likely an event like a flood is going to happen again in the future, based on how often it happened in the past. It can be adapted to bring in things like climate change causing wetter winters and drier summers.
The Delphi method (/ d lfa / DEL-fy) is a structured communication technique or method, originally developed as a systematic, interactive forecasting method which relies on a panel of experts. The experts answer questionnaires in two or more rounds. After each round, a facilitator or change agent provides an anonymous summary of the experts  forecasts from the previous round as well as the reasons they provided for their judgments. Thus, experts are encouraged to revise their earlier answers in light of the replies of other members of their panel. It is believed that during this process the range of the answers will decrease and the group will converge towards the "correct" answer. Finally, the process is stopped after a predefined stop criterion (e.g. number of rounds, achievement of consensus, stability of results) and the mean or median scores of the final rounds determine the results. Delphi is based on the principle that forecasts (or decisions) from a structured group of individuals are more accurate than those from unstructured groups. The technique can also be adapted for use in face-to-face meetings, and is then called mini-Delphi or Estimate-Talk-Estimate (ETE). Delphi has been widely used for business forecasting and has certain advantages over another structured forecasting approach, prediction markets.
Statistical software are specialized computer programs for analysis in statistics and econometrics.
In probability theory   specifically in the theory of stochastic processes, a stationary sequence is a random sequence whose joint probability distribution is invariant over time. If a random sequence X j is stationary then the following holds:  where F is the joint cumulative distribution function of the random variables in the subscript. If a sequence is stationary then it is wide-sense stationary. If a sequence is stationary then it has a constant mean (which may not be finite):
Mixed-data sampling (MIDAS) is an econometric regression or filtering method developed by Ghysels et al. A simple regression example has the independent variable appearing at a higher frequency than the dependent variable:  where y is the dependent variable, x is the regressor, m denotes the frequency   for instance if y is yearly  is quarterly    is the disturbance and  is a lag distribution, for instance the Beta function or the Almon Lag. The regression models can be viewed in some cases as substitutes for the Kalman filter when applied in the context of mixed frequency data. Bai, Ghysels and Wright (2010) examine the relationship between MIDAS regressions and Kalman filter state space models applied to mixed frequency data. In general, the latter involve a system of equations, whereas in contrast MIDAS regressions involve a (reduced form) single equation. As a consequence, MIDAS regressions might be less efficient, but also less prone to specification errors. In cases where the MIDAS regression is only an approximation, the approximation errors tend to be small.
In statistics, the Freedman Diaconis rule, named after David A. Freedman and Persi Diaconis, can be used to select the size of the bins to be used in a histogram. The general equation for the rule is:  where  is the interquartile range of the data and  is the number of observations in the sample
In marketing, Geodemographic segmentation is a multivariate statistical classification technique for discovering whether the individuals of a population fall into different groups by making quantitative comparisons of multiple characteristics with the assumption that the differences within any group should be less than the differences between groups.
The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary metrics of distances between datapoints instead of . This method was proposed in 1987 for the work with  norm and other distances. k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette. It is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances. A medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster.
Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between them is based on the likeness of their meaning or semantic content as opposed to similarity which can be estimated regarding their syntactical representation (e.g. their string format). These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature. The term semantic similarity is often confused with semantic relatedness. Semantic relatedness includes any relation between two terms, while semantic similarity only includes "is a" relations. For example, "car" is similar to "bus", but is also related to "road" and "driving". Computationally, semantic similarity can be estimated by defining a topological similarity, by using ontologies to define the distance between terms/concepts. For example, a naive metric for the comparison of concepts ordered in a partially ordered set and represented as nodes of a directed acyclic graph (e.g., a taxonomy), would be the shortest-path linking the two concept nodes. Based on text analyses, semantic relatedness between units of language (e.g., words, sentences) can also be estimated using statistical means such as a vector space model to correlate words and textual contexts from a suitable text corpus. An extensive survey dedicated to the notion of semantic measures and semantic similarity is proposed in: Semantic Similarity from Natural Language and Ontology Analysis.
In probability theory, random element is a generalization of the concept of random variable to more complicated spaces than the simple real line. The concept was introduced by Maurice Fre chet (1948) who commented that the  development of probability theory and expansion of area of its applications have led to necessity to pass from schemes where (random) outcomes of experiments can be described by number or a finite set of numbers, to schemes where outcomes of experiments represent, for example, vectors, functions, processes, fields, series, transformations, and also sets or collections of sets.  The modern day usage of  random element  frequently assumes the space of values is a topological vector space, often a Banach or Hilbert space with a specified natural sigma algebra of subsets.
In statistics, Lukacs's proportion-sum independence theorem is a result that is used when studying proportions, in particular the Dirichlet distribution. It is named for Eugene Lukacs.
The Jadad scale, sometimes known as Jadad scoring or the Oxford quality scoring system, is a procedure to independently assess the methodological quality of a clinical trial. It is the most widely used such assessment in the world, and as of 2008, its seminal paper has been cited in over 3000 scientific works. The Jadad scale is named after Alejandro Jadad-Bechara, a Colombian physician who worked as a Research Fellow at the Oxford Pain Relief Unit, Nuffield Department of Anaesthetics, at the University of Oxford. Jadad felt that the randomised controlled trial was of great importance for the advancement of medical science, describing it in a 2007 book as "one of the simplest, most powerful and revolutionary forms of research". He and his team outlined their views of the effectiveness of blinding on published studies in a 1996 paper in the Journal of Controlled Clinical Trials. An appendix to the paper described a scale allocating trials a score of between zero (very poor) and five (rigorous).
In the statistical analysis of observational data, propensity score matching (PSM) is a statistical matching technique that attempts to estimate the effect of a treatment, policy, or other intervention by accounting for the covariates that predict receiving the treatment. PSM attempts to reduce the bias due to confounding variables that could be found in an estimate of the treatment effect obtained from simply comparing outcomes among units that received the treatment versus those that did not. The technique was first published by Paul Rosenbaum and Donald Rubin in 1983, and implements the Rubin causal model for observational studies. The possibility of bias arises because the apparent difference in outcome between these two groups of units may depend on characteristics that affected whether or not a unit received a given treatment instead of due to the effect of the treatment per se. In randomized experiments, the randomization enables unbiased estimation of treatment effects; for each covariate, randomization implies that treatment-groups will be balanced on average, by the law of large numbers. Unfortunately, for observational studies, the assignment of treatments to research subjects is typically not random. Matching attempts to mimic randomization by creating a sample of units that received the treatment that is comparable on all observed covariates to a sample of units that did not receive the treatment. For example, one may be interested to know the consequences of smoking or the consequences of going to university. The people 'treated' are simply those the smokers, or the university graduates who in the course of everyday life undergo whatever it is that is being studied by the researcher. In both of these cases it is unfeasible (and perhaps unethical) to randomly assign people to smoking or a university education, so observational studies are required. The treatment effect estimated by simply comparing a particular outcome rate of cancer or life time earnings between those who smoked and did not smoke or attended university and did not attend university would be biased by any factors that predict smoking or university attendance, respectively. PSM attempts to control for these differences to make the groups receiving treatment and not-treatment more comparable.
In probability theory and statistics, the Poisson distribution (French pronunciation [pwas  ]; in English usually / pw  s n/), named after French mathematician Sime on Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume. For instance, an individual keeping track of the amount of mail they receive each day may notice that they receive an average number of 4 letters per day. If receiving any particular piece of mail doesn't affect the arrival times of future pieces of mail, i.e., if pieces of mail from a wide range of sources arrive independently of one another, then a reasonable assumption is that the number of pieces of mail received per day obeys a Poisson distribution. Other examples that may follow a Poisson: the number of phone calls received by a call center per hour, the number of decay events per second from a radioactive source, or the number of pedicabs in queue in a particular street in a given hour of a day.
A kernel smoother is a statistical technique for estimating a real valued function  by using its noisy observations, when no parametric model for this function is known. The estimated function is smooth, and the level of smoothness is set by a single parameter. This technique is most appropriate for low-dimensional (p < 3) data visualization purposes. Actually, the kernel smoother represents the set of irregular data points as a smooth line or surface.
A stem-and-leaf display is a device for presenting quantitative data in a graphical format, similar to a histogram, to assist in visualizing the shape of a distribution. They evolved from Arthur Bowley's work in the early 1900s, and are useful tools in exploratory data analysis. Stemplots became more commonly used in the 1980s after the publication of John Tukey's book on exploratory data analysis in 1977. The popularity during those years is attributable to their use of monospaced (typewriter) typestyles that allowed computer technology of the time to easily produce the graphics. Modern computers' superior graphic capabilities have meant these techniques are less often used. A stem-and-leaf display is often called a stemplot, but the latter term often refers to another chart type. A simple stem plot may refer to plotting a matrix of y values onto a common x axis, and identifying the common x value with a vertical line, and the individual y values with symbols on the line. Unlike histograms, stem-and-leaf displays retain the original data to at least two significant digits, and put the data in order, thereby easing the move to order-based inference and non-parametric statistics. A basic stem-and-leaf display contains two columns separated by a vertical line. The left column contains the stems and the right column contains the leaves.  
The positive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results. The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test; they depend also on the prevalence. The PPV can be derived using Bayes' theorem. Although sometimes used synonymously, a positive predictive value generally refers to what is established by control groups, while a post-test probability refers to a probability for an individual. Still, if the individual's pre-test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal. In information retrieval, the PPV statistic is often called the precision.
"Natural parameter" links here. For the usage of this term in differential geometry, see differential geometry of curves. In probability and statistics, an exponential family is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, on account of some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. The concept of exponential families is credited to E. J. G. Pitman, G. Darmois, and B. O. Koopman in 1935 36. The term exponential class is sometimes used in place of "exponential family". The exponential families include many of the most common distributions, including the normal, exponential, gamma, chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, Wishart, Inverse Wishart and many others. A number of common distributions are exponential families only when certain parameters are considered fixed and known, e.g. binomial (with fixed number of trials), multinomial (with fixed number of trials), and negative binomial (with fixed number of failures). Examples of common distributions that are not exponential families are Student's t, most mixture distributions, and even the family of uniform distributions with unknown bounds. See the section below on examples for more discussion. Consideration of exponential-family distributions provides a general framework for selecting a possible alternative parameterisation of the distribution, in terms of natural parameters, and for defining useful sample statistics, called the natural sufficient statistics of the family. For more information, see below.
Bayesian experimental design provides a general probability-theoretical framework from which other theories on experimental design can be derived. It is based on Bayesian inference to interpret the observations/data acquired during the experiment. This allows accounting for both any prior knowledge on the parameters to be determined as well as uncertainties in observations. The theory of Bayesian experimental design is to a certain extent based on the theory for making optimal decisions under uncertainty. The aim when designing an experiment is to maximize the expected utility of the experiment outcome. The utility is most commonly defined in terms of a measure of the accuracy of the information provided by the experiment (e.g. the Shannon information or the negative variance), but may also involve factors such as the financial cost of performing the experiment. What will be the optimal experiment design depends on the particular utility criterion chosen.
Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involve random objective functions or random constraints, for example. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.
A geographic information system or geographical information system (GIS) is a system designed to capture, store, manipulate, analyze, manage, and present all types of spatial or geographical data. The acronym GIS is sometimes used for geographic information science (GIScience) to refer to the academic discipline that studies geographic information systems and is a large domain within the broader academic discipline of geoinformatics. What goes beyond a GIS is a spatial data infrastructure, a concept that has no such restrictive boundaries. In a general sense, the term describes any information system that integrates, stores, edits, analyzes, shares, and displays geographic information. GIS applications are tools that allow users to create interactive queries (user-created searches), analyze spatial information, edit data in maps, and present the results of all these operations. Geographic information science is the science underlying geographic concepts, applications, and systems. GIS is a broad term that can refer to a number of different technologies, processes, and methods. It is attached to many operations and has many applications related to engineering, planning, management, transport/logistics, insurance, telecommunications, and business. For that reason, GIS and location intelligence applications can be the foundation for many location-enabled services that rely on analysis and visualization. GIS can relate unrelated information by using location as the key index variable. Locations or extents in the Earth space time may be recorded as dates/times of occurrence, and x, y, and z coordinates representing, longitude, latitude, and elevation, respectively. All Earth-based spatial temporal location and extent references should, ideally, be relatable to one another and ultimately to a "real" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry.
In probability theory, Boole's inequality, also known as the union bound, says that for any finite or countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events. Boole's inequality is named after George Boole. Formally, for a countable set of events A1, A2, A3, ..., we have  In measure-theoretic terms, Boole's inequality follows from the fact that a measure (and certainly any probability measure) is  -sub-additive.
In statistics, the Sobel test is a method of testing the significance of a mediation effect. The test is based on the work of Michael E. Sobel, a statistics professor at Columbia University in New York, NY. In mediation, the relationship between the independent variable and the dependent variable is hypothesized to be an indirect effect that exists due to the influence of a third variable (the mediator). As a result when the mediator is included in a regression analysis model with the independent variable, the effect of the independent variable is reduced and the effect of the mediator remains significant. The Sobel test is basically a specialized t test that provides a method to determine whether the reduction in the effect of the independent variable, after including the mediator in the model, is a significant reduction and therefore whether the mediation effect is statistically significant.
The average treatment effect (ATE) is a measure used to compare treatments (or interventions) in randomized experiments, evaluation of policy interventions, and medical trials. The ATE measures the difference in mean (average) outcomes between units assigned to the treatment and units assigned to the control. In a randomized trial (i.e., an experimental study), the average treatment effect can be estimated from a sample using a comparison in mean outcomes for treated and untreated units. However, the ATE is generally understood as a causal parameter (i.e., an estimand or property of a population) that a researcher desires to know, defined without reference to the study design or estimation procedure. Both observational and experimental study designs may enable one to estimate an ATE in a variety of ways.
The Upside-Potential Ratio is a measure of a return of an investment asset relative to the minimal acceptable return. The measurement allows a firm or individual to choose investments which have had relatively good upside performance, per unit of downside risk.  where the returns  have been put into increasing order. Here  is the probability of the return  and  which occurs at  is the minimal acceptable return. In the secondary formula  and . The Upside-Potential Ratio may also be expressed as a ratio of partial moments since  is the first upper moment and  is the second lower partial moment. The measure was developed by Frank A. Sortino.  
Data quality refers to the level of quality of data. There are many definitions of data quality but data are generally considered high quality if, "they are fit for their intended uses in operations, decision making and planning." (Tom Redman<Redman, T.C. (2008). Data driven: Profiting from your most important business asset (p. 56). Boston, Mass.: Harvard Business Press.>). Alternatively, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as data volume increases, the question of internal consistency within data becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose.
In probability theory, the law of total covariance, covariance decomposition formula, or ECCE states that if X, Y, and Z are random variables on the same probability space, and the covariance of X and Y is finite, then  The nomenclature in this article's title parallels the phrase law of total variance. Some writers on probability call this the "conditional covariance formula" or use other names. (The conditional expected values E( X | Z ) and E( Y | Z ) are random variables in their own right, whose values depends on the value of Z. Notice that the conditional expected value of X given the event Z = z is a function of z (this is where adherence to the conventional rigidly case-sensitive notation of probability theory becomes important!). If we write E( X | Z = z) = g(z) then the random variable E( X | Z ) is just g(Z). Similar comments apply to the conditional covariance.)
In statistics, truncation results in values that are limited above or below, resulting in a truncated sample. Truncation is similar to but distinct from the concept of statistical censoring. A truncated sample can be thought of as being equivalent to an underlying sample with all values outside the bounds entirely omitted, with not even a count of those omitted being kept. With statistical censoring, a note would be recorded documenting which bound (upper or lower) had been exceeded and the value of that bound. With truncated sampling, no note is recorded.
Higher-order factor analysis is a statistical method consisting of repeating steps factor analysis   oblique rotation   factor analysis of rotated factors. Its merit is to enable the researcher to see the hierarchical structure of studied phenomena. To interpret the results, one proceeds either by post-multiplying the primary factor pattern matrix by the higher-order factor pattern matrices (Gorsuch, 1983) and perhaps applying a Varimax rotation to the result (Thompson, 1990) or by using a Schmid-Leiman solution (SLS, Schmid & Leiman, 1957, also known as Schmid-Leiman transformation) which attributes the variation from the primary factors to the second-order factors.
The contact process is a model of an interacting particle system. It is a continuous time Markov process with state space , where  is a finite or countable graph, usually Z. The process is usually interpreted as a model for the spread of an infection: if the state of the process at a given time is , then a site  in  is "infected" if  and healthy if . Infected sites become healthy at a constant rate, while healthy sites become infected at a rate proportional to the number infected neighbors. One can generalize the state space to , such is called the multitype contact process. It represents a model when more than one type of infection is competing for space.
The general linear model is a statistical linear model. It may be written as  where Y is a matrix with series of multivariate measurements, X is a matrix that might be a design matrix, B is a matrix containing parameters that are usually to be estimated and U is a matrix containing errors or noise. The errors are usually assumed to be uncorrelated across measurements, and follow a multivariate normal distribution. If the errors do not follow a multivariate normal distribution, generalized linear models may be used to relax assumptions about Y and U. The general linear model incorporates a number of different statistical models: ANOVA, ANCOVA, MANOVA, MANCOVA, ordinary linear regression, t-test and F-test. The general linear model is a generalization of multiple linear regression model to the case of more than one dependent variable. If Y, B, and U were column vectors, the matrix equation above would represent multiple linear regression. Hypothesis tests with the general linear model can be made in two ways: multivariate or as several independent univariate tests. In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are tested independently, i.e., as multiple univariate tests with the same design matrix.
In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags. This function plays an important role in data analyses aimed at identifying the extent of the lag in an autoregressive model. The use of this function was introduced as part of the Box Jenkins approach to time series modelling, where by plotting the partial autocorrelative functions one could determine the appropriate lags p in an AR (p) model or in an extended ARIMA (p,d,q) model.
In probability theory, Donsker's theorem (also known as Donsker's invariance principle, or the functional central limit theorem), named after Monroe D. Donsker, is a functional extension of the central limit theorem. Let  be a sequence of independent and identically distributed (i.i.d.) random variables with mean 0 and variance 1. Let . The stochastic process  is known as a random walk. Define the diffusively rescaled random walk by  The central limit theorem asserts that  converges in distribution to a standard Gaussian random variable  as . Donsker's invariance principle extends this convergence to the whole function . More precisely, in its modern form, Donsker's invariance principle states that: As random variables taking values in the Skorokhod space , the random function  converges in distribution to a standard Brownian motion  as
In probability theory, the theory of large deviations concerns the asymptotic behaviour of remote tails of sequences of probability distributions. While some basic ideas of the theory can be traced to Laplace, the formalization started with insurance mathematics, namely ruin theory with Crame r and Lundberg. A unified formalization of large deviation theory was developed in 1966, in a paper by Varadhan. Large deviations theory formalizes the heuristic ideas of concentration of measures and widely generalizes the notion of convergence of probability measures. Roughly speaking, large deviations theory concerns itself with the exponential decline of the probability measures of certain kinds of extreme or tail events.
A utilization distribution is a probability distribution constructed from data providing the location of an individual in space at different points in time.
Bayesian inference of phylogeny uses a likelihood function to create a quantity called the posterior probability of trees using a model of evolution, based on some prior probabilities, producing the most likely phylogenetic tree for the given data. The Bayesian approach has become popular due to advances in computing speeds and the integration of Markov chain Monte Carlo (MCMC) algorithms. Bayesian inference has a number of applications in molecular phylogenetics and systematics.
ASReml is a statistical software package for fitting linear mixed models using restricted maximum likelihood, a technique commonly used in plant and animal breeding and quantitative genetics as well as other fields. It is notable for its ability to fit very large and complex data sets efficiently, due to its use of the average information algorithm and sparse matrix methods. It was originally developed by Arthur Gilmour. ASREML can be used in Windows, Linux, and as an add-on to S-PLUS and R.
In probability theory and statistics, the noncentral F-distribution is a continuous probability distribution that is a generalization of the (ordinary) F-distribution. It describes the distribution of the quotient (X/n1)/(Y/n2), where the numerator X has a noncentral chi-squared distribution with n1 degrees of freedom and the denominator Y has a central chi-squared distribution n2 degrees of freedom. It is also required that X and Y are statistically independent of each other. It is the distribution of the test statistic in analysis of variance problems when the null hypothesis is false. The noncentral F-distribution is used to find the power function of such a test.
In mathematics and statistics, a stationary process (or strict(ly) stationary process or strong(ly) stationary process) is a stochastic process whose joint probability distribution does not change when shifted in time. Consequently, parameters such as the mean and variance, if they are present, also do not change over time and do not follow any trends. Stationarity is used as a tool in time series analysis, where the raw data is often transformed to become stationary; for example, economic data are often seasonal and/or dependent on a non-stationary price level. An important type of non-stationary process that does not include a trend-like behavior is the cyclostationary process. Note that a "stationary process" is not the same thing as a "process with a stationary distribution". Indeed, there are further possibilities for confusion with the use of "stationary" in the context of stochastic processes; for example a "time-homogeneous" Markov chain is sometimes said to have "stationary transition probabilities". Besides, all stationary Markov random processes are time-homogeneous.
In algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself. That is, the matrix M is idempotent if and only if MM = M. For this product MM to be defined, M must necessarily be a square matrix. Viewed this way, idempotent matrices are idempotent elements of matrix rings.
In mathematics, the Paley Zygmund inequality bounds the probability that a positive random variable is small, in terms of its mean and variance (i.e., its first two moments). The inequality was proved by Raymond Paley and Antoni Zygmund. Theorem: If Z   0 is a random variable with finite variance, and if , then  Proof: First,  The first addend is at most , while the second is at most  by the Cauchy Schwarz inequality. The desired inequality then follows.  
In statistics, a central tendency (or, more commonly, a measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s. The most common measures of central tendency are the arithmetic mean, the median and the mode. A central tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote "the tendency of quantitative data to cluster around some central value."  The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysts may judge whether data has a strong or a weak central tendency based on its dispersion.
The term kernel has several distinct meanings in statistics.
Statistics education is the practice of teaching and learning of statistics, along with the associated scholarly research. Statistics is both a formal science and a practical theory of scientific inquiry, and both aspects are considered in statistics education. Education in statistics has similar concerns as does education in other mathematical sciences, like logic, mathematics, and computer science. At the same time, statistics is concerned with evidence-based reasoning, particularly with the analysis of data. Therefore education in statistics has strong similarities to education in empirical disciplines like psychology and chemistry, in which education is closely tied to "hands-on" experimentation. Mathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities). Statistics courses have been sometimes taught by non-statisticians, against the recommendations of some professional organizations of statisticians and of mathematicians. Statistics education research is an emerging field that grew out of different disciplines and is currently establishing itself as a unique field that is devoted to the improvement of teaching and learning statistics at all educational levels.
LISREL, an acronym for linear structural relations, is a statistical software package used in structural equation modeling (SEM) for manifest and latent variables. It requires a "fairly high level of statistical sophistication".
There are many longstanding unsolved problems in mathematics for which a solution has still not yet been found. The unsolved problems in statistics are generally of a different flavor; according to John Tukey, "difficulties in identifying problems have delayed statistics far more than difficulties in solving problems." A list of "one or two open problems" (in fact 22 of them) was given by David Cox.
Among the kinds of data that national leaders need are the demographic statistics of their population. Records of births, deaths, marriages, immigration and emigration and a regular census of population provide information that is key to making sound decisions about national policy. A useful summary of such data is the population pyramid. It provides data about the sex and age distribution of the population in an accessible graphical format. Another summary is called the life table. For a cohort of persons born in the same year, it traces and projects their life experiences from birth to death. For a given cohort, the proportion expected to survive each year (or decade in an abridged life table) is presented in tabular or graphical form. The ratio of males to females by age indicates the consequences of differing mortality rates on the sexes. Thus, while values above one are common for newborns, the ratio dwindles until it is well below one for the older population.
The Hardy Weinberg principle, also known as the Hardy Weinberg equilibrium, model, theorem, or law, states that allele and genotype frequencies in a population will remain constant from generation to generation in the absence of other evolutionary influences. These influences include mate choice, mutation, selection, genetic drift, gene flow and meiotic drive. Because one or more of these influences are typically present in real populations, the Hardy Weinberg principle describes an ideal condition against which the effects of these influences can be analyzed. In the simplest case of a single locus with two alleles denoted A and a with frequencies f(A) = p and f(a) = q, respectively, the expected genotype frequencies are f(AA) = p2 for the AA homozygotes, f(aa) = q2 for the aa homozygotes, and f(Aa) = 2pq for the heterozygotes. The genotype proportions p2, 2pq, and q2 are called the Hardy Weinberg proportions. Note that the sum of all genotype frequencies of this case is the binomial expansion of the square of the sum of p and q, and such a sum, as it represents the total of all possibilities, must be equal to 1. Therefore, (p + q)2 = p2 + 2pq + q2 = 1. The realistic solution of this equation is q = 1   p. If union of gametes to produce the next generation is random, it can be shown that the new frequency f  satisfies  and . That is, allele frequencies are constant between generations. This principle was named after G. H. Hardy and Wilhelm Weinberg, who first demonstrated it mathematically.
In probability theory, Popoviciu's inequality, named after Tiberiu Popoviciu, is an upper bound on the variance of any bounded probability distribution. Let M and m be upper and lower bounds on the values of any random variable with a particular probability distribution. Then Popoviciu's inequality states:  Sharma et al. have proved an improvement of the Popoviciu's inequality that says that:  Equality holds precisely when half of the probability is concentrated at each of the two bounds. Popoviciu's inequality is weaker than the Bhatia Davis inequality.
In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed. The LLN is important because it "guarantees" stable long-term results for the averages of some random events. For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the parameters of the game. It is important to remember that the LLN only applies (as the name indicates) when a large number of observations are considered. There is no principle that a small number of observations will coincide with the expected value or that a streak of one value will immediately be "balanced" by the others (see the gambler's fallacy).
Monte Carlo methods are used in finance and mathematical finance to value and analyze (complex) instruments, portfolios and investments by simulating the various sources of uncertainty affecting their value, and then determining their average value over the range of resultant outcomes. This is usually done by help of stochastic asset models. The advantage of Monte Carlo methods over other techniques increases as the dimensions (sources of uncertainty) of the problem increase. Monte Carlo methods were first introduced to finance in 1964 by David B. Hertz through his Harvard Business Review article, discussing their application in Corporate Finance. In 1977, Phelim Boyle pioneered the use of simulation in derivative valuation in his seminal Journal of Financial Economics paper. This article discusses typical financial problems in which Monte Carlo methods are used. It also touches on the use of so-called "quasi-random" methods such as the use of Sobol sequences.
A therapeutic effect is a consequence of a medical treatment of any kind, the results of which are judged to be desirable and beneficial. This is true whether the result was expected, unexpected, or even an unintended consequence of the treatment. An adverse effect, on the other hand, is a harmful and undesired effect. What constitutes a therapeutic effect versus a side effect is a matter of both the nature of the situation in which a treatment is used and the goals of treatment. There is no inherent difference between therapeutic and undesired side effects; both responses are behavioral/physiologic changes which occur as a response to the treatment strategy or agent. However, those changes which are viewed as desirable, given the situation, are called therapeutic; those undesirable for the situation are viewed as harmful.
Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s (cf. Bellman 1957). A core body of research on Markov decision processes resulted from Ronald A. Howard's book published in 1960, Dynamic Programming and Markov Processes. They are used in a wide area of disciplines, including robotics, automated control, economics, and manufacturing. More precisely, a Markov Decision Process is a discrete time stochastic control process. At each time step, the process is in some state , and the decision maker may choose any action  that is available in state . The process responds at the next time step by randomly moving into a new state , and giving the decision maker a corresponding reward . The probability that the process moves into its new state  is influenced by the chosen action. Specifically, it is given by the state transition function . Thus, the next state  depends on the current state  and the decision maker's action . But given  and , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP process satisfies the Markov property. Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state and all rewards are the same (e.g., zero), a Markov decision process reduces to a Markov chain.
In time series analysis, a fan chart is a chart that joins a simple line chart for observed past data, by showing ranges for possible values of future data together with a line showing a central estimate or most likely value for the future outcomes. As predictions become increasingly uncertain the further into the future one goes, these forecast ranges spread out, creating distinctive wedge or "fan" shapes, hence the term. Alternative forms of the chart can also include uncertainty for past data, such as preliminary data that is subject to revision. The term "fan chart" was coined by the Bank of England, which has been using these charts and this term since 1997 in its "Inflation Report"  to describe its best prevision of future inflation to the general public. Fan charts have been used extensively in finance and monetary policy, for instance to represent forecasts of inflation.
Bertrand's box paradox is a classic paradox of elementary probability theory. It was first posed by Joseph Bertrand in his Calcul des probabilite s, published in 1889. There are three boxes: a box containing two gold coins, a box containing two silver coins, a box containing one gold coin and a silver coin. The 'paradox' is in the probability, after choosing a box at random and withdrawing one coin at random, if that happens to be a gold coin, of the next coin also being a gold coin. These simple but counterintuitive puzzles are used as a standard example in teaching probability theory. Their solution illustrates some basic principles, including the Kolmogorov axioms.
In probability and statistics, the Yule Simon distribution is a discrete probability distribution named after Udny Yule and Herbert A. Simon. Simon originally called it the Yule distribution. The probability mass function (pmf) of the Yule Simon ( ) distribution is , for integer  and real , where  is the beta function. Equivalently the pmf can be written in terms of the falling factorial as , where  is the gamma function. Thus, if  is an integer, . The parameter  can be estimated using a fixed point algorithm. The probability mass function f has the property that for sufficiently large k we have .  This means that the tail of the Yule Simon distribution is a realization of Zipf's law:  can be used to model, for example, the relative frequency of the th most frequent word in a large collection of text, which according to Zipf's law is inversely proportional to a (typically small) power of .  
The following outline is provided as an overview and guide to the variety of topics included within the subject of statistics: Statistics pertains to the collection, analysis, interpretation, and presentation of data. It is applicable to a wide variety of academic disciplines, from the physical and social sciences to the humanities; it is also used and misused for making informed decisions in all areas of business and government.  
In statistics, adaptive or "variable-bandwidth" kernel density estimation is a form of kernel density estimation in which the size of the kernels used in the estimate are varied depending upon either the location of the samples or the location of the test point. It is a particularly effective technique when the sample space is multi-dimensional.
A run chart, also known as a run-sequence plot is a graph that displays observed data in a time sequence. Often, the data displayed represent some aspect of the output or performance of a manufacturing or other business process. It is therefore a form of line chart.
In queueing theory, a discipline within the mathematical theory of probability, an M/D/1 queue represents the queue length in a system having a single server, where arrivals are determined by a Poisson process and job service times are fixed (deterministic). The model name is written in Kendall's notation. Agner Krarup Erlang first published on this model in 1909, starting the subject of queueing theory. An extension of this model with more than one server is the M/D/c queue.
The relativistic Breit Wigner distribution (after the 1936 nuclear resonance formula of Gregory Breit and Eugene Wigner) is a continuous probability distribution with the following probability density function,  where k is a constant of proportionality, equal to    with    (This equation is written using natural units,   = c = 1.) It is most often used to model resonances (unstable particles) in high-energy physics. In this case, E is the center-of-mass energy that produces the resonance, M is the mass of the resonance, and   is the resonance width (or decay width), related to its mean lifetime according to   = 1/ . (With units included, the formula is   =  / .) The probability of producing the resonance at a given energy E is proportional to f (E), so that a plot of the production rate of the unstable particle as a function of energy traces out the shape of the relativistic Breit Wigner distribution. Note that for values of E off the maximum at M such that |E2 M2| = M , (hence |E M| =  /2 for M  ), the distribution f has attenuated to half its maximum value, which justifies the name for  , width at half-maximum. In the limit of vanishing width,   0, the particle becomes stable as the Lorentzian distribution f sharpens infinitely to 2M  (E2 M2). In general,   can also be a function of E; this dependence is typically only important when   is not small compared to M and the phase space-dependence of the width needs to be taken into account. (For example, in the decay of the rho meson into a pair of pions.) The factor of M 2 that multiplies  2 should also be replaced with E 2 (or E 4/M 2, etc.) when the resonance is wide. The form of the relativistic Breit Wigner distribution arises from the propagator of an unstable particle, which has a denominator of the form p2   M2 + iM . (Here, p2 is the square of the four-momentum carried by that particle in the tree Feynman diagram involved.) The propagator in its rest frame then is proportional to the quantum-mechanical amplitude for the decay utilized to reconstruct that resonance,  The resulting probability distribution is proportional to the absolute square of the amplitude, so then the above relativistic Breit Wigner distribution for the probability density function. The form of this distribution is similar to the solution of the classical equation of motion for a driven harmonic oscillator damped and driven by a sinusoidal external force. It has the standard resonance form of the Lorentz, or Cauchy distribution, but involves relativistic variables s=p 2, here =E 2. The distribution is the solution of the differential equation, analogous to that for the time averaged input power of the above classical forced oscillator, .
Population ecology or autecology is a sub-field of ecology that deals with the dynamics of species populations and how these populations interact with the environment. It is the study of how the population sizes of species change over time and space. The development of population ecology owes much to demography and actuarial life tables. Population ecology is important in conservation biology, especially in the development of population viability analysis (PVA) which makes it possible to predict the long-term probability of a species persisting in a given habitat patch. Although population ecology is a subfield of biology, it provides interesting problems for mathematicians and statisticians who work in population dynamics.
A cluster randomised controlled trial is a type of randomised controlled trial in which groups of subjects (as opposed to individual subjects) are randomised. Cluster randomised controlled trials are also known as cluster randomised trials, group-randomised trials, and place-randomized trials. A 2004 bibliometric study documented an increasing number of publications in the medical literature on cluster randomised controlled trials since the 1980s. Advantages of cluster randomised controlled trials over individually randomised controlled trials include the ability to study interventions that cannot be directed toward selected individuals (e.g., a radio show about lifestyle changes) and the ability to control for "contamination" across individuals (e.g., one individual's changing behaviors may influence another individual to do so). Disadvantages compared with individually randomised controlled trials include greater complexity in design and analysis, and a requirement for more participants to obtain the same statistical power. Specifically, the cluster randomised designs introduce dependence (or clustering) between individual units sampled. An example would be an educational intervention in which schools are randomised to one of several new teaching methods. When comparing differences in outcome achieved under the new methods, researchers must account for the fact that two students sampled from a single school are more likely to be similar (in terms of outcomes) than two students sampled from different schools. Multilevel or similar statistical models are typically used to correct for this non-independence.
Statistics play an important role in summarizing baseball performance and evaluating players in the sport. Since the flow of a baseball game has natural breaks to it, and normally players act individually rather than performing in clusters, the sport lends itself to easy record-keeping and statistics. Statistics have been kept for professional baseball since the creation of the National League and American League, now part of Major League Baseball. Many statistics are also available from outside of Major League Baseball, from leagues such as the National Association of Professional Base Ball Players and the Negro Leagues, although the consistency of whether these records were kept, of the standards with respect to which they were calculated, and of their accuracy varied from league to league.
Multistage sampling refers to sampling plans where the sampling is carried out in stages using smaller and smaller sampling units at each stage. Multistage sampling can be a complex form of cluster sampling... Cluster because sampling is a type of sampling which involves dividing the population into groups (or clusters). Then, one or more clusters are chosen at random and everyone within the chosen cluster is sampled. Using all the sample elements in all the selected clusters may be prohibitively expensive or unnecessary. Under these circumstances, multistage cluster sampling becomes useful. Instead of using all the elements contained in the selected clusters, the researcher randomly selects elements from each cluster. Constructing the clusters is the first stage. Deciding what elements within the cluster to use is the second stage. The technique is used frequently when a complete list of all members of the population does not exist and is inappropriate. In some cases, several levels of cluster selection may be applied before the final sample elements are reached. For example, household surveys conducted by the Australian Bureau of Statistics begin by dividing metropolitan regions into 'collection districts' and selecting some of these collection districts (first stage). The selected collection districts are then divided into blocks, and blocks are chosen from within each selected collection district (second stage). Next, dwellings are listed within each selected block, and some of these dwellings are selected (third stage). This method makes it unnecessary to create a list of every dwelling in the region and necessary only for selected blocks. In remote areas, an additional stage of clustering is used, in order to reduce travel requirements. Although cluster sampling and stratified sampling bear some superficial similarities, they are substantially different. In stratified sampling, a random sample is drawn from all the strata, where in cluster sampling only the selected clusters are studied, either in single- or multi-stage. Advantages Cost and speed that the survey can be done in Convenience of finding the survey sample Normally more accurate than cluster sampling for the same size sample Disadvantages Not as accurate as Simple Random Sample if the sample is the same size More testing is difficult to do
In probability theory and statistics, the Poisson distribution (French pronunciation [pwas  ]; in English usually / pw  s n/), named after French mathematician Sime on Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume. For instance, an individual keeping track of the amount of mail they receive each day may notice that they receive an average number of 4 letters per day. If receiving any particular piece of mail doesn't affect the arrival times of future pieces of mail, i.e., if pieces of mail from a wide range of sources arrive independently of one another, then a reasonable assumption is that the number of pieces of mail received per day obeys a Poisson distribution. Other examples that may follow a Poisson: the number of phone calls received by a call center per hour, the number of decay events per second from a radioactive source, or the number of pedicabs in queue in a particular street in a given hour of a day.
A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The relative position and angle of the axes is typically uninformative. The radar chart is also known as web chart, spider chart, star chart, star plot, cobweb chart, irregular polygon, polar chart, or kiviat diagram.
In statistics, a proxy or proxy variable is a variable that is not in itself directly relevant, but that serves in place of an unobservable or immeasurable variable. In order for a variable to be a good proxy, it must have a close correlation, not necessarily linear, with the variable of interest. This correlation might be either positive or negative.
A statistic is biased if it is calculated in such a way that it is only systematically different from the population parameter of interest. The following lists some types of biases, which can overlap. Selection bias involves individuals being more likely to be selected for study than others, biasing the sample. This can also be termed Berksonian bias.Spectrum bias arises from evaluating diagnostic tests on biased patient samples, leading to an overestimate of the sensitivity and specificity of the test.  The bias of an estimator is the difference between an estimator's expectations and the true value of the parameter being estimated. Omitted-variable bias is the bias that appears in estimates of parameters in a regression analysis when the assumed specification omits an independent variable that should be in the model.  In statistical hypothesis testing, a test is said to be unbiased when the probability of committing a type I error (i.e. false positive) is less than the significance level, and that of getting a true positive (rejecting the null hypothesis when the alternative hypothesis is true) is at least that of the significance level. Detection bias occurs when a phenomenon is more likely to be observed for a particular set of study subjects. For instance, the syndemic involving obesity and diabetes may mean doctors are more likely to look for diabetes in obese patients than in thinner patients, leading to an inflation in diabetes among obese patients because of skewed detection efforts. Funding bias may lead to selection of outcomes, test samples, or test procedures that favor a study's financial sponsor. Reporting bias involves a skew in the availability of data, such that observations of a certain kind are more likely to be reported. Analytical bias arise due to the way that the results are evaluated. Exclusion bias arise due to the systematic exclusion of certain individuals from the study. Attrition bias arises due to a loss of participants e.g. loss to follow up during a study. Recall bias arises due to differences in the accuracy or completeness of participant recollections of past events. e.g. a patient cannot recall how many cigarettes they smoked last week exactly, leading to over-estimation or under-estimation. Observer bias arises when the researcher subconsciously influences the experiment due to cognitive bias where judgement may alter how an experiment is carried out / how results are recorded.
The absolute difference of two real numbers x, y is given by |x   y|, the absolute value of their difference. It describes the distance on the real line between the points corresponding to x and y. It is a special case of the Lp distance for all 1   p     and is the standard metric used for both the set of rational numbers Q and their completion, the set of real numbers R. As with any metric, the metric properties hold: |x   y|   0, since absolute value is always non-negative. |x   y| = 0   if and only if   x = y. |x   y| = |y   x|     (symmetry or commutativity). |x   z|   |x   y| + |y   z|     (triangle inequality); in the case of the absolute difference, equality holds if and only if x   y   z. By contrast, simple subtraction is not non-negative or commutative, but it does obey the second and fourth properties above, since x   y = 0 if and only if x = y, and x   z = (x   y) + (y   z). The absolute difference is used to define other quantities including the relative difference, the L1 norm used in taxicab geometry, and graceful labelings in graph theory. When it is desirable to avoid the absolute value function   for example because it is expensive to compute, or because its derivative is not continuous   it can sometimes be eliminated by the identity |x   y| < |z   w| if and only if (x   y)2 < (z   w)2. This follows since |x   y|2 = (x   y)2 and squaring is monotonic on the nonnegative reals.  
In statistics, a marginal likelihood function, or integrated likelihood, is a likelihood function in which some parameter variables have been marginalized. In the context of Bayesian statistics, it may also be referred to as the evidence or model evidence. Given a set of independent identically distributed data points  where  according to some probability distribution parameterized by  , where   itself is a random variable described by a distribution, i.e.  the marginal likelihood in general asks what the probability  is, where   has been marginalized out (integrated out):  The above definition is phrased in the context of Bayesian statistics. In classical (frequentist) statistics, the concept of marginal likelihood occurs instead in the context of a joint parameter  =( , ), where   is the actual parameter of interest, and   is a non-interesting nuisance parameter. If there exists a probability distribution for  , it is often desirable to consider the likelihood function only in terms of  , by marginalizing out  :  Unfortunately, marginal likelihoods are generally difficult to compute. Exact solutions are known for a small class of distributions, particularly when the marginalized-out parameter is the conjugate prior of the distribution of the data. In other cases, some kind of numerical integration method is needed, either a general method such as Gaussian integration or a Monte Carlo method, or a method specialized to statistical problems such as the Laplace approximation, Gibbs sampling or the EM algorithm. It is also possible to apply the above considerations to a single random variable (data point) x, rather than a set of observations. In a Bayesian context, this is equivalent to the prior predictive distribution of a data point.
In probability theory and statistics, the cumulants  n of a probability distribution are a set of quantities that provide an alternative to the moments of the distribution. The moments determine the cumulants in the sense that any two probability distributions whose moments are identical will have identical cumulants as well, and similarly the cumulants determine the moments. In some cases theoretical treatments of problems in terms of cumulants are simpler than those using moments. Just as for moments, where joint moments are used for collections of random variables, it is possible to define joint cumulants.
In probability theory, the telegraph process is a memoryless continuous-time stochastic process that shows two distinct values. If these are called a and b, the process can be described by the following master equations:  and  The process is also known under the names Kac process , dichotomous random process.
In statistics, a mediation model is one that seeks to identify and explicate the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (also a mediating variable, intermediary variable, or intervening variable). Rather than a direct causal relationship between the independent variable and the dependent variable, a mediation model proposes that the independent variable influences the (non-observable) mediator variable, which in turn influences the dependent variable. Thus, the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables. Mediation analyses are employed to understand a known relationship by exploring the underlying mechanism or process by which one variable influences another variable through a mediator variable. Mediation analysis facilitates a better understanding of the relationship between the independent and dependent variables when the variables appear to not have a definite connection. They are studied by means of operational definitions and have no existence apart.
Significance analysis of microarrays (SAM) is a statistical technique, established in 2001 by Virginia Tusher, Robert Tibshirani and Gilbert Chu, for determining whether changes in gene expression are statistically significant. With the advent of DNA microarrays, it is now possible to measure the expression of thousands of genes in a single hybridization experiment. The data generated is considerable, and a method for sorting out what is significant and what isn't is essential. SAM is distributed by Stanford University in an R-package. SAM identifies statistically significant genes by carrying out gene specific t-tests and computes a statistic dj for each gene j, which measures the strength of the relationship between gene expression and a response variable. This analysis uses non-parametric statistics, since the data may not follow a normal distribution. The response variable describes and groups the data based on experimental conditions. In this method, repeated permutations of the data are used to determine if the expression of any gene is significant related to the response. The use of permutation-based analysis accounts for correlations in genes and avoids parametric assumptions about the distribution of individual genes. This is an advantage over other techniques (e.g., ANOVA and Bonferroni), which assume equal variance and/or independence of genes.
In epidemiology, the attack rate is the biostatistical measure of frequency of morbidity, or speed of spread, in an at risk population. It is used in hypothetical predictions and during actual outbreaks of disease. An at risk population is defined as one that has no immunity to the attacking pathogen which can be either a novel pathogen or an established pathogen. It is used to project the number of victims to expect during an epidemic. This aids in marshalling resources for delivery of medical care as well as production of vaccines and/or anti-viral and anti-bacterial medicines. The rate is arrived at by taking the number of new cases in the population at risk and dividing by the number of persons at risk in the population.  Rates are determined from the beginning of the outbreak to its end. The term should probably not be described as a rate because its time dimension is uncertain. While the duration of an epidemic can be predicted given other variables such as early intervention, it cannot be known in absolute terms. In epidemiology, a rate requires a defined unit change (in this instance, time) over which the rate applies. For this reason, it is often referred to as an attack ratio.
Many probability distributions are so important in theory or applications that they have been given specific names.
A wait list control group, also called a wait list comparison, is a group of participants included in an outcome study that is assigned to a waiting list and receives intervention after the active treatment group. This control group serves as an untreated comparison group during the study, but eventually goes on to receive treatment at a later date. Wait list control groups are often used when it would be unethical to deny participants access to treatment, provided the wait is still shorter than that for routine services.
The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSD is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent.
In statistics, the Breusch Pagan test, developed in 1979 by Trevor Breusch and Adrian Pagan, is used to test for heteroskedasticity in a linear regression model. It was independently suggested with some extension by R. Dennis Cook and Sanford Weisberg in 1983. It tests whether the estimated variance of the residuals from a regression are dependent on the values of the independent variables. In that case, heteroskedasticity is present. Suppose that we estimate the regression model  and obtain from this fitted model a set of values for , the residuals. Ordinary least squares constrains these so that their mean is 0 and so, given the assumption that their variance does not depend on the independent variables, an estimate of this variance can be obtained from the average of the squared values of the residuals. If the assumption is not held to be true, a simple model might be that the variance is linearly related to independent variables. Such a model can be examined by regressing the squared residuals on the independent variables, using an auxiliary regression equation of the form  This is the basis of the Breusch Pagan test. If an F-test confirms that the independent variables are jointly significant then the null hypothesis of homoskedasticity can be rejected. The Breusch Pagan test tests for conditional heteroskedasticity. It is a chi-squared test: the test statistic is n 2 with k degrees of freedom. It tests the null hypothesis of homoskedasticity. If the Chi Squared value is significant with p-value below an appropriate threshold (e.g. p<0.05) then the null hypothesis of homoskedasticity is rejected and heteroskedasticity assumed. If the Breusch Pagan test shows that there is conditional heteroskedasticity, the original regression can be corrected by using the Hansen method, using robust standard errors, or re-thinking the regression equation by changing and/or transforming independent variables.
In statistics and econometrics, the term panel data refers to multi-dimensional data frequently involving measurements over time. Panel data contain observations of multiple phenomena obtained over multiple time periods for the same firms or individuals. In biostatistics, the term longitudinal data is often used instead, wherein a subject or cluster constitutes a panel member or individual in a longitudinal study. Time series and cross-sectional data can be thought of as special cases of panel data that are in one dimension only (one panel member or individual for the former, one time point for the latter).
Many animals, including humans, tend to live in groups, herds, flocks, bands, packs, shoals, or colonies (hereafter: groups) of conspecific individuals. The size of these groups, as expressed by the number of participant individuals, is an important aspect of their social environment. Group size tend to be highly variable even within the same species, thus we often need statistical measures to quantify group size and statistical tests to compare these measures between two or more samples. Group size measures are notoriously hard to handle statistically since groups sizes typically follow an aggregated (right-skewed) distribution: most groups are small, few are large, and a very few are very large. Statistical measures of group size roughly fall into two categories.
In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions. According to the principle of maximum entropy, if nothing is known about a distribution except that it belongs to a certain class (usually defined in terms of specified properties or measures), then the distribution with the largest entropy should be chosen as the least-informative default. The motivation is twofold: first, maximizing entropy minimizes the amount of prior information built into the distribution; second, many physical systems tend to move towards maximal entropy configurations over time.
In economics, hedonic regression or hedonic demand theory is a revealed preference method of estimating demand or value. It decomposes the item being researched into its constituent characteristics, and obtains estimates of the contributory value of each characteristic. This requires that the composite good being valued can be reduced to its constituent parts and that the market values those constituent parts. Hedonic models are most commonly estimated using regression analysis, although more generalized models, such as sales adjustment grids, are special cases of hedonic models. An attribute vector, which may be a dummy or panel variable, is assigned to each characteristic or group of characteristics. Hedonic models can accommodate non-linearity, variable interaction, or other complex valuation situations. Hedonic models are commonly used in real estate appraisal, real estate economics, and Consumer Price Index (CPI) calculations. In CPI calculations hedonic regression is used to control the effect of changes in product quality. Price changes that are due to substitution effects are subject to hedonic quality adjustments.
In epidemiology, the standardized mortality ratio or SMR, is a quantity, expressed as either a ratio or percentage quantifying the increase or decrease in mortality of a study cohort with respect to the general population.
A line chart or line graph is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time   a time series   thus the line is often drawn chronologically. In these cases they are known as run charts.
Littlewood's law states that a person can expect to experience an event with odds of one in a million (defined by the law as a "miracle") at the rate of about one per month.
Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus "error" terms. The information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis originated in psychometrics and is used in behavioral sciences, social sciences, marketing, product management, operations research, and other fields that deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. Factor analysis is related to principal component analysis (PCA), but the two are not identical. There has been significant controversy in the field over differences between the two techniques (see section on exploratory factor analysis versus principal components analysis) below. Clearly though, PCA is a more basic version of exploratory factor analysis (EFA) that was developed in the early days prior to the advent of high-speed computers. From the point of view of exploratory analysis, the eigenvalues of PCA are inflated component loadings, i.e., contaminated with error variance.
Ito  calculus, named after Kiyoshi Ito , extends the methods of calculus to stochastic processes such as Brownian motion (see Wiener process). It has important applications in mathematical finance and stochastic differential equations. The central concept is the Ito  stochastic integral, a stochastic generalization of the Riemann Stieltjes integral in analysis. The integrands and the integrators are now stochastic processes:  where H is a locally square-integrable process adapted to the filtration generated by X (Revuz & Yor 1999, Chapter IV), which is a Brownian motion or, more generally, a semimartingale. The result of the integration is then another stochastic process. Concretely, the integral from 0 to any particular t is a random variable, defined as a limit of a certain sequence of random variables. The paths of Brownian motion fail to satisfy the requirements to be able to apply the standard techniques of calculus. So with the integrand a stochastic process, the Ito  stochastic integral amounts to an integral with respect to a function which is not differentiable at any point and has infinite variation over every time interval. The main insight is that the integral can be defined as long as the integrand H is adapted, which loosely speaking means that its value at time t can only depend on information available up until this time. Roughly speaking, one chooses a sequence of partitions of the interval from 0 to t and construct Riemann sums. Every time we are computing a Riemann sum, we are using a particular instantiation of the integrator. It is crucial which point in each of the small intervals is used to compute the value of the function. The limit then is taken in probability as the mesh of the partition is going to zero. Numerous technical details have to be taken care of to show that this limit exists and is independent of the particular sequence of partitions. Typically, the left end of the interval is used. Important results of Ito  calculus include the integration by parts formula and Ito 's lemma, which is a change of variables formula. These differ from the formulas of standard calculus, due to quadratic variation terms. In mathematical finance, the described evaluation strategy of the integral is conceptualized as that we are first deciding what to do, then observing the change in the prices. The integrand is how much stock we hold, the integrator represents the movement of the prices, and the integral is how much money we have in total including what our stock is worth, at any given moment. The prices of stocks and other traded financial assets can be modeled by stochastic processes such as Brownian motion or, more often, geometric Brownian motion (see Black Scholes). Then, the Ito  stochastic integral represents the payoff of a continuous-time trading strategy consisting of holding an amount Ht of the stock at time t. In this situation, the condition that H is adapted corresponds to the necessary restriction that the trading strategy can only make use of the available information at any time. This prevents the possibility of unlimited gains through high-frequency trading: buying the stock just before each uptick in the market and selling before each downtick. Similarly, the condition that H is adapted implies that the stochastic integral will not diverge when calculated as a limit of Riemann sums (Revuz & Yor 1999, Chapter IV).
The discrete phase-type distribution is a probability distribution that results from a system of one or more inter-related geometric distributions occurring in sequence, or phases. The sequence in which each of the phases occur may itself be a stochastic process. The distribution can be represented by a random variable describing the time until absorption of an absorbing Markov chain with one absorbing state. Each of the states of the Markov chain represents one of the phases. It has continuous time equivalent in the phase-type distribution.  
See also: Conjoint analysis (in marketing), Conjoint analysis (in healthcare), IDDEA, Rule Developing Experimentation, Value based pricing. Conjoint analysis, also called multi-attribute compositional models or stated preference analysis, is a statistical technique that originated in mathematical psychology. It is used in surveys developed in applied sciences, often on behalf of marketing, product management, and operations research. It is not to be confused with the theory of conjoint measurement. Conjoint analysis is a particular application of regression analysis. There is no precise statistical definition of it. Usually two or three of the following properties are applicable: data are collected among multiple individuals (respondents) whereas there are multiple data points for each individual, which makes it a layered model the dependent variable reflects a choice or trade-off situation the independent variables are categorical, thus coded as binary numbers (0,1)
In survey sampling, total survey error includes all forms of survey error including sampling variability, interviewer effects, frame errors, response bias, and non-response bias. Total survey error is discussed in detail in many sources including Salant and Dillman.
In science and engineering, a semi-log graph or semi-log plot is a way of visualizing data that are related according to an exponential relationship. One axis is plotted on a logarithmic scale. This kind of plot is useful when one of the variables being plotted covers a large range of values and the other has only a restricted range   the advantage being that it can bring out features in the data that would not easily be seen if both variables had been plotted linearly. All equations of the form  form straight lines when plotted semi-logarithmically, since taking logs of both sides gives  This can easily be seen as a line in slope-intercept form with  as the slope and  as the vertical intercept. To facilitate use with logarithmic tables, one usually takes logs to base 10 or e, or sometimes base 2:  The term log-lin is used to describe a semi-log plot with a logarithmic scale on the y-axis, and a linear scale on the x-axis. Likewise, a lin-log plot uses a logarithmic scale on the x-axis, and a linear scale on the y-axis. Note that the naming is output-input (y-x), the opposite order from (x, y). On a semi-log plot the spacing of the scale on the y-axis (or x-axis) is proportional to the logarithm of the number, not the number itself. It is equivalent to converting the y values (or x values) to their log, and plotting the data on lin-lin scales. A log-log plot uses the logarithmic scale for both axes, and hence is not a semi-log plot.
Probabilistic design is a discipline within engineering design. It deals primarily with the consideration of the effects of random variability upon the performance of an engineering system during the design phase. Typically, these effects are related to quality and reliability. Thus, probabilistic design is a tool that is mostly used in areas that are concerned with quality and reliability. For example, product design, quality control, systems engineering, machine design, civil engineering (particularly useful in limit state design) and manufacturing. It differs from the classical approach to design by assuming a small probability of failure instead of using the safety factor.
In probability theory, the probability distribution of the sum of two or more independent random variables is the convolution of their individual distributions. The term is motivated by the fact that the probability mass function or probability density function of a sum of random variables is the convolution of their corresponding probability mass functions or probability density functions respectively. Many well known distributions have simple convolutions. The following is a list of these convolutions. Each statement is of the form  where  are independent and identically distributed random variables. In place of  and  the names of the corresponding distributions and their parameters have been indicated.
The sample mean or empirical mean and the sample covariance are statistics computed from a collection (the sample) of data on one or more random variables. The sample mean and sample covariance are estimators of the population mean and population covariance, where the term population refers to the set from which the sample was taken. The sample mean is a vector each of whose elements is the sample mean of one of the random variables   that is, each of whose elements is the arithmetic average of the observed values of one of the variables. The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables. If only one variable has had values observed, then the sample mean is a single number (the arithmetic average of the observed values of that variable) and the sample covariance matrix is also simply a single value (a 1x1 matrix containing a single number, the sample variance of the observed values of that variable). Due to their ease of calculation and other desirable characteristics, the sample mean and sample covariance are widely used in statistics and applications to numerically represent the location and dispersion, respectively, of a distribution.
In filtering theory the Kushner equation (after Harold Kushner) is an equation for the conditional probability density of the state of a stochastic non-linear dynamical system, given noisy measurements of the state. It therefore provides the solution of the nonlinear filtering problem in estimation theory. The equation is sometimes referred to as the Stratonovich Kushner (or Kushner Stratonovich) equation. However, the correct equation in terms of Ito  calculus was first derived by Kushner although a more heuristic Stratonovich version of it appeared already in Stratonovich's works in late fifties. However, the derivation in terms of Ito  calculus is due to Richard Bucy.
In statistics, Self-Exciting Threshold AutoRegressive (SETAR) models are typically applied to time series data as an extension of autoregressive models, in order to allow for higher degree of flexibility in model parameters through a regime switching behaviour. Given a time series of data xt, the SETAR model is a tool for understanding and, perhaps, predicting future values in this series, assuming that the behaviour of the series changes once the series enters a different regime. The switch from one regime to another depends on the past values of the x series (hence the Self-Exciting portion of the name). The model consists of k autoregressive (AR) parts, each for a different regime. The model is usually referred to as the SETAR(k, p) model where k is the number of regimes and p is the order of the autoregressive part (since those can differ between regimes, the p portion is sometimes dropped and models are denoted simply as SETAR(k).
The concept of a random sequence is essential in probability theory and statistics. The concept generally relies on the notion of a sequence of random variables and many statistical discussions begin with the words "let X1,...,Xn be independent random variables...". Yet as D. H. Lehmer stated in 1951: "A random sequence is a vague notion... in which each term is unpredictable to the uninitiated and whose digits pass a certain number of tests traditional with statisticians". Axiomatic probability theory deliberately avoids a definition of a random sequence. Traditional probability theory does not state if a specific sequence is random, but generally proceeds to discuss the properties of random variables and stochastic sequences assuming some definition of randomness. The Bourbaki school considered the statement "let us consider a random sequence" an abuse of language. During the 20th century various technical approaches to defining random sequences were developed and now three distinct paradigms can be identified.
A natural experiment is an empirical study in which individuals (or clusters of individuals) exposed to the experimental and control conditions are determined by nature or by other factors outside the control of the investigators, yet the process governing the exposures arguably resembles random assignment. Thus, natural experiments are observational studies and are not controlled in the traditional sense of a randomized experiment. Natural experiments are most useful when there has been a clearly defined exposure involving a well defined subpopulation (and the absence of exposure in a similar subpopulation) such that changes in outcomes may be plausibly attributed to the exposure. In this sense the difference between a natural experiment and a non-experimental observational study is that the former includes a comparison of conditions that pave the way for causal inference, while the latter does not. Natural experiments are employed as study designs when controlled experimentation is extremely difficult to implement or unethical, such as in several research areas addressed by epidemiology (e.g., evaluating the health impact of varying degrees of exposure to ionizing radiation in people living near Hiroshima at the time of the atomic blast) and economics (e.g., estimating the economic return on amount of schooling in US adults).  
In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent yes/no experiments, each of which yields success with probability p. A success/failure experiment is also called a Bernoulli experiment or Bernoulli trial; when n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance. The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for N much larger than n, the binomial distribution is a good approximation, and widely used.
In statistics, the generalized Dirichlet distribution (GD) is a generalization of the Dirichlet distribution with a more general covariance structure and almost twice the number of parameters. Random variables with a GD distribution are not completely neutral . The density function of  is  where we define . Here  denotes the Beta function. This reduces to the standard Dirichlet distribution if  for  ( is arbitrary). For example, if k=4, then the density function of  is  where  and . Connor and Mosimann define the PDF as they did for the following reason. Define random variables  with . Then  have the generalized Dirichlet distribution as parametrized above, if the  are iid beta with parameters , .  
In probability theory and statistics, the zeta distribution is a discrete probability distribution. If X is a zeta-distributed random variable with parameter s, then the probability that X takes the integer value k is given by the probability mass function  where  (s) is the Riemann zeta function (which is undefined for s = 1). The multiplicities of distinct prime factors of X are independent random variables. The Riemann zeta function being the sum of all terms  for positive integer k, it appears thus as the normalization of the Zipf distribution. Indeed the terms "Zipf distribution" and the "zeta distribution" are often used interchangeably. But note that while the Zeta distribution is a probability distribution by itself, it is not associated to the Zipf's law with same exponent. See also Yule Simon distribution  
In statistics, minimum chi-square estimation is a method of estimation of unobserved quantities based on observed data. In certain chi-square tests, one rejects a null hypothesis about a population distribution if a specified test statistic is too large, when that statistic would have approximately a chi-square distribution if the null hypothesis is true. In minimum chi-square estimation, one finds the values of parameters that make that test statistic as small as possible. Among the consequences of its use is that the test statistic actually does have approximately a chi-square distribution when the sample size is large. Generally, one reduces by 1 the number of degrees of freedom for each parameter estimated by this method.
The Encyclopedia of Statistical Sciences is an encyclopaedia of statistics published by John Wiley & Sons. The first edition, in nine volumes, was edited by Norman Lloyd Johnson and Samuel Kotz and appeared in 1982. The second edition, in 16 volumes, was published in 2006. Samuel Kotz was the senior editor.
In probability theory and mathematical statistics, the law of total cumulance is a generalization to cumulants of the law of total probability, the law of total expectation, and the law of total variance. It has applications in the analysis of time series. It was introduced by David Brillinger. It is most transparent when stated in its most general form, for joint cumulants, rather than for cumulants of a specified order for just one random variable. In general, we have  where  (X1, ..., Xn) is the joint cumulant of n random variables X1, ..., Xn, and the sum is over all partitions  of the set { 1, ..., n } of indices, and "B    " means B runs through the whole list of "blocks" of the partition  , and  (Xi : i   B | Y) is a conditional cumulant given the value of the random variable Y. It is therefore a random variable in its own right a function of the random variable Y.
In psychometrics, predictive validity is the extent to which a score on a scale or test predicts scores on some criterion measure. For example, the validity of a cognitive test for job performance is the correlation between test scores and, for example, supervisor performance ratings. Such a cognitive test would have predictive validity if the observed correlation were statistically significant. Predictive validity shares similarities with concurrent validity in that both are generally measured as correlations between a test and some criterion measure. In a study of concurrent validity the test is administered at the same time as the criterion is collected. This is a common method of developing validity evidence for employment tests: A test is administered to incumbent employees, then a rating of those employees' job performance is, or has already been, obtained independently of the test (often, as noted above, in the form of a supervisor rating). Note the possibility for restriction of range both in test scores and performance scores: The incumbent employees are likely to be a more homogeneous and higher performing group than the applicant pool at large. In a strict study of predictive validity, the test scores are collected first; then at some later time the criterion measure is collected. For predictive validity, the example is slightly different: Tests are administered, perhaps to job applicants, and then after those individuals work in the job for a year, their test scores are correlated with their first year job performance scores. Another relevant example is SAT scores: These are validated by collecting the scores during the examinee's senior year and high school and then waiting a year (or more) to correlate the scores with their first year college grade point average. Thus predictive validity provides somewhat more useful data about test validity because it has greater fidelity to the real situation in which the test will be used. After all, most tests are administered to find out something about future behavior. As with many aspects of social science, the magnitude of the correlations obtained from predictive validity studies is usually not high. A typical predictive validity for an employment test might obtain a correlation in the neighborhood of r=.35. Higher values are occasionally seen and lower values are very common. Nonetheless the utility (that is the benefit obtained by making decisions using the test) provided by a test with a correlation of .35 can be quite substantial. More information, and an explanation of the relationship between variance and predictive validity, can be found here.
In statistics, a unit root test tests whether a time series variable is non-stationary using an autoregressive model. A well-known test that is valid in large samples is the augmented Dickey Fuller test. The optimal finite sample tests for a unit root in autoregressive models were developed by Denis Sargan and Alok Bhargava. Another test is the Phillips Perron test. These tests use the existence of a unit root as the null hypothesis.
In probability theory and statistics, the geometric distribution is either of two discrete probability distributions: The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ...} The probability distribution of the number Y = X   1 of failures before the first success, supported on the set { 0, 1, 2, 3, ... } Which of these one calls "the" geometric distribution is a matter of convention and convenience. These two different geometric distributions should not be confused with each other. Often, the name shifted geometric distribution is adopted for the former one (distribution of the number X); however, to avoid ambiguity, it is considered wise to indicate which is intended, by mentioning the support explicitly. It s the probability that the first occurrence of success requires k number of independent trials, each with success probability p. If the probability of success on each trial is p, then the probability that the kth trial (out of k trials) is the first success is  for k = 1, 2, 3, .... The above form of geometric distribution is used for modeling the number of trials up to and including the first success. By contrast, the following form of the geometric distribution is used for modeling the number of failures until the first success:  for k = 0, 1, 2, 3, .... In either case, the sequence of probabilities is a geometric sequence. For example, suppose an ordinary die is thrown repeatedly until the first time a "1" appears. The probability distribution of the number of times it is thrown is supported on the infinite set { 1, 2, 3, ... } and is a geometric distribution with p = 1/6.
A stochastic investment model tries to forecast how returns and prices on different assets or asset classes, (e. g. equities or bonds) vary over time. Stochastic models are not applied for making point estimation rather interval estimation and they use different stochastic processes. Investment models can be classified into single-asset and multi-asset models. They are often used for actuarial work and financial planning to allow optimization in asset allocation or asset-liability-management (ALM).
In statistics, generalized least squares (GLS) is a technique for estimating the unknown parameters in a linear regression model. GLS can be used to perform linear regression when there is a certain degree of correlation between the residuals in a regression model. In these cases, ordinary least squares and weighted least squares can be statistically inefficient, or even give misleading inferences. GLS was first described by Alexander Aitken in 1934.
In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference. Important special cases of the order statistics are the minimum and maximum value of a sample, and (with some qualifications discussed below) the sample median and other sample quantiles. When using probability theory to analyze order statistics of random samples from a continuous distribution, the cumulative distribution function is used to reduce the analysis to the case of order statistics of the uniform distribution.
In statistics, when a usual one-way ANOVA is performed, it is assumed that the group variances are statistically equal. If this assumption is not valid, then the resulting F-test is invalid. The Brown Forsythe test is a statistical test for the equality of group variances based on performing an ANOVA on a transformation of the response variable. The Brown Forsythe test statistic is the F statistic resulting from an ordinary one-way analysis of variance on the absolute deviations from the median.
A random r-regular graph is a graph selected from , which denotes the probability space of all r-regular graphs on n vertices, where 3   r < n and nr is even. It is therefore a particular kind of random graph, but the regularity restriction significantly alters the properties that will hold, since most graphs are not regular.
The Hubbert curve is an approximation of the production rate of a resource over time. It is a symmetric logistic distribution curve, often confused with the "normal" gaussian function. It first appeared in "Nuclear Energy and the Fossil Fuels," geologist M. King Hubbert's 1956 presentation to the American Petroleum Institute, as an idealized symmetric curve, during his tenure at the Shell Oil Company. It has gained a high degree of popularity in the scientific community for predicting the depletion of various natural resources. The curve is the main component of Hubbert peak theory, which has led to the rise of peak oil concerns. Basing his calculations on the peak of oil well discovery in 1948, Hubbert used his model in 1956 to create a curve which accurately predicted that oil production in the contiguous United States would peak around 1970.
Randomness tests (or tests for randomness), in data evaluation, are used to analyze the distribution of a set of data to see if it is random (patternless). In stochastic modeling, as in some computer simulations, the hoped-for randomness of potential input data can be verified, by a formal test for randomness, to show that the data are valid for use in simulation runs. In some cases, data reveals an obvious non-random pattern, as with so-called "runs in the data" (such as expecting random 0 9 but finding "4 3 2 1 0 4 3 2 1..." and rarely going above 4). If a selected set of data fails the tests, then parameters can be changed or other randomized data can be used which does pass the tests for randomness. There are many practical measures of randomness for a binary sequence. These include measures based on statistical tests, transforms, and complexity or a mixture of these.
The Black Scholes / bl k   o lz/ or Black Scholes Merton model is a mathematical model of a financial market containing derivative investment instruments. From the model, one can deduce the Black Scholes formula, which gives a theoretical estimate of the price of European-style options. The formula led to a boom in options trading and legitimised scientifically the activities of the Chicago Board Options Exchange and other options markets around the world. lt is widely used, although often with adjustments and corrections, by options market participants. Many empirical tests have shown that the Black Scholes price is "fairly close" to the observed prices, although there are well-known discrepancies such as the "option smile". The Black Scholes model was first published by Fischer Black and Myron Scholes in their 1973 paper, "The Pricing of Options and Corporate Liabilities", published in the Journal of Political Economy. They derived a partial differential equation, now called the Black Scholes equation, which estimates the price of the option over time. The key idea behind the model is to hedge the option by buying and selling the underlying asset in just the right way and, as a consequence, to eliminate risk. This type of hedging is called delta hedging and is the basis of more complicated hedging strategies such as those engaged in by investment banks and hedge funds. Robert C. Merton was the first to publish a paper expanding the mathematical understanding of the options pricing model, and coined the term "Black Scholes options pricing model". Merton and Scholes received the 1997 Nobel Memorial Prize in Economic Sciences for their work. Though ineligible for the prize because of his death in 1995, Black was mentioned as a contributor by the Swedish Academy. The model's assumptions have been relaxed and generalized in many directions, leading to a plethora of models that are currently used in derivative pricing and risk management. It is the insights of the model, as exemplified in the Black-Scholes formula, that are frequently used by market participants, as distinguished from the actual prices. These insights include no-arbitrage bounds and risk-neutral pricing. The Black-Scholes equation, a partial differential equation that governs the price of the option, is also important as it enables pricing when an explicit formula is not possible. The Black Scholes formula has only one parameter that cannot be observed in the market: the average future volatility of the underlying asset. Since the formula is increasing in this parameter, it can be inverted to produce a "volatility surface" that is then used to calibrate other models, e.g. for OTC derivatives.
The innovations vector or residual vector is the difference between the measurement vector and the predicted measurement vector. Each difference represents the deviation of the observed random variable from the predicted response. The innovation vector is often used to check the validity of a model.
In statistics, Bartlett's test (see Snedecor and Cochran, 1989) is used to test if k samples are from populations with equal variances. Equal variances across samples is called homoscedasticity or homogeneity of variances. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The Bartlett test can be used to verify that assumption. Bartlett's test is sensitive to departures from normality. That is, if the samples come from non-normal distributions, then Bartlett's test may simply be testing for non-normality. Levene's test and the Brown Forsythe test are alternatives to the Bartlett test that are less sensitive to departures from normality. The test is named after Maurice Stevenson Bartlett.
Biostatistics (or biometry) is the application of statistics to a wide range of topics in biology. The science of biostatistics encompasses the design of biological experiments, especially in medicine, pharmacy, agriculture and fishery; the collection, summarization, and analysis of data from those experiments; and the interpretation of, and inference from, the results. A major branch of this is medical biostatistics, which is exclusively concerned with medicine and health.
Official statistics are statistics published by government agencies or other public bodies such as international organizations. They provide quantitative or qualitative information on all major areas of citizens' lives, such as economic and social development, living conditions, health, education, and the environment. During the 16th and 17th centuries, statistics were a method for counting and listing populations and State resources. The term statistics comes from the New Latin statisticum collegium (council of state) and refers to science of the state. According to the Organization for Economic Cooperation and Development, official statistics are statistics disseminated by the national statistical system, excepting those that are explicitly not to be official". Of course, governmental agencies at all levels, including municipal, county, and state administrations, may generate and disseminate official statistics. This broader possibility is accommodated by later definitions. For example: "Almost every country in the world has one or more government agencies (usually national institutes) that supply decision-makers and other users including the general public and the research community with a continuing flow of information (...). This bulk of data is usually called official statistics. Official statistics should be objective and easily accessible and produced on a continuing basis so that measurement of change is possible." Official statistics result from the collection and processing of data into statistical information by a government institution or international organisation. They are then disseminated to help users develop their knowledge about a particular topic or geographical area, make comparisons between countries or understand changes over time. Official statistics make information on economic and social development accessible to the public, allowing the impact of government policies to be assessed, thus improving accountability.
National accounts or national account systems (NAS) are the implementation of complete and consistent accounting techniques for measuring the economic activity of a nation. These include detailed underlying measures that rely on double-entry accounting. By design, such accounting makes the totals on both sides of an account equal even though they each measure different characteristics, for example production and the income from it. As a method, the subject is termed national accounting or, more generally, social accounting. Stated otherwise, national accounts as systems may be distinguished from the economic data associated with those systems. While sharing many common principles with business accounting, national accounts are based on economic concepts. One conceptual construct for representing flows of all economic transactions that take place in an economy is a social accounting matrix with accounts in each respective row-column entry. National accounting has developed in tandem with macroeconomics from the 1930s with its relation of aggregate demand to total output through interaction of such broad expenditure categories as consumption and investment. Economic data from national accounts are also used for empirical analysis of economic growth and development.
A field of applied statistics, survey methodology studies the sampling of individual units from a population and the associated survey data collection techniques, such as questionnaire construction and methods for improving the number and accuracy of responses to surveys. Statistical surveys are undertaken with a view towards making statistical inferences about the population being studied, and this depends strongly on the survey questions used. Polls about public opinion, public health surveys, market research surveys, government surveys and censuses are all examples of quantitative research that use contemporary survey methodology to answer questions about a population. Although censuses do not include a "sample", they do include other aspects of survey methodology, like questionnaires, interviewers, and nonresponse follow-up techniques. Surveys provide important information for all kinds of public information and research fields, e.g., marketing research, psychology, health professionals and sociology.  
In mathematics, progressive measurability is a property in the theory of stochastic processes. A progressively measurable process, while defined quite technically, is important because it implies the stopped process is measurable. Being progressively measurable is a strictly stronger property than the notion of being an adapted process. Progressively measurable processes are important in the theory of Ito  integrals.
Queueing theory is the mathematical study of waiting lines, or queues. In queueing theory a model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service. Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing and the design of factories, shops, offices and hospitals.
The iterative proportional fitting procedure (IPFP, also known as biproportional fitting in statistics, RAS algorithm in economics and matrix raking or matrix scaling in computer science) is an iterative algorithm for estimating cell values of a contingency table such that the marginal totals remain fixed and the estimated table decomposes into an outer product. First introduced by Deming and Stephan in 1940 (they proposed IPFP as an algorithm leading to a minimizer of the Pearson X-squared statistic, which it does not, and even failed to prove convergence), it has seen various extensions and related research. A rigorous proof of convergence by means of differential geometry is due to Fienberg (1970). He interpreted the family of contingency tables of constant crossproduct ratios as a particular (IJ   1)-dimensional manifold of constant interaction and showed that the IPFP is a fixed-point iteration on that manifold. Nevertheless, he assumed strictly positive observations. Generalization to tables with zero entries is still considered a hard and only partly solved problem. An exhaustive treatment of the algorithm and its mathematical foundations can be found in the book of Bishop et al. (1975). The first general proof of convergence, built on non-trivial measure theoretic theorems and entropy minimization, is due to Csisza r (1975). Relatively new results on convergence and error behavior have been published by Pukelsheim and Simeone (2009) . They proved simple necessary and sufficient conditions for the convergence of the IPFP for arbitrary two-way tables (i.e. tables with zero entries) by analysing an -error function. Other general algorithms can be modified to yield the same limit as the IPFP, for instance the Newton Raphson method and the EM algorithm. In most cases, IPFP is preferred due to its computational speed, numerical stability and algebraic simplicity.
Mathematical models can project how infectious diseases progress to show the likely outcome of an epidemic and help inform public health interventions. Models use some basic assumptions and mathematics to find parameters for various infectious diseases and use those parameters to calculate the effects of possible interventions, like mass vaccination programmes.
In statistics, non-sampling error is a catch-all term for the deviations of estimates from their true values that are not a function of the sample chosen, including various systematic errors and random errors that are not due to sampling. Non-sampling errors are much harder to quantify than sampling errors. Non-sampling errors in survey estimates can arise from: Coverage errors, such as failure to accurately represent all population units in the sample, or the inability to obtain information about all sample cases; Response errors by respondents due for example to definitional differences, misunderstandings, or deliberate misreporting; Mistakes in recording the data or coding it to standard classifications; Other errors of collection, nonresponse, processing, or imputation of values for missing or inconsistent data. An excellent discussion of issues pertaining to non-sampling error can be found in several sources such as Kalton (1983) and Salant and Dillman (1995),  
In mathematics and statistics, deviation is a measure of difference between the observed value of a variable and some other value, often that variable's mean. The sign of the deviation (positive or negative), reports the direction of that difference (the deviation is positive when the observed value exceeds the reference value). The magnitude of the value indicates the size of the difference.
In probability theory and statistics, the split normal distribution also known as the two-piece normal distribution results from joining at the mode the corresponding halves of two normal distributions with the same mode but different variances. It is claimed by Johnson et al. that this distribution was introduced by Gibbons and Mylroie and by John. But these are two of several independent rediscoveries of the Zweiseitige Gauss'sche Gesetz introduced in the posthumously published Kollektivmasslehre (1897) of Gustav Theodor Fechner (1801-1887).
Statistical shape analysis is an analysis of the geometrical properties of some given set of shapes by statistical methods. For instance, it could be used to quantify differences between male and female Gorilla skull shapes, normal and pathological bone shapes, leaf outlines with and without herbivory by insects, etc. Important aspects of shape analysis are to obtain a measure of distance between shapes, to estimate mean shapes from (possibly random) samples, to estimate shape variability within samples, to perform clustering and to test for differences between shapes. One of the main methods used is principal component analysis(PCA). Statistical shape analysis has applications in various fields, including medical imaging, computer vision, Computational anatomy, sensor measurement, and geographical profiling.
In the comparison of various statistical procedures, efficiency is a measure of the optimality of an estimator, of an experimental design, or of a hypothesis testing procedure. Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance. This article primarily deals with efficiency of estimators. The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional "best possible" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure. Efficiencies are often defined using the variance or mean square error as the measure of desirability.
In queueing models, a discipline within the mathematical theory of probability, the quasi-birth death process describes a generalisation of the birth death process. As with the birth-death process it moves up and down between levels one at a time, but the time between these transitions can have a more complicated distribution encoded in the blocks.
Model selection is the task of selecting a statistical model from a set of candidate models, given data. In the simplest cases, a pre-existing set of data is considered. However, the task can also involve the design of experiments such that the data collected is well-suited to the problem of model selection. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice. Konishi & Kitagawa (2008, p. 75) state, "The majority of the problems in statistical inference can be considered to be problems related to statistical modeling". Relatedly, Sir David Cox (2006, p. 197) has said, "How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis".
In statistics, Basu's theorem states that any boundedly complete sufficient statistic is independent of any ancillary statistic. This is a 1955 result of Debabrata Basu. It is often used in statistics as a tool to prove independence of two statistics, by first demonstrating one is complete sufficient and the other is ancillary, then appealing to the theorem. An example of this is to show that the sample mean and sample variance of a normal distribution are independent statistics, which is done in the Examples section below. This property (independence of sample mean and sample variance) characterizes normal distributions.
Health care analytics is a product category used in the marketing of business software and consulting services. It makes extensive use of data, statistical and qualitative analysis, explanatory and predictive modeling.
In statistics, a Galbraith plot (also known as Galbraith's radial plot or just radial plot), is one way of displaying several estimates of the same quantity that have different standard errors.  It can be used to examine heterogeneity in a meta-analysis, as an alternative or supplement to a forest plot. A Galbraith plot is produced by first calculating the standardized estimates or z-statistics by dividing each estimate by its standard error (SE). The Galbraith plot is then a scatter plot of each z-statistic (vertical axis) against 1/SE (horizontal axis). Larger studies (with smaller SE and larger 1/SE) will be observed to aggregate away from the origin.
In statistics, the log-rank test is a hypothesis test to compare the survival distributions of two samples. It is a nonparametric test and appropriate to use when the data are right skewed and censored (technically, the censoring must be non-informative). It is widely used in clinical trials to establish the efficacy of a new treatment in comparison with a control treatment when the measurement is the time to event (such as the time from initial treatment to a heart attack). The test is sometimes called the Mantel Cox test, named after Nathan Mantel and David Cox. The log-rank test can also be viewed as a time-stratified Cochran Mantel Haenszel test. The test was first proposed by Nathan Mantel and was named the log-rank test by Richard and Julian Peto.
An increasing process is a stochastic process  where the random variables  which make up the process are increasing almost surely and adapted:  A continuous increasing process is such a process where the set  is continuous.
In statistics, a confounding variable (also confounding factor, a confound, or confounder) is an extraneous variable in a statistical model that correlates (directly or inversely) with both the dependent variable and the independent variable. A spurious relationship is a perceived relationship between an independent variable and a dependent variable that has been estimated incorrectly because the estimate fails to account for a confounding factor. The incorrect estimation suffers from omitted-variable bias. While specific definitions may vary, in essence a confounding variable fits the following four criteria, here given in a hypothetical situation with variable of interest "V", confounding variable "C" and outcome of interest "O": C is associated (inversely or directly) with O C is associated with O, independent of V C is associated (inversely or directly) with V C is not in the causal pathway of V to O (C is not a direct consequence of V, not a way by which V produces O) The preceding correlation-based definition, however, is metaphorical at best   a growing number of analysts agree that confounding is a causal concept, and as such, cannot be described in terms of correlations nor associations  (see causal definition).  
In statistics, an L-estimator is an estimator which is an L-statistic   a linear combination of order statistics of the measurements. This can be as little as a single point, as in the median (of an odd number of values), or as many as all points, as in the mean. The main benefits of L-estimators are that they are often extremely simple, and often robust statistics: assuming sorted data, they are very easy to calculate and interpret, and are often resistant to outliers. They thus are useful in robust statistics, as descriptive statistics, in statistics education, and when computation is difficult. However, they are inefficient, and in modern robust statistics M-estimators are preferred, though these are much more difficult computationally. In many circumstances L-estimators are reasonably efficient, and thus adequate for initial estimation.
In the design of experiments, treatments are applied to experimental units in the treatment group(s). In comparative experiments, members of the complementary group, the control group, receive either no treatment or a standard treatment. For the conclusions drawn from the results of an experiment to have validity, it is essential that the items or patients assigned to treatment and control groups be representative of the same population. In some experiments, such as many in agriculture or psychology, this can be achieved by randomly assigning items from a common population to one of the treatment and control groups. In studies of twins involving just one treatment group and a control group, it is statistically efficient to do this random assignment separately for each pair of twins, so that one is in the treatment group and one in the control group. In some medical studies, where it may be unethical not to treat patients who present with symptoms, controls may be given a standard treatment, rather than no treatment at all. Another alternative is to select controls from a wider population, provided that this population is well-defined and that those presenting with symptoms at the clinic are representative of those in the wider population.
In statistics, Stein's unbiased risk estimate (SURE) is an unbiased estimator of the mean-squared error of "a nearly arbitrary, nonlinear biased estimator." In other words, it provides an indication of the accuracy of a given estimator. This is important since the true mean-squared error of an estimator is a function of the unknown parameter to be estimated, and thus cannot be determined exactly. The technique is named after its discoverer, Charles Stein.
Chernoff faces, invented by Herman Chernoff, display multivariate data in the shape of a human face. The individual parts, such as eyes, ears, mouth and nose represent values of the variables by their shape, size, placement and orientation. The idea behind using faces is that humans easily recognize faces and notice small changes without difficulty. Chernoff faces handle each variable differently. Because the features of the faces vary in perceived importance, the way in which variables are mapped to the features should be carefully chosen (e.g. eye size and eyebrow-slant have been found to carry significant weight). Chernoff faces themselves can be plotted on a standard X-Y graph; the faces can be positioned X-Y based on the two most important variables, and then the faces themselves represent the rest of the dimensions for each item. Edward Tufte, presenting such a diagram, says this kind of Chernoff face graph would "reduce well, maintaining legibility even with individual areas of .05 square inches as shown...with cartoon faces and even numbers becoming data measures, we would appear to have reached the limit of graphical economy of presentation, imagination, and let it be admitted, eccentricity."
Anscombe's quartet comprises four datasets that have nearly identical simple statistical properties, yet appear very different when graphed. Each dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties. For all four datasets: The first scatter plot (top left) appears to be a simple linear relationship, corresponding to two variables correlated and following the assumption of normality. The second graph (top right) is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear, and the Pearson correlation coefficient is not relevant (a more general regression and the corresponding coefficient of determination would be more appropriate). In the third graph (bottom left), the distribution is linear, but with a different regression line, which is offset by the one outlier which exerts enough influence to alter the regression line and lower the correlation coefficient from 1 to 0.816 (a robust regression would have been called for). Finally, the fourth graph (bottom right) shows an example when one outlier is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear. The quartet is still often used to illustrate the importance of looking at a set of data graphically before starting to analyze according to a particular type of relationship, and the inadequacy of basic statistic properties for describing realistic datasets. The datasets are as follows. The x values are the same for the first three datasets. A procedure to generate similar data sets with identical statistics and dissimilar graphics has since been developed.
Computer random number generators are important in mathematics, cryptography and gambling (on game servers). This list includes many common types, regardless of quality.
The q-Gaussian is a probability distribution arising from the maximization of the Tsallis entropy under appropriate constraints. It is one example of a Tsallis distribution. The q-Gaussian is a generalization of the Gaussian in the same way that Tsallis entropy is a generalization of standard Boltzmann Gibbs entropy or Shannon entropy. The normal distribution is recovered as q   1. The q-Gaussian has been applied to problems in the fields of statistical mechanics, geology, anatomy, astronomy, economics, finance, and machine learning. The distribution is often favored for its heavy tails in comparison to the Gaussian for 1 < q < 3. A generalized q-analog of the classical central limit theorem was proposed in 2008, in which the independence constraint for the i.i.d. variables is relaxed to an extent defined by the q parameter, with independence being recovered as q   1. However, a proof of such a theorem is still lacking. In the heavy tail regions, the distribution is equivalent to the Student's t-distribution with a direct mapping between q and the degrees of freedom. A practitioner using one of these distributions can therefore parameterize the same distribution in two different ways. The choice of the q-Gaussian form may arise if the system is non-extensive, or if there is lack of a connection to small samples sizes.
In stochastic processes, the Stratonovich integral (developed simultaneously by Ruslan L. Stratonovich and D. L. Fisk) is a stochastic integral, the most common alternative to the Ito  integral. Although the Ito  integral is the usual choice in applied mathematics, the Stratonovich integral is frequently used in physics. In some circumstances, integrals in the Stratonovich definition are easier to manipulate. Unlike the Ito  calculus, Stratonovich integrals are defined such that the chain rule of ordinary calculus holds. Perhaps the most common situation in which these are encountered is as the solution to Stratonovich stochastic differential equations (SDEs). These are equivalent to Ito  SDEs and it is possible to convert between the two whenever one definition is more convenient.
In statistics, familywise error rate (FWER) is the probability of making one or more false discoveries, or type I errors, among all the hypotheses when performing multiple hypotheses tests.
A fan chart is made of a group of dispersion fan diagrams, which may be positioned according to two categorising dimensions. A dispersion fan diagram is a circular diagram which reports the same information about a dispersion as a box plot: namely median, quartiles, and two extreme values.
In statistics, Somers  D, sometimes incorrectly referred to as Somer s D, is a measure of ordinal association between two variables  and . Somers  D takes values between  when all pairs of the variables disagree and  when all pairs of the variables agree. Somers  D is named after R. H. Somers, who proposed it in 1962. Somers  D plays a central role in rank statistics and is the parameter behind many nonparametric methods. It is also used as a quality measure of logistic regressions and credit scoring models.
In probability and statistics, the Dirichlet distribution (after Peter Gustav Lejeune Dirichlet), often denoted , is a family of continuous multivariate probability distributions parameterized by a vector  of positive reals. It is the multivariate generalization of the beta distribution. Dirichlet distributions are very often used as prior distributions in Bayesian statistics, and in fact the Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution. The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process.
Autocorrelation, also known as serial correlation or cross-autocorrelation, is the cross-correlation of a signal with itself at different points in time (that is what the cross stands for). Informally, it is the similarity between observations as a function of the time lag between them. It is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals.  
In statistics, the variance inflation factor (VIF) quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.
In epidemiology and biostatistics, the control event rate (CER) is a measure of how often a particular statistical event (such as response to a drug, adverse event or death) occurs within the scientific control group of an experiment. This value is very useful in determining the therapeutic benefit or risk to patients in experimental groups, in comparison to patients in placebo or traditionally treated control groups. Three statistical terms rely on CER for their calculation: Absolute risk reduction, Relative risk reduction and Number needed to treat.
In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be Markov random field if it satisfies Markov properties. A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite. When the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model. In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision.
Vapnik Chervonenkis theory (also known as VC theory) was developed during 1960 1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view. VC theory is related to statistical learning theory and to empirical processes. Richard M. Dudley and Vladimir Vapnik, among others, have applied VC-theory to empirical processes.
In statistics, a generalized linear mixed model (GLMM) is an extension to the generalized linear model in which the linear predictor contains random effects in addition to the usual fixed effects. They also extend the idea of linear mixed models to non-normal data.
Squared deviations from the mean (SDM) are involved in various calculations. In probability theory and statistics, the definition of variance is either the SDM expected value (when considering a theoretical distribution) or its average value (for actual experimental data). Computations for analysis of variance involve the partitioning of a sum of SDM.
In mathematics, Ito 's lemma is an identity used in Ito  calculus to find the differential of a time-dependent function of a stochastic process. It serves as the stochastic calculus counterpart of the chain rule. It can be heuristically derived by forming the Taylor series expansion of the function up to its second derivatives and retaining terms up to first order in the time increment and second order in the Wiener process increment. The lemma is widely employed in mathematical finance, and its best known application is in the derivation of the Black Scholes equation for option values. Ito 's lemma, which is named after Kiyosi Ito , is occasionally referred to as the Ito  Doeblin theorem in recognition of the recently discovered work of Wolfgang Doeblin. Note that while Ito's lemma was proved by Kiyosi Ito , Ito 's theorem, a result in group theory, is due to Noboru Ito .
Dataplot is a public domain software system for scientific visualization and statistical analysis. It was developed at the National Institute of Standards and Technology. Dataplot's source code is available.
Multifactor dimensionality reduction (MDR) is a data mining approach for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify interactions among discrete variables that influence a binary outcome and is considered a nonparametric alternative to traditional statistical methods such as logistic regression. The basis of the MDR method is a constructive induction algorithm that converts two or more variables or attributes to a single attribute. This process of constructing a new attribute changes the representation space of the data. The end goal is to create or discover a representation that facilitates the detection of nonlinear or nonadditive interactions among the attributes such that prediction of the class variable is improved over that of the original representation of the data.
In probability theory and statistics, variance measures how far a set of numbers are spread out. A variance of zero indicates that all the values are identical. Variance is always non-negative: a small variance indicates that the data points tend to be very close to the mean (expected value) and hence to each other, while a high variance indicates that the data points are very spread out around the mean and from each other. An equivalent measure is the square root of the variance, called the standard deviation. The standard deviation has the same dimension as the data, and hence is comparable to deviations from the mean. As standard deviation is often represented with the symbol   (lowercase sigma), so variance is often represented with the symbol  2 (sigma squared). There are two distinct concepts that are both called "variance". One variance is a characteristic of a set of observations. The other is part of a theoretical probability distribution and is defined by an equation. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below. The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance. The variance is one of several descriptors of a probability distribution. In particular, the variance is one of the moments of a distribution. In that context, it forms part of a systematic approach to distinguishing between probability distributions. While other such approaches have been developed, those based on moments are advantageous in terms of mathematical and computational simplicity.
Reliability in statistics and psychometrics is the overall consistency of a measure. A measure is said to have a high reliability if it produces similar results under consistent conditions. For example, measurements of people's height and weight are often extremely reliable.
In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) which are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning. For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, the output neuron that determines which character was read is activated. Like other machine learning methods    systems that learn from data    neural networks have been used to solve a wide variety of tasks, like computer vision and speech recognition, that are hard to solve using ordinary rule-based programming.
The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with true experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation. In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is reflected in a variable called the predictor. The change in the predictor is generally hypothesized to result in a change in the second variable, hence called the outcome variable. Experimental design involves not only the selection of suitable predictors and outcomes, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources. Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the predictor, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity. Correctly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making.
Statgraphics is a statistics package that performs and explains basic and advanced statistical functions. The software was created in 1980 by Dr. Neil Polhemus while working as a professor of statistics at Princeton university. The current version of the program, Statgraphics Centurion XVII, was released in fall 2014. Version XVII, available in both 32-bit and 64-bit editions, is available in five languages: English, French, Spanish, German and Italian. Statgraphics is distributed by Statpoint Technologies, Inc., a privately held company based in Warrenton, Virginia. Statgraphics is frequently used with Six Sigma process improvement. The program has also been used in various health and nutrition-related studies. During spring 2006, Statgraphics Mobile was released as the first sophisticated statistical program designed to run on hand-held computers (Pocket PC, Pocket PC Phone Edition, or compatible device running Windows Mobile 5 or Windows Pocket PC 2003). In September 2008 the Statgraphics Online version was released. Statgraphics Online is a statistical package that runs within a web browser. Users can enter data directly into the data editor or import data from text files, Excel files, or other formats. The calculations are performed remotely on a web server and the results returned to the user's browser as HTML with embedded graphics images. In July 2012 Statgraphics Sigma express was released. It is an add-in for Microsoft Excel that enables users to perform various calculations required when learning or applying Six Sigma. It adds a menu selection to Excel containing sections for each item of the DMAIC paradigm (Define, Measure, Analyze, Improve and Control) plus additional menu items for Tools and Help. The program is designed to meet the needs of Six Sigma yellow belts, green belts and most black belts. Sigma express is available in English and French. In October 2014 Statpoint Technologies Incorporated released Centurion XVII, the companies flagship software. Centurion XVII included 32 new statistical procedures. With Centurion XVII the company placed a new emphases on data visualization. Following extensive development, Statgraphics released its cloud based "Stratus" in June 2015. Billed as one of the first cloud analytic tools on the market, it is designed to work either as a stand-alone program or in conjunction with Centurion XVII. Stratus was designed by Statpoint Technologies Incorporated to work on PCs, Macs, tablets and handheld devices.
In the statistical area of survival analysis, an accelerated failure time model (AFT model) is a parametric model that provides an alternative to the commonly used proportional hazards models. Whereas a proportional hazards model assumes that the effect of a covariate is to multiply the hazard by some constant, an AFT model assumes that the effect of a covariate is to accelerate or decelerate the life course of a disease by some constant. This is especially appealing in a technical context where the 'disease' is a result of some mechanical process with a known sequence of intermediary stages.  
The cross-entropy (CE) method attributed to Reuven Rubinstein is a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling. The method originated from the field of rare event simulation, where very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema. In a nutshell the CE method consists of two phases: Generate a random data sample (trajectories, vectors, etc.) according to a specified mechanism. Update the parameters of the random mechanism based on the data to produce a "better" sample in the next iteration. This step involves minimizing the cross-entropy or Kullback Leibler divergence.
In probability theory, a real valued process X is called a semimartingale if it can be decomposed as the sum of a local martingale and an adapted finite-variation process. Semimartingales are "good integrators", forming the largest class of processes with respect to which the Ito  integral and the Stratonovich integral can be defined. The class of semimartingales is quite large (including, for example, all continuously differentiable processes, Brownian motion and Poisson processes). Submartingales and supermartingales together represent a subset of the semimartingales.
Self-similar processes are types of stochastic processes that exhibit the phenomenon of self-similarity. A self-similar phenomenon behaves the same when viewed at different degrees of magnification, or different scales on a dimension (space or time). Self-similar processes can sometimes be described using heavy-tailed distributions, also known as long-tailed distributions. Example of such processes include traffic processes such as packet inter-arrival times and burst lengths. Self-similar processes can exhibit long-range dependency.
In statistics, polychoric correlation is a technique for estimating the correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables. Tetrachoric correlation is a special case of the polychoric correlation applicable when both observed variables are dichotomous. These names derive from the polychoric and tetrachoric series which are used for estimation of these correlations. These series' were mathematical expansions once but not anymore.
In mathematics and statistics, the Fre chet mean is a generalization of centroids to metric spaces, giving a single representative point or central tendency for a cluster of points. It is named after Maurice Fre chet. Karcher means are a closely related construction named after Hermann Karcher. On the real numbers, the arithmetic mean, median, geometric mean, and harmonic mean can all be interpreted as Fre chet means for different distance functions.
The five-number summary is a descriptive statistic that provides information about a set of observations. It consists of the five most important sample percentiles: the sample minimum (smallest observation) the lower quartile or first quartile the median (middle value) the upper quartile or third quartile the sample maximum (largest observation) In order for these statistics to exist the observations must be from a univariate variable that can be measured on an ordinal, interval or ratio scale.
In probability theory and statistics, the law of the unconscious statistician (sometimes abbreviated LOTUS) is a theorem used to calculate the expected value of a function g(X) of a random variable X when one knows the probability distribution of X but one does not explicitly know the distribution of g(X). The form of the law can depend on the form in which one states the probability distribution of the random variable X. If it is a discrete distribution and one knows its probability mass function  X (but not  g(X)), then the expected value of g(X) is  where the sum is over all possible values x of X. If it is a continuous distribution and one knows its probability density function  X (but not  g(X)), then the expected value of g(X) is  (provided the values of X are real numbers as opposed to vectors, complex numbers, etc.). Regardless of continuity-versus-discreteness and related issues, if one knows the cumulative probability distribution function FX (but not Fg(X)), then the expected value of g(X) is given by a Riemann Stieltjes integral  (again assuming X is real-valued). However, the result is so well known that it is usually used without stating a name for it: the name is not extensively used. For justifications of the result for discrete and continuous random variables see.
In probability theory and statistics, the index of dispersion, dispersion index, coefficient of dispersion, relative variance, or variance-to-mean ratio (VMR), like the coefficient of variation, is a normalized measure of the dispersion of a probability distribution: it is a measure used to quantify whether a set of observed occurrences are clustered or dispersed compared to a standard statistical model. It is defined as the ratio of the variance  to the mean ,  It is also known as the Fano factor, though this term is sometimes reserved for windowed data (the mean and variance are computed over a subpopulation), where the index of dispersion is used in the special case where the window is infinite. Windowing data is frequently done: the VMR is frequently computed over various intervals in time or small regions in space, which may be called "windows", and the resulting statistic called the Fano factor. It is only defined when the mean  is non-zero, and is generally only used for positive statistics, such as count data or time between events, or where the underlying distribution is assumed to be the exponential distribution or Poisson distribution.
In statistics, Wold's decomposition or the Wold representation theorem (not to be confused with the Wold theorem that is the discrete-time analog of the Wiener Khinchin theorem) named after Herman Wold, says that every covariance-stationary time series  can be written as the sum of two time series, one deterministic and one stochastic. Formally  where:   is the time series being considered,   is an uncorrelated sequence which is the innovation process to the process    that is, a white noise process that is input to the linear filter .   is the possibly infinite vector of moving average weights (coefficients or parameters)   is a deterministic time series, such as one represented by a sine wave.  Note that the moving average coefficients have these properties: Stable, that is square summable  <  Causal (i.e. there are no terms with j < 0) Minimum delay Constant ( independent of t) It is conventional to define  This theorem can be considered as an existence theorem: any stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model. Imagine creating a process that is a moving average but not satisfying these properties 1 4. For example, the coefficients  could define an acausal and non-minimum delay model. Nevertheless the theorem assures the existence of a causal minimum delay moving average that exactly represents this process. How this all works for the case of causality and the minimum delay property is discussed in Scargle (1981), where an extension of the Wold Decomposition is discussed. The usefulness of the Wold Theorem is that it allows the dynamic evolution of a variable  to be approximated by a linear model. If the innovations  are independent, then the linear model is the only possible representation relating the observed value of  to its past evolution. However, when  is merely an uncorrelated but not independent sequence, then the linear model exists but it is not the only representation of the dynamic dependence of the series. In this latter case, it is possible that the linear model may not be very useful, and there would be a nonlinear model relating the observed value of  to its past evolution. However, in practical time series analysis, it is often the case that only linear predictors are considered, partly on the grounds of simplicity, in which case the Wold decomposition is directly relevant. The Wold representation depends on an infinite number of parameters, although in practice they usually decay rapidly. The autoregressive model is an alternative that may have only a few coefficients if the corresponding moving average has many. These two models can be combined into an autoregressive-moving average (ARMA) model, or an autoregressive-integrated-moving average (ARIMA) model if non-stationarity is involved. See Scargle (1981) and references there.
Group method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models. GMDH is used in such fields as data mining, knowledge discovery, prediction, complex systems modeling, optimization and pattern recognition. GMDH algorithms are characterized by inductive procedure that performs sorting-out of gradually complicated polynomial models and selecting the best solution by means of the so-called external criterion. A GMDH model with multiple inputs and one output is a subset of components of the base function (1):  where f are elementary functions dependent on different sets of inputs, a are coefficients and m is the number of the base function components. In order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called partial models. Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an external criterion. This process is called self-organization of models. The most popular base function used in GMDH is the gradually complicated Kolmogorov-Gabor polynomial (2):  The resulting models are also known as polynomial neural networks. Ju rgen Schmidhuber cites GDMH as one of the earliest deep learning methods, remarking that it was used to train eight-layer neural nets as early as 1971.
In information theory, the limiting density of discrete points is an adjustment to the formula of Claude Shannon for differential entropy. It was formulated by Edwin Thompson Jaynes to address defects in the initial definition of differential entropy.
In statistical surveys conducted by means of structured interviews or questionnaires, a subset of the survey items having binary (e.g., YES or NO) answers forms a Guttman scale (named after Louis Guttman) if they can be ranked in some order so that, for a rational respondent, the response pattern can be captured by a single index on that ordered scale. In other words, on a Guttman scale, items are arranged in an order so that an individual who agrees with a particular item also agrees with items of lower rank-order. For example, a series of items could be (1) "I am willing to be near ice cream"; (2) "I am willing to smell ice cream"; (3) "I am willing to eat ice cream"; and (4) "I love to eat ice cream". Agreement with any one item implies agreement with the lower-order items. This contrasts with topics studied using a Likert scale or a Thurstone scale. The concept of Guttman scale likewise applies to series of items in other kinds of tests, such as achievement tests, that have binary outcomes. For example, a test of math achievement might order questions based on their difficulty and instruct the examinee to begin in the middle. The assumption is if the examinee can successfully answer items of that difficulty (e.g., summing two 3-digit numbers), s/he would be able to answer the earlier questions (e.g., summing two 2-digit numbers). Some achievement tests are organized in a Guttman scale to reduce the duration of the test. By designing surveys and tests such that they contain Guttman scales, researchers can simplify the analysis of the outcome of surveys and increase the robustness. Guttman scales also make it possible to detect and discard randomized answer patterns, as may be given by uncooperative respondents. A hypothetical, perfect Guttman scale consists of a unidimensional set of items that are ranked in order of difficulty from least extreme to most extreme position. For example, a person scoring a "7" on a ten item Guttman scale, will agree with items 1-7 and disagree with items 8,9,10. An important property of Guttman's model is that a person's entire set of responses to all items can be predicted from their cumulative score because the model is deterministic. A well-known example of a Guttman scale is the Bogardus Social Distance Scale. Another example is the original Beaufort wind force scale, assigning a single number to observed conditions of the sea surface ("Flat", ..., "Small waves", ..., "Sea heaps up and foam begins to streak", ...), which was in fact a Guttman scale. The observation "Flat = YES" implies "Small waves = NO".
In probability theory and directional statistics, a wrapped normal distribution is a wrapped probability distribution that results from the "wrapping" of the normal distribution around the unit circle. It finds application in the theory of Brownian motion and is a solution to the heat equation for periodic boundary conditions. It is closely approximated by the von Mises distribution, which, due to its mathematical simplicity and tractability, is the most commonly used distribution in directional statistics.
In a clinical research trial, a clinical endpoint generally refers to occurrence of a disease, symptom, sign or laboratory abnormality that constitutes one of the target outcomes of the trial, but may also refer to any such disease or sign that strongly motivates the withdrawal of that individual or entity from the trial, then often termed humane (clinical) endpoint.  The primary endpoint of a clinical trial is the endpoint for which subjects are randomized and for which the trial is powered. Secondary endpoints are endpoints that are analyzed post hoc, for which the trial may not be powered nor randomized.
In mathematics, Doob's martingale inequality is a result in the study of stochastic processes. It gives a bound on the probability that a stochastic process exceeds any given value over a given interval of time. As the name suggests, the result is usually given in the case that the process is a non-negative martingale, but the result is also valid for non-negative submartingales. The inequality is due to the American mathematician Joseph L. Doob.
Stochastic dominance is a form of stochastic ordering. The concept arises in decision theory and decision analysis in situations where one gamble (a probability distribution over possible outcomes, also known as prospects) can be ranked as superior to another gamble for a broad class of decision-makers. It is based on shared preferences regarding sets of possible outcomes and their associated probabilities. Only limited knowledge of preferences is required for determining dominance. Risk aversion is a factor only in second order stochastic dominance. Stochastic dominance does not give a total order, but rather only a partial order: for some pairs of gambles, neither one stochastically dominates the other, since different members of the broad class of decision-makers will differ regarding which gamble is preferable without them generally being considered to be equally attractive. A related concept not included under stochastic dominance is deterministic dominance, which occurs when the least preferable outcome of gamble A is more valuable than the most highly preferred outcome of gamble B.
In statistics, the size of a test is the probability of falsely rejecting the null hypothesis. That is, it is the probability of making a Type I error. It is denoted by the Greek letter   (alpha). For a simple hypothesis,  In the case of a composite null hypothesis, the size is the supremum over all null hypotheses.  A test is said to have significance level  if its size is less than or equal to . In many cases the size and level of a test are equal.
Sinkov statistics, also known as log-weight statistics, is a specialized field of statistics that was developed by Abraham Sinkov, while working for the small Signal Intelligence Service organization, the primary mission of which was to compile codes and ciphers for use by the U.S. Army. The mathematics involved include modular arithmetic, a bit of number theory, some linear algebra of two dimensions with matrices, some combinatorics, and a little statistics.  Sinkov did not explain the theoretical underpinnings of his statistics, or characterized its distribution, nor did he give a decision procedure for accepting or rejecting candidate plaintexts on the basis of their S1 scores. The situation becomes more difficult when comparing strings of different lengths because Sinkov does not explain how the distribution of his statistics changes with length, especially when applied to higher-order grams. As for how to accept or reject a candidate plaintext, Sinkov simply said to try all possibilities and to pick the one with the highest S1 value. Although the procedure works for some applications, it is inadequate for applications that require on-line decisions. Furthermore, it is desirable to have a meaningful interpretation of the S1 values.
In population genetics, the Balding Nichols model is a statistical description of the allele frequencies in the components of a sub-divided population. With background allele frequency p the allele frequencies, in sub-populations separated by Wright's FST F, are distributed according to independent draws from  where B is the Beta distribution. This distribution has mean p and variance Fp(1   p). The model is due to David Balding and Richard Nichols and is widely used in the forensic analysis of DNA profiles and in population models for genetic epidemiology. Differential equation  
The Cochran Armitage test for trend, named for William Cochran and Peter Armitage, is used in categorical data analysis when the aim is to assess for the presence of an association between a variable with two categories and a variable with k categories. It modifies the Pearson chi-squared test to incorporate a suspected ordering in the effects of the k categories of the second variable. For example, doses of a treatment can be ordered as 'low', 'medium', and 'high', and we may suspect that the treatment benefit cannot become smaller as the dose increases. The trend test is often used as a genotype-based test for case-control genetic association studies.
In statistics, sampling error is incurred when the statistical characteristics of a population are estimated from a subset, or sample, of that population. Since the sample does not include all members of the population, statistics on the sample, such as means and quantiles, generally differ from the characteristics of the entire population, which are known as parameters. For example, if one measures the height of a thousand individuals from a country of one million, the average height of the thousand is typically not the same as the average height of all one million people in the country. Since sampling is typically done to determine the characteristics of a whole population, the difference between the sample and population values is considered a sampling error. Exact measurement of sampling error is generally not feasible since the true population values are unknown; however, sampling error can often be estimated by probabilistic modeling of the sample.
In statistics and signal processing, step detection (also known as step smoothing, step filtering, shift detection, jump detection or edge detection) is the process of finding abrupt changes (steps, jumps, shifts) in the mean level of a time series or signal. It is usually considered as a special case of the statistical method known as change detection or change point detection. Often, the step is small and the time series is corrupted by some kind of noise, and this makes the problem challenging because the step may be hidden by the noise. Therefore, statistical and/or signal processing algorithms are often required. The step detection problem occurs in multiple scientific and engineering contexts, for example in statistical process control (the control chart being the most directly related method), in exploration geophysics (where the problem is to segment a well-log recording into stratigraphic zones), in genetics (the problem of separating microarray data into similar copy-number regimes), and in biophysics (detecting state transitions in a molecular machine as recorded in time-position traces). For 2D signals, the related problem of edge detection has been studied intensively for image processing.
In statistics, an adaptive estimator is an estimator in a parametric or semiparametric model with nuisance parameters such that the presence of these nuisance parameters does not affect efficiency of estimation.
Data mining is an interdisciplinary subfield of computer science. It is the computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the "knowledge discovery in databases" process, or KDD. The term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics   or, when referring to actual methods, artificial intelligence and machine learning   are more appropriate. The actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps. The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
In probability theory and statistics, the generalized inverse Gaussian distribution (GIG) is a three-parameter family of continuous probability distributions with probability density function  where Kp is a modified Bessel function of the second kind, a > 0, b > 0 and p a real parameter. It is used extensively in geostatistics, statistical linguistics, finance, etc. This distribution was first proposed by E tienne Halphen. It was rediscovered and popularised by Ole Barndorff-Nielsen, who called it the generalized inverse Gaussian distribution. It is also known as the Sichel distribution, after Herbert Sichel. Its statistical properties are discussed in Bent J rgensen's lecture notes.
The h-index is an author-level metric that attempts to measure both the productivity and citation impact of the publications of a scientist or scholar. The index is based on the set of the scientist's most cited papers and the number of citations that they have received in other publications. The index can also be applied to the productivity and impact of a scholarly journal as well as a group of scientists, such as a department or university or country. The index was suggested in 2005 by Jorge E. Hirsch, a physicist at UCSD, as a tool for determining theoretical physicists' relative quality and is sometimes called the Hirsch index or Hirsch number.
In queueing theory, a discipline within the mathematical theory of probability, a fork join queue is a queue where incoming jobs are split on arrival for service by numerous servers and joined before departure. The model is often used for parallel computations or systems where products need to be obtained simultaneously from different suppliers (in a warehouse or manufacturing setting). The key quantity of interest in this model is usually the time taken to service a complete job. The model has been described as a "key model for the performance analysis of parallel and distributed systems." Few analytical results exist for fork join queues, but various approximations are known. The situation where jobs arrive according to a Poisson process and service times are exponentially distributed is sometimes referred to as a Flatto Hahn Wright model or FHW model.
In statistics, a trimmed estimator is an estimator derived from another estimator by excluding some of the extreme values, a process called truncation. This is generally done to obtain a more robust statistic, and the extreme values are considered outliers. Trimmed estimators also often have higher efficiency for mixture distributions and heavy-tailed distributions than the corresponding untrimmed estimator, at the cost of lower efficiency for other distributions, such as the normal distribution. Given an estimator, the n% trimmed version is obtained by discarding the n% lowest and highest observations: it is a statistic on the middle of the data. For instance, the 5% trimmed mean is obtained by taking the mean of the 5% to 95% range. In some cases a trimmed estimator discards a fixed number of points (such as maximum and minimum) instead of a percentage.
In statistics, the mean percentage error (MPE) is the computed average of percentage errors by which forecasts of a model differ from actual values of the quantity being forecast. The formula for the mean percentage error is  where at is the actual value of the quantity being forecast, ft is the forecast, and n is the number of different times for which the variable is forecast. Because actual rather than absolute values of the forecast errors are used in the formula, positive and negative forecast errors can offset each other; as a result the formula can be used as a measure of the bias in the forecasts. A disadvantage of this measure is that it is undefined whenever a single actual value is zero.
The deviance information criterion (DIC) is a hierarchical modeling generalization of the AIC (Akaike information criterion) and BIC (Bayesian information criterion, also known as the Schwarz criterion). It is particularly useful in Bayesian model selection problems where the posterior distributions of the models have been obtained by Markov chain Monte Carlo (MCMC) simulation. Like AIC and BIC it is an asymptotic approximation as the sample size becomes large. It is only valid when the posterior distribution is approximately multivariate normal. Define the deviance as , where  are the data,  are the unknown parameters of the model and  is the likelihood function.  is a constant that cancels out in all calculations that compare different models, and which therefore does not need to be known. The expectation  is a measure of how well the model fits the data; the larger this is, the worse the fit. There are two calculations in common usage for the effective number of parameters of the model. The first, as described in Spiegelhalter et al. (2002, p. 587) is , where  is the expectation of . The second, as described in Gelman et al. (2004, p. 182) is . The larger the effective number of parameters is, the easier it is for the model to fit the data, and so the deviance needs to be penalized. The deviance information criterion is calculated as  or equivalently as  From this latter form, the connection with Akaike's information criterion is evident. The idea is that models with smaller DIC should be preferred to models with larger DIC. Models are penalized both by the value of , which favors a good fit, but also (in common with AIC and BIC) by the effective number of parameters . Since  will decrease as the number of parameters in a model increases, the  term compensates for this effect by favoring models with a smaller number of parameters. The advantage of DIC over other criteria in the case of Bayesian model selection is that the DIC is easily calculated from the samples generated by a Markov chain Monte Carlo simulation. AIC and BIC require calculating the likelihood at its maximum over , which is not readily available from the MCMC simulation. But to calculate DIC, simply compute  as the average of  over the samples of , and  as the value of  evaluated at the average of the samples of . Then the DIC follows directly from these approximations. Claeskens and Hjort (2008, Ch. 3.5) show that the DIC is large-sample equivalent to the natural model-robust version of the AIC. In the derivation of DIC, it is assumed that the specified parametric family of probability distributions that generate future observations encompasses the true model. This assumption does not always hold, and it is desirable to consider model assessment procedures in that scenario. Also, the observed data are used both to construct the posterior distribution and to evaluate the estimated models. Therefore, DIC tends to select over-fitted models. Recently, these issues are resolved by Ando (2007), Bayesian predictive information criterion, BPIC. Ando (2010, Ch. 8) provided a discussion of various Bayesian model selection criteria. To avoid the over-fitting problems of DIC, Ando (2011) developed Bayesian model selection criteria from a predictive view point. The criterion is calculated as  The first term is a measure of how well the model fits the data, while the second term is a penalty on the model complexity. Note, that the p in this expression is the predictive distribution rather than the likelihood above.
In graph theory, the Erdo s Re nyi model is either of two closely related models for generating random graphs. They are named after Paul Erdo s and Alfre d Re nyi, who first introduced one of the models in 1959; the other model was introduced independently and contemporaneously by Edgar Gilbert. In the model introduced by Erdo s and Re nyi, all graphs on a fixed vertex set with a fixed number of edges are equally likely; in the model introduced by Gilbert, each edge has a fixed probability of being present or absent, independently of the other edges. These models can be used in the probabilistic method to prove the existence of graphs satisfying various properties, or to provide a rigorous definition of what it means for a property to hold for almost all graphs.
In time series modeling, a nonlinear autoregressive exogenous model (NARX) is a nonlinear autoregressive model which has exogenous inputs. This means that the model relates the current value of a time series where one would like to explain or predict to both: past values of the same series; and current and past values of the driving (exogenous) series   that is, of the externally determined series that influences the series of interest. In addition, the model contains: an "error" term which relates to the fact that knowledge of the other terms will not enable the current value of the time series to be predicted exactly. Such a model can be stated algebraically as  Here y is the variable of interest, and u is the externally determined variable. In this scheme, information about u helps predict y, as do previous values of y itself. Here   is the error term (sometimes called noise). For example, y may be air temperature at noon, and u may be the day of the year (day-number within year). The function F is some nonlinear function, such as a polynomial. F can be a neural network, a wavelet network, a sigmoid network and so on. To test for non-linearity in a time series, the BDS test (Brock-Dechert-Scheinkman test) developed for econometrics can be used.
In statistics, the Horvitz Thompson estimator, named after Daniel G. Horvitz and Donovan J. Thompson, is a method for estimating the total and mean of a superpopulation in a stratified sample. Inverse probability weighting is applied to account for different proportions of observations within strata in a target population. The Horvitz Thompson estimator is frequently applied in survey analyses and can be used to account for missing data.
Orthogonal array testing is a black box testing technique that is a systematic, statistical way of software testing. It is used when the number of inputs to the system is relatively small, but too large to allow for exhaustive testing of every possible input to the systems. It is particularly effective in finding errors associated with faulty logic within computer software systems. Orthogonal arrays can be applied in user interface testing, system testing, regression testing, configuration testing and performance testing. The permutations of factor levels comprising a single treatment are so chosen that their responses are uncorrelated and therefore each treatment gives a unique piece of information. The net effects of organizing the experiment in such treatments is that the same piece of information is gathered in the minimum number of experiments.
In statistics, an ecological correlation is a correlation between two variables that are group means, in contrast to a correlation between two variables that describe individuals. For example, one might study the correlation between physical activity and weight among sixth-grade children. A study at the individual level might make use of 100 children, then measure both physical activity and weight; the correlation between the two variables would be at the individual level. By contrast, another study might make use of 100 classes of sixth-grade students, then measure the mean physical activity and the mean weight of each of the 100 classes. A correlation between these group means would be an example of an ecological correlation. Because a correlation describes the measured strength of a relationship, correlations at the group level can be much higher than those at the individual level. Thinking both are equal is an example of ecological fallacy.
Diffusion-limited aggregation (DLA) is the process whereby particles undergoing a random walk due to Brownian motion cluster together to form aggregates of such particles. This theory, proposed by T.A. Witten Jr. and L.M. Sander in 1981, is applicable to aggregation in any system where diffusion is the primary means of transport in the system. DLA can be observed in many systems such as electrodeposition, Hele-Shaw flow, mineral deposits, and dielectric breakdown.  The clusters formed in DLA processes are referred to as Brownian trees. These clusters are an example of a fractal. In 2-D these fractals exhibit a dimension of approximately 1.71 for free particles that are unrestricted by a lattice, however computer simulation of DLA on a lattice will change the fractal dimension slightly for a DLA in the same embedding dimension. Some variations are also observed depending on the geometry of the growth, whether it be from a single point radially outward or from a plane or line for example. Two examples of aggregates generated using a microcomputer by allowing random walkers to adhere to an aggregate (originally (i) a straight line consisting 1300 particles and (ii) one particle at center) are shown on the right.  Computer simulation of DLA is one of the primary means of studying this model. Several methods are available to accomplish this. Simulations can be done on a lattice of any desired geometry of embedding dimension, in fact this has been done in up to 8 dimensions, or the simulation can be done more along the lines of a standard molecular dynamics simulation where a particle is allowed to freely random walk until it gets within a certain critical range at which time it is pulled onto the cluster. Of critical importance is that the number of particles undergoing Brownian motion in the system is kept very low so that only the diffusive nature of the system is present.
Statistical literacy is the ability to understand statistics. Statistical literacy is necessary for citizens to understand material presented in publications such as newspapers, television, and the Internet. Numeracy is a prerequisite to being statistically literate. Being statistically literate is sometimes taken to include having both the ability to critically evaluate statistical material and to appreciate the relevance of statistically-based approaches to all aspects of life in general. H.G. Wells is often cited as saying that statistical understanding will one day be as important as being able to read or write but he may have been referring more to the older idea of political arithmetic than modern statistics.
In statistics, projection pursuit regression (PPR) is a statistical model developed by Jerome H. Friedman and Werner Stuetzle which is an extension of additive models. This model adapts the additive models in that it first projects the data matrix of explanatory variables in the optimal direction before applying smoothing functions to these explanatory variables.
Least squares support vector machines (LS-SVM) are least squares versions of support vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis. In this version one finds the solution by solving a set of linear equations instead of a convex quadratic programming (QP) problem for classical SVMs. Least squares SVM classifiers, were proposed by Suykens and Vandewalle. LS-SVMs are a class of kernel-based learning methods.
In mathematical analysis and in probability theory, a  -algebra (also sigma-algebra,  -field, sigma-field) on a set X is a collection   of subsets of X that includes the empty subset, is closed under complement, and is closed under union or intersection of infinitely-countably many subsets. The pair (X,  ) is called a measurable space. A  -algebra specializes the concept of an algebra of sets. An algebra of sets needs only to be closed under the union or intersection of finitely many subsets. The main use of  -algebras is in the definition of measures; specifically, the collection of those subsets for which a given measure is defined is necessarily a  -algebra. This concept is important in mathematical analysis as the foundation for Lebesgue integration, and in probability theory, where it is interpreted as the collection of events which can be assigned probabilities. Also, in probability,  -algebras are pivotal in the definition of conditional expectation. In statistics, (sub)  -algebras are needed for a formal mathematical definition of sufficient statistic, particularly when the statistic is a function or a random process and the notion of conditional density is not applicable. If X = {a, b, c, d}, one possible  -algebra on X is   = {  , {a, b}, {c, d}, {a, b, c, d} }, where   is the empty set. In general, a finite algebra is always a  -algebra. If {A1, A2, A3, ...} is a countable partition of X then the collection of all unions of sets in the partition (including the empty set) is a  -algebra. A more useful example is the set of subsets of the real line formed by starting with all open intervals and adding in all countable unions, countable intersections, and relative complements and continuing this process (by transfinite iteration through all countable ordinals) until the relevant closure properties are achieved (a construction known as the Borel hierarchy).
In statistics, overdispersion is the presence of greater variability (statistical dispersion) in a data set than would be expected based on a given statistical model. A common task in applied statistics is choosing a parametric model to fit a given set of empirical observations. This necessitates an assessment of the fit of the chosen model. It is usually possible to choose the model parameters in such a way that the theoretical population mean of the model is approximately equal to the sample mean. However, especially for simple models with few parameters, theoretical predictions may not match empirical observations for higher moments. When the observed variance is higher than the variance of a theoretical model, overdispersion has occurred. Conversely, underdispersion means that there was less variation in the data than predicted. Overdispersion is a very common feature in applied data analysis because in practice, populations are frequently heterogeneous (non-uniform) contrary to the assumptions implicit within widely used simple parametric models.
In statistics, a generalized estimating equation (GEE) is used to estimate the parameters of a generalized linear model with a possible unknown correlation between outcomes. Parameter estimates from the GEE are consistent even when the covariance structure is misspecified, under mild regularity conditions. The focus of the GEE is on estimating the average response over the population ("population-averaged" effects) rather than the regression parameters that would enable prediction of the effect of changing one or more covariates on a given individual. GEEs are usually used in conjunction with Huber White standard error estimates, also known as "robust standard error" or "sandwich variance" estimates. In the case of a linear model with a working independence variance structure, these are known as "heteroscedasticity consistent standard error" estimators. Indeed, the GEE unified several independent formulations of these standard error estimators in a general framework. GEEs belong to a class of semiparametric regression techniques because they rely on specification of only the first two moments. Under correct model specification and mild regularity conditions, parameter estimates from GEEs are consistent. They are a popular alternative to the likelihood based generalized linear mixed model which is more sensitive to variance structure specification. They are commonly used in large epidemiological studies, especially multi-site cohort studies because they can handle many types of unmeasured dependence between outcomes.
In randomized statistical experiments, generalized randomized block designs (GRBDs) are used to study the interaction between blocks and treatments. For a GRBD, each treatment is replicated at least two times in each block; this replication allows the estimation and testing of an interaction term in the linear model (without making parametric assumptions about a normal distribution for the error).
The sign test is a statistical method to test for consistent differences between pairs of observations, such as the weight of subjects before and after treatment. Given pairs of observations (such as weight pre- and post-treatment) for each subject, the sign test determines if one member of the pair (such as pre-treatment) tends to be greater than (or less than) the other member of the pair (post-treatment). The paired observations may be designated x and y. For comparisons of paired observations (x,y), the sign test is most useful if comparisons can only be expressed as x > y, x = y, or x < y. If, instead, the observations can be expressed as numeric quantities (x = 7, y = 18), or as ranks (rank of x = 1st, rank of y = 8th), then the paired t-test or the Wilcoxon signed-rank test. will usually have greater power than the sign test to detect consistent differences. If X and Y are quantitative variables, the sign test can be used to test the hypothesis that the difference between the median of X and the median of Y is zero, assuming continuous distributions of the two random variables X and Y, in the situation when we can draw paired samples from X and Y. The sign test can also test if the median of a collection of numbers is significantly greater than or less than a specified value. For example, given a list of student grades in a class, the sign test can determine if the median grade is significantly different from, say, 75 out of 100. The sign test is a non-parametric test which makes very few assumptions about the nature of the distributions under test - this means that it has very general applicability but may lack the statistical power of the alternative tests.
In probability theory, a hyperexponential distribution is a continuous probability distribution whose probability density function of the random variable X is given by  where each Yi is an exponentially distributed random variable with rate parameter  i, and pi is the probability that X will take on the form of the exponential distribution with rate  i. It is named the hyperexponential distribution since its coefficient of variation is greater than that of the exponential distribution, whose coefficient of variation is 1, and the hypoexponential distribution, which has a coefficient of variation smaller than one. While the exponential distribution is the continuous analogue of the geometric distribution, the hyperexponential distribution is not analogous to the hypergeometric distribution. The hyperexponential distribution is an example of a mixture density. An example of a hyperexponential random variable can be seen in the context of telephony, where, if someone has a modem and a phone, their phone line usage could be modeled as a hyperexponential distribution where there is probability p of them talking on the phone with rate  1 and probability q of them using their internet connection with rate  2.
Curve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a "smooth" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data.
The Neyer D-Optimal Test is a sensitivity test. It can be used to answer questions such as "How far can a carton of eggs fall, on average, before one breaks " If these egg cartons are very expensive, the person running the test would like to minimize the number of cartons dropped, to keep the experiment cheaper and to perform it faster. The Neyer test allows the experimenter to choose the experiment that gives the most information. In this case, given the history of egg cartons which have already been dropped, and whether those cartons broke or not, the Neyer test says "you will learn the most if you drop the next egg carton from a height of 32.123 meters."
In statistics, the odds ratio (usually abbreviated "OR") is one of three main ways to quantify how strongly the presence or absence of property A is associated with the presence or absence of property B in a given population. If each individual in a population either does or does not have a property "A", (e.g. "high blood pressure"), and also either does or does not have a property "B" (e.g. "moderate alcohol consumption") where both properties are appropriately defined, then a ratio can be formed which quantitatively describes the association between the presence/absence of "A" (high blood pressure) and the presence/absence of "B" (moderate alcohol consumption) for individuals in the population. This ratio is the odds ratio (OR) and can be computed following these steps: For a given individual that has "B" compute the odds that the same individual has "A" For a given individual that does not have "B" compute the odds that the same individual has "A" Divide the odds from step 1 by the odds from step 2 to obtain the odds ratio (OR). The term "individual" in this usage does not have to refer to a human being, as a statistical population can measure any set of entities, whether living or inanimate. If the OR is greater than 1, then having "A" is considered to be "associated" with having "B" in the sense that the having of "B" raises (relative to not-having "B") the odds of having "A". Note that this does not establish that B is a contributing cause of "A": it could be that the association is due to a third property, "C", which is a contributing cause of both "A" and "B" (confounding). The two other major ways of quantifying association are the risk ratio ("RR") and the absolute risk reduction ("ARR"). In clinical studies and many other settings, the parameter of greatest interest is often actually the RR, which is determined in a way that is similar to the one just described for the OR, except using probabilities instead of odds. Frequently, however, the available data only allows the computation of the OR; notably, this is so in the case of case-control studies, as explained below. On the other hand, if one of the properties (say, A) is sufficiently rare (the "rare disease assumption"), then the OR of having A given that the individual has B is a good approximation to the corresponding RR (the specification "A given B" is needed because, while the OR treats the two properties symmetrically, the RR and other measures do not). In a more technical language, the OR is a measure of effect size, describing the strength of association or non-independence between two binary data values. It is used as a descriptive statistic, and plays an important role in logistic regression.
The VEGAS algorithm, due to G. P. Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral. The VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function , so that the points are concentrated in the regions that make the largest contribution to the integral.
In estimation theory in statistics, stochastic equicontinuity is a property of estimators or of estimation procedures that is useful in dealing with their asymptotic behaviour as the amount of data increases. It is a version of equicontinuity used in the context of functions of random variables: that is, random functions. The property relates to the rate of convergence of sequences of random variables and requires that this rate is essentially the same within a region of the parameter space being considered. For instance, stochastic equicontinuity, along with other conditions, can be used to show uniform weak convergence, which can be used to prove the convergence of extremum estimators.
In probability theory, heavy-tailed distributions are probability distributions whose tails are not exponentially bounded: that is, they have heavier tails than the exponential distribution. In many applications it is the right tail of the distribution that is of interest, but a distribution may have a heavy left tail, or both tails may be heavy. There are three important subclasses of heavy-tailed distributions: the fat-tailed distributions, the long-tailed distributions and the subexponential distributions. In practice, all commonly used heavy-tailed distributions belong to the subexponential class. There is still some discrepancy over the use of the term heavy-tailed. There are two other definitions in use. Some authors use the term to refer to those distributions which do not have all their power moments finite; and some others to those distributions that do not have a finite variance. The definition given in this article is the most general in use, and includes all distributions encompassed by the alternative definitions, as well as those distributions such as log-normal that possess all their power moments, yet which are generally acknowledged to be heavy-tailed. (Occasionally, heavy-tailed is used for any distribution that has heavier tails than the normal distribution.)
Multilevel models (also hierarchical linear models, nested models, mixed models, random coefficient, random-effects models, random parameter models, or split-plot designs) are statistical models of parameters that vary at more than one level. An example could be a model of student performance that contains measures for individual students as well as measures for classrooms within which the students are grouped. These models can be seen as generalizations of linear models (in particular, linear regression), although they can also extend to non-linear models. These models became much more popular after sufficient computing power and software became available. Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level (i.e., nested data). The units of analysis are usually individuals (at a lower level) who are nested within contextual/aggregate units (at a higher level). While the lowest level of data in multilevel models is usually an individual, repeated measurements of individuals may also be examined. As such, multilevel models provide an alternative type of analysis for univariate or multivariate analysis of repeated measures. Individual differences in growth curves may be examined (see growth model). Furthermore, multilevel models can be used as an alternative to ANCOVA, where scores on the dependent variable are adjusted for covariates (i.e., individual differences) before testing treatment differences. Multilevel models are able to analyze these experiments without the assumptions of homogeneity-of-regression slopes that is required by ANCOVA. Multilevel models can be used on data with many levels, although 2-level models are the most common and the rest of this article deals only with these. The dependent variable must be examined at the lowest level of analysis.
In probability theory and statistics, a sequence or other collection of random variables is independent and identically distributed (i.i.d.) if each random variable has the same probability distribution as the others and all are mutually independent. The abbreviation i.i.d. is particularly common in statistics (often as iid, sometimes written IID), where observations in a sample are often assumed to be effectively i.i.d. for the purposes of statistical inference. The assumption (or requirement) that observations be i.i.d. tends to simplify the underlying mathematics of many statistical methods (see mathematical statistics and statistical theory). However, in practical applications of statistical modeling the assumption may or may not be realistic. To test how realistic the assumption is on a given data set, the autocorrelation can be computed, lag plots drawn or turning point test performed. The generalization of exchangeable random variables is often sufficient and more easily met. The assumption is important in the classical form of the central limit theorem, which states that the probability distribution of the sum (or average) of i.i.d. variables with finite variance approaches a normal distribution. Note that IID refers to sequences of random variables. "Independent and identically distributed" implies an element in the sequence is independent of the random variables that came before it. In this way, an IID sequence is different from a Markov sequence, where the probability distribution for the nth random variable is a function of the previous random variable in the sequence (for a first order Markov sequence). An IID sequence does not imply the probabilities for all elements of the sample space or event space must be the same. For example, repeated throws of loaded dice will produce a sequence that is IID, despite the outcomes being biased.
In probability theory, Proebsting's paradox is an argument that appears to show that the Kelly criterion can lead to ruin. Although it can be resolved mathematically, it raises some interesting issues about the practical application of Kelly, especially in investing. It was named and first discussed by Edward O. Thorp in 2008. The paradox was named for Todd Proebsting, its creator.
A histogram is a graphical representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable (quantitative variable) and was first introduced by Karl Pearson. To construct a histogram, the first step is to "bin" the range of values that is, divide the entire range of values into a series of intervals and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are usually equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency, the number of cases in each bin. In general, however, bins need not be of equal width; in that case, the erected rectangle has area proportional to the frequency of cases in the bin The vertical axis is not frequency but density: the number of cases per unit of the variable on the horizontal axis. A histogram may also be normalized displaying relative frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Another alternative is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are often confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, and the areas of the rectangles are meaningful, while a bar chart is a plot of categorical variables and the discontinuity should be indicated by having gaps between the rectangles, from which only the length is meaningful. Often this is neglected, which may lead to a bar chart being confused for a histogram.
In statistical mechanics, an Ursell function or connected correlation function, is a cumulant of a random variable. It is also called a connected correlation function as it can often be obtained by summing over connected Feynman diagrams (the sum over all Feynman diagrams gives the correlation functions). The Ursell function was named after Harold Ursell, who introduced it in 1927.
Detection theory or signal detection theory is a means to quantify the ability to discern between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the detection machine and of the nervous system of the operator). In the field of electronics, the separation of such patterns from a disguising background is referred to as signal recovery. According to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be. The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed. When the detecting system is a human being, experience, expectations, physiological state (e.g., fatigue) and other factors can affect the threshold applied. For instance, a sentry in wartime might be likely to detect fainter stimuli than the same sentry in peacetime due to a lower criterion, however they might also be more likely to treat innocuous stimuli as a threat. Much of the early work in detection theory was done by radar researchers. By 1954, the theory was fully developed on the theoretical side as described by Peterson, Birdsall and Fox and the foundation for the psychological theory was made by Wilson P. Tanner, David M. Green, and John A. Swets, also in 1954. Detection theory was used in 1966 by John A. Swets and David M. Green for psychophysics. Green and Swets criticized the traditional methods of psychophysics for their inability to discriminate between the real sensitivity of subjects and their (potential) response biases. Detection theory has applications in many fields such as diagnostics of any kind, quality control, telecommunications, and psychology. The concept is similar to the signal to noise ratio used in the sciences and confusion matrices used in artificial intelligence. It is also usable in alarm management, where it is important to separate important events from background noise.
In robust statistics, Peirce's criterion is a rule for eliminating outliers from data sets, which was devised by Benjamin Peirce.
In survival analysis, relative survival of a disease is calculated by dividing the overall survival after diagnosis by the survival as observed in a similar population that was not diagnosed with that disease. A similar population is composed of individuals with at least age and gender similar to those diagnosed with the disease. When describing the survival experience of a group of people or patients typically the method of overall survival is used, and it presents estimates of the proportion of people or patients alive at a certain point in time. The problem with measuring overall survival using Kaplan-Meier or actuarial survival methods, is that the estimates include two causes of death: 1) deaths due to the disease of interest and; 2) deaths due to all other causes, which includes old age, other cancers, trauma and any other possible cause of death. In general, survival analysis is interested in the deaths due to a disease rather than all causes, and therefore a "cause-specific survival analysis" is employed to measure disease-specific survival. Thus, there are two ways in performing a cause-specific survival analysis "competing risks survival analysis" and "relative survival".
In inferential statistics, the term "null hypothesis" usually refers to a general statement or default position that there is no relationship between two measured phenomena, or no difference among groups. Rejecting or disproving the null hypothesis and thus concluding that there are grounds for believing that there is a relationship between two phenomena (e.g. that a potential treatment has a measurable effect) is a central task in the modern practice of science, and gives a precise criterion for rejecting a hypothesis. The null hypothesis is generally assumed to be true until evidence indicates otherwise. In statistics, it is often denoted H0 (read  H-naught , "H-null", or "H-zero"). The concept of a null hypothesis is used differently in two approaches to statistical inference. In the significance testing approach of Ronald Fisher, a null hypothesis is rejected if the observed data is significantly unlikely if the null hypothesis were true. In this case the null hypothesis is rejected and an alternative hypothesis is accepted in its place. If the data are consistent with the null hypothesis, then the null hypothesis is not rejected (i.e., accepted). In neither case is the null hypothesis or its alternative proven; the null hypothesis is tested with data and a decision is made based on how likely or unlikely the data is. This is analogous to a criminal trial, in which the defendant is assumed to be innocent (null is not rejected) until proven guilty (null is rejected) beyond a reasonable doubt (to a statistically significant degree). In the hypothesis testing approach of Jerzy Neyman and Egon Pearson, a null hypothesis is contrasted with an alternative hypothesis, and the two hypotheses are distinguished on the basis of data, with certain error rates. Proponents of each approach criticize the other approach. Nowadays, though, a hybrid approach is widely practiced and presented in textbooks. The hybrid is in turn criticized as incorrect and incoherent for details, see Statistical hypothesis testing. Statistical inference can be done without a null hypothesis, thus avoiding the criticisms under debate. An approach to statistical inference that does not involve a null hypothesis is the following: for each candidate hypothesis, specify a statistical model that corresponds to the hypothesis; then, use model selection techniques to choose the most appropriate model. (The most common selection techniques are based on either Akaike information criterion or Bayes factor.)
In probability theory, a compound Poisson distribution is the probability distribution of the sum of a number of independent identically-distributed random variables, where the number of terms to be added is itself a Poisson-distributed variable. In the simplest cases, the result can be either a continuous or a discrete distribution.
In probability theory, the Landau distribution is a probability distribution named after Lev Landau. Because of the distribution's long tail, the moments of the distribution, like mean or variance, are undefined. The distribution is a special case of the stable distribution.
The modifiable areal unit problem (MAUP) is a source of statistical bias that can radically affect the results of statistical hypothesis tests. It affects results when point-based measures of spatial phenomena (e.g., population density) are aggregated into districts. The resulting summary values (e.g., totals, rates, proportions) are influenced by the choice of district boundaries. For example, census data may be aggregated into census enumeration districts, or postcode areas, or police precincts, or any other spatial partition (thus, the "areal units" are "modifiable"). The issue was recognized in 1934 and later described in detail by Stan Openshaw, who lamented that "the areal units (zonal objects) used in many geographical studies are arbitrary, modifiable, and subject to the whims and fancies of whoever is doing, or did, the aggregating."
Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as classification function: Sensitivity (also called the true positive rate, or the recall in some fields) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition). Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition). Thus sensitivity quantifies the avoiding of false negatives, as specificity does for false positives. For any test, there is usually a trade-off between the measures. For instance, in an airport security setting in which one is testing for potential threats to safety, scanners may be set to trigger on low-risk items like belt buckles and keys (low specificity), in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard (high sensitivity). This trade-off can be represented graphically as a receiver operating characteristic curve. A perfect predictor would be described as 100% sensitive (e.g., all sick are identified as sick) and 100% specific (e.g., no healthy are identified as sick); however, theoretically any predictor will possess a minimum error bound known as the Bayes error rate.
The gravity model of international trade in international economics, similar to other gravity models in social science, predicts bilateral trade flows based on the economic sizes (often using GDP measurements) and distance between two units. The model was first used by Jan Tinbergen in 1962. The basic model for trade between two countries (i and j) takes the form of:  Where F is the trade flow, M is the economic mass of each country, D is the distance and G is a constant. The model has also been used in international relations to evaluate the impact of treaties and alliances on trade, and it has been used to test the effectiveness of trade agreements and organizations such as the North American Free Trade Agreement (NAFTA) and the World Trade Organization (WTO). The model has also been applied to other bilateral flow data (also 'dyadic' data) such as migration, traffic, remittances and foreign direct investment.
Each entry below presents a list of topics about a specific nation or state (country), followed by a link to the main article for that country. Entries for nations are in bold type, while those for subnational entities are in normal (unbolded) type.
The following tables compare general and technical information for a number of statistical analysis packages.
An empirical statistical law or (in popular terminology) a law of statistics represents a type of behaviour that has been found across a number of datasets and, indeed, across a range of types of data sets. Many of these observances have been formulated and proved as statistical or probabilistic theorems and the term "law" has been carried over to these theorems. There are other statistical and probabilistic theorems that also have "law" as a part of their names that have not obviously derived from empirical observations. However, both types of "law" may be considered instances of a scientific law in the field of statistics. For example, both Zipf's law and Heaps' law have been described as "empirical statistical laws" in the field of linguistics. Examples of empirically inspired statistical laws that have a firm theoretical basis include:  Statistical regularity Law of large numbers Law of truly large numbers Central limit theorem Regression towards the mean  Examples of "laws" with a weaker foundation include:  Safety in numbers Benford's law  Examples of "laws" which are more general observations than having a theoretical background:  Rank-size distribution  Examples of supposed "laws" which are incorrect include:  Law of averages
A forecast bias occurs when there are consistent differences between actual outcomes and previously generated forecasts of those quantities; that is: forecasts may have a general tendency to be too high or too low. A normal property of a good forecast is that it is not biased. As a quantitative measure, the "forecast bias" can be specified as a probabilistic or statistical property of the forecast error. A typical measure of bias of forecasting procedure is the arithmetic mean or expected value of the forecast errors, but other measures of bias are possible. For example, a median-unbiased forecast would be one where half of the forecasts are too low and half too high: see Bias of an estimator. In contexts where forecasts are being produced on a repetitive basis, the performance of the forecasting system may be monitored using a tracking signal, which provides an automatically maintained summary of the forecasts produced up to any given time. This can be used to monitor for deteriorating performance of the system.
A violin plot is a method of plotting numeric data. It is a box plot with a rotated kernel density plot on each side. The violin plot is similar to box plots, except that they also show the probability density of the data at different values (in the simplest case this could be a histogram). Typically violin plots will include a marker for the median of the data and a box indicating the interquartile range, as in standard box plots. Overlaid on this box plot is a kernel density estimation. Violin plots are available as extensions to a number of software packages, including the R libraries vioplot, wvioplot, caroline, UsingR, lattice and ggplot2, the Stata add-on command vioplot, and the Python libraries matplotlib and Seaborn.
Statistical inference might be thought of as gambling theory applied to the world around. The myriad applications for logarithmic information measures tell us precisely how to take the best guess in the face of partial information. In that sense, information theory might be considered a formal expression of the theory of gambling. It is no surprise, therefore, that information theory has applications to games of chance.
In statistics, a likelihood ratio test is a statistical test used to compare the goodness of fit of two models, one of which (the null model) is a special case of the other (the alternative model). The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio, or equivalently its logarithm, can then be used to compute a p-value, or compared to a critical value to decide whether to reject the null model in favour of the alternative model. When the logarithm of the likelihood ratio is used, the statistic is known as a log-likelihood ratio statistic, and the probability distribution of this test statistic, assuming that the null model is true, can be approximated using Wilks  theorem. In the case of distinguishing between two models, each of which has no unknown parameters, use of the likelihood ratio test can be justified by the Neyman Pearson lemma, which demonstrates that such a test has the highest power among all competitors.
In mathematics, the Kolmogorov extension theorem (also known as Kolmogorov existence theorem or Kolmogorov consistency theorem) is a theorem that guarantees that a suitably "consistent" collection of finite-dimensional distributions will define a stochastic process. It is credited to the Soviet mathematician Andrey Nikolaevich Kolmogorov.
In statistics, the residual sum of squares (RSS), also known as the sum of squared residuals (SSR) or the sum of squared errors of prediction (SSE), is the sum of the squares of residuals (deviations of predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection. In general, total sum of squares = explained sum of squares + residual sum of squares. For a proof of this in the multivariate ordinary least squares (OLS) case, see partitioning in the general OLS model.
In statistics, the Fisher Tippett Gnedenko theorem (also the Fisher Tippett theorem or the extreme value theorem) is a general result in extreme value theory regarding asymptotic distribution of extreme order statistics. The maximum of a sample of iid random variables after proper renormalization can only converge in distribution to one of 3 possible distributions, the Gumbel distribution, the Fre chet distribution, or the Weibull distribution. Credit for the extreme value theorem (or convergence to types theorem) is given to Gnedenko (1948), previous versions were stated by Ronald Fisher and Leonard Henry Caleb Tippett in 1928 and Fre chet in 1927. The role of the extremal types theorem for maxima is similar to that of central limit theorem for averages, except that the central limit theorem applies to the average of a sample from any distribution with finite variance, while the Fisher-Tippet-Gnedenko theorem only states that if the distribution of a normalized maximum converges, then the limit has to be one of a particular class of distributions. It does not state that the distribution of the normalized maximum does converge.
In statistics and econometrics, the multivariate probit model is a generalization of the probit model used to estimate several correlated binary outcomes jointly. For example, if it is believed that the decisions of sending at least one child to public school and that of voting in favor of a school budget are correlated (both decisions are binary), then the multivariate probit model would be appropriate for jointly predicting these two choices on an individual-specific basis.
For the measure of downside risk, see Variance#Semivariance In spatial statistics, the empirical semivariance is described by  where z is a datum at a particular location, h is the distance between ordered data, and n(h) is the number of paired data at a distance of h. The semivariance is half the variance of the increments , but the whole variance of z-values at given separation distance h (Bachmaier and Backes, 2008). A plot of semivariances versus distances between ordered data in a graph is known as a semivariogram rather than a variogram. Many authors call  a variogram, others use the terms variogram and semivariogram synonymously. However, Bachmaier and Backes (2008), who discussed this confusion, have shown that  should be called a variogram, terms like semivariogram or semivariance should be avoided.
Statistics, like all mathematical disciplines, does not infer valid conclusions from nothing. Inferring interesting conclusions about real statistical populations almost always requires some background assumptions. Those assumptions must be made carefully, because incorrect assumptions can generate wildly inaccurate conclusions. Here are some examples of statistical assumptions. Independence of observations from each other (this assumption is an especially common error). Independence of observational error from potential confounding effects. Exact or approximate normality of observations. Linearity of graded responses to quantitative stimuli, e.g. in linear regression.
In statistics, Fisher consistency, named after Ronald Fisher, is a desirable property of an estimator asserting that if the estimator were calculated using the entire population rather than a sample, the true value of the estimated parameter would be obtained.
Lotka's law, named after Alfred J. Lotka, is one of a variety of special applications of Zipf's law. It describes the frequency of publication by authors in any given field. It states that the number of authors making  contributions in a given period is a fraction of the number making a single contribution, following the formula  where a nearly always equals two, i.e., an approximate inverse-square law, where the number of authors publishing a certain number of articles is a fixed ratio to the number of authors publishing a single article. As the number of articles published increases, authors producing that many publications become less frequent. There are 1/4 as many authors publishing two articles within a specified time period as there are single-publication authors, 1/9 as many publishing three articles, 1/16 as many publishing four articles, etc. Though the law itself covers many disciplines, the actual ratios involved (as a function of 'a') are discipline-specific. The general formula says:  or  where X is the number of publications, Y the relative frequency of authors with X publications, and n and  are constants depending on the specific field (). This law is believed to have applications in other fields, for example in the military for fighter pilot kills.
Randomized response is a research method used in structured survey interview. It was first proposed by S. L. Warner in 1965 and later modified by B. G. Greenberg in 1969. It allows respondents to respond to sensitive issues (such as criminal behavior or sexuality) while maintaining confidentiality. Chance decides, unknown to the interviewer, whether the question is to be answered truthfully, or "yes", regardless of the truth. For example, social scientists have used it to ask people whether they use drugs, whether they have illegally installed telephones, or whether they have evaded paying taxes. Before abortions were legal, social scientists used the method to ask women whether they had had abortions.
A quadrat is a plot used in ecology and geography to isolate a standard unit of area for study of the distribution of an item over a large area. While originally rectangular, modern quadrats can be rectangular, circular, irregular, etc.,. The quadrat is suitable for sampling plants, slow-moving animals (such as millipedes and insects), and some aquatic organisms. When an ecologist wants to know how many organisms there are in a particular habitat, it would not be feasible to count them all. Instead, they would be forced to count a smaller representative part of the population, called a sample. Sampling of plants or animals that do not move much (such as snails), can be done using a sampling square called a quadrat. A suitable size of a quadrat depends on the size of the organisms being sampled. For example, to count plants growing on a school field, one could use a quadrat with sides 0.5 or 1 meter in length. Choice of quadrat size depends to a large extent on the type of survey being conducted. For instance, it would be difficult to gain any meaningful results using a 0.5m2 quadrat in a study of a woodland canopy. It is important that sampling in an area is carried out at random, to avoid bias. For example, if you were sampling from a school field, but for convenience only placed quadrats next to a path, this might not give a sample that was representative of the whole field. It would be an unrepresentative, or biased, sample. One way one can sample randomly is to place the quadrats at coordinates on a numbered grid. Long-term studies may require that the same quadrats be revisited months or even years after initial sampling. Methods of relocating the precise area of study vary widely in accuracy, and include measurement from nearby permanent markers, use of total station theodolites, consumer-grade GPS, and differential GPS.
In educational statistics, a normal curve equivalent (NCE), developed for the United States Department of Education by the RMC Research Corporation, is a way of standardizing scores received on a test into a 0-100 scale similar to a percentile-rank, but preserving the valuable equal-interval properties of a z-score. It is defined as: 50 + 49/qnorm(.99)   z or, approximately 50 + 21.063   z, where z is the standard score or "z-score", i.e. z is how many standard deviations above the mean the raw score is (z is negative if the raw score is below the mean). The reason for the choice of the number 21.06 is to bring about the following result: If the scores are normally distributed (i.e. they follow the "bell-shaped curve") then the normal equivalent score is 99 if the percentile rank of the raw score is 99; the normal equivalent score is 50 if the percentile rank of the raw score is 50; the normal equivalent score is 1 if the percentile rank of the raw score is 1. This relationship between normal equivalent scores and percentile ranks does not hold at values other than 1, 50, and 99. It also fails to hold in general if scores are not normally distributed. The number 21.06 was chosen because It is desired that a score of 99 correspond to the 99th percentile; The 99th percentile in a normal distribution is 2.3263 standard deviations above the mean; 99 is 49 more than 50 thus 49 points above the mean; 49/2.3263 = 21.06. Normal curve equivalents are on an equal-interval scale (see [1] and [2] for examples). This is advantageous compared to percentile rank scales, which suffer from the problem that the difference between any two scores is not the same as that between any other two scores (see below or percentile rank for more information). The major advantage of NCEs over percentile ranks is that NCEs can be legitimately averaged.
In Bayesian statistics, a maximum a posterior probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to Fisher's method of maximum likelihood (ML), but employs an augmented optimization objective which incorporates a prior distribution over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of ML estimation.
In statistics, the Ramsey Regression Equation Specification Error Test (RESET) test is a general specification test for the linear regression model. More specifically, it tests whether non-linear combinations of the fitted values help explain the response variable. The intuition behind the test is that if non-linear combinations of the explanatory variables have any power in explaining the response variable, the model is mis-specified. The test was developed by James B. Ramsey as part of his Ph.D. thesis at the University of Wisconsin Madison in 1968, and later published in the Journal of the Royal Statistical Society in 1969.
In statistics and in particular statistical theory, unbiased estimation of a standard deviation is the calculation from a statistical sample of an estimated value of the standard deviation (a measure of statistical dispersion) of a population of values, in such a way that the expected value of the calculation equals the true value. Except in some important situations, outlined later, the task has little relevance to applications of statistics since its need is avoided by standard procedures, such as the use of significance tests and confidence intervals, or by using Bayesian analysis. However, for statistical theory, it provides an exemplar problem in the context of estimation theory which is both simple to state and for which results cannot be obtained in closed form. It also provides an example where imposing the requirement for unbiased estimation might be seen as just adding inconvenience, with no real benefit.
The following tables compare general and technical information for a number of statistical analysis packages.
The proposition in probability theory known as the law of total expectation, the law of iterated expectations, the tower rule, the smoothing theorem, and Adam's Law among other names, states that if X is an integrable random variable (i.e., a random variable satisfying E( | X | ) <  ) and Y is any random variable, not necessarily integrable, on the same probability space, then  i.e., the expected value of the conditional expected value of X given Y is the same as the expected value of X. The conditional expected value E( X | Y ) is a random variable in its own right, whose value depends on the value of Y. Notice that the conditional expected value of X given the event Y = y is a function of y. If we write E( X | Y = y) = g(y) then the random variable E( X | Y ) is just g(Y). One special case states that if  is a partition of the whole outcome space, i.e. these events are mutually exclusive and exhaustive, then
In mathematics, the error function (also called the Gauss error function) is a special function (non-elementary) of sigmoid shape that occurs in probability, statistics, and partial differential equations describing diffusion. It is defined as:  The complementary error function, denoted erfc, is defined as  which also defines erfcx, the scaled complementary error function (which can be used instead of erfc to avoid arithmetic underflow). Another form of  is known as Craig's formula:  The imaginary error function, denoted erfi, is defined as  where D(x) is the Dawson function (which can be used instead of erfi to avoid arithmetic overflow). Despite the name "imaginary error function",  is real when x is real. When the error function is evaluated for arbitrary complex arguments z, the resulting complex error function is usually discussed in scaled form as the Faddeeva function:  
In statistics, a Q Q plot ("Q" stands for quantile) is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. First, the set of intervals for the quantiles is chosen. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the (number of the) interval for the quantile. If the two distributions being compared are similar, the points in the Q Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q Q plot will approximately lie on a line, but not necessarily on the line y = x. Q Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions. A Q Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Q Q plots can be used to compare collections of data, or theoretical distributions. The use of Q Q plots to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions. A Q Q plot is generally a more powerful approach to do this than the common technique of comparing histograms of the two samples, but requires more skill to interpret. Q Q plots are commonly used to compare a data set to a theoretical model. This can provide an assessment of "goodness of fit" that is graphical, rather than reducing to a numerical summary. Q Q plots are also used to compare two theoretical distributions to each other. Since Q Q plots compare distributions, there is no need for the values to be observed as pairs, as in a scatter plot, or even for the numbers of values in the two groups being compared to be equal. The term "probability plot" sometimes refers specifically to a Q Q plot, sometimes to a more general class of plots, and sometimes to the less commonly used P P plot. The probability plot correlation coefficient is a quantity derived from the idea of Q Q plots, which measures the agreement of a fitted distribution with observed data and which is sometimes used as a means of fitting a distribution to data.
JMP (pronounced "jump") is a computer program for statistics developed by the JMP business unit of SAS Institute. It was created in the 1980s to take advantage of the graphical user interface introduced by the Macintosh. It has since been improved and made available for the Windows operating system. JMP is used in applications such as Six Sigma, quality control and engineering, design of experiments and scientific research. The software consists of five products: JMP, JMP Pro, JMP Clinical, JMP Genomics and the JMP Graph Builder App for the iPad; a scripting language is also available. The software is focused on exploratory analytics, whereby users investigate and explore data, rather than testing a hypothesis.
In statistics, sufficient dimension reduction (SDR) is a paradigm for analyzing data that combines the ideas of dimension reduction with the concept of sufficiency. Dimension reduction has long been a primary goal of regression analysis. Given a response variable y and a p-dimensional predictor vector , regression analysis aims to study the distribution of , the conditional distribution of  given . A dimension reduction is a function  that maps  to a subset of , k < p, thereby reducing the dimension of . For example,  may be one or more linear combinations of . A dimension reduction  is said to be sufficient if the distribution of  is the same as that of . In other words, no information about the regression is lost in reducing the dimension of  if the reduction is sufficient.
In queueing theory, a discipline within the mathematical theory of probability, the Gordon Newell theorem is an extension of Jackson's theorem from open queueing networks to closed queueing networks of exponential servers where customers cannot leave the network. Jackson's theorem cannot be applied to closed networks because the queue length at a node in the closed network is limited by the population of the network. The Gordon Newell theorem calculates the open network solution and then eliminates the infeasible states by renormalizing the probabilities. Calculation of the normalizing constant makes the treatment more awkward as the whole state space must be enumerated. Buzen's algorithm or mean value analysis can be used to calculate the normalizing constant more efficiently.
Statistical software are specialized computer programs for analysis in statistics and econometrics.
In probability theory, the expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents. For example, the expected value in rolling a six-sided die is 3.5 because, roughly speaking, the average of all the numbers that come up in an extremely large number of rolls is very nearly always quite close to three and a half. Less roughly, the law of large numbers states that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions approaches infinity. The expected value is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment. More practically, the expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same principle applies to a continuous random variable, except that an integral of the variable with respect to its probability density replaces the sum. The formal definition subsumes both of these and also works for distributions which are neither discrete nor continuous: the expected value of a random variable is the integral of the random variable with respect to its probability measure. The expected value does not exist for random variables having some distributions with large "tails", such as the Cauchy distribution. For random variables such as these, the long-tails of the distribution prevent the sum/integral from converging. The expected value is a key aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a measure of dispersion of the possible values of the random variable around the expected value. The variance itself is defined in terms of two expectations: it is the expected value of the squared deviation of the variable's value from the variable's expected value. The expected value plays important roles in a variety of contexts. In regression analysis, one desires a formula in terms of observed data that will give a "good" estimate of the parameter giving the effect of some explanatory variable upon a dependent variable. The formula will give different estimates using different samples of data, so the estimate it gives is itself a random variable. A formula is typically considered good in this context if it is an unbiased estimator that is, if the expected value of the estimate (the average value it would give over an arbitrarily large number of separate samples) can be shown to equal the true value of the desired parameter. In decision theory, and in particular in choice under uncertainty, an agent is described as making an optimal choice in the context of incomplete information. For risk neutral agents, the choice involves using the expected values of uncertain quantities, while for risk averse agents it involves maximizing the expected value of some objective function such as a von Neumann Morgenstern utility function. One example of using expected value in reaching optimal decisions is the Gordon Loeb model of information security investment. According to the model, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).
Proportional reduction in loss (PRL) refers to a general framework for developing and evaluating measures of the reliability of particular ways of making observations which are possibly subject to errors of all types. Such measures quantify how much having the observations available has reduced the loss (cost) of the uncertainty about the intended quantity compared with not having those observations. Proportional reduction in error is a more restrictive framework widely used in statistics, in which the general loss function is replaced by a more direct measure of error such as the mean square error. Examples are the coefficient of determination and Goodman and Kruskal's lambda. The concept of proportional reduction in loss was proposed by Bruce Cooil and Roland T. Rust in their 1994 paper. Many commonly used reliability measures for quantitative data (such as continuous data in an experimental design) are PRL measures, including Cronbach's alpha and measures proposed by Ben J. Winer (1971). It also provides a general way of developing measures for the reliability of qualitative data. For example, this framework provides several possible measures that are applicable when a researcher wants to assess the consensus between judges who are asked to code a number of items into mutually exclusive qualitative categories (Cooil and Rust, 1995). Measures of this latter type have been proposed by several researchers, including Perrault and Leigh (1989).
In applied statistics, a partial regression plot attempts to show the effect of adding another variable to a model already having one or more independent variables. Partial regression plots are also referred to as added variable plots, adjusted variable plots, and individual coefficient plots. When performing a linear regression with a single independent variable, a scatter plot of the response variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, things become more complicated. Although it can still be useful to generate scatter plots of the response variable against each of the independent variables, this does not take into account the effect of the other independent variables in the model. Partial regression plots are formed by: Computing the residuals of regressing the response variable against the independent variables but omitting Xi Computing the residuals from regressing Xi against the remaining independent variables Plotting the residuals from (1) against the residuals from (2). Velleman and Welsch (see References below) express this mathematically as:  where Y.[i] = residuals from regressing Y (the response variable) against all the independent variables except Xi Xi.[i] = residuals from regressing Xi against the remaining independent variables. Velleman and Welsch list the following useful properties for this plot: The least squares linear fit to this plot has the slope  and intercept zero. The residuals from the least squares linear fit to this plot are identical to the residuals from the least squares fit of the original model (Y against all the independent variables including Xi). The influences of individual data values on the estimation of a coefficient are easy to see in this plot. It is easy to see many kinds of failures of the model or violations of the underlying assumptions (nonlinearity, heteroscedasticity, unusual patterns). Partial regression plots are widely discussed in the regression diagnostics literature (e.g., see the References section below). Since the strengths and weaknesses of partial regression plots are widely discussed in the literature, it is not discussed in any detail here. Partial regression plots are related to, but distinct from, partial residual plots. Partial regression plots are most commonly used to identify data points with high leverage and influential data points that might not have high leverage. Partial residual plots are most commonly used to identify the nature of the relationship between Y and Xi (given the effect of the other independent variables in the model). Note that since the simple correlation between the two sets of residuals plotted is equal to the partial correlation between the response variable and Xi, partial regression plots will show the correct strength of the linear relationship between the response variable and Xi. This is not true for partial residual plots. On the other hand, for the partial regression plot, the x-axis is not Xi. This limits its usefulness in determining the need for a transformation (which is the primary purpose of the partial residual plot).
In statistics a uniformly minimum-variance unbiased estimator or minimum-variance unbiased estimator (UMVUE or MVUE) is an unbiased estimator that has lower variance than any other unbiased estimator for all possible values of the parameter. For practical statistics problems, it is important to determine the UMVUE if one exists, since less-than-optimal procedures would naturally be avoided, other things being equal. This has led to substantial development of statistical theory related to the problem of optimal estimation. While the particular specification of "optimal" here   requiring unbiasedness and measuring "goodness" using the variance   may not always be what is wanted for any given practical situation, it is one where useful and generally applicable results can be found.  
Analysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, in different business, science, and social science domains. Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes. Business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. In statistical applications, some people divide data analysis into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis. Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.
In econometrics and time series analysis, the Epps effect, named after T. W. Epps, is the phenomenon that the empirical correlation between the returns of two different stocks decreases as the sampling frequency of data increases. The phenomenon is caused by non-synchronous/asynchronous trading  and discretization effects. However, a current study shows that the effect originates in investors' herd behaviour.
Statistica is an advanced analytics software package originally developed by StatSoft which was acquired by Dell in March, 2014. Statistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures. Statistica product categories include Enterprise (for use across a site or organization), Web-Based (for use with a server and web browser), Concurrent Network Desktop, and Single-User Desktop.
Geostatistics is a branch of statistics focusing on spatial or spatiotemporal datasets. Developed originally to predict probability distributions of ore grades for mining operations, it is currently applied in diverse disciplines including petroleum geology, hydrogeology, hydrology, meteorology, oceanography, geochemistry, geometallurgy, geography, forestry, environmental control, landscape ecology, soil science, and agriculture (esp. in precision farming). Geostatistics is applied in varied branches of geography, particularly those involving the spread of diseases (epidemiology), the practice of commerce and military planning (logistics), and the development of efficient spatial networks. Geostatistical algorithms are incorporated in many places, including geographic information systems (GIS) and the R statistical environment.
Principal stratification is a statistical technique used in causal inference when adjusting results for post-treatment covariates. The idea is to identify underlying strata and then compute causal effects only within strata. It is a generalization of the Local Average Treatment Effect (LATE).
In mathematical finance, a Monte Carlo option model uses Monte Carlo methods  to calculate the value of an option with multiple sources of uncertainty or with complicated features. The first application to option pricing was by Phelim Boyle in 1977 (for European options). In 1996, M. Broadie and P. Glasserman showed how to price Asian options by Monte Carlo. In 2001 F. A. Longstaff and E. S. Schwartz developed a practical Monte Carlo method for pricing American-style options.
The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling.
Pedometric mapping, or statistical soil mapping, is data-driven generation of soil property and class maps that is based on use of statistical methods. The main objective of pedometric mapping is to predict values of some soil variable at unobserved locations and access the uncertainty of that estimate using statistical inference i.e. statistically optimal approaches. From the application point of view, the main objective of soil mapping is to accurately predict response of a soil-plant ecosystem to various soil management strategies. In other words, the main objective of pedometric mapping is to generate maps of soil properties and soil classes that can be used to feed other environmental models or for decision making. Pedometric mapping is largely based on applying geostatistics in soil science and other statistical methods used in pedometrics. Although pedometric mapping is mainly data-driven, it can also largely be based on use of expert knowledge. The expert knowledge, however, needs to be plugged-in into a pedometric computational framework so that it can be used to produce more accurate prediction models. For example, data assimilation techniques, such as the space time Kalman filter, can be used to integrate pedogenetic knowledge and field observations. In the information theory context, the objective of pedometric mapping is to describe the spatial complexity of soils (information content of soil variables over a geographical area), then represent this complexity using maps, summary measures, mathematical models and simulations. Simulations are a preferred way of visualizing soil patterns as they represent both the deterministic pattern due to the landscape, geographic hot-spots and short range variability (see image below).
The Panjer recursion is an algorithm to compute the probability distribution approximation of a compound random variable . where both  and  are random variables and of special types. In more general cases the distribution of S is a compound distribution. The recursion for the special cases considered was introduced in a paper  by Harry Panjer (Emeritus professor, University of Waterloo). It is heavily used in actuarial science (see also systemic risk).
In statistics, the Robbins lemma, named after Herbert Robbins, states that if X is a random variable having a Poisson distribution with parameter  , and f is any function for which the expected value E(f(X)) exists, then  Robbins introduced this proposition while developing empirical Bayes methods.
In statistics, an effect size is a quantitative measure of the strength of a phenomenon. Examples of effect sizes are the correlation between two variables, the regression coefficient in a regression, the mean difference, or even the risk with which something happens, such as how many people survive after a heart attack for every one person that does not survive. For each type of effect-size, a larger absolute value always indicates a stronger effect. Effect sizes complement statistical hypothesis testing, and play an important role in power analyses, sample size planning, and in meta-analyses. They are the first item (magnitude) in the MAGIC criteria for evaluating the strength of a statistical claim. Especially in meta-analysis, where the purpose is to combine multiple effect-sizes, the standard error (S.E.) of effect-size is of critical importance. The S.E. of effect-size is used to weight effect-sizes when combining studies, so that large studies are considered more important than small studies in the analysis. The S.E. of effect-size is calculated differently for each type of effect-size, but generally only requires knowing the study's sample size (N), or the number of observations in each group (n's). Reporting effect sizes is considered good practice when presenting empirical research findings in many fields. The reporting of effect sizes facilitates the interpretation of the substantive, as opposed to the statistical, significance of a research result. Effect sizes are particularly prominent in social and medical research. Relative and absolute measures of effect size convey different information, and can be used complementarily. A prominent task force in the psychology research community expressed the following recommendation:  Always present effect sizes for primary outcomes...If the units of measurement are meaningful on a practical level (e.g., number of cigarettes smoked per day), then we usually prefer an unstandardized measure (regression coefficient or mean difference) to a standardized measure (r or d).
In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the conditional probability that is assigned after the relevant evidence or background is taken into account. Similarly, the posterior probability distribution is the probability distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained from an experiment or survey. "Posterior", in this context, means after taking into account the relevant evidence related to the particular case being examined.
Multivariate analysis of covariance (MANCOVA) is an extension of analysis of covariance (ANCOVA) methods to cover cases where there is more than one dependent variable and where the control of concomitant continuous independent variables   covariates   is required. The most prominent benefit of the MANCOVA design over the simple MANOVA is the 'factoring out' of noise or error that has been introduced by the covariant. A commonly used multivariate version of the ANOVA F-statistic is Wilks' Lambda ( ), which represents the ratio between the error variance (or covariance) and the effect variance (or covariance).
In probability and statistics, a mixture distribution is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized. The underlying random variables may be random real numbers, or they may be random vectors (each having the same dimension), in which case the mixture distribution is a multivariate distribution. In cases where each of the underlying random variables is continuous, the outcome variable will also be continuous and its probability density function is sometimes referred to as a mixture density. The cumulative distribution function (and the probability density function if it exists) can be expressed as a convex combination (i.e. a weighted sum, with non-negative weights that sum to 1) of other distribution functions and density functions. The individual distributions that are combined to form the mixture distribution are called the mixture components, and the probabilities (or weights) associated with each component are called the mixture weights. The number of components in mixture distribution is often restricted to being finite, although in some cases the components may be countably infinite. More general cases (i.e. an uncountable set of component distributions), as well as the countable case, are treated under the title of compound distributions. A distinction needs to be made between a random variable whose distribution function or density is the sum of a set of components (i.e. a mixture distribution) and a random variable whose value is the sum of the values of two or more underlying random variables, in which case the distribution is given by the convolution operator. As an example, the sum of two jointly normally distributed random variables, each with different means, will still have a normal distribution. On the other hand, a mixture density created as a mixture of two normal distributions with different means will have two peaks provided that the two means are far enough apart, showing that this distribution is radically different from a normal distribution. Mixture distributions arise in many contexts in the literature and arise naturally where a statistical population contains two or more subpopulations. They are also sometimes used as a means of representing non-normal distributions. Data analysis concerning statistical models involving mixture distributions is discussed under the title of mixture models, while the present article concentrates on simple probabilistic and statistical properties of mixture distributions and how these relate to properties of the underlying distributions.
In statistical classification, the Fisher kernel, named after Ronald Fisher, is a function that measures the similarity of two objects on the basis of sets of measurements for each object and a statistical model. In a classification procedure, the class for a new object (whose real class is unknown) can be estimated by minimising, across classes, an average of the Fisher kernel distance from the new object to each known member of the given class. The Fisher kernel was introduced in 1998. It combines the advantages of generative statistical models (like the hidden Markov model) and those of discriminative methods (like support vector machines): generative models can process data of variable length (adding or removing data is well-supported) discriminative methods can have flexible criteria and yield better results.
In statistics, the binomial test is an exact test of the statistical significance of deviations from a theoretically expected distribution of observations into two categories.
In mathematical optimization, statistics, decision theory and machine learning, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (sometimes called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. In statistics, typically a loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century. In the context of economics, for example, this is usually economic cost or regret. In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Crame r in the 1920s. In optimal control the loss is the penalty for failing to achieve a desired value. In financial risk management the function is precisely mapped to a monetary loss.
In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by   and  , that appear as exponents of the random variable and control the shape of the distribution. The beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines. For example, it has been used as a statistical description of allele frequencies in population genetics; time allocation in project management / control systems; sunshine data; variability of soil properties; proportions of the minerals in rocks in stratigraphy; and heterogeneity in the probability of HIV transmission. In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions. The usual formulation of the beta distribution is also known as the beta distribution of the first kind, whereas beta distribution of the second kind is an alternative name for the beta prime distribution.
In probability theory, the slash distribution is the probability distribution of a standard normal variate divided by an independent standard uniform variate. In other words, if the random variable Z has a normal distribution with zero mean and unit variance, the random variable U has a uniform distribution on [0,1] and Z and U are statistically independent, then the random variable X = Z / U has a slash distribution. The slash distribution is an example of a ratio distribution. The distribution was named by William H. Rogers and John Tukey in a paper published in 1972. The probability density function (pdf) is  where  (x) is the probability density function of the standard normal distribution. The result is undefined at x = 0, but the discontinuity is removable:  The most common use of the slash distribution is in simulation studies. It is a useful distribution in this context because it has heavier tails than a normal distribution, but it is not as pathological as the Cauchy distribution.
The Nakagami distribution or the Nakagami-m distribution is a probability distribution related to the gamma distribution. It has two parameters: a shape parameter  and a second parameter controlling spread, .
In statistics, analysis of rhythmic variance (ANORVA) is a method for detecting rhythms in biological time series, published by Peter Celec (Biol Res. 2004, 37(4 Suppl A):777 82). It is a procedure for detecting cyclic variations in biological time series and quantification of their probability. ANORVA is based on the premise that the variance in groups of data from rhythmic variables is low when a time distance of one period exists between the data entries.
Abductive reasoning (also called abduction, abductive inference or retroduction) is a form of logical inference which goes from an observation to a theory which accounts for the observation, ideally seeking to find the simplest and most likely explanation. In abductive reasoning, unlike in deductive reasoning, the premises do not guarantee the conclusion. One can understand abductive reasoning as "inference to the best explanation". The fields of law, computer science, and artificial intelligence research renewed interest in the subject of abduction. Diagnostic expert systems frequently employ abduction.
Selection bias is the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. The phrase "selection bias" most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate.
In statistics, ordinal regression (also called "ordinal classification") is a type of regression analysis used for predicting an ordinal variable, i.e. a variable whose value exists on an arbitrary scale where only the relative ordering between different values is significant. It can be considered an intermediate problem in between (metric) regression and classification. Ordinal regression turns up often in the social sciences, for example in the modeling of human levels of preference (on a scale from, say, 1 5 for "very poor" through "excellent"), as well as in information retrieval. In machine learning, ordinal regression may also be called ranking learning.
The Kruskal Wallis test by ranks, Kruskal Wallis H test (named after William Kruskal and W. Allen Wallis), or One-way ANOVA on ranks is a non-parametric method for testing whether samples originate from the same distribution. It is used for comparing two or more independent samples of equal or different sample sizes. It extends the Mann Whitney U test when there are more than two groups. The parametric equivalent of the Kruskal-Wallis test is the one-way analysis of variance (ANOVA). A significant Kruskal-Wallis test indicates that at least one sample stochastically dominates one other sample. The test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains. Dunn's test would help analyze the specific sample pairs for stochastic dominance. Since it is a non-parametric method, the Kruskal Wallis test does not assume a normal distribution of the residuals, unlike the analogous one-way analysis of variance. If the researcher can make the less stringent assumptions of an identically shaped and scaled distribution for all groups, except for any difference in medians, then the null hypothesis is that the medians of all groups are equal, and the alternative hypothesis is that at least one population median of one group is different from the population median of at least one other group.
Empirical likelihood (EL) is an estimation method in statistics. Empirical likelihood estimates require few assumptions about the error distribution compared to similar methods like maximum likelihood. EL can handle data well as long as it is independent and identically distributed (iid). EL performs well even when the distribution is asymmetric or censored. EL methods are also useful since they can easily incorporate constraints and prior information. Art Owen pioneered work in this area with his 1988 paper.
Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The basic concept is the reduction of multitudinous amounts of data down to the meaningful parts. When information is derived from instrument readings there may also be a transformation from analog to digital form. When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, coding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed. Often the data reduction is undertaken in the presence of reading or measurement errors. Some idea of the nature of these errors is needed before the most likely value may be determined. An example in astronomy is the data reduction in the Kepler satellite. This satellite records 95-megapixel images once every six seconds, generating tens of megabytes of data per second, which is orders of magnitudes more than the downlink bandwidth of 550 KBps. The on-board data reduction encompasses co-adding the raw frames for thirty minutes, reducing the bandwidth by a factor of 300. Furthermore, interesting targets are pre-selected and only the relevant pixels are processed, which is 6% of the total. This reduced data is then sent to Earth where it is processed further.
In economics and finance, an index is a statistical measure of changes in a representative group of individual data points. These data may be derived from any number of sources, including company performance, prices, productivity, and employment. Economic indices track economic health from different perspectives. Influential global financial indices such as the Global Dow, and the NASDAQ Composite track the performance of selected large and powerful companies in order to evaluate and predict economic trends. The Dow Jones Industrial Average and the S&P 500 primarily track U.S. markets, though some legacy international companies are included. The consumer price index tracks the variation in prices for different consumer goods and services over time in a constant geographical location, and is integral to calculations used to adjust salaries, bond interest rates, and tax thresholds for inflation. The GDP Deflator Index, or real GDP, measures the level of prices of all new, domestically produced, final goods and services in an economy. Market performance indices include the labour market index/job index and proprietary stock market index investment instruments offered by brokerage houses. Some indices display market variations that cannot be captured in other ways. For example, the Economist provides a Big Mac Index that expresses the adjusted cost of a globally ubiquitous Big Mac as a percentage over or under the cost of a Big Mac in the U.S. in USD (estimated: $3.57). The least relatively expensive Big Mac price occurs in Hong Kong, at a 52% reduction from U.S. prices, or $1.71 U.S. Such indices can be used to help forecast currency values. From this example, it would be assumed that Hong Kong currency is undervalued, and provides a currency investment opportunity.
In mathematical finance, the Cox Ingersoll Ross model (or CIR model) describes the evolution of interest rates. It is a type of "one factor model" (short rate model) as it describes interest rate movements as driven by only one source of market risk. The model can be used in the valuation of interest rate derivatives. It was introduced in 1985 by John C. Cox, Jonathan E. Ingersoll and Stephen A. Ross as an extension of the Vasicek model.
Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically. NMF finds applications in such fields as computer vision, document clustering, chemometrics, audio signal processing and recommender systems.  
In statistics, regression validation is the process of deciding whether the numerical results quantifying hypothesized relationships between variables, obtained from regression analysis, are acceptable as descriptions of the data. The validation process can involve analyzing the goodness of fit of the regression, analyzing whether the regression residuals are random, and checking whether the model's predictive performance deteriorates substantially when applied to data that were not used in model estimation.
The Spearman Brown prediction formula, also known as the Spearman Brown prophecy formula, is a formula relating psychometric reliability to test length and used by psychometricians to predict the reliability of a test after changing the test length. The method was published independently by Spearman (1910) and Brown (1910).
In statistics, the projection matrix , sometimes also called the influence matrix or hat matrix , maps the vector of response values (dependent variable values) to the vector of fitted values (or predicted values). It describes the influence each response value has on each fitted value. The diagonal elements of the projection matrix are the leverages, which describe the influence each response value has on the fitted value for that same observation.
The junction tree algorithm (also known as 'Clique Tree') is a method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The basic premise is to eliminate cycles by clustering them into single nodes.
Analysis of covariance (ANCOVA) is a general linear model which blends ANOVA and regression. ANCOVA evaluates whether population means of a dependent variable (DV) are equal across levels of a categorical independent variable (IV) often called a treatment, while statistically controlling for the effects of other continuous variables that are not of primary interest, known as covariates (CV) or nuisance variables. Mathematically, ANCOVA decomposes the variance in the DV into variance explained by the CV(s), variance explained by the categorical IV, and residual variance. Intuitively, ANCOVA can be thought of as 'adjusting' the DV by the group means of the CV(s). The ANCOVA procedure is described as follows, assuming that a linear relationship between the response (DV) and covariate (CV) exists:  where  is the jth observation under the ith categorical group,  is the grand mean,  is the effect of the ith level of the IV,  is the jth observation of the covariate under the ith group,  is the ith group mean, and  is the associated unobserved error term. Under this specification, we assume that the categorical treatment effects sum to zero  The standard assumptions of the linear regression model are also assumed to hold, as discussed below.
Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. The algorithm for inducing Breiman's random forest was developed by Leo Breiman and Adele Cutler, and "Random Forests" is their trademark. The method combines Breiman's "bagging" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance. The selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.
In statistics and computational geometry, the notion of centerpoint is a generalization of the median to data in higher-dimensional Euclidean space. Given a set of points in d-dimensional space, a centerpoint of the set is a point such that any hyperplane that goes through that point divides the set of points in two roughly equal subsets: the smaller part should have at least a 1/(d + 1) fraction of the points. Like the median, a centerpoint need not be one of the data points. Every non-empty set of points (with no duplicates) has at least one centerpoint.
The law of comparative judgment was conceived by L. L. Thurstone. In modern-day terminology, it is more aptly described as a model that is used to obtain measurements from any process of pairwise comparison. Examples of such processes are the comparison of perceived intensity of physical stimuli, such as the weights of objects, and comparisons of the extremity of an attitude expressed within statements, such as statements about capital punishment. The measurements represent how we perceive objects, rather than being measurements of actual physical properties. This kind of measurement is the focus of psychometrics and psychophysics. In somewhat more technical terms, the law of comparative judgment is a mathematical representation of a discriminal process, which is any process in which a comparison is made between pairs of a collection of entities with respect to magnitudes of an attribute, trait, attitude, and so on. The theoretical basis for the model is closely related to item response theory and the theory underlying the Rasch model, which are used in psychology and education to analyse data from questionnaires and tests.
Generalized multidimensional scaling (GMDS) is an extension of metric multidimensional scaling, in which the target space is non-Euclidean. When the dissimilarities are distances on a surface and the target space is another surface, GMDS allows finding the minimum-distortion embedding of one surface into another. GMDS is an emerging research direction. Currently, main applications are recognition of deformable objects (e.g. for three-dimensional face recognition) and texture mapping.  
Systematic sampling is a statistical method involving the selection of elements from an ordered sampling frame. The most common form of systematic sampling is an equal-probability method. In this approach, progression through the list is treated circularly, with a return to the top once the end of the list is passed. The sampling starts by selecting an element from the list at random and then every kth element in the frame is selected, where k, the sampling interval (sometimes known as the skip): this is calculated as:  where n is the sample size, and N is the population size. Using this procedure each element in the population has a known and equal probability of selection. This makes systematic sampling functionally similar to simple random sampling. However it is not the same as an SRS because not every possible sample of a certain size has an equal chance of being chosen (e.g. samples with at least two elements adjacent to each other will never be chosen by systematic sampling). It is however, much more efficient (if variance within systematic sample is more than variance of population). Systematic sampling is to be applied only if the given population is logically homogeneous, because systematic sample units are uniformly distributed over the population. The researcher must ensure that the chosen sampling interval does not hide a pattern. Any pattern would threaten randomness. Example: Suppose a supermarket wants to study buying habits of their customers, then using systematic sampling they can choose every 10th or 15th customer entering the supermarket and conduct the study on this sample. This is random sampling with a system. From the sampling frame, a starting point is chosen at random, and choices thereafter are at regular intervals. For example, suppose you want to sample 8 houses from a street of 120 houses. 120/8=15, so every 15th house is chosen after a random starting point between 1 and 15. If the random starting point is 11, then the houses selected are 11, 26, 41, 56, 71, 86, 101, and 116. As an aside, if every 15th house was a "corner house" then this corner pattern could destroy the randomness of the population. If, as more frequently, the population is not evenly divisible (suppose you want to sample 8 houses out of 125, where 125/8=15.625), should you take every 15th house or every 16th house  If you take every 16th house, 8*16=128, so there is a risk that the last house chosen does not exist. On the other hand, if you take every 15th house, 8*15=120, so the last five houses will never be selected. The random starting point should instead be selected as a noninteger between 0 and 15.625 (inclusive on one endpoint only) to ensure that every house has equal chance of being selected; the interval should now be nonintegral (15.625); and each noninteger selected should be rounded up to the next integer. If the random starting point is 3.6, then the houses selected are 4, 20, 35, 50, 66, 82, 98, and 113, where there are 3 cyclic intervals of 15 and 4 intervals of 16. To illustrate the danger of systematic skip concealing a pattern, suppose we were to sample a planned neighbourhood where each street has ten houses on each block. This places houses No. 1, 10, 11, 20, 21, 30... on block corners; corner blocks may be less valuable, since more of their area is taken up by streetfront etc. that is unavailable for building purposes. If we then sample every 10th household, our sample will either be made up only of corner houses (if we start at 1 or 10) or have no corner houses (any other start); either way, it will not be representative. Systematic sampling may also be used with non-equal selection probabilities. In this case, rather than simply counting through elements of the population and selecting every kth unit, we allocate each element a space along a number line according to its selection probability. We then generate a random start from a uniform distribution between 0 and 1, and move along the number line in steps of 1. Example: We have a population of 5 units (A to E). We want to give unit A a 20% probability of selection, unit B a 40% probability, and so on up to unit E (100%). Assuming we maintain alphabetical order, we allocate each unit to the following interval:  A: 0 to 0.2 B: 0.2 to 0.6 (= 0.2 + 0.4) C: 0.6 to 1.2 (= 0.6 + 0.6) D: 1.2 to 2.0 (= 1.2 + 0.8) E: 2.0 to 3.0 (= 2.0 + 1.0)  If our random start was 0.156, we would first select the unit whose interval contains this number (i.e. A). Next, we would select the interval containing 1.156 (element C), then 2.156 (element E). If instead our random start was 0.350, we would select from points 0.350 (B), 1.350 (D), and 2.350 (E).
In probability theory and statistics, the logistic distribution is a continuous probability distribution. Its cumulative distribution function is the logistic function, which appears in logistic regression and feedforward neural networks. It resembles the normal distribution in shape but has heavier tails (higher kurtosis). The Tukey lambda distribution can be considered a generalization of the logistic distribution since it adds a shape parameter,   (the Tukey distribution is logistic when   is zero).
In statistics the assumed mean is a method for calculating the arithmetic mean and standard deviation of a data set. It simplifies calculating accurate values by hand. Its interest today is chiefly historical but it can be used to quickly estimate these statistics. There are other rapid calculation methods which are more suited for computers which also ensure more accurate results than the obvious methods.
In quantum mechanics, information theory, and Fourier analysis, the entropic uncertainty or Hirschman uncertainty is defined as the sum of the temporal and spectral Shannon entropies. It turns out that Heisenberg's uncertainty principle can be expressed as a lower bound on the sum of these entropies. This is stronger than the usual statement of the uncertainty principle in terms of the product of standard deviations. In 1957, Hirschman considered a function f and its Fourier transform g such that  where the " " indicates convergence in L2, and normalized so that (by Plancherel's theorem),  He showed that for any such functions the sum of the Shannon entropies is non-negative,  A tighter bound,  was conjectured by Hirschman and Everett, proven in 1975 by W. Beckner and in the same year interpreted by as a generalized quantum mechanical uncertainty principle by Bia ynicki-Birula and Mycielski. The equality holds in the case of Gaussian distributions. Note, however, that the above entropic uncertainty function is distinctly different from the quantum Von Neumann entropy represented in phase space.
In probability theory and statistics, the coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. It is often expressed as a percentage, and is defined as the ratio of the standard deviation  to the mean  (or its absolute value, ). The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay. It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R.
In statistics, the Anscombe transform, named after Francis Anscombe, is a variance-stabilizing transformation that transforms a random variable with a Poisson distribution into one with an approximately standard Gaussian distribution. The Anscombe transform is widely used in photon-limited imaging (astronomy, X-ray) where images naturally follow the Poisson law. The Anscombe transform is usually used to pre-process the data in order to make the standard deviation approximately constant. Then denoising algorithms designed for the framework of additive white Gaussian noise are used; the final estimate is then obtained by applying an inverse Anscombe transformation to the denoised data.
In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other. A drawback of this method is that it tends to produce long thin clusters in which nearby elements of the same cluster have small distances, but elements at opposite ends of a cluster may be much farther from each other than to elements of other clusters. This may lead to difficulties in defining classes that could usefully subdivide the data.
Bioinformatics / ba .o   nf r m t ks/ is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques. Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.
In mathematics, more specifically in the theory of Monte Carlo methods, variance reduction is a procedure used to increase the precision of the estimates that can be obtained for a given number of iterations. Every output random variable from the simulation is associated with a variance which limits the precision of the simulation results. In order to make a simulation statistically efficient, i.e., to obtain a greater precision and smaller confidence intervals for the output random variable of interest, variance reduction techniques can be used. The main ones are: Common random numbers, antithetic variates, control variates, importance sampling and stratified sampling. Under these headings are a variety of specialized techniques; for example, particle transport simulations make extensive use of "weight windows" and "splitting/Russian roulette" techniques, which are a form of importance sampling.
In statistics and quantitative research methodology, a data sample is a set of data collected and/or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Typically, the population is very large, making a census or a complete enumeration of all the values in the population impractical or impossible. The sample usually represents a subset of manageable size. Samples are collected and statistics are calculated from the samples so that one can make inferences or extrapolations from the sample to the population. The data sample may be drawn from a population without replacement, in which case it is a subset of a population; or with replacement, in which case it is a multisubset.
The Nelson Aalen estimator is a non-parametric estimator of the cumulative hazard rate function in case of censored data or incomplete data. It is used in survival theory, reliability engineering and life insurance to estimate the cumulative number of expected events. An "event" can be the failure of a non-repairable component, the death of a human being, or any occurrence for which the experimental unit remains in the "failed" state (e.g., death) from the point at which it changed on. The estimator is given by  with  the number of events at  and  the total individuals at risk at . The curvature of the Nelson Aalen estimator gives an idea of the hazard rate shape. A concave shape is an indicator for infant mortality while a convex shape indicates wear out mortality. It can be used for example when testing the homogeneity of Poisson processes.
In statistics, resampling is any of a variety of methods for doing one of the following: Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping) Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests) Validating models by using random subsets (bootstrapping, cross validation) Common resampling techniques include bootstrapping, jackknifing and permutation tests.
In probability theory, the continuous mapping theorem states that continuous functions are limit-preserving even if their arguments are sequences of random variables. A continuous function, in Heine s definition, is such a function that maps convergent sequences into convergent sequences: if xn   x then g(xn)   g(x). The continuous mapping theorem states that this will also be true if we replace the deterministic sequence {xn} with a sequence of random variables {Xn}, and replace the standard notion of convergence of real numbers     with one of the types of convergence of random variables. This theorem was first proved by Mann & Wald (1943), and it is therefore sometimes called the Mann Wald theorem.
In statistics, the median absolute deviation (MAD) is a robust measure of the variability of a univariate sample of quantitative data. It can also refer to the population parameter that is estimated by the MAD calculated from a sample. For a univariate data set X1, X2, ..., Xn, the MAD is defined as the median of the absolute deviations from the data's median:  that is, starting with the residuals (deviations) from the data's median, the MAD is the median of their absolute values.
The Unistat computer program is a statistical data analysis tool featuring two modes of operation: The stand-alone user interface is a complete workbench for data input, analysis and visualization while the Microsoft Excel add-in mode extends the features of the mainstream spreadsheet application with powerful analytical capabilities. With its first release in 1984, Unistat soon differentiated itself by targeting the new generation of microcomputers that were becoming commonplace in offices and homes at a time when data analysis was largely the domain of big iron mainframe and minicomputers. Since then, the product has gone through several major revisions targeting various desktop computing platforms, but its development has always been focused on user interaction and dynamic visualization. As desktop computing has continued to proliferate throughout the 1990s and onwards, Unistat's end-user oriented interface has attracted a following amongst biomedicine researchers, social scientists, market researchers, government departments and students, enabling them to perform complex data analysis without the need for large manuals and scripting languages. Procedures supported by Unistat include: Statistical graphics: Scatter plot, Line chart, Box plot, Probability plot, Histogram, Stem-and-leaf plot, Open-high-low-close chart, Bland-Altman plot Parametric statistics: Student's t-test, F test, Levene's test, equivalence tests for means Goodness of fit: Kolmogorov-Smirnov test, chi-squared test, Shapiro Wilk test, Lilliefors test, Anderson Darling test, Crame r von Mises statistic Correlations: Pearson product-moment correlation coefficient, Spearman's rank correlation coefficient, Kendall tau rank correlation coefficient, Partial correlation, Intraclass correlation Non-parametric statistics: Mann Whitney U, Hodges Lehmann estimator, Wald Wolfowitz runs test, Moses Extreme Reaction test, Median test, Wilcoxon signed-rank test, Sign test, binomial test, Noninferiority Test, Superiority test, Equivalence test, Odds ratio, Relative risk, Fisher's exact test, McNemar's test, Tetrachoric Correlation, Sensitivity and specificity, Prevalence, Youden's index, Positive predictive value, Negative predictive value, Likelihood ratios Kruskal Wallis one-way analysis of variance, Friedman two-way analysis of variance, Cochran's Q test, Cohen's kappa Contingency table: Pearson's chi-squared test, Phi coefficient, Kendall's tau, Kendall's W, Crame r's V, Goodman and Kruskal's lambda Regression analysis: Linear regression, Stepwise regression, Nonlinear regression, logit/probit/gompit, logistic regression, multinomial logit, Poisson regression, Box-Cox transformation, Cox regression ROC analysis Meta analysis Analysis of variance General linear model Multiple comparisons / Post-hoc analysis: Tukey's HSD, Scheffe method, Studentized range, Duncan's new multiple range test, Tukey's range test, Bonferroni, Student-Newman-Keuls Multivariate analysis: Cluster analysis, Principal components analysis, Linear discriminant analysis, canonical analysis, Multidimensional scaling, Canonical correlation analysis Time series: ARIMA, Exponential moving average Reliability analysis Survival analysis: Life table, Kaplan Meier analysis, Cox regression Quality control / Statistical process control: Control chart, Run chart, EWMA chart, Pareto chart, Process capability, ANOVA Gage R&R, Weibull distribution Bioassay Analysis: This optional module features potency estimation with Dilution assay, parallel line, slope ratio and quantal response methods, with Fieller confidence intervals, validity tests, ED50 and graphical representations.
The term cosmic variance is the statistical uncertainty inherent in observations of the universe at extreme distances. It has three different but closely related meanings: It is sometimes used, incorrectly, to mean sample variance - the difference between different finite samples of the same parent population. Such differences follow a Poissonian distribution, and in this case the term sample variance should be used instead. It is sometimes used, mainly by cosmologists, to mean the uncertainty because we can only observe one realization of all the possible observable universes. For example, we can only observe one Cosmic Microwave Background, so the measured positions of the peaks in the Cosmic Microwave Background spectrum, integrated over the visible sky, are limited by the fact that only one spectrum is observable from Earth. The observable universe viewed from another Galaxy will have the peaks in slightly different places, while remaining consistent with the same physical laws, inflation, etc. This second meaning may be regarded as a special case of the third meaning. The most widespread use, to which the rest of this article refers, reflects the fact that measurements are affected by cosmic large-scale structure, so a measurement of any region of sky (viewed from Earth) may differ from a measurement of a different region of sky (also viewed from Earth) by an amount that may be much greater than the sample variance. This most widespread use of the term is based on the idea that it is only possible to observe part of the universe at one particular time, so it is difficult to make statistical statements about cosmology on the scale of the entire universe, as the number of observations (sample size) must be too small.
Optimal Discriminant Analysis (ODA) and the related classification tree analysis (CTA) are exact statistical methods that maximize predictive accuracy. For any specific sample and exploratory or confirmatory hypothesis, optimal discriminant analysis (ODA) identifies the statistical model that yields maximum predictive accuracy, assesses the exact Type I error rate, and evaluates potential cross-generalizability. Optimal discriminant analysis may be applied to > 0 dimensions, with the one-dimensional case being referred to as UniODA and the multidimensional case being referred to as MultiODA. Classification tree analysis is a generalization of optimal discriminant analysis to non-orthogonal trees. Classification tree analysis has more recently been called "hierarchical optimal discriminant analysis". Optimal discriminant analysis and classification tree analysis may be used to find the combination of variables and cut points that best separate classes of objects or events. These variables and cut points may then be used to reduce dimensions and to then build a statistical model that optimally describes the data. Optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while optimal discriminant analysis gives a dependent variable that is a class variable.
SOFA Statistics is an open-source statistical package, with an emphasis on ease of use, learn as you go, and beautiful output. The name stands for Statistics Open For All. It has a graphical user interface and can connect directly to MySQL, PostgreSQL, SQLite, MS Access (mdb), Microsoft SQL Server, and CUBRID. Data can also be imported from CSV and Tab-Separated files or spreadsheets (Microsoft Excel, OpenOffice.org Calc, Gnumeric, Google Docs). The main statistical tests available are Independent and Paired t-tests, Wilcoxon signed ranks, Mann Whitney U, Pearson's chi squared, Kruskal Wallis H, one-way ANOVA, Spearman's R, and Pearson's R. Nested tables can be produced with row and column percentages, totals, sd, mean, median, lower and upper quartiles, and sum. Simple but dynamic bar charts (freq or means), clustered bar charts (freq or means), pie charts, single or multiple line charts (freq or means), area charts (freq or means), histograms, scatterplots, and box and whisker plots are available. It is also possible to create chart series. Installation packages are available for several Operating Systems such as Microsoft Windows, Ubuntu, ArchLinux, Linux Mint, and Mac OS X (Leopard upwards). SOFA Statistics is written in Python, and the widget toolkit used is wxPython. The statistical analyses are based on functions available through the Scipy stats module. Analysis and reporting can be automated using Python scripts   either exported directly from SOFA Statistics or manually written.
See also: Conjoint analysis, Conjoint analysis (in healthcare), IDDEA, Rule Developing Experimentation, Discrete choice models. Conjoint analysis is a statistical technique used in market research to determine how people value different attributes (feature, function, benefits) that make up an individual product or service. The objective of conjoint analysis is to determine what combination of a limited number of attributes is most influential on respondent choice or decision making. A controlled set of potential products or services is shown to respondents and by analyzing how they make preferences between these products, the implicit valuation of the individual elements making up the product or service can be determined. These implicit valuations (utilities or part-worths) can be used to create market models that estimate market share, revenue and even profitability of new designs. Conjoint originated in mathematical psychology and was developed by marketing professor Paul Green at the Wharton School of the University of Pennsylvania and Data Chan. Other prominent conjoint analysis pioneers include professor V.  Seenu  Srinivasan of Stanford University who developed a linear programming (LINMAP) procedure for rank ordered data as well as a self-explicated approach, Richard Johnson (founder of Sawtooth Software) who developed the Adaptive Conjoint Analysis technique in the 1980s and Jordan Louviere (University of Iowa) who invented and developed Choice-based approaches to conjoint analysis and related techniques such as Best-Worst Scaling. Today it is used in many of the social sciences and applied sciences including marketing, product management, and operations research. It is used frequently in testing customer acceptance of new product designs, in assessing the appeal of advertisements and in service design. It has been used in product positioning, but there are some who raise problems with this application of conjoint analysis (see disadvantages). Conjoint analysis techniques may also be referred to as multiattribute compositional modelling, discrete choice modelling, or stated preference research, and is part of a broader set of trade-off analysis tools used for systematic analysis of decisions. These tools include Brand-Price Trade-Off, Simalto, and mathematical approaches such as AHP, evolutionary algorithms or Rule Developing Experimentation.
The Box Muller transform, by George Edward Pelham Box and Mervin Edgar Muller 1958, is a pseudo-random number sampling method for generating pairs of independent, standard, normally distributed (zero expectation, unit variance) random numbers, given a source of uniformly distributed random numbers. It is commonly expressed in two forms. The basic form as given by Box and Muller takes two samples from the uniform distribution on the interval (0, 1] and maps them to two standard, normally distributed samples. The polar form takes two samples from a different interval, [ 1, +1], and maps them to two normally distributed samples without the use of sine or cosine functions. The Box Muller transform was developed as a more computationally efficient alternative to the inverse transform sampling method. The Ziggurat algorithm gives an even more efficient method. Furthermore, the Box Muller transform can be also employed from drawing from truncated bivariate Gaussian densities.
In statistics, a covariate is a variable that is possibly predictive of the outcome under study. A covariate may be of direct interest or it may be a confounding or interacting variable. The alternative terms explanatory variable, independent variable, or predictor, are used in a regression analysis. In econometrics, the term "control variable" is usually used instead of "covariate". In a more specific usage, a covariate is a secondary variable that can affect the relationship between the dependent variable and other independent variables of primary interest. An example is provided by the analysis of trend in sea level by Woodworth (1987). Here the dependent variable (and variable of most interest) was the annual mean sea level at a given location for which a series of yearly values were available. The primary independent variable was time. Use was made of a covariate consisting of yearly values of annual mean atmospheric pressure at sea level. The results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained, compared to analyses which omitted the covariate.
In estimation theory, estimation of signal parameters via rotational invariant techniques (ESPRIT) is a technique to determine parameters of a mixture of sinusoids in a background noise.
In statistical significance testing, a one-tailed test and a two-tailed test are alternative ways of computing the statistical significance of a parameter inferred from a data set, in terms of a test statistic. A two-tailed test is used if deviations of the estimated parameter in either direction from some benchmark value are considered theoretically possible; in contrast, a one-tailed test is used if only deviations in one direction are considered possible. Alternative names are one-sided and two-sided tests; the terminology "tail" is used because the extreme portions of distributions, where observations lead to rejection of the null hypothesis, are small and often "tail off" toward zero as in the normal distribution or "bell curve", pictured above right.
In mathematics, the beta function, also called the Euler integral of the first kind, is a special function defined by  for  The beta function was studied by Euler and Legendre and was given its name by Jacques Binet; its symbol   is a Greek capital   rather than the similar Latin capital B.
Predictive intake modelling uses mathematical modelling strategies to estimate intake of food, personal care products, and their formulations.
Used in a number of sciences, ranging from econometrics to meteorology, consensus forecasts are predictions of the future that are created by combining together several separate forecasts which have often been created using different methodologies. Also known as combining forecasts, forecast averaging or model averaging (in econometrics and statistics) and committee machines, ensemble averaging or expert aggregation (in machine learning). Applications can range from forecasting the weather to predicting the annual Gross Domestic Product of a country or the number of cars a company or an individual dealer is likely to sell in a year. While forecasts are often made for future values of a time series, they can also be for one-off events such as the outcome of a presidential election or a football match.
In machine learning, the Markov blanket for a node  in a Bayesian network is the set of nodes  composed of 's parents, its children, and its children's other parents. In a Markov network, the Markov blanket of a node is its set of neighboring nodes. A Markov blanket may also be denoted by . Every set of nodes in the network is conditionally independent of  when conditioned on the set , that is, when conditioned on the Markov blanket of the node . The probability has the Markov property; formally, for distinct nodes  and :  The Markov blanket of a node contains all the variables that shield the node from the rest of the network. This means that the Markov blanket of a node is the only knowledge needed to predict the behavior of that node. The term was coined by Pearl in 1988. In a Bayesian network, the values of the parents and children of a node evidently give information about that node; however, its children's parents also have to be included, because they can be used to explain away the node in question. In a Markov random field, the Markov blanket for a node is simply its adjacent nodes.  
In statistical inference, the concept of a confidence distribution (CD) has often been loosely referred to as a distribution function on the parameter space that can represent confidence intervals of all levels for a parameter of interest. Historically, it has typically been constructed by inverting the upper limits of lower sided confidence intervals of all levels, and it was also commonly associated with a fiducial interpretation (fiducial distribution), although it is a purely frequentist concept. A confidence distribution is NOT a probability distribution function of the parameter of interest, but may still be a function useful for making inferences. In recent years, there has been a surge of renewed interest in confidence distributions. In the more recent developments, the concept of confidence distribution has emerged as a purely frequentist concept, without any fiducial interpretation or reasoning. Conceptually, a confidence distribution is no different from a point estimator or an interval estimator (confidence interval), but it uses a sample-dependent distribution function on the parameter space (instead of a point or an interval) to estimate the parameter of interest. A simple example of a confidence distribution, that has been broadly used in statistical practice, is a bootstrap distribution. The development and interpretation of a bootstrap distribution does not involve any fiducial reasoning; the same is true for the concept of a confidence distribution. But the notion of confidence distribution is much broader than that of a bootstrap distribution. In particular, recent research suggests that it encompasses and unifies a wide range of examples, from regular parametric cases (including most examples of the classical development of Fisher's fiducial distribution) to bootstrap distributions, p-value functions, normalized likelihood functions and, in some cases, Bayesian priors and Bayesian posteriors. Just as a Bayesian posterior distribution contains a wealth of information for any type of Bayesian inference, a confidence distribution contains a wealth of information for constructing almost all types of frequentist inferences, including point estimates, confidence intervals and p-values, among others. Some recent developments have highlighted the promising potentials of the CD concept, as an effective inferential tool.
Proportional hazards models are a class of survival models in statistics. Survival models relate the time that passes before some event occurs to one or more covariates that may be associated with that quantity of time. In a proportional hazards model, the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate. For example, taking a drug may halve one's hazard rate for a stroke occurring, or, changing the material from which a manufactured component is constructed may double its hazard rate for failure. Other types of survival models such as accelerated failure time models do not exhibit proportional hazards. The accelerated failure time model describes a situation where the biological or mechanical life history of an event is accelerated.
In mathematics, Owen's T function T(h, a), named after statistician Donald Bruce Owen, is defined by  The function was first introduced by Owen in 1956.
In statistics, a simple random sample is a subset of individuals (a sample) chosen from a larger set (a population). Each individual is chosen randomly and entirely by chance, such that each individual has the same probability of being chosen at any stage during the sampling process, and each subset of k individuals has the same probability of being chosen for the sample as any other subset of k individuals. This process and technique is known as simple random sampling, and should not be confused with systematic random sampling. A simple random sample is an unbiased surveying technique. Simple random sampling is a basic type of sampling, since it can be a component of other more complex sampling methods. The principle of simple random sampling is that every object has the same probability of being chosen. For example, suppose N college students want to get a ticket for a basketball game, but there are only X < N tickets for them, so they decide to have a fair way to see who gets to go. Then, everybody is given a number in the range from 0 to N-1, and random numbers are generated, either electronically or from a table of random numbers. Numbers outside the range from 0 to N-1 are ignored, as are any numbers previously selected. The first X numbers would identify the lucky ticket winners. In small populations and often in large ones, such sampling is typically done "without replacement", i.e., one deliberately avoids choosing any member of the population more than once. Although simple random sampling can be conducted with replacement instead, this is less common and would normally be described more fully as simple random sampling with replacement. Sampling done without replacement is no longer independent, but still satisfies exchangeability, hence many results still hold. Further, for a small sample from a large population, sampling without replacement is approximately the same as sampling with replacement, since the odds of choosing the same individual twice is low. An unbiased random selection of individuals is important so that if a large number of samples were drawn, the average sample would accurately represent the population. However, this does not guarantee that a particular sample is a perfect representation of the population. Simple random sampling merely allows one to draw externally valid conclusions about the entire population based on the sample. Conceptually, simple random sampling is the simplest of the probability sampling techniques. It requires a complete sampling frame, which may not be available or feasible to construct for large populations. Even if a complete frame is available, more efficient approaches may be possible if other useful information is available about the units in the population. Advantages are that it is free of classification error, and it requires minimum advance knowledge of the population other than the frame. Its simplicity also makes it relatively easy to interpret data collected in this manner. For these reasons, simple random sampling best suits situations where not much information is available about the population and data collection can be efficiently conducted on randomly distributed items, or where the cost of sampling is small enough to make efficiency less important than simplicity. If these conditions do not hold, stratified sampling or cluster sampling may be a better choice.
In statistics, the Fano factor, like the coefficient of variation, is a measure of the dispersion of a probability distribution of a Fano noise. It is named after Ugo Fano, an Italian American physicist. The Fano factor is defined as  where  is the variance and  is the mean of a random process in some time window W. The Fano factor can be viewed as a kind of noise-to-signal ratio; it is a measure of the reliability with which the random variable could be estimated from a time window that on average contains several random events. For a Poisson process, the variance in the count equals the mean count, so F = 1 (normalization). If the time window is chosen to be infinity, the Fano factor is similar to the variance-to-mean ratio (VMR) which in statistics is also known as the index of dispersion.
The Shapiro Wilk test is a test of normality in frequentist statistics. It was published in 1965 by Samuel Sanford Shapiro and Martin Wilk.
The Underprivileged Area Score is an index to measure socio-economic variation across small geographical areas. The score is an outcome of the need identified in the Acheson Committee Report (into General Practitioner (GP) services in the UK) to create an index to identify 'underprivileged areas' where there were high numbers of patients and hence pressure on general practitioner services. Its creation involved the random distribution of a questionnaire among general practitioners throughout the UK. This was then used to obtain statistical weights for a calculation of a composite index of underprivileged areas based on GPs' perceptions of workload and patient need. (Jarman, 1984)
Parametric statistics is a branch of statistics which assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. Most well-known elementary statistical methods are parametric. Conversely a non-parametric model differs precisely in that the parameter set (or feature set in machine learning) is not fixed and can increase, or even decrease if new relevant information is collected. A parametric model as it relies on a fixed parameter set assumes more about a given population than non-parametric methods. When the assumptions are correct, parametric methods will produce more accurate and precise estimates than non-parametric methods, i.e. have more statistical power. As more is assumed when the assumptions are not correct they have a greater chance of failing, and for this reason are not a robust statistical method. On the other hand, parametric formulae are often simpler to write down and faster to compute. For this reason their simplicity can make up for their lack of robustness, especially if care is taken to examine diagnostic statistics.
In statistics, the matrix normal distribution is a probability distribution that is a generalization of the multivariate normal distribution to matrix-valued random variables.
Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball.
In statistics, the monotone likelihood ratio property is a property of the ratio of two probability density functions (PDFs). Formally, distributions  (x) and g(x) bear the property if  that is, if the ratio is nondecreasing in the argument . If the functions are first-differentiable, the property may sometimes be stated  For two distributions that satisfy the definition with respect to some argument x, we say they "have the MLRP in x." For a family of distributions that all satisfy the definition with respect to some statistic T(X), we say they "have the MLR in T(X)."
A candlestick chart is a style of financial chart used to describe price movements of a security, derivative, or currency. Each "candlestick" typically shows one day; so for example a one-month chart may show the 20 trading days as 20 "candlesticks". It is like a combination of line-chart and a bar-chart: each bar represents all four important pieces of information for that day: the open, the close, the high and the low. Candlestick charts are most often used in technical analysis of equity and currency price patterns. They appear superficially similar to box plots, but are unrelated.
In statistics, the interclass correlation (or interclass correlation coefficient) measures a bivariate relation among variables. The Pearson correlation coefficient is the most commonly used interclass correlation. The interclass correlation contrasts with the intraclass correlation.
In statistics, a mediation model is one that seeks to identify and explicate the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (also a mediating variable, intermediary variable, or intervening variable). Rather than a direct causal relationship between the independent variable and the dependent variable, a mediation model proposes that the independent variable influences the (non-observable) mediator variable, which in turn influences the dependent variable. Thus, the mediator variable serves to clarify the nature of the relationship between the independent and dependent variables. Mediation analyses are employed to understand a known relationship by exploring the underlying mechanism or process by which one variable influences another variable through a mediator variable. Mediation analysis facilitates a better understanding of the relationship between the independent and dependent variables when the variables appear to not have a definite connection. They are studied by means of operational definitions and have no existence apart.
The Tobit model is a statistical model proposed by James Tobin (1958) to describe the relationship between a non-negative dependent variable  and an independent variable (or vector) . The term Tobit was derived from Tobin's name by truncating and adding -it by analogy with the probit model. The model supposes that there is a latent (i.e. unobservable) variable . This variable linearly depends on  via a parameter (vector)  which determines the relationship between the independent variable (or vector)  and the latent variable  (just as in a linear model). In addition, there is a normally distributed error term  to capture random influences on this relationship. The observable variable  is defined to be equal to the latent variable whenever the latent variable is above zero and zero otherwise.  where  is a latent variable:
In random matrix theory, the Marchenko Pastur distribution, or Marchenko Pastur law, describes the asymptotic behavior of singular values of large rectangular random matrices. The theorem is named after Ukrainian mathematicians Vladimir Marchenko and Leonid Pastur who proved this result in 1967. If  denotes a  random matrix whose entries are independent identically distributed random variables with mean 0 and variance , let  and let  be the eigenvalues of  (viewed as random variables). Finally, consider the random measure  Theorem. Assume that  so that the ratio . Then  (in weak* topology in distribution), where  and  with  The Marchenko Pastur law also arises as the free Poisson law in free probability theory, having rate  and jump size .
An intention-to-treat (ITT) analysis of the results of an experiment is based on the initial treatment assignment and not on the treatment eventually received. ITT analysis is intended to avoid various misleading artifacts that can arise in intervention research such as non-random attrition of participants from the study or crossover. ITT is also simpler than other forms of study design and analysis because it does not require observation of compliance status for units assigned to different treatments or incorporation of compliance into the analysis. Although ITT analysis is widely employed in published clinical trials, it can be incorrectly described and there are some issues with its application. Furthermore, there is no consensus on how to carry out an ITT analysis in the presence of missing outcome data.
In statistics, self-selection bias arises in any situation in which individuals select themselves into a group, causing a biased sample with nonprobability sampling. It is commonly used to describe situations where the characteristics of the people which cause them to select themselves in the group create abnormal or undesirable conditions in the group. It is closely related to the Non-response bias, describing when the group of people responding has different responses than the group of people not responding. Self-selection bias is a major problem in research in sociology, psychology, economics and many other social sciences. In such fields, a poll suffering from such bias is termed a self-selected listener opinion poll or "SLOP". The term is also used in criminology to describe the process by which specific predispositions may lead an offender to choose a criminal career and lifestyle. While the effects of self-selection bias are closely related to those of selection bias, the problem arises for rather different reasons; thus there may be a purposeful intent on the part of respondents leading to self-selection bias whereas other types of selection bias may arise more inadvertently, possibly as the result of mistakes by those designing any given study.  
Fleiss' kappa (named after Joseph L. Fleiss) is a statistical measure for assessing the reliability of agreement between a fixed number of raters when assigning categorical ratings to a number of items or classifying items. This contrasts with other kappas such as Cohen's kappa, which only work when assessing the agreement between two raters. The measure calculates the degree of agreement in classification over that which would be expected by chance. There is no generally agreed-upon measure of significance, although guidelines have been given. Fleiss' kappa can be used only with binary or nominal-scale ratings. No version is available for ordered-categorical ratings.
In medical testing with binary classification, the diagnostic odds ratio is a measure of the effectiveness of a diagnostic test. It is defined as the ratio of the odds of the test being positive if the subject has a disease relative to the odds of the test being positive if the subject does not have the disease. The rationale for the diagnostic odds ratio is that it is a single indicator of test performance (like accuracy and Youden's J statistic) but which is independent of prevalence (unlike accuracy) and is presented as an odds ratio, which is familiar to medical practitioners.
The Johnson's SU-distribution is a four-parameter family of probability distributions first investigated by N. L. Johnson in 1949. Johnson proposed it as a transformation of the normal distribution:  where .  
The Richardson Lucy algorithm, also known as Lucy Richardson deconvolution, is an iterative procedure for recovering a latent image that has been blurred by a known point spread function. It was named after William Richardson and Leon Lucy, who described it independently.
In probability, a singular distribution is a probability distribution concentrated on a set of Lebesgue measure zero, where the probability of each point in that set is zero.  
The vector autoregression (VAR) is an econometric model used to capture the linear interdependencies among multiple time series. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR are treated symmetrically in a structural sense (although the estimated quantitative response coefficients will not in general be the same); each variable has an equation explaining its evolution based on its own lags and the lags of the other model variables. VAR modeling does not require as much knowledge about the forces influencing a variable as do structural models with simultaneous equations: The only prior knowledge required is a list of variables which can be hypothesized to affect each other intertemporally.
In statistics, the theory of minimum norm quadratic unbiased estimation (MINQUE) was developed by C.R. Rao. Its application was originally to the estimation of variance components in random effects models. The theory involves three stages:  defining a general class of potential estimators as quadratic functions of the observed data, where the estimators relate to a vector of model parameters; specifying certain constraints on the desired properties of the estimators, such as unbiasedness; choosing the optimal estimator by minimising a "norm" which measures the size of the covariance matrix of the estimators.
A statistical hypothesis is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables. A statistical hypothesis test is a method of statistical inference. Commonly, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets. The comparison is deemed statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability the significance level. Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance. The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors (type 1 & type 2), and by specifying parametric limits on e.g. how much type 1 error will be permitted. An alternative framework for statistical hypothesis testing is to specify a set of statistical models, one for each candidate hypothesis, and then use model selection techniques to choose the most appropriate model. The most common selection techniques are based on either Akaike information criterion or Bayes factor. Statistical hypothesis testing is sometimes called confirmatory data analysis. It can be contrasted with exploratory data analysis, which may not have pre-specified hypotheses.
The weighted arithmetic mean is similar to an ordinary arithmetic mean (the most common type of average), except that instead of each of the data points contributing equally to the final average, some data points contribute more than others. The notion of weighted mean plays a role in descriptive statistics and also occurs in a more general form in several other areas of mathematics. If all the weights are equal, then the weighted mean is the same as the arithmetic mean. While weighted means generally behave in a similar fashion to arithmetic means, they do have a few counterintuitive properties, as captured for instance in Simpson's paradox.
In statistics, the Goldfeld Quandt test checks for homoscedasticity in regression analyses. It does this by dividing a dataset into two parts or groups, and hence the test is sometimes called a two-group test. The Goldfeld Quandt test is one of two tests proposed in a 1965 paper by Stephen Goldfeld and Richard Quandt. Both a parametric and nonparametric test are described in the paper, but the term "Goldfeld Quandt test" is usually associated only with the former.
In probability theory, the Rice distribution, Rician distribution or Ricean distribution is the probability distribution of the magnitude of a circular bivariate normal random variable with potentially non-zero mean. It was named after Stephen O. Rice.
In statistics, an additive model (AM) is a nonparametric regression method. It was suggested by Jerome H. Friedman and Werner Stuetzle (1981) and is an essential part of the ACE algorithm. The AM uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a p-dimensional smoother. Furthermore, the AM is more flexible than a standard linear model, while being more interpretable than a general regression surface at the cost of approximation errors. Problems with AM include model selection, overfitting, and multicollinearity.
In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. (This term should be distinguished from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.) In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of y given the value of X is assumed to be an affine function of X; less commonly, the median or some other quantile of the conditional distribution of y given X is expressed as a linear function of X. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of y given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis. Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine. Linear regression has many practical uses. Most applications fall into one of the following two broad categories: If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of y and X values. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a prediction of the value of y. Given a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj may have no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y. Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares loss function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.
A test score is a piece of information, usually a number, that conveys the performance of an examinee on a test. One formal definition is that it is "a summary of the evidence contained in an examinee's responses to the items of a test that are related to the construct or constructs being measured." Test scores are interpreted with a norm-referenced or criterion-referenced interpretation, or occasionally both. A norm-referenced interpretation means that the score conveys meaning about the examinee with regards to their standing among other examinees. A criterion-referenced interpretation means that the score conveys information about the examinee with regards a specific subject matter, regardless of other examinees' scores.
In mathematics, more specifically measure theory, there are various notions of the convergence of measures. For an intuitive general sense of what is meant by convergence in measure, consider a sequence of measures  n on a space, sharing a common collection of measurable sets. Such a sequence might represent an attempt to construct 'better and better' approximations to a desired measure   that is difficult to obtain directly. The meaning of 'better and better' is subject to all the usual caveats for taking limits; for any error tolerance   > 0 we require there be N sufficiently large for n   N to ensure the 'difference' between  n and   is smaller than  . Various notions of convergence specify precisely what the word 'difference' should mean in that description; these notions are not equivalent to one another, and vary in strength. Three of the most common notions of convergence are described below.
In probability and statistics a Markov renewal process is a random process that generalizes the notion of Markov jump processes. Other random processes like Markov chain, Poisson process, and renewal process can be derived as a special case of an MRP (Markov renewal process).
In mathematics, statistics and elsewhere, sums of squares occur in a number of contexts:
This article itemizes the various lists of statistics topics. Some of these lists link to hundreds of articles; some link only to a few.
The following is a glossary of terms used in the mathematical sciences statistics and probability. atomic event Another name for elementary event bias 1.  A sample that is not representative of the population 2.  The difference between the expected value of an estimator and the true value binary data Data that can take only two values, usually represented by 0 and 1 causal study A statistical study in which the objective is to measure the effect of some variable on an outcome relative to a different variable. For example, how will my headache feel if I take aspirin, versus if I do not take aspirin  Causal studies may be either experimental or observational. concomitants In a statistical study, concomitants are any variables whose values are unaffected by treatments, such as a unit s age, gender, and cholesterol level before starting a diet (treatment). conditional distribution Given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X (written "Y | X") is the probability distribution of Y when X is known to be a particular value conditional probability The probability of some event A, assuming event B. Conditional probability is written P(A|B), and is read "the probability of A, given B" confidence interval In inferential statistics, a CI is a range of plausible values for the population mean. For example, based on a study of sleep habits among 100 people, a researcher may estimate that the overall population sleeps somewhere between 5 and 9 hours per night. This is different from the sample mean, which can be measured directly. confidence level Also known as a confidence coefficient, the confidence level indicates the probability that the confidence interval (range) captures the true population mean. For example, a confidence interval with a 95 percent confidence level has a 95 percent chance of capturing the population mean. Technically, this means that, if the experiment were repeated many times, 95 percent of the CIs would contain the true population mean. correlation Also called correlation coefficient, a numeric measure of the strength of linear relationship between two random variables (one can use it to quantify, for example, how shoe size and height are correlated in the population). An example is the Pearson product-moment correlation coefficient, which is found by dividing the covariance of the two variables by the product of their standard deviations. Independent variables have a correlation of 0 count data Data arising from counting that can take only non-negative integer values covariance Given two random variables X and Y, with expected values  and , covariance is defined as the expected value of random variable , and is written . It is used for measuring correlation data set A sample and the associated data points data point A typed measurement   it can be a Boolean value, a real number, a vector (in which case it's also called a data vector), etc elementary event An event with only one element. For example, when pulling a card out of a deck, "getting the jack of spades" is an elementary event, while "getting a king or an ace" is not estimator A function of the known data that is used to estimate an unknown parameter; an estimate is the result from the actual application of the function to a particular set of data. The mean can be used as an estimator expected value The sum of the probability of each possible outcome of the experiment multiplied by its payoff ("value"). Thus, it represents the average amount one "expects" to win per bet if bets with identical odds are repeated many times. For example, the expected value of a six-sided die roll is 3.5. The concept is similar to the mean. The expected value of random variable X is typically written E(X) for the operator and  (mu) for the parameter experiment Any procedure that can be infinitely repeated and has a well-defined set of outcomes event A subset of the sample space (a possible experiment's outcome), to which a probability can be assigned. For example, on rolling a die, "getting a five or a six" is an event (with a probability of one third if the die is fair) joint distribution Given two random variables X and Y, the joint distribution of X and Y is the probability distribution of X and Y together joint probability The probability of two events occurring together. The joint probability of A and B is written  or  kurtosis A measure of the "peakedness" of the probability distribution of a real-valued random variable. Higher kurtosis means more of the variance is due to infrequent extreme deviations, as opposed to frequent modestly sized deviations likelihood function A conditional probability function considered a function of its second argument with its first argument held fixed. For example, imagine pulling a numbered ball with the number k from a bag of n balls, numbered 1 to n. Then you could describe a likelihood function for the random variable N as the probability of getting k given that there are n balls : the likelihood will be 1/n for n greater or equal to k, and 0 for n smaller than k. Unlike a probability distribution function, this likelihood function will not sum up to 1 on the sample space marginal distribution Given two jointly distributed random variables X and Y, the marginal distribution of X is simply the probability distribution of X ignoring information about Y marginal probability The probability of an event, ignoring any information about other events. The marginal probability of A is written P(A). Contrast with conditional probability mean 1.  The expected value of a random variable 2.  The arithmetic mean is the average of a set of numbers, or the sum of the values divided by the number of values multivariate random variable A vector whose components are random variables on the same probability space mutual independence A collection of events is mutually independent if for any subset of the collection, the joint probability of all events occurring is equal to the product of the joint probabilities of the individual events. Think of the result of a series of coin-flips. This is a stronger condition than pairwise independence null hypothesis The statement being tested in a test of statistical significance Usually the null hypothesis is a statement of 'no effect' or 'no difference'." For example, if one wanted to test whether light has an effect on sleep, the null hypothesis would be that there is no effect. It is often symbolized as H0. pairwise independence A pairwise independent collection of random variables is a set of random variables any two of which are independent parameter Can be a population parameter, a distribution parameter, an unobserved parameter (with different shades of meaning). In statistics, this is often a quantity to be estimated prior probability In Bayesian inference, this represents prior beliefs or other information that is available before new data or observations are taken into account population parameter See parameter posterior probability The result of a Bayesian analysis that encapsulates the combination of prior beliefs or information with observed data probability density Describes the probability in a continuous probability distribution. For example, you can't say that the probability of a man being six feet tall is 20%, but you can say he has 20% of chances of being between five and six feet tall. Probability density is given by a probability density function. Contrast with probability mass probability density function Gives the probability distribution for a continuous random variable probability distribution A function that gives the probability of all elements in a given space: see List of probability distributions probability measure The probability of events in a probability space probability space A sample space over which a probability measure has been defined random variable A measurable function on a probability space, often real-valued. The distribution function of a random variable gives the probability of different results. We can also derive the mean and variance of a random variable  range The length of the smallest interval which contains all the data responses In a statistical study, any variables whose values may have been affected by the treatments, such as cholesterol levels after following a particular diet for six months. sample That part of a population which is actually observed sample mean The arithmetic mean of a sample of values drawn from the population. It is denoted by . An example is the average test score of a subset of 10 students from a class. Sample mean is used as an estimator of the population mean, which in this example would be the average test score of all of the students in the class. sample space The set of possible outcomes of an experiment. For example, the sample space for rolling a six-sided die will be {1, 2, 3, 4, 5, 6} sampling A process of selecting observations to obtain knowledge about a population. There are many methods to choose on which sample to do the observations sampling distribution The probability distribution, under repeated sampling of the population, of a given statistic skewness A measure of the asymmetry of the probability distribution of a real-valued random variable. Roughly speaking, a distribution has positive skew (right-skewed) if the higher tail is longer and negative skew (left-skewed) if the lower tail is longer (confusing the two is a common error) standard deviation The most commonly used measure of statistical dispersion. It is the Square root of the variance, and is generally written  (Sigma) statistic The result of applying a statistical algorithm to a data set. It can also be described as an observable random variable statistical independence Two events are independent if the outcome of one does not affect that of the other (for example, getting a 1 on one die roll does not affect the probability of getting a 1 on a second roll). Similarly, when we assert that two random variables are independent, we intuitively mean that knowing something about the value of one of them does not yield any information about the value of the other statistical inference Inference about a population from a random sample drawn from it or, more generally, about a random process from its observed behavior during a finite period of time statistical population A set of entities about which statistical inferences are to be drawn, often based on random sampling. One can also talk about a population of measurements or values statistical dispersion Statistical variability is a measure of how diverse some data is. It can be expressed by the variance or the standard deviation statistical parameter A parameter that indexes a family of probability distributions treatments Variables in a statistical study that are conceptually manipulable. For example, in a health study, following a certain diet is a treatment whereas age is not. trial Can refer to each individual repetition when talking about an experiment composed of any fixed number of them. As an example, one can think of an experiment being any number from one to n coin tosses, say 17. In this case, one toss can be called a trial to avoid confusion, since the whole experiment is composed of 17 ones. units In a statistical study, the objects to which treatments are assigned. For example, in a study examining the effects of smoking cigarettes, the units would be people. variance A measure of its statistical dispersion of a random variable, indicating how far from the expected value its values typically are. The variance of random variable X is typically designated as , , or simply
In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The common exponential distribution and chi-squared distribution are special cases of the gamma distribution. There are three different parametrizations in common use: With a shape parameter k and a scale parameter  . With a shape parameter   = k and an inverse scale parameter   = 1/ , called a rate parameter. With a shape parameter k and a mean parameter   = k/ . In each of these three forms, both parameters are positive real numbers. The parameterization with k and   appears to be more common in econometrics and certain other applied fields, where e.g. the gamma distribution is frequently used to model waiting times. For instance, in life testing, the waiting time until death is a random variable that is frequently modeled with a gamma distribution. The parameterization with   and   is more common in Bayesian statistics, where the gamma distribution is used as a conjugate prior distribution for various types of inverse scale (aka rate) parameters, such as the   of an exponential distribution or a Poisson distribution   or for that matter, the   of the gamma distribution itself. (The closely related inverse gamma distribution is used as a conjugate prior for scale parameters, such as the variance of a normal distribution.) If k is a positive integer, then the distribution represents an Erlang distribution; i.e., the sum of k independent exponentially distributed random variables, each of which has a mean of  . The gamma distribution is the maximum entropy probability distribution for a random variable X for which E[X] = k  =  /  is fixed and greater than zero, and E[ln(X)] =  (k) + ln( ) =  ( )   ln( ) is fixed (  is the digamma function).
In the study of stochastic processes in mathematics, a disorder problem or quickest detection problem (formulated by Kolmogorov) is the problem of using ongoing observations of a stochastic process to detect as soon as possible when the probabilistic properties of the process have changed. This is a type of change detection problem. An example case is to detect the change in the drift parameter of a Wiener process.  
In probability theory, a piecewise-deterministic Markov process (PDMP) is a process whose behaviour is governed by random jumps at points in time, but whose evolution is deterministically governed by an ordinary differential equation between those times. The class of models is "wide enough to include as special cases virtually all the non-diffusion models of applied probability." The process is defined by three quantities: the flow, the jump rate, and the transition measure. The model was first introduced in a paper by Mark H. A. Davis in 1984.
Statistical parametric mapping or SPM is a statistical technique created by Karl Friston for examining differences in brain activity recorded during functional neuroimaging experiments using neuroimaging technologies such as fMRI or PET. It may also refer to a specific piece of software created by the Wellcome Department of Imaging Neuroscience (part of University College London) to carry out such analyses.
In statistics, a forecast error is the difference between the actual or real and the predicted or forecast value of a time series or any other phenomenon of interest. In simple cases, a forecast is compared with an outcome at a single time-point and a summary of forecast errors is constructed over a collection of such time-points. Here the forecast may be assessed using the difference or using a proportional error. By convention, the error is defined using the value of the outcome minus the value of the forecast. In other cases, a forecast may consist of predicted values over a number of lead-times; in this case an assessment of forecast error may need to consider more general ways of assessing the match between the time-profiles of the forecast and the outcome. If a main application of the forecast is to predict when certain thresholds will be crossed, one possible way of assessing the forecast is to use the timing-error the difference in time between when the outcome crosses the threshold and when the forecast does so. When there is interest in the maximum value being reached, assessment of forecasts can be done using any of: the difference of times of the peaks; the difference in the peak values in the forecast and outcome; the difference between the peak value of the outcome and the value forecast for that time point. Forecast error can be a calendar forecast error or a cross-sectional forecast error, when we want to summarize the forecast error over a group of units. If we observe the average forecast error for a time-series of forecasts for the same product or phenomenon, then we call this a calendar forecast error or time-series forecast error. If we observe this for multiple products for the same period, then this is a cross-sectional performance error. Reference class forecasting has been developed to reduce forecast error. Combining forecasts has also been shown to reduce forecast error.
Plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition.
In statistics, the Johansen test, named after S ren Johansen, is a procedure for testing cointegration of several, say k, I(1) time series. This test permits more than one cointegrating relationship so is more generally applicable than the Engle Granger test which is based on the Dickey Fuller (or the augmented) test for unit roots in the residuals from a single (estimated) cointegrating relationship. There are two types of Johansen test, either with trace or with eigenvalue, and the inferences might be a little bit different. The null hypothesis for the trace test is that the number of cointegration vectors is r=r*<k, vs. the alternative that r=k. Testing proceeds sequentially for r*=1,2,etc. and the first non-rejection of the null is taken as an estimate of r. The null hypothesis for the "maximum eigenvalue" test is as for the trace test but the alternative is r=r*+1 and, again, testing proceeds sequentially for r*=1,2,etc., with the first non-rejection used as an estimator for r. Just like a unit root test, there can be a constant term, a trend term, both, or neither in the model. For a general VAR(p) model:  There are two possible specifications for error correction: that is, two VECM (vector error correction models): 1. The longrun VECM:  where  2. The transitory VECM:  where  Be aware that the two are the same. In both VECM (Vector Error Correction Model),  Inferences are drawn on  , and they will be the same, so is the explanatory power.
In statistics, a consistent estimator or asymptotically consistent estimator is an estimator a rule for computing estimates of a parameter  0 having the property that as the number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to  0. This means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to  0 converges to one. In practice one constructs an estimator as a function of an available sample of size n, and then imagines being able to keep collecting data and expanding the sample ad infinitum. In this way one would obtain a sequence of estimates indexed by n, and consistency is a property of what occurs as the sample size  grows to infinity . If the sequence of estimates can be mathematically shown to converge in probability to the true value  0, it is called a consistent estimator; otherwise the estimator is said to be inconsistent. Consistency as defined here is sometimes referred to as weak consistency. When we replace convergence in probability with almost sure convergence, then the estimator is said to be strongly consistent. Consistency is related to bias; see bias versus consistency.
In statistics, time series data is data collected at regular intervals. When there are patterns that repeat over known, fixed periods of time within the data set it is considered to be seasonality, seasonal variation, periodic variation, or periodic fluctuations. This variation can be either regular or semi-regular. Seasonality may be caused by various factors, such as weather, vacation, and holidays and usually consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series. Seasonality can repeat on a weekly, monthly or quarterly basis, these periods of time are structured and occur in a length of time less than a year. Seasonal fluctuations in a time series can be contrasted with cyclical patterns. The latter occur in a period of time that extends beyond a single year, these fluctuations are usually of at least two year and do not repeat over fixed periods of time. Organizations facing seasonal variations, such as ice-cream vendors, are often interested in knowing their performance relative to the normal seasonal variation. Seasonal variations in the labor market can be attributed to the entrance of school leavers into the job market; as they aim to contribute to the workforce during their vacations, or upon the completion of their schooling. These regular changes are of less interest to those who study employment data than the variations that occur due to the underlying state of the economy. Where their focus is on how unemployment in the workforce has changed, despite the impact of the regular seasonal variations. It is necessary for organizations to identify and measure seasonal variations within their market to help them plan for the future. This can prepare them for the temporary increases or decreases in labor requirements and inventory as demand for their product or service fluctuates over certain periods. This may require training, periodic maintenance, and so forth that can be organized in advance. Apart from these considerations, the organizations need to know if variation they have experienced have been more or less than the expected amount, beyond what the usual seasonal variations account for.  
The Heckman correction (the two-stage method, Heckman's lambda or the Heckit method) is any of a number of related statistical methods developed by James Heckman at the University of Chicago in 1976 to 1979 which allow the researcher to correct for selection bias. Selection bias problems are endemic to applied econometric problems, which make Heckman s original technique, and subsequent refinements by both himself and others, indispensable to applied econometricians. Heckman received the Economics Nobel Prize in 2000 for this achievement.
In statistics, a power law is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: one quantity varies as a power of another. For instance, considering the area of a square in terms of the length of its side, if the length is doubled, the area is multiplied by a factor of four.
Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object lies within its cluster. It was first described by Peter J. Rousseeuw in 1986. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.
In statistics, qualitative comparative analysis (QCA) is a data analysis technique for determining which logical conclusions a data set supports. The analysis begins with listing and counting all the combinations of variables observed in the data set, followed by applying the rules of logical inference to determine which descriptive inferences or implications the data supports. The technique was originally developed by Charles Ragin in 1987.
Probability bounds analysis (PBA) is a collection of methods of uncertainty propagation for making qualitative and quantitative calculations in the face of uncertainties of various kinds. It is used to project partial information about random variables and other quantities through mathematical expressions. For instance, it computes sure bounds on the distribution of a sum, product, or more complex function, given only sure bounds on the distributions of the inputs. Such bounds are called probability boxes, and constrain cumulative probability distributions (rather than densities or mass functions). This bounding approach permits analysts to make calculations without requiring overly precise assumptions about parameter values, dependence among variables, or even distribution shape. Probability bounds analysis is essentially a combination of the methods of standard interval analysis and classical probability theory. Probability bounds analysis gives the same answer as interval analysis does when only range information is available. It also gives the same answers as Monte Carlo simulation does when information is abundant enough to precisely specify input distributions and their dependencies. Thus, it is a generalization of both interval analysis and probability theory. The diverse methods comprising probability bounds analysis provide algorithms to evaluate mathematical expressions when there is uncertainty about the input values, their dependencies, or even the form of mathematical expression itself. The calculations yield results that are guaranteed to enclose all possible distributions of the output variable if the input p-boxes were also sure to enclose their respective distributions. In some cases, a calculated p-box will also be best-possible in the sense that the bounds could be no tighter without excluding some of the possible distributions. P-boxes are usually merely bounds on possible distributions. The bounds often also enclose distributions that are not themselves possible. For instance, the set of probability distributions that could result from adding random values without the independence assumption from two (precise) distributions is generally a proper subset of all the distributions enclosed by the p-box computed for the sum. That is, there are distributions within the output p-box that could not arise under any dependence between the two input distributions. The output p-box will, however, always contain all distributions that are possible, so long as the input p-boxes were sure to enclose their respective underlying distributions. This property often suffices for use in risk analysis and other fields requiring calculations under uncertainty.
A Poincare  plot, named after Henri Poincare , is used to quantify self-similarity in processes, usually periodic functions. It is also known as a return map. Poincare  plots can be used to distinguish chaos from randomness by embedding a data set into a higher-dimensional state space. Given a time series of the form  a return map in its simplest form first plots (xt, xt+1), then plots (xt+1, xt+2), then (xt+2, xt+3), and so on.
Minimum distance estimation (MDE) is a statistical method for fitting a mathematical model to data, usually the empirical distribution.
A data set (or dataset) is a collection of data. Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. The term data set may also be used more loosely, to refer to the data in a collection of closely related tables, corresponding to a particular experiment or event. An example of this type is the data sets collected by space agencies performing experiments with instruments aboard space probes.
In probability theory and statistics, there are several algebraic formulae for the variance available for deriving the variance of a random variable. The usefulness of these depends on what is already known about the random variable; for example a random variable may be defined in terms of its probability density function or by construction from other random variables. The context here is that of deriving algebraic expressions for the theoretical variance of a random variable, in contrast to questions of estimating the variance of a population from sample data for which there are special considerations in implementing computational algorithms.
Equipossibility is a philosophical concept in possibility theory that is a precursor to the notion of equiprobability in probability theory. It is used to distinguish what can occur in a probability experiment. For example, when considering rolling a six-sided die, why do we typically view the possible outcomes as {1,2,3,4,5,6} rather than, say, {6, not 6}  The former set contains equally possible alternatives, while the latter does not because there are five times as many alternatives inherent in 'not 6' as in 6. This is true even if the die is biased so that 6 and 'not 6' are equally likely to occur. By the Principle of Indifference of Laplace, equipossible alternatives may be accorded equal probabilities if nothing more is known about the underlying probability distribution. It is a matter of contention whether the concept of equipossibility, also called equispecificity (from equispecific), can truly be distinguished from the concept of equiprobability. In Bayesian inference, a widely used definition of equipossibility is "a transformation group which leaves invariant one's state of knowledge". Equiprobability is then defined by normalizing the Haar measure of this symmetry group. This is known as the principle of transformation groups.
Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences among group means and their associated procedures (such as "variation" among and between groups), developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is less conservative (results in less type I error) and is therefore suited to a wide range of practical problems.
In statistics, a robust measure of scale is a robust statistic that quantifies the statistical dispersion in a set of numerical data. The most common such statistics are the interquartile range (IQR) and the median absolute deviation (MAD). These are contrasted with conventional measures of scale, such as sample variance or sample standard deviation, which are non-robust, meaning greatly influenced by outliers. These robust statistics are particularly used as estimators of a scale parameter, and have the advantages of both robustness and superior efficiency on contaminated data, at the cost of inferior efficiency on clean data from distributions such as the normal distribution. To illustrate robustness, the standard deviation can be made arbitrarily large by increasing exactly one observation (it has a breakdown point of 0, as it can be contaminated by a single point), a defect that is not shared by robust statistics.
1.96 is the approximate value of the 97.5 percentile point of the normal distribution used in probability and statistics. 95% of the area under a normal curve lies within roughly 1.96 standard deviations of the mean, and due to the central limit theorem, this number is therefore used in the construction of approximate 95% confidence intervals. Its ubiquity is due to the arbitrary but common convention of using confidence intervals with 95% coverage rather than other coverages (such as 90% or 99%). This convention seems particularly common in medical statistics, but is also common in other areas of application, such as earth sciences, social sciences and business research. There is no single accepted name for this number; it is also commonly referred to as the "standard normal deviate", "normal score" or "Z score" for the 97.5 percentile point, or .975 point. If X has a standard normal distribution, i.e. X ~ N(0,1),  and as the normal distribution is symmetric,  One notation for this number is z.025. From the probability density function of the normal distribution, the exact value of z.025 is determined by
Determining the number of clusters in a data set, a quantity often labeled k as in the k-means algorithm, is a frequent problem in data clustering, and is a distinct issue from the process of actually solving the clustering problem. For a certain class of clustering algorithms (in particular k-means, k-medoids and expectation maximization algorithm), there is a parameter commonly referred to as k that specifies the number of clusters to detect. Other algorithms such as DBSCAN and OPTICS algorithm do not require the specification of this parameter; hierarchical clustering avoids the problem altogether. The correct choice of k is often ambiguous, with interpretations depending on the shape and scale of the distribution of points in a data set and the desired clustering resolution of the user. In addition, increasing k without penalty will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when k equals the number of data points, n). Intuitively then, the optimal choice of k will strike a balance between maximum compression of the data using a single cluster, and maximum accuracy by assigning each data point to its own cluster. If an appropriate value of k is not apparent from prior knowledge of the properties of the data set, it must be chosen somehow. There are several categories of methods for making this decision.
In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It was developed by William Sealy Gosset under the pseudonym Student. Whereas a normal distribution describes a full population, t-distributions describe samples drawn from a full population; accordingly, the t-distribution for each sample size is different, and the larger the sample, the more the distribution resembles a normal distribution. The t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. The Student's t-distribution also arises in the Bayesian analysis of data from a normal family. If we take a sample of n observations from a normal distribution, then the t-distribution with  degrees of freedom can be defined as the distribution of the location of the true mean, relative to the sample mean and divided by the sample standard deviation, after multiplying by the normalizing term . In this way, the t-distribution can be used to estimate how likely it is that the true mean lies in any given range. The t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. This makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero. The Student's t-distribution is a special case of the generalised hyperbolic distribution.
The ratio estimator is a statistical parameter and is defined to be the ratio of means of two variates. Ratio estimates are biased and corrections must be made when they are used in experimental or survey work. The ratio estimates are asymmetrical and symmetrical tests such as the t test should not be used to generate confidence intervals. The bias is of the order O(1/n) (see big O notation) so as the sample size (n) increases, the bias will asymptotically approach 0. Therefore, the estimator is approximately unbiased for large sample sizes.
In statistics, a variable is termed a collider when it is the outcome of two (or more) variables (that may or may not themselves be correlated) (See Figure 1). The name "collider" reflects the fact that in graphical models, the arrow heads from variables that lead into the collider appear to "collide" on the node that is the collider  The result of having a collider in the path is that the collider blocks the association between the variables that influence it. Thus in the example shown on the right, "controlling" for the collider will cause the correlation between predictors X and Y to be biased (Berkson's paradox). The collider in this way "blocks" the association between its predictors. Colliders can confound attempts to test causal theories: By' "controlling" for what they consider to be a background variable such as education, researchers can unwittingly inducing false correlations among the variables of interest, and thus risk promoting theories for which there is in fact no support.
In statistics, the Pearson product-moment correlation coefficient (/ p  rs n/) (sometimes referred to as the PPMCC or PCC or Pearson's r) is a measure of the linear correlation between two variables X and Y, giving a value between +1 and  1 inclusive, where 1 is total positive correlation, 0 is no correlation, and  1 is total negative correlation. It is widely used in the sciences as a measure of the degree of linear dependence between two variables. It was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s. Early work on the distribution of the sample correlation coefficient was carried out by Anil Kumar Gain and R. A. Fisher from the University of Cambridge.
A ranking is a relationship between a set of items such that, for any two items, the first is either 'ranked higher than', 'ranked lower than' or 'ranked equal to' the second. In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. By reducing detailed measures to a sequence of ordinal numbers, rankings make it possible to evaluate complex information according to certain criteria. Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their relevance, making it possible for the user quickly to select the pages they are likely to want to see. Analysis of data obtained by ranking commonly requires non-parametric statistics.
In statistical hypothesis testing, the alternative hypothesis (or maintained hypothesis or research hypothesis) and the null hypothesis are the two rival hypotheses which are compared by a statistical hypothesis test. In the domain of science two rival hypotheses can be compared by explanatory power and predictive power.
gretl is an open-source statistical package, mainly for econometrics. The name is an acronym for Gnu Regression, Econometrics and Time-series Library. It has a graphical user interface (GUI) and can be used together with TRAMO/SEATS, R, Stata, Python, Octave, and Ox. It is written in C, uses GTK+ as widget toolkit for creating its GUI, and uses gnuplot for generating graphs. As a complement to the GUI it also has a command-line interface. gretl can output models as LaTeX files. Besides English, gretl is also available in Greek, Chinese, Basque, Catalan, Czech, German, French, Italian, Albanian, Polish, Portuguese, Russian, Spanish and Turkish. Gretl has been reviewed several times in the Journal of Applied Econometrics and in the Journal of Statistical Software.
In statistics, econometrics, political science, epidemiology, and related disciplines, a regression discontinuity design (RDD) is a quasi-experimental pretest-posttest design that elicits the causal effects of interventions by assigning a cutoff or threshold above or below which an intervention is assigned. By comparing observations lying closely on either side of the threshold, it is possible to estimate the local Average treatment effect in environments in which randomization was unfeasible. First applied by Donald Thistlethwaite and Donald Campbell to the evaluation of scholarship programs, the RDD has become increasingly popular in recent years.
The regression (or regressive) fallacy is an informal fallacy. It ascribes cause where none exists. The flaw is failing to account for natural fluctuations. It is frequently a special kind of the post hoc fallacy.
In probability theory and mathematical physics, a random matrix (sometimes stochastic matrix) is a matrix-valued random variable that is, a matrix some or all of whose elements are random variables. Many important properties of physical systems can be represented mathematically as matrix problems. For example, the thermal conductivity of a lattice can be computed from the dynamical matrix of the particle-particle interactions within the lattice.  
The power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H1) when it is true that is, the ability of a test to detect an effect, if the effect actually exists. That is,  The power of a test sometimes, less formally, refers to the probability of rejecting the null when it is not correct, though this is not the formal definition stated above. The power is in general a function of the possible distributions, often determined by a parameter, under the alternative hypothesis. As the power increases, there are decreasing chances of a Type II error (false negative), which are also referred to as the false negative rate ( ) since the power is equal to 1  , again, under the alternative hypothesis. A similar concept is Type I error, also referred to as the  false positive rate  or the level of a test under the null hypothesis. Power analysis can be used to calculate the minimum sample size required so that one can be reasonably likely to detect an effect of a given size. For example:  how many times do I need to toss a coin to conclude it is rigged   Power analysis can also be used to calculate the minimum effect size that is likely to be detected in a study using a given sample size. In addition, the concept of power is used to make comparisons between different statistical testing procedures: for example, between a parametric and a nonparametric test of the same hypothesis.
In probability theory and statistics, given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X is the probability distribution of Y when X is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value x of X as a parameter. In case that both "X" and "Y" are categorical variables, a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable. If the conditional distribution of Y given X is a continuous distribution, then its probability density function is known as the conditional density function. The properties of a conditional distribution, such as the moments, are often referred to by corresponding names such as the conditional mean and conditional variance. More generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables.
In statistics, quantile normalization is a technique for making two distributions identical in statistical properties. To quantile-normalize a test distribution to a reference distribution of the same length, sort the test distribution and sort the reference distribution. The highest entry in the test distribution then takes the value of the highest entry in the reference distribution, the next highest entry in the reference distribution, and so on, until the test distribution is a perturbation of the reference distribution. To quantile normalize two or more distributions to each other, without a reference distribution, sort as before, then set to the average (usually, arithmetic mean) of the distributions. So the highest value in all cases becomes the mean of the highest values, the second highest value becomes the mean of the second highest values, and so on. Generally a reference distribution will be one of the standard statistical distributions such as the Gaussian distribution or the Poisson distribution. The reference distribution can be generated randomly or from taking regular samples from the cumulative distribution function of the distribution. However, any reference distribution can be used. Quantile normalization is frequently used in microarray data analysis. It was introduced as quantile standardization and then renamed as quantile normalization.
In probability theory, comonotonicity mainly refers to the perfect positive dependence between the components of a random vector, essentially saying that they can be represented as increasing functions of a single random variable. In two dimensions it is also possible to consider perfect negative dependence, which is called countermonotonicity. Comonotonicity is also related to the comonotonic additivity of the Choquet integral. The concept of comonotonicity has applications in financial risk management and actuarial science, see e.g. Dhaene et al. (2002a) and Dhaene et al. (2002b). In particular, the sum of the components X1 + X2 +       + Xn is the riskiest if the joint probability distribution of the random vector (X1, X2, . . . , Xn) is comonotonic. Furthermore, the  -quantile of the sum equals of the sum of the  -quantiles of its components, hence comonotonic random variables are quantile-additive. In practical risk management terms it means that there is minimal (or eventually no) variance reduction from diversification. For extensions of comonotonicity, see Jouini & Napp (2004) and Puccetti & Scarsini (2010).
The relative index of inequality (RII) is a regression-based index which summarizes the magnitude of socio-economic status (SES) as a source of inequalities in health. RII is useful because it takes into account the size of the population and the relative disadvantage experienced by different groups. The disease outcome is regressed on the proportion of the population that has a higher position in the hierarchy. The RII is particularly valuable when comparing risk factors (independent variables) that are on very different scales (e.g. low SES, low IQ, cigarette smoking). The RII is calculated in the following way: Rank cases on each of the variables For tied ranks and for categorical variables, assign the mean rank Divide the ranks by the sample size, creating a value ranging from 0 to 1  ^ Mackenbach, Johan; Anton Kunst (1997). "Measuring the magnitude of socio-economic inequalities in health: An overview of available measures illustrated with two examples from Europe". Social Science & Medicine 44 (6): 757 771. doi:10.1016/S0277-9536(96)00073-1.   ^ Batty, David; Geoff Der; Sally Macintyre; Ian Deary (2006). "Does IQ explain socioeconomic inequalities in health  Evidence from a population based cohort study in the west of Scotland". BMJ 332 (7541): 580 584. doi:10.1136/bmj.38723.660637.AE. PMC 1397779. PMID 16452104. Retrieved 2010-12-02.
In probability theory, Chernoff's distribution, named after Herman Chernoff, is the probability distribution of the random variable  where W is a "two-sided" Wiener process (or two-sided "Brownian motion") satisfying W(0) = 0. If  then V(0, c) has density  where gc has Fourier transform given by  and where Ai is the Airy function. Thus fc is symmetric about 0 and the density  Z =  1. Groeneboom (1989) shows that  where  is the largest zero of the Airy function Ai and where .  
In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into "spam" or "non-spam" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance. Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. "A", "B", "AB" or "O", for blood type), ordinal (e.g. "large", "medium" or "small"), integer-valued (e.g. the number of occurrences of a part word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term "classifier" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term "classification" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.
Statistics are supposed to make something easier to understand but when used in a misleading fashion can trick the casual observer into believing something other than what the data shows. That is, a misuse of statistics occurs when a statistical argument asserts a falsehood. In some cases, the misuse may be accidental. In others, it is purposeful and for the gain of the perpetrator. When the statistical reason involved is false or misapplied, this constitutes a statistical fallacy. The false statistics trap can be quite damaging to the quest for knowledge. For example, in medical science, correcting a falsehood may take decades and cost lives. Misuses can be easy to fall into. Professional scientists, even mathematicians and professional statisticians, can be fooled by even some simple methods, even if they are careful to check everything. Scientists have been known to fool themselves with statistics due to lack of knowledge of probability theory and lack of standardization of their tests.
In descriptive statistics, the seven-number summary is a collection of seven summary statistics, and is an extension of the five-number summary. There are two similar, common forms. As with the five-number summary, it can be represented by a modified box plot, adding hatch-marks on the "whiskers" for two of the additional numbers.
Pharmaceutical Statistics is a peer-reviewed scientific journal that publishes papers related to pharmaceutical statistics. It is the official journal of Statisticians in the Pharmaceutical Industry and is published by John Wiley & Sons.
In probability theory, Bernstein inequalities give bounds on the probability that the sum of random variables deviates from its mean. In the simplest case, let X1, ..., Xn be independent Bernoulli random variables taking values +1 and  1 with probability 1/2 (this distribution is also known as the Rademacher distribution), then for every positive ,  Bernstein inequalities were proved and published by Sergei Bernstein in the 1920s and 1930s. Later, these inequalities were rediscovered several times in various forms. Thus, special cases of the Bernstein inequalities are also known as the Chernoff bound, Hoeffding's inequality and Azuma's inequality.
In the theory of finite population sampling, Poisson sampling is a sampling process where each element of the population that is sampled is subjected to an independent Bernoulli trial which determines whether the element becomes part of the sample during the drawing of a single sample. Each element of the population may have a different probability of being included in the sample. The probability of being included in a sample during the drawing of a single sample is denoted as the first-order inclusion probability of that element. If all first-order inclusion probabilities are equal, Poisson sampling becomes equivalent to Bernoulli sampling, which can therefore be considered to be a special case of Poisson sampling.
The erlang (symbol E) is a dimensionless unit that is used in telephony as a measure of offered load or carried load on service-providing elements such as telephone circuits or telephone switching equipment. For example, a single cord circuit has the capacity to be used for 60 minutes in one hour. If one hundred six-minute calls are received on a group of such circuits, then assuming no other calls are placed for the rest of the hour, the total traffic in that hour will be six hundred minutes, or 10 erlangs. In 1946, the CCITT named the international unit of telephone traffic the erlang in honor of Agner Krarup Erlang.
The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.
Discriminant function analysis is a statistical analysis to predict a categorical dependent variable (called a grouping variable) by one or more continuous or binary independent variables (called predictor variables). The original dichotomous discriminant analysis was developed by Sir Ronald Fisher in 1936. It is different from an ANOVA or MANOVA, which is used to predict one (ANOVA) or multiple (MANOVA) continuous dependent variables by one or more independent categorical variables. Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership. Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type. Moreover, it is a useful follow-up procedure to a MANOVA instead of doing a series of one-way ANOVAs, for ascertaining how the groups differ on the composite of dependent variables. In this case, a significant F test allows classification based on a linear combination of predictor variables. Terminology can get confusing here, as in MANOVA, the dependent variables are the predictor variables, and the independent variables are the grouping variables.
CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases that is more robust to outliers and identifies clusters having non-spherical shapes and size variances.
In statistics, M-estimators are a broad class of estimators, which are obtained as the minima of sums of functions of the data. Least-squares estimators are M-estimators. The definition of M-estimators was motivated by robust statistics, which contributed new types of M-estimators. The statistical procedure of evaluating an M-estimator on a data set is called M-estimation. More generally, an M-estimator may be defined to be a zero of an estimating function. This estimating function is often the derivative of another statistical function: For example, a maximum-likelihood estimate is often defined to be a zero of the derivative of the likelihood function with respect to the parameter: thus, a maximum-likelihood estimator is often a critical point of the score function. In many applications, such M-estimators can be thought of as estimating characteristics of the population.
In probability theory, Gauss's inequality (or the Gauss inequality) gives an upper bound on the probability that a unimodal random variable lies more than any given distance from its mode. Let X be a unimodal random variable with mode m, and let   2 be the expected value of (X   m)2. (  2 can also be expressed as (    m)2 +   2, where   and   are the mean and standard deviation of X.) Then for any positive value of k,  The theorem was first proved by Carl Friedrich Gauss in 1823.
In spatial ecology and macroecology, scaling pattern of occupancy (SPO), also known as the area-of-occupancy is the way in which species distribution changes across spatial scales. In physical geography and image analysis, it is similar to the modifiable areal unit problem. Simon A. Levin (1992) states that the problem of relating phenomena across scales is the central problem in biology and in all of science. Understanding the SPO is thus one central theme in ecology.
Pearson's chi-squared test ( 2) is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It is suitable for unpaired data from large samples. It is the most widely used of many chi-squared tests (e.g., Yates, likelihood ratio, portmanteau test in time series, etc.)   statistical procedures whose results are evaluated by reference to the chi-squared distribution. Its properties were first investigated by Karl Pearson in 1900. In contexts where it is important to improve a distinction between the test statistic and its distribution, names similar to Pearson  -squared test or statistic are used. It tests a null hypothesis stating that the frequency distribution of certain events observed in a sample is consistent with a particular theoretical distribution. The events considered must be mutually exclusive and have total probability 1. A common case for this is where the events each cover an outcome of a categorical variable. A simple example is the hypothesis that an ordinary six-sided die is "fair" (i. e., all six outcomes are equally likely to occur).
In statistics, a confounding variable (also confounding factor, a confound, or confounder) is an extraneous variable in a statistical model that correlates (directly or inversely) with both the dependent variable and the independent variable. A spurious relationship is a perceived relationship between an independent variable and a dependent variable that has been estimated incorrectly because the estimate fails to account for a confounding factor. The incorrect estimation suffers from omitted-variable bias. While specific definitions may vary, in essence a confounding variable fits the following four criteria, here given in a hypothetical situation with variable of interest "V", confounding variable "C" and outcome of interest "O": C is associated (inversely or directly) with O C is associated with O, independent of V C is associated (inversely or directly) with V C is not in the causal pathway of V to O (C is not a direct consequence of V, not a way by which V produces O) The preceding correlation-based definition, however, is metaphorical at best   a growing number of analysts agree that confounding is a causal concept, and as such, cannot be described in terms of correlations nor associations  (see causal definition).  
In statistics, a fixed effects model is a statistical model that represents the observed quantities in terms of explanatory variables that are treated as if the quantities were non-random. This is in contrast to random effects models and mixed models in which either all or some of the explanatory variables are treated as if they arise from random causes. Contrast this to the biostatistics definitions, as biostatisticians use "fixed" and "random" effects to respectively refer to the population-average and subject-specific effects (and where the latter are generally assumed to be unknown, latent variables). Often the same structure of model, which is usually a linear regression model, can be treated as any of the three types depending on the analyst's viewpoint, although there may be a natural choice in any given situation. In panel data analysis, the term fixed effects estimator (also known as the within estimator) is used to refer to an estimator for the coefficients in the regression model. If we assume fixed effects, we impose time independent effects for each entity that are possibly correlated with the regressors.
In statistics, the Jarque Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. The test is named after Carlos Jarque and Anil K. Bera. The test statistic JB is defined as  where n is the number of observations (or degrees of freedom in general); S is the sample skewness, C is the sample kurtosis, and k is the number of regressors:  where  and  are the estimates of third and fourth central moments, respectively,  is the sample mean, and  is the estimate of the second central moment, the variance. If the data comes from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom, so the statistic can be used to test the hypothesis that the data are from a normal distribution. The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero. Samples from a normal distribution have an expected skewness of 0 and an expected excess kurtosis of 0 (which is the same as a kurtosis of 3). As the definition of JB shows, any deviation from this increases the JB statistic. For small samples the chi-squared approximation is overly sensitive, often rejecting the null hypothesis when it is true. Furthermore, the distribution of p-values departs from a uniform distribution and becomes a right-skewed uni-modal distribution, especially for small p-values. This leads to a large Type I error rate. The table below shows some p-values approximated by a chi-squared distribution that differ from their true alpha levels for small samples.  (These values have been approximated by using Monte Carlo simulation in Matlab) In MATLAB's implementation, the chi-squared approximation for the JB statistic's distribution is only used for large sample sizes (> 2000). For smaller samples, it uses a table derived from Monte Carlo simulations in order to interpolate p-values.
The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect "mere" correlations, but Clive Granger argued that causality in economics could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of "true causality" is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, econometricians assert that the Granger test finds only "predictive causality". A time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y. Granger also stressed that some studies using "Granger causality" testing in areas outside economics reached "ridiculous" conclusions. "Of course, many ridiculous papers appeared", he said in his Nobel lecture. However, it remains a popular method for causality analysis in time series due to its computational simplicity. The original definition of Granger causality does not account for latent confounding effects and does not capture instantaneous and non-linear causal relationships, though several extensions have been proposed to address these issues.
In statistics, Dixon's Q test, or simply the Q test, is used for identification and rejection of outliers. This assumes normal distribution and per Dean and Dixon, and others, this test should be used sparingly and never more than once in a data set. To apply a Q test for bad data, arrange the data in order of increasing values and calculate Q as defined:  Where gap is the absolute difference between the outlier in question and the closest number to it. If Q > Qtable, where Qtable is a reference value corresponding to the sample size and confidence level, then reject the questionable point. Note that only one point may be rejected from a data set using a Q test.
In statistics, the Cuzick Edwards test is a significance test whose aim is to detect the possible clustering of sub-populations within a clustered or non-uniformly-spread overall population. Possible applications of the test include examining the spatial clustering of childhood leukemia and lymphoma within the general population, given that the general population is spatially clustered. The test is based on:  using control locations within the general population as the basis of a second or "control" sub-population in addition to the original "case" sub-population; using "nearest-neighbour" analyses to form statistics based on either: the number of other "cases" among the neighbours of each case; the number "cases" which are nearer to each given case than the k-th nearest "control" for that case.  An example application of this test was to spatial clustering of leukaemias and lymphomas among young people in New Zealand.
In statistics, an approximate entropy (ApEn) is a technique used to quantify the amount of regularity and the unpredictability of fluctuations over time-series data. For example, there are two series of data: series 1: (10,20,10,20,10,20,10,20,10,20,10,20...), which alternates 10 and 20. series 2: (10,10,20,10,20,20,20,10,10,20,10,20,20...), which has either a value of 10 or 20, chosen randomly, each with probability 1/2. Moment statistics, such as mean and variance, will not distinguish between these two series. Nor will rank order statistics distinguish between these series. Yet series 1 is "perfectly regular"; knowing one term has the value of 20 enables one to predict with certainty that the next term will have the value of 10. Series 2 is randomly valued; knowing one term has the value of 20 gives no insight into what value the next term will have. Regularity was originally measured by exact regularity statistics, which has mainly centered on various entropy measures. However, accurate entropy calculation requires vast amounts of data, and the results will be greatly influenced by system noise, therefore it is not practical to apply these methods to experimental data. ApEn was developed by Steve M. Pincus to handle these limitations by modifying an exact regularity statistic, Kolmogorov Sinai entropy. ApEn was initially developed to analyze medical data, such as heart rate, and later spread its applications in finance, psychology, and human factors engineering.
In actuarial science and demography, a life table (also called a mortality table or actuarial table) is a table which shows, for each age, what the probability is that a person of that age will die before his or her next birthday ("probability of death"). In other words, it represents the survivorship of people from a certain population.  There are two types of life tables used in actuarial science. The period life table represents mortality rates during a specific time period of a certain population. A cohort life table, often referred to as a generation life table, is used to represent the overall mortality rates of a certain population s entire lifetime. They must have had to be born during the same specific time interval. A cohort life table is more frequently used because it is able to make a prediction of any expected changes in mortality rates of a population in the future. This type of table also analyzes patterns in mortality rates that can be observed over time.  Both of these types of life tables are created based on an actual population from the present, as well as an educated prediction of the experience of a population in the near future. Other life tables in historical demography may be based on historical records, although these often undercount infants and understate infant mortality, on comparison with other regions with better records, and on mathematical adjustments for varying mortality levels and life expectancies at birth. From this starting point, a number of inferences can be derived. the probability of surviving any particular year of age remaining life expectancy for people at different ages Life tables are also used extensively in biology and epidemiology. An area that uses this tool is Social Security. It examines the mortality rates of all the people who have Social Security to decide which actions to take. The concept is also of importance in product life cycle management.
Ecological regression is a statistical technique used especially in political science and history to estimate group voting behavior from aggregate data. For example, if counties have a known Democratic vote (in percentage) D, and a known percentage of Catholics, C, then run the linear regression of dependent variable D against independent variable C. This gives D = a + bC. When C = 1 (100% Catholic) this gives the estimated Democratic vote as a+b. When C = 0 (0% Catholic), this gives the estimated non-Catholic vote as a. For example, if the regression gives D = .22 + .45C, then the estimated Catholic vote is 67% Democratic and the non-Catholic vote is 22% Democratic. The technique has been often used in litigation brought under the Voting Rights Act of 1965 to see how blacks and whites voted.  
V-statistics are a class of statistics named for Richard von Mises who developed their asymptotic distribution theory in a fundamental paper in 1947. V-statistics are closely related to U-statistics (U for  unbiased ) introduced by Wassily Hoeffding in 1948. A V-statistic is a statistical function (of a sample) defined by a particular statistical functional of a probability distribution.
The reliability theory of aging is an attempt to apply the principles of reliability theory to human biology. The theory was published in Russian by Leonid A. Gavrilov and Natalia S. Gavrilova as Biologiia prodolzhitel nosti zhizni in 1986, and in English translation as The Biology of Life Span: A Quantitative Approach in 1991. The hypothesis is based on the unusual premise that humans are born in a highly defective state. According to the model, this is then made worse by environmental and mutational damage; redundancy allows the organism to survive for a while.
In statistics, a generalized Wiener process (named after Norbert Wiener) is a continuous time random walk with drift and random jumps at every point in time. Formally:  where a and b are deterministic functions, t is a continuous index for time, x is a set of exogenous variables that may change with time, dt is a differential in time, and   is a random draw from a standard normal distribution at each instant.
In statistics and machine learning, one of the most common tasks is to fit a "model" to a set of training data, so as to be able to make reliable predictions on general untrained data. Overfitting occurs when a statistical model describes random error or noise instead of the underlying relationship. Overfitting generally occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overfit will generally have poor predictive performance, as it can exaggerate minor fluctuations in the data. The possibility of overfitting exists because the criterion used for training the model is not the same as the criterion used to judge the efficacy of a model. In particular, a model is typically trained by maximizing its performance on some set of training data. However, its efficacy is determined not by its performance on the training data but by its ability to perform well on unseen data. Overfitting occurs when a model begins to "memorize" training data rather than "learning" to generalize from trend. As an extreme example, if the number of parameters is the same as or greater than the number of observations, a simple model or learning process can perfectly predict the training data simply by memorizing the training data in its entirety, but such a model will typically fail drastically when making predictions about new or unseen data, since the simple model has not learned to generalize at all. The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting. In particular, the value of the coefficient of determination will shrink relative to the original training data. In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, Bayesian priors on parameters or model comparison), that can indicate when further training is not resulting in better generalization. The basis of some techniques is either (1) to explicitly penalize overly complex models, or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.
In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others. In case of perfect multicollinearity the design matrix is singular and therefore cannot be inverted. Under these circumstances, for a general linear model , the ordinary least-squares estimator  does not exist. Note that in statements of the assumptions underlying regression analyses such as ordinary least squares, the phrase "no multicollinearity" is sometimes used to mean the absence of perfect multicollinearity, which is an exact (non-stochastic) linear relation among the regressors.
The Cantor distribution is the probability distribution whose cumulative distribution function is the Cantor function. This distribution has neither a probability density function nor a probability mass function, as it is not absolutely continuous with respect to Lebesgue measure, nor has it any point-masses. It is thus neither a discrete nor an absolutely continuous probability distribution, nor is it a mixture of these. Rather it is an example of a singular distribution. Its cumulative distribution function is continuous everywhere but horizontal almost everywhere, so is sometimes referred to as the Devil's staircase, although that term has a more general meaning.
A long-tailed or heavy-tailed probability distribution is one that assigns relatively high probabilities to regions far from the mean or median. A more formal mathematical definition is given below. In the context of teletraffic engineering a number of quantities of interest have been shown to have a long-tailed distribution. For example, if we consider the sizes of files transferred from a web-server, then, to a good degree of accuracy, the distribution is heavy-tailed, that is, there are a large number of small files transferred but, crucially, the number of very large files transferred remains a major component of the volume downloaded. Many processes are technically long-range dependent but not self-similar. The differences between these two phenomena are subtle. Heavy-tailed refers to a probability distribution, and long-range dependent refers to a property of a time series and so these should be used with care and a distinction should be made. The terms are distinct although superpositions of samples from heavy-tailed distributions aggregate to form long-range dependent time series. Additionally there is Brownian motion which is self-similar but not long-range dependent.
In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used.
In artificial intelligence, Thompson sampling, named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.
In electrical engineering and applied mathematics, blind deconvolution is deconvolution without explicit knowledge of the impulse response function used in the convolution. In microscopy the term is used to describe deconvolution without knowledge of the microscopes point spread function. This is usually achieved by making appropriate assumptions of the input to estimate the impulse response by analyzing the output.
The Gini coefficient (also known as the Gini index or Gini ratio) (/d ini/ jee-nee) is a measure of statistical dispersion intended to represent the income distribution of a nation's residents, and is the most commonly used measure of inequality. It was developed by the Italian statistician and sociologist Corrado Gini and published in his 1912 paper Variability and Mutability (Italian: Variabilita  e mutabilita ). The Gini coefficient measures the inequality among values of a frequency distribution (for example, levels of income). A Gini coefficient of zero expresses perfect equality, where all values are the same (for example, where everyone has the same income). A Gini coefficient of 1 (or 100%) expresses maximal inequality among values (e.g., for a large number of people, where only one person has all the income or consumption, and all others have none, the Gini coefficient will be very nearly one). However, a value greater than one may occur if some persons represent negative contribution to the total (for example, having negative income or wealth). For larger groups, values close to or above 1 are very unlikely in practice. Given the normalization of both the cumulative population and the cumulative share of income used to calculate the Gini coefficient, the measure is not overly sensitive to the specifics of the income distribution, but rather only on how incomes vary relative to the other members of a population. The exception to this is in the redistribution of wealth resulting in a minimum income for all people. When the population is sorted, if their income distribution were to approximate a well known function, then some representative values could be calculated. The Gini coefficient was proposed by Gini as a measure of inequality of income or wealth. For OECD countries, in the late 20th century, considering the effect of taxes and transfer payments, the income Gini coefficient ranged between 0.24 and 0.49, with Slovenia the lowest and Chile the highest. African countries had the highest pre-tax Gini coefficients in 2008 2009, with South Africa the world's highest, variously estimated to be 0.63 to 0.7, although this figure drops to 0.52 after social assistance is taken into account, and drops again to 0.47 after taxation. The global income Gini coefficient in 2005 has been estimated to be between 0.61 and 0.68 by various sources. There are some issues in interpreting a Gini coefficient. The same value may result from many different distribution curves. The demographic structure should be taken into account. Countries with an aging population, or with a baby boom, experience an increasing pre-tax Gini coefficient even if real income distribution for working adults remains constant. Scholars have devised over a dozen variants of the Gini coefficient.  
In mathematics, the harmonic mean (sometimes called the subcontrary mean) is one of several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of rates is desired. The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals. As a simple example, the harmonic mean of 1, 2, and 4 is  The harmonic mean H of the positive real numbers  is defined to be  From the third formula in the above equation, it is more apparent that the harmonic mean is related to the arithmetic and geometric means. It is the reciprocal dual of the arithmetic mean for positive inputs:  The harmonic mean is a Schur-concave function, and dominated by the minimum of its arguments, in the sense that for any positive set of arguments, . Thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged).
OpenEpi is a free, web-based, open source, operating system-independent series of programs for use in epidemiology, biostatistics, public health, and medicine, providing a number of epidemiologic and statistical tools for summary data. OpenEpi was developed in JavaScript and HTML, and can be run in modern web browsers. The program can be run from the OpenEpi website or downloaded and run without a web connection. The source code and documentation is downloadable and freely available for use by other investigators. OpenEpi has been reviewed, both by media organizations and in research journals. The OpenEpi developers have had extensive experience in the development and testing of Epi Info, a program developed by the Centers for Disease Control and Prevention (CDC) and widely used around the world for data entry and analysis. OpenEpi was developed to perform analyses found in the DOS version of Epi Info modules StatCalc and EpiTable, to improve upon the types of analyses provided by these modules, and to provide a number of tools and calculations not currently available in Epi Info. It is the first step toward an entirely web-based set of epidemiologic software tools. OpenEpi can be thought of as an important companion to Epi Info and to other programs such as SAS, PSPP, SPSS, Stata, SYSTAT, Minitab, Epidata, and R (see the R programming language). Another functionally similar Windows-based program is Winpepi. See also list of statistical packages and comparison of statistical packages. Both OpenEpi and Epi Info were developed with the goal of providing tools for low and moderate resource areas of the world. The initial development of OpenEpi was supported by a grant from the Bill and Melinda Gates Foundation to Emory University. The types of calculations currently performed by OpenEpi include: Various confidence intervals for proportions, rates, standardized mortality ratio, mean, median, percentiles 2x2 crude and stratified tables for count and rate data Matched case-control analysis Test for trend with count data Independent t-test and one-way ANOVA Diagnostic and screening test analyses with receiver operating characteristic (ROC) curves Sample size for proportions, cross-sectional surveys, unmatched case-control, cohort, randomized controlled trials, and comparison of two means Power calculations for proportions (unmatched case-control, cross-sectional, cohort, randomized controlled trials) and for the comparison of two means Random number generator For epidemiologists and other health researchers, OpenEpi performs a number of calculations based on tables not found in most epidemiologic and statistical packages. For example, for a single 2x2 table, in addition to the results presented in other programs, OpenEpi provides estimates for: Etiologic or prevented fraction in the population and in exposed with confidence intervals, based on risk, odds, or rate data The cross-product and MLE odds ratio estimate Mid-p exact p-values and confidence limits for the odds ratio Calculations of rate ratios and rate differences with confidence intervals and statistical tests. For stratified 2x2 tables with count data, OpenEpi provides: Mantel-Haenszel (MH) and precision-based estimates of the risk ratio and odds ratio Precision-based adjusted risk difference Tests for interaction for the risk ratio, odds ratio, and risk difference Four different confidence limit methods for the odds ratio. Similar to Epi Info, in a stratified analysis, both crude and adjusted estimates are provided so that the assessment of confounding can be made. With rate data, OpenEpi provides adjusted rate ratio s and rate differences, and tests for interaction. Finally, with count data, OpenEpi also performs a test for trend, for both crude data and stratified data. In addition to being used to analyze data by health researchers, OpenEpi has been used as a training tool for teaching epidemiology to students at: Emory University, University of Massachusetts, University of Michigan, University of Minnesota, Morehouse College, Columbia University, University of Wisconsin, San Jose State University, University of Medicine and Dentistry of New Jersey, University of Washington, and elsewhere. This includes campus-based and distance learning courses. Because OpenEpi is easy to use, requires no programming experience, and can be run on the internet, students can use the program and focus on the interpretation of results. Users can run the program in English, French, Spanish, Portuguese or Italian. Comments and suggestions for improvements are welcomed and the developers respond to user queries. The developers encourage others to develop modules that could be added to OpenEpi and provide a developer s tool at the website. Planned future development include improvements to existing modules, development of new modules, translation into other languages, and add the ability to cut and paste data and/or read data files.
Failure rate is the frequency with which an engineered system or component fails, expressed in failures per unit of time. It is often denoted by the Greek letter   (lambda) and is highly used in reliability engineering. The failure rate of a system usually depends on time, with the rate varying over the life cycle of the system. For example, an automobile's failure rate in its fifth year of service may be many times greater than its failure rate during its first year of service. One does not expect to replace an exhaust pipe, overhaul the brakes, or have major transmission problems in a new vehicle. In practice, the mean time between failures (MTBF, 1/ ) is often reported instead of the failure rate. This is valid and useful if the failure rate may be assumed constant   often used for complex units / systems, electronics   and is a general agreement in some reliability standards (Military and Aerospace). It does in this case only relate to the flat region of the bathtub curve, also called the "useful life period". Because of this, it is incorrect to extrapolate MTBF to give an estimate of the service life time of a component, which will typically be much less than suggested by the MTBF due to the much higher failure rates in the "end-of-life wearout" part of the "bathtub curve". The reason for the preferred use for MTBF numbers is that the use of large positive numbers (such as 2000 hours) is more intuitive and easier to remember than very small numbers (such as 0.0005 per hour). The MTBF is an important system parameter in systems where failure rate needs to be managed, in particular for safety systems. The MTBF appears frequently in the engineering design requirements, and governs frequency of required system maintenance and inspections. In special processes called renewal processes, where the time to recover from failure can be neglected and the likelihood of failure remains constant with respect to time, the failure rate is simply the multiplicative inverse of the MTBF (1/ ). A similar ratio used in the transport industries, especially in railways and trucking is "mean distance between failures", a variation which attempts to correlate actual loaded distances to similar reliability needs and practices. Failure rates are important factors in the insurance, finance, commerce and regulatory industries and fundamental to the design of safe systems in a wide variety of applications.
A Markov logic network (or MLN) is a probabilistic logic which applies the ideas of a Markov network to first-order logic, enabling uncertain inference. Markov logic networks generalize first-order logic, in the sense that, in a certain limit, all unsatisfiable statements have a probability of zero, and all tautologies have probability one.
Sample size determination is the act of choosing the number of observations or replicates to include in a statistical sample. The sample size is an important feature of any empirical study in which the goal is to make inferences about a population from a sample. In practice, the sample size used in a study is determined based on the expense of data collection, and the need to have sufficient statistical power. In complicated studies there may be several different sample sizes involved in the study: for example, in a stratified survey there would be different sample sizes for each stratum. In a census, data are collected on the entire population, hence the sample size is equal to the population size. In experimental design, where a study may be divided into different treatment groups, there may be different sample sizes for each group. Sample sizes may be chosen in several different ways: experience - For example, include those items readily available or convenient to collect. A choice of small sample sizes, though sometimes necessary, can result in wide confidence intervals or risks of errors in statistical hypothesis testing. using a target variance for an estimate to be derived from the sample eventually obtained using a target for the power of a statistical test to be applied once the sample is collected.
Process control is an engineering discipline that deals with architectures, mechanisms and algorithms for maintaining the output of a specific process within a desired range. For instance, the temperature of a chemical reactor may be controlled to maintain a consistent product output. Process control is extensively used in industry and enables mass production of consistent products from continuously operated processes such as oil refining, paper manufacturing, chemicals, power plants and many others. Process control enables automation, by which a small staff of operating personnel can operate a complex process from a central control room.
In probability theory and statistics, coherence can have several different meanings.
In statistical quality control, the c-chart is a type of control chart used to monitor "count"-type data, typically total number of nonconformities per unit. It is also occasionally used to monitor the total number of events occurring in a given unit of time. The c-chart differs from the p-chart in that it accounts for the possibility of more than one nonconformity per inspection unit, and that (unlike the p-chart and u-chart) it requires a fixed sample size. The p-chart models "pass"/"fail"-type inspection only, while the c-chart (and u-chart) give the ability to distinguish between (for example) 2 items which fail inspection because of one fault each and the same two items failing inspection with 5 faults each; in the former case, the p-chart will show two non-conformant items, while the c-chart will show 10 faults. Nonconformities may also be tracked by type or location which can prove helpful in tracking down assignable causes. Examples of processes suitable for monitoring with a c-chart include: Monitoring the number of voids per inspection unit in injection molding or casting processes Monitoring the number of discrete components that must be re-soldered per printed circuit board Monitoring the number of product returns per day The Poisson distribution is the basis for the chart and requires the following assumptions: The number of opportunities or potential locations for nonconformities is very large The probability of nonconformity at any location is small and constant The inspection procedure is same for each sample and is carried out consistently from sample to sample The control limits for this chart type are  where  is the estimate of the long-term process mean established during control-chart setup.
In filtering theory the Zakai equation is a linear stochastic partial differential equation for the un-normalized density of a hidden state. In contrast, the Kushner equation gives a non-linear stochastic partial differential equation for the normalized density of the hidden state. In principle either approach allows one to estimate a quantity function (the state of a Dynamical system) from noisy measurements, even when the system is non-linear (thus generalizing the earlier results of Wiener and Kalman for linear systems and solving a central problem in estimation theory). The application of this approach to a specific engineering situation may be problematic however, as these equations are quite complex. The Zakai equation is a bilinear stochastic partial differential equation. It was named after Moshe Zakai.
Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences among group means and their associated procedures (such as "variation" among and between groups), developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is less conservative (results in less type I error) and is therefore suited to a wide range of practical problems.
In probability theory and statistics, a Markov process or Markoff process, named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property. A Markov process can be thought of as 'memoryless': loosely speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history. i.e., conditional on the present state of the system, its future and past are independent.
CHAID is a type of decision tree technique, based upon adjusted significance testing (Bonferroni testing). The technique was developed in South Africa and was published in 1980 by Gordon V. Kass, who had completed a PhD thesis on this topic. CHAID can be used for prediction (in a similar fashion to regression analysis, this version of CHAID being originally known as XAID) as well as classification, and for detection of interaction between variables. CHAID stands for CHi-squared Automatic Interaction Detection, based upon a formal extension of the US AID (Automatic Interaction Detection) and THAID (THeta Automatic Interaction Detection) procedures of the 1960s and 1970s, which in turn were extensions of earlier research, including that performed in the UK in the 1950s. In practice, CHAID is often used in the context of direct marketing to select groups of consumers and predict how their responses to some variables affect other variables, although other early applications were in the field of medical and psychiatric research. Like other decision trees, CHAID's advantages are that its output is highly visual and easy to interpret. Because it uses multiway splits by default, it needs rather large sample sizes to work effectively, since with small sample sizes the respondent groups can quickly become too small for reliable analysis. One important advantage of CHAID over alternatives such as multiple regression is that it is non-parametric.
In queueing theory, a discipline within the mathematical theory of probability, Ross's conjecture gives a lower bound for the average waiting-time experienced by a customer when arrivals to the queue do not follow the simplest model for random arrivals. It was proposed by Sheldon M. Ross in 1978 and proved in 1981 by Tomasz Rolski. Equality can be obtained in the bound; and the bound does not hold for finite buffer queues.
Common and special causes are the two distinct origins of variation in a process, as defined in the statistical thinking and methods of Walter A. Shewhart and W. Edwards Deming. Briefly, "common causes", also called Natural patterns, are the usual, historical, quantifiable variation in a system, while "special causes" are unusual, not previously observed, non-quantifiable variation. The distinction is fundamental in philosophy of statistics and philosophy of probability, with different treatment of these issues being a classic issue of probability interpretations, being recognised and discussed as early as 1703 by Gottfried Leibniz; various alternative names have been used over the years. The distinction has been particularly important in the thinking of economists Frank Knight, John Maynard Keynes and G. L. S. Shackle.
Bayesian econometrics is a branch of econometrics which applies Bayesian principles to economic modelling. Bayesianism is based on a degree-of-belief interpretation of probability, as opposed to a relative-frequency interpretation. The Bayesian principle relies on Bayes' theorem which states that the probability of B conditional on A is the ratio of joint probability of A and B divided by probability of B. Bayesian econometricians assume that coefficients in the model have prior distributions. This approach was first propagated by Arnold Zellner.
The Skellam distribution is the discrete probability distribution of the difference  of two statistically independent random variables  and  each Poisson-distributed with respective expected values  and  It is useful in describing the statistics of the difference of two images with simple photon noise, as well as describing the point spread distribution in sports where all scored points are equal, such as baseball, hockey and soccer. The distribution is also applicable to a special case of the difference of dependent Poisson random variables, but just the obvious case where the two variables have a common additive random contribution which is cancelled by the differencing: see Karlis & Ntzoufras (2003) for details and an application. The probability mass function for the Skellam distribution for a difference  between two independent Poisson-distributed random variables with means  and  is given by:  where Ik(z) is the modified Bessel function of the first kind. Note that since k is an integer we have that Ik(z)=I|k|(z).  
In statistics, signal processing, and time series analysis, a sinusoidal model to approximate a sequence Yi is:  where C is constant defining a mean level,   is an amplitude for the sine wave,   is the frequency, Ti is a time variable,   is the phase, and Ei is the error sequence in approximating the sequence Yi by the model. This sinusoidal model can be fit using nonlinear least squares; to obtain a good fit, nonlinear least squares routines may require good starting values for the constant, the amplitude, and the frequency. Fitting a model with a single sinusoid is a special case of least-squares spectral analysis.
Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of minimum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). The PLS algorithm is employed in partial least squares path modeling, a method of modeling a "causal" network of latent variables (causes cannot be determined without experimental or quasi-experimental methods, but one typically bases a latent variable model on the prior theoretical assumption that latent variables cause manifestations in their measured indicators). This technique is argued to be a form of structural equation modeling, distinguished from the classical method by being component-based rather than covariance-based. Yet, others dispute that this is the case. Partial least squares was introduced by the Swedish statistician Herman Wold, who then developed it with his son, Svante Wold. An alternative term for PLS (and more correct according to Svante Wold) is projection to latent structures, but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience and anthropology. In contrast, PLS path modeling is most often used in social sciences, econometrics, marketing and strategic management. However, within the realm of psychology, it has received criticism for being an unreliable estimation and testing tool.
In probability theory and statistics, kurtosis (from Greek:        , kyrtos or kurtos, meaning "curved, arching") is a measure of the "tailedness" of the probability distribution of a real-valued random variable. In a similar way to the concept of skewness, kurtosis is a descriptor of the shape of a probability distribution and, just as for skewness, there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Depending on the particular measure of kurtosis that is used, there are various interpretations of kurtosis, and of how particular measures should be interpreted. The standard measure of kurtosis, originating with Karl Pearson, is based on a scaled version of the fourth moment of the data or population. This number measures heavy tails, and not peakedness; hence, the historical "peakedness" definition is wrong. For this measure, higher kurtosis means more of the variance is the result of infrequent extreme deviations, as opposed to frequent modestly sized deviations. The kurtosis of any univariate normal distribution is 3. It is common to compare the kurtosis of a distribution to this value. Distributions with kurtosis less than 3 are said to be platykurtic, although this does not imply the distribution is "flat-topped" as sometimes reported. Rather, it means the distribution produces fewer and less extreme outliers than does the normal distribution. An example of a platykurtic distribution is the uniform distribution, which does not produce outliers. Distributions with kurtosis greater than 3 are said to be leptokurtic. An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian, and therefore produces more outliers than the normal distribution. It is also common practice to use an adjusted version of Pearson's kurtosis, the excess kurtosis, which is the kurtosis minus 3, to provide the comparison to the normal distribution. Some authors use "kurtosis" by itself to refer to the excess kurtosis. For the reason of clarity and generality, however, this article follows the non-excess convention and explicitly indicates where excess kurtosis is meant. Alternative measures of kurtosis are: the L-kurtosis, which is a scaled version of the fourth L-moment; measures based on 4 population or sample quantiles. These correspond to the alternative measures of skewness that are not based on ordinary moments.
In statistics, a generalized p-value is an extended version of the classical p-value, which except in a limited number of applications, provides only approximate solutions. Conventional statistical methods do not provide exact solutions to many statistical problems, such as those arising in mixed models and MANOVA, especially when the problem involves many nuisance parameters. As a result, practitioners often resort to approximate statistical methods or asymptotic statistical methods that are valid only with large samples. With small samples, such methods often have poor performance. Use of approximate and asymptotic methods may lead to misleading conclusions or may fail to detect truly significant results from experiments. Tests based on generalized p-values are exact statistical methods in that they are based on exact probability statements. While conventional statistical methods do not provide exact solutions to such problems as testing variance components or ANOVA under unequal variances, exact tests for such problems can be obtained based on generalized p-values. In order to overcome the shortcomings of the classical p-values, Tsui and Weerahandi extended the classical definition so that one can obtain exact solutions for such problems as the Behrens Fisher problem and testing variance components. This is accomplished by allowing test variables to depend on observable random vectors as well as their observed values, as in the Bayesian treatment of the problem, but without having to treat constant parameters as random variables.
In time series analysis, the moving-average (MA) model is a common approach for modeling univariate time series. The notation MA(q) refers to the moving average model of order q:  where   is the mean of the series, the  1, ...,  q are the parameters of the model and the  t,  t 1,...,  t q are white noise error terms. The value of q is called the order of the MA model. This can be equivalently written in terms of the backshift operator B as  Thus, a moving-average model is conceptually a linear regression of the current value of the series against current and previous (unobserved) white noise error terms or random shocks. The random shocks at each point are assumed to be mutually independent and to come from the same distribution, typically a normal distribution, with location at zero and constant scale.
Predictive informatics (PI) is the combination of predictive modeling and informatics applied to healthcare, pharmaceutical, life sciences and business industries. Predictive informatics enables researchers, analysts, physicians and decision-makers to aggregate and analyze disparate types of data, recognize patterns and trends within that data, and make more informed decisions in an effort to preemptively alter future outcomes.
In statistics, the method of moments is a method of estimation of population parameters. One starts with deriving equations that relate the population moments (i.e., the expected values of powers of the random variable under consideration) to the parameters of interest. Then a sample is drawn and the population moments are estimated from the sample. The equations are then solved for the parameters of interest, using the sample moments in place of the (unknown) population moments. This results in estimates of those parameters. The method of moments was introduced by Karl Pearson in 1894.
In statistics, a confidence region is a multi-dimensional generalization of a confidence interval. It is a set of points in an n-dimensional space, often represented as an ellipsoid around a point which is an estimated solution to a problem, although other shapes can occur.
In mathematics, the Khintchine inequality, named after Aleksandr Khinchin and spelled in multiple ways in the Roman alphabet, is a theorem from probability, and is also frequently used in analysis. Heuristically, it says that if we pick  complex numbers , and add them together each multiplied by a random sign , then the expected value of its modulus, or the modulus it will be closest to on average, will be not too far off from .
In statistics, interval estimation is the use of sample data to calculate an interval of possible (or probable) values of an unknown population parameter, in contrast to point estimation, which is a single number. Jerzy Neyman (1937) identified interval estimation ("estimation by interval") as distinct from point estimation ("estimation by unique estimate"). In doing so, he recognised that then-recent work quoting results in the form of an estimate plus-or-minus a standard deviation indicated that interval estimation was actually the problem statisticians really had in mind. The most prevalent forms of interval estimation are: confidence intervals (a frequentist method); and credible intervals (a Bayesian method). Other common approaches to interval estimation, which are encompassed by statistical theory, are: Tolerance intervals Prediction intervals - used mainly in Regression Analysis Likelihood intervals There is another approach to statistical inference, namely fiducial inference, that also considers interval estimation. Non-statistical methods that can lead to interval estimates include fuzzy logic. An interval estimate is one type of outcome of a statistical analysis. Some other types of outcome are point estimates and decisions.
In statistics, the Cochran Mantel Haenszel statistics are a collection of test statistics used in the analysis of stratified categorical data. They are named after William G. Cochran, Nathan Mantel and William Haenszel. One of these test statistics is the Cochran Mantel Haenszel (CMH) test, which allows the comparison of two groups on a dichotomous/categorical response. It is used when the effect of the explanatory variable on the response variable is influenced by covariates that can be measured/observed ("controlled for" to use common but somewhat misleading terminology). Unobserved covariates pose greater problems. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but influencing covariates can. In the CMH test, the data are arranged in a series of associated 2   2 contingency tables, the null hypothesis is that the observed response is independent of the treatment used in any 2   2 contingency table. The CMH test's use of associated 2   2 contingency tables increases the ability of the test to detect associations (the power of the test is increased).  
Software that is used for designing factorial experiments plays an important role in scientific experiments and represents a route to the implementation of design of experiments procedures that derive from statistical and combinatorial theory. In principle, easy-to-use design of experiments (DOE) software should be available to all experimenters to foster use of DOE.
In combinatorial mathematics, the rencontres numbers are a triangular array of integers that enumerate permutations of the set { 1, ..., n } with specified numbers of fixed points: in other words, partial derangements. (Rencontre is French for encounter. By some accounts, the problem is named after a solitaire game.) For n   0 and 0   k   n, the rencontres number Dn, k is the number of permutations of { 1, ..., n } that have exactly k fixed points. For example, if seven presents are given to seven different people, but only two are destined to get the right present, there are D7, 2 = 924 ways this could happen. Another often cited example is that of a dance school with 7 couples, where after tea-break the participants are told to randomly find a partner to continue, and there are D7, 2 = 924 possibilities once more, now, that 2 previous couples meet again just by chance.
Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words. An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called Latent Semantic Indexing (LSI).
In machine learning, one-class classification, also known as unary classification, tries to identify objects of a specific class amongst all objects, by learning from a training set containing only the objects of that class. This is different from and more difficult than the traditional classification problem, which tries to distinguish between two or more classes with the training set containing objects from all the classes. An example is the classification of the operational status of a nuclear plant as 'normal': In this scenario, there are (fortunately) few or no examples of catastrophic system states, only the statistics of normal operation are known. The term One-class classification was coined by Moya & Hush (1996) and many applications can be found in scientific literature, for example outlier detection, anomaly detection, novelty detection. A similar problem is PU learning, in which a binary classifier is learned in a semi-supervised way from only positive and unlabeled samples.
Info-gap decision theory is a non-probabilistic decision theory that seeks to optimize robustness to failure   or opportuneness for windfall   under severe uncertainty, in particular applying sensitivity analysis of the stability radius type to perturbations in the value of a given estimate of the parameter of interest. It has some connections with Wald's maximin model; some authors distinguish them, others consider them instances of the same principle. It has been developed since the 1980s by Yakov Ben-Haim, and has found many applications and described as a theory for decision-making under "severe uncertainty". It has been criticized as unsuited for this purpose, and alternatives proposed, including such classical approaches as robust optimization.
Response rate (also known as completion rate or return rate) in survey research refers to the number of people who answered the survey divided by the number of people in the sample. It is usually expressed in the form of a percentage. The term is also used in direct marketing to refer to the number of people who responded to an offer. The general consensus in academic surveys is to choose one of the six definitions summarized by the American Association for Public Opinion Research (AAPOR). These definitions are endorsed by the National Research Council and the Journal of the American Medical Association, among other well recognized institutions. They are: Response Rate 1 (RR1) -- or the minimum response rate, is the number of complete interviews divided by the number of interviews (complete plus partial) plus the number of non-interviews (refusal and break-off plus non-contacts plus others) plus all cases of unknown eligibility (unknown if housing unit, plus unknown, other). Response Rate 2 (RR2) -- RR1 + counting partial interviews as respondents. Response Rate 3 (RR3) -- estimates what proportion of cases of unknown eligibility is actually eligible. Those respondents estimated to be ineligible are excluded from the denominator. The method of estimation *must* be explicitly stated with RR3. Response Rate 4 (RR4) -- allocates cases of unknown eligibility as in RR3, but also includes partial interviews as respondents as in RR2. Response Rate 5 (RR5) -- is either a special case of RR3 in that it assumes that there are no eligible cases among the cases of unknown eligibility or the rare case in which there are no cases of unknown eligibility. RR5 is only appropriate when it is valid to assume that none of the unknown cases are eligible ones, or when there are no unknown cases. Response Rate 6 (RR6) -- makes that same assumption as RR5 and also includes partial interviews as respondents. RR6 represents the maximum response rate. The six AAPOR definitions vary with respect to whether or not the surveys are partially or entirely completed and how researchers deal with unknown nonrespondents. Definition #1, for example, does NOT include partially completed surveys in the numerator, while definition #2 does. Definitions 3-6 deal with the unknown eligibility of potential respondents who could not be contacted. For example, there is no answer at the doors of 10 houses you attempted to survey. Maybe 5 of those you already know house people who qualify for your survey based on neighbors telling you whom lived there, but the other 5 are completely unknown. Maybe the dwellers fit your target population, maybe they don't. This may or may not be considered in your response rate, depending on which definition you use. Example: if 1,000 surveys were sent by mail, and 257 were successfully completed (entirely) and returned, then the response rate would be 25.7%.
Transiogram is the accompanying spatial correlation measure of simplified Markov chain random field (MCRF) models based on the conditional independence assumption and an important part of Markov chain geostatistics. It is defined as a transition probability function over the distance lag. Simply, a transiogram refers to a transition probability-lag diagram. Transiograms include auto-transiograms and cross-transiograms. The former represent the spatial auto-correlation of a single category, and the latter represent the spatial interclass relationships among different categories. Experimental transiograms can be directly estimated from sparse sample data. Transiogram models, which provide transition probabilities at any lags for Markov chain modeling, can be further acquired through model fitting of experimental transiograms. In general, the transiogram is a spatial correlation measure following the style of variogram, and it includes a set of concepts and a set of methods for obtaining transition probability values from sample data and provide transition probabilities values for simplified MCRF models.
In statistics, intra-rater reliability is the degree of agreement among repeated administrations of a diagnostic test performed by a single rater.
In economics, game theory, and decision theory the expected utility hypothesis is a hypothesis concerning people's preferences with regard to choices that have uncertain outcomes (gambles). This hypothesis states that if specific axioms are satisfied, the subjective value associated with an individual's gamble is the statistical expectation of that individual's valuations of the outcomes of that gamble. This hypothesis has proved useful to explain some popular choices that seem to contradict the expected value criterion (which takes into account only the sizes of the payouts and the probabilities of occurrence), such as occur in the contexts of gambling and insurance. Daniel Bernoulli initiated this hypothesis in 1738. Until the mid-twentieth century, the standard term for the expected utility was the moral expectation, contrasted with "mathematical expectation" for the expected value. The von Neumann Morgenstern utility theorem provides necessary and sufficient conditions under which the expected utility hypothesis holds. From relatively early on, it was accepted that some of these conditions would be violated by real decision-makers in practice but that the conditions could be interpreted nonetheless as 'axioms' of rational choice. Work by Anand (1993) argues against this normative interpretation and shows that 'rationality' does not require transitivity, independence or completeness. This view is now referred to as the 'modern view' and Anand argues that despite the normative and evidential difficulties the general theory of decision-making based on expected utility is an insightful first order approximation that highlights some important fundamental principles of choice, even if it imposes conceptual and technical limits on analysis which need to be relaxed in real world settings where knowledge is less certain or preferences are more sophisticated.
A short-rate model, in the context of interest rate derivatives, is a mathematical model that describes the future evolution of interest rates by describing the future evolution of the short rate, usually written .
In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the conditional probability that is assigned after the relevant evidence or background is taken into account. Similarly, the posterior probability distribution is the probability distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained from an experiment or survey. "Posterior", in this context, means after taking into account the relevant evidence related to the particular case being examined.
In statistics, a varimax rotation is used to simplify the expression of a particular sub-space in terms of just a few major items each. The actual coordinate system is unchanged, it is the orthogonal basis that is being rotated to align with those coordinates. The sub-space found with principal component analysis or factor analysis is expressed as a dense basis with many non-zero weights which makes it hard to interpret. Varimax is so called because it maximizes the sum of the variances of the squared loadings (squared correlations between variables and factors). Preserving orthogonality requires that it is a rotation that leaves the sub-space invariant. Intuitively, this is achieved if, (a) any given variable has a high loading on a single factor but near-zero loadings on the remaining factors and if (b) any given factor is constituted by only a few variables with very high loadings on this factor while the remaining variables have near-zero loadings on this factor. If these conditions hold, the factor loading matrix is said to have "simple structure," and varimax rotation brings the loading matrix closer to such simple structure (as much as the data allow). From the perspective of individuals measured on the variables, varimax seeks a basis that most economically represents each individual that is, each individual can be well described by a linear combination of only a few basis functions. One way of expressing the varimax criterion formally is this:  Suggested by Henry Felix Kaiser in 1958, it is a popular scheme for orthogonal rotation (where all factors remain uncorrelated with one another). A technical discussion of advantages and disadvantages of various rotation approaches are discussed at the website of Columbia University.
In statistics, and especially in biostatistics, cophenetic correlation (more precisely, the cophenetic correlation coefficient) is a measure of how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points. Although it has been most widely applied in the field of biostatistics (typically to assess cluster-based models of DNA sequences, or other taxonomic models), it can also be used in other fields of inquiry where raw data tend to occur in clumps, or clusters. This coefficient has also been proposed for use as a test for nested clusters.
In finance, the beta (  or beta coefficient) of an investment indicates whether the investment is more or less volatile than the market. In general, a beta less than 1 indicates that the investment is less volatile than the market, while a beta more than 1 indicates that the investment is more volatile than the market. Volatility is measured as the fluctuation of the price around the mean: the standard deviation. Beta is a measure of the risk arising from exposure to general market movements as opposed to idiosyncratic factors. The market portfolio of all investable assets has a beta of exactly 1. A beta below 1 can indicate either an investment with lower volatility than the market, or a volatile investment whose price movements are not highly correlated with the market. An example of the first is a treasury bill: the price does not go up or down a lot, so it has a low beta. An example of the second is gold. The price of gold does go up and down a lot, but not in the same direction or at the same time as the market. A beta greater than one generally means that the asset both is volatile and tends to move up and down with the market. An example is a stock in a big technology company. Negative betas are possible for investments that tend to go down when the market goes up, and vice versa. There are few fundamental investments with consistent and significant negative betas, but some derivatives like put options can have large negative betas. Beta is important because it measures the risk of an investment that cannot be reduced by diversification. It does not measure the risk of an investment held on a stand-alone basis, but the amount of risk the investment adds to an already-diversified portfolio. In the capital asset pricing model, beta risk is the only kind of risk for which investors should receive an expected return higher than the risk-free rate of interest. The definition above covers only theoretical beta. The term is used in many related ways in finance. For example, the betas commonly quoted in mutual fund analyses generally measure the risk of the fund arising from exposure to a benchmark for the fund, rather than from exposure to the entire market portfolio. Thus they measure the amount of risk the fund adds to a diversified portfolio of funds of the same type, rather than to a portfolio diversified among all fund types. Beta decay refers to the tendency for a company with a high beta coefficient (  > 1) to have its beta coefficient decline to the market beta. It is an example of regression toward the mean.
In statistics, observable variables or manifest variables, as opposed to latent variables, are those variables that can be observed and directly measured.
DataDetective is a data mining platform developed by Sentient Information Systems. Since 1992, this software is being applied in organizations that have the need for retrieving patterns and relations in their typically large databases. DataDetective does this by offering a broad spectrum of data analysis techniques, such as associative memory prediction, clustering, decision trees, fuzzy matching, but also basic statistics, graphs and geographical visualization. Several Dutch police forces use DataDetective for crime analysis.
In mathematics, the Bernoulli scheme or Bernoulli shift is a generalization of the Bernoulli process to more than two possible outcomes. Bernoulli schemes are important in the study of dynamical systems, as most such systems (such as Axiom A systems) exhibit a repellor that is the product of the Cantor set and a smooth manifold, and the dynamics on the Cantor set are isomorphic to that of the Bernoulli shift. This is essentially the Markov partition. The term shift is in reference to the shift operator, which may be used to study Bernoulli schemes. The Ornstein isomorphism theorem shows that Bernoulli shifts are isomorphic when their entropy is equal.  
In probability theory and statistics, a sequence of independent Bernoulli trials with probability 1/2 of success on each trial is metaphorically called a fair coin. One for which the probability is not 1/2 is called a biased or unfair coin. In theoretical studies, the assumption that a coin is fair is often made by referring to an ideal coin. Some coins have been alleged to be unfair when spun on a table, but the results have not been substantiated or are not significant. There are statistical procedures for checking whether a coin is fair.
In probability theory, Etemadi's inequality is a so-called "maximal inequality", an inequality that gives a bound on the probability that the partial sums of a finite collection of independent random variables exceed some specified bound. The result is due to Nasrollah Etemadi.  
People in these lists may have either unspecified traits, or specific characteristics (e. g. the people of Brazil, Spain or the people of the Plains). Lists of people include
The Sethi model was developed by Suresh P. Sethi and describes the process of how sales evolve over time in response to advertising. The rate of change in sales depend on three effects: response to advertising that acts positively on the unsold portion of the market, the loss due to forgetting or possibly due to competitive factors that act negatively on the sold portion of the market, and a random effect that can go either way. Suresh Sethi published his paper "Deterministic and Stochastic Optimization of a Dynamic Advertising Model" in 1983. The Sethi model is a modification as well as a stochastic extension of the Vidale-Wolfe advertising model. The model and its competitive extensions have been used extensively in the literature. Moreover, some of these extensions have been also tested empirically.
In data analysis involving geographical locations, geo-imputation or geographical imputation methods are steps taken to replace missing values for exact locations with approximate locations derived from associated data. They assign a reasonable location or geographic based attribute (e.g., census tract) to a person by using both the demographic characteristics of the person and the population characteristics from a larger geographic aggregate area in which the person was geocoded (e.g., postal delivery area or county). For example if a person's census tract was known and no other address information was available then geo-imputation methods could be used to probabilistically assign that person to a smaller geographic area, such as a census block group.
Confirmation bias, also called confirmatory bias or myside bias, is the tendency to search for, interpret, favor, and recall information in a way that confirms one's preexisting beliefs or hypotheses, while giving disproportionately less consideration to alternative possibilities. It is a type of cognitive bias and a systematic error of inductive reasoning. People display this bias when they gather or remember information selectively, or when they interpret it in a biased way. The effect is stronger for emotionally charged issues and for deeply entrenched beliefs. People also tend to interpret ambiguous evidence as supporting their existing position. Biased search, interpretation and memory have been invoked to explain attitude polarization (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence), belief perseverance (when beliefs persist after the evidence for them is shown to be false), the irrational primacy effect (a greater reliance on information encountered early in a series) and illusory correlation (when people falsely perceive an association between two events or situations). A series of experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. In certain situations, this tendency can bias people's conclusions. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another explanation is that people show confirmation bias because they are weighing up the costs of being wrong, rather than investigating in a neutral, scientific way. Confirmation biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. Poor decisions due to these biases have been found in political and organizational contexts.
The scaled inverse chi-squared distribution is the distribution for x = 1/s2, where s2 is a sample mean of the squares of   independent normal random variables that have mean 0 and inverse variance 1/ 2 =  2. The distribution is therefore parametrised by the two quantities   and  2, referred to as the number of chi-squared degrees of freedom and the scaling parameter, respectively. This family of scaled inverse chi-squared distributions is closely related to two other distribution families, those of the inverse-chi-squared distribution and the inverse gamma distribution. Compared to the inverse-chi-squared distribution, the scaled distribution has an extra parameter  2, which scales the distribution horizontally and vertically, representing the inverse-variance of the original underlying process. Also, the scale inverse chi-squared distribution is presented as the distribution for the inverse of the mean of   squared deviates, rather than the inverse of their sum. The two distributions thus have the relation that if    then    Compared to the inverse gamma distribution, the scaled inverse chi-squared distribution describes the same data distribution, but using a different parametrization, which may be more convenient in some circumstances. Specifically, if    then    Either form may be used to represent the maximum entropy distribution for a fixed first inverse moment  and first logarithmic moment . The scaled inverse chi-squared distribution also has a particular use in Bayesian statistics, somewhat unrelated to its use as a predictive distribution for x = 1/s2. Specifically, the scaled inverse chi-squared distribution can be used as a conjugate prior for the variance parameter of a normal distribution. In this context the scaling parameter is denoted by  02 rather than by  2, and has a different interpretation. The application has been more usually presented using the inverse gamma distribution formulation instead; however, some authors, following in particular Gelman et al. (1995/2004) argue that the inverse chi-squared parametrisation is more intuitive.
In probability theory, Bennett's inequality provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. Bennett's inequality was proved by George Bennett of the University of New South Wales in 1962. Let X1, ... Xn be independent random variables, and assume (for simplicity but without loss of generality) they all have zero expected value. Further assume |Xi|   a almost surely for all i, and let  Then for any t   0,  where h(u) = (1 + u)log(1 + u)   u. See also Freedman (1975)  and Fan et al. (2012)  for a martingale version of Bennett's inequality and its improvement, respectively.
In statistics and econometrics, a distributed lag model is a model for time series data in which a regression equation is used to predict current values of a dependent variable based on both the current values of an explanatory variable and the lagged (past period) values of this explanatory variable. The starting point for a distributed lag model is an assumed structure of the form  or the form  where yt is the value at time period t of the dependent variable y, a is the intercept term to be estimated, and wi is called the lag weight (also to be estimated) placed on the value i periods previously of the explanatory variable x. In the first equation, the dependent variable is assumed to be affected by values of the independent variable arbitrarily far in the past, so the number of lag weights is infinite and the model is called an infinite distributed lag model. In the alternative, second, equation, there are only a finite number of lag weights, indicating an assumption that there is a maximum lag beyond which values of the independent variable do not affect the dependent variable; a model based on this assumption is called a finite distributed lag model. In an infinite distributed lag model, an infinite number of lag weights need to be estimated; clearly this can be done only if some structure is assumed for the relation between the various lag weights, with the entire infinitude of them expressible in terms of a finite number of assumed underlying parameters. In a finite distributed lag model, the parameters could be directly estimated by ordinary least squares (assuming the number of data points sufficiently exceeds the number of lag weights); nevertheless, such estimation may give very imprecise results due to extreme multicollinearity among the various lagged values of the independent variable, so again it may be necessary to assume some structure for the relation between the various lag weights. The concept of distributed lag models easily generalizes to the context of more than one right-side explanatory variable.  
In statistics, the predicted residual error sum of squares (PRESS) statistic is a form of cross-validation used in regression analysis to provide a summary measure of the fit of a model to a sample of observations that were not themselves used to estimate the model. It is calculated as the sums of squares of the prediction residuals for those observations. A fitted model having been produced, each observation in turn is removed and the model is refitted using the remaining observations. The out-of-sample predicted value is calculated for the omitted observation in each case, and the PRESS statistic is calculated as the sum of the squares of all the resulting prediction errors:  Given this procedure, the PRESS statistic can be calculated for a number of candidate model structures for the same dataset, with the lowest values of PRESS indicating the best structures. Models that are over-parameterised (over-fitted) would tend to give small residuals for observations included in the model-fitting but large residuals for observations that are excluded.
The unseen species problem is a statistical statement of a problem that arises in ecology and other fields. Given a sample from a population, where each individual in the sample is classified by kind or species, the problem is to answer these related questions: How many species are there in the population, including "unseen species" that do not appear in the sample  For each species, seen or unseen, what is the prevalence in the population; that is, what fraction of the population belongs to each species  If additional samples are collected from the same species, how many more species do we expect to discover  The answer to this question might be expressed in the form of a species discovery curve. How many additional samples are needed to achieve a desired level of coverage (fraction of species observed)  Solutions to these problems are usually based on the assumption that each member of the population is equally likely to appear in the sample.
Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as classification function: Sensitivity (also called the true positive rate, or the recall in some fields) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition). Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition). Thus sensitivity quantifies the avoiding of false negatives, as specificity does for false positives. For any test, there is usually a trade-off between the measures. For instance, in an airport security setting in which one is testing for potential threats to safety, scanners may be set to trigger on low-risk items like belt buckles and keys (low specificity), in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard (high sensitivity). This trade-off can be represented graphically as a receiver operating characteristic curve. A perfect predictor would be described as 100% sensitive (e.g., all sick are identified as sick) and 100% specific (e.g., no healthy are identified as sick); however, theoretically any predictor will possess a minimum error bound known as the Bayes error rate.
In statistics, the likelihood principle is that, given a statistical model, all of the evidence in a sample relevant to model parameters is contained in the likelihood function. A likelihood function arises from a conditional probability distribution considered as a function of its distributional parameterization argument, conditioned on the data argument. For example, consider a model which gives the probability density function of observable random variable X as a function of a parameter  . Then for a specific value x of X, the function L(  | x) = P(X=x |  ) is a likelihood function of  : it gives a measure of how "likely" any particular value of   is, if we know that X has the value x. Two likelihood functions are equivalent if one is a scalar multiple of the other. The likelihood principle states that all information from the data relevant to inferences about the value of   is found in the equivalence class. The strong likelihood principle applies this same criterion to cases such as sequential experiments where the sample of data that is available results from applying a stopping rule to the observations earlier in the experiment.
Seasonal subseries plots are a graphical tool to visualize and detect seasonality in a time series. Seasonal subseries plots involves the extraction of the seasons from a time series into a subseries based on a selected periodicity and are placed into mini time plots. Seasonal subseries plots allows one to detect changes between different seasons , changes within a particular season over time as well as any underlying seasonal patterns. This plot is only useful if the period of the seasonality is already known. In many cases, this will in fact be known. For example, monthly data typically has a period of 12. If the period is not known, an autocorrelation plot or spectral plot can be used to determine it. If there is a large number of observations, then a box plot may be preferable.
In statistics, Halton sequences are sequences used to generate points in space for numerical methods such as Monte Carlo simulations. Although these sequences are deterministic they are of low discrepancy, that is, appear to be random for many purposes. They were first introduced in 1960 and are an example of a quasi-random number sequence. They generalise the one-dimensional van der Corput sequences, consult that article for a precise definition.
In probability theory and statistics, the skew normal distribution is a continuous probability distribution that generalises the normal distribution to allow for non-zero skewness.
In econometrics, the Frisch Waugh Lovell (FWL) theorem is named after the econometricians Ragnar Frisch, Frederick V. Waugh, and Michael C. Lovell. The Frisch Waugh Lovell theorem states that if the regression we are concerned with is:  where  and  are  and  matrices respectively and where  and  are conformable, then the estimate of  will be the same as the estimate of it from a modified regression of the form:  where  projects onto the orthogonal complement of the image of the projection matrix . Equivalently, MX1 projects onto the orthogonal complement of the column space of X1. Specifically,  known as the annihilator matrix. This result implies that all these secondary regressions are unnecessary: using projection matrices to make the explanatory variables orthogonal to each other will lead to the same results as running the regression with all non-orthogonal explanators included.  
A game of chance is a game whose outcome is strongly influenced by some randomizing device, and upon which contestants may choose to wager money or anything of monetary value. Common devices used include dice, spinning tops, playing cards, roulette wheels, or numbered balls drawn from a container. A game of chance may have some skill element to it, however, chance generally plays a greater role in determining the outcome than skill. A game of skill, on the other hand, also has an element of chance, but with skill playing a greater role in determining the outcome. Any game of chance that involves anything of monetary value is gambling. Gambling is known in nearly all human societies, even though many have passed laws restricting it. Early people used the knucklebones of sheep as dice. Some people develop a psychological addiction to gambling, and will risk even food and shelter to continue. Some games of chance may also involve a certain degree of skill. This is especially true where the player or players have decisions to make based upon previous or incomplete knowledge, such as blackjack. In other games like roulette and punto banco (baccarat) the player may only choose the amount of bet and the thing he/she wants to bet on, the rest is up to chance, therefore these games are still considered games of chance with small amount of skills required. The distinction between 'chance' and 'skill' is relevant as in some countries chance games are illegal or at least regulated, where skill games are not.
A cancer cluster is a disease cluster in which a high number of cancer cases occurs in a group of people in a particular geographic area over a limited period of time. Historical examples of work-related cancer clusters are well documented in the medical literature. Notable examples include: scrotal cancer among chimney sweeps in 18th century London; osteosarcoma among female watch dial painters in the 20th century; skin cancer in farmers; bladder cancer in dye workers exposed to aniline compounds; and leukemia and lymphoma in chemical workers exposed to benzene. Cancer cluster suspicions usually arise when members of the general public report that their family members, friends, neighbors, or coworkers have been diagnosed with the same or related cancers. State or local health departments will investigate the possibility of a cancer cluster when a claim is filed. In order to justify investigating such claims, health departments conduct a preliminary review. Data will be collected and verified regarding: the types of cancer reported, numbers of cases, geographic area of the cases, and the patients clinical history. At this point, a committee of medical professionals will examine the data and determine whether or not an investigation (often lengthy and expensive) is justified. In the U.S., state and local health departments respond to more than 1,000 inquiries about suspected cancer clusters each year. It is possible that a suspected cancer cluster may be due to chance alone; however, only clusters that have a disease rate that is statistically significantly greater than the disease rate of the general population are investigated. Given the number of inquiries it is likely that many of these are due to chance alone. It is a well-known problem in interpreting data known as pareidolia that random cases of cancer can appear to form clumps that are misinterpreted as a cluster. A cluster is less likely to be coincidental if the case consists of one type of cancer, a rare type of cancer, or a type of cancer that is not usually found in a certain age group. Between 5% to 15% of suspected cancer clusters are statistically significant.
In non-parametric statistics, there is a method for robustly fitting a line to a set of points (simple linear regression) that chooses the median slope among all lines through pairs of two-dimensional sample points. It has been called the Theil Sen estimator, Sen's slope estimator, slope selection, the single median method, the Kendall robust line-fit method, and the Kendall Theil robust line. It is named after Henri Theil and Pranab K. Sen, who published papers on this method in 1950 and 1968 respectively, and after Maurice Kendall. This estimator can be computed efficiently, and is insensitive to outliers. It can be significantly more accurate than non-robust simple linear regression for skewed and heteroskedastic data, and competes well against non-robust least squares even for normally distributed data in terms of statistical power. It has been called "the most popular nonparametric technique for estimating a linear trend".
Laboratory quality control is designed to detect, reduce, and correct deficiencies in a laboratory's internal analytical process prior to the release of patient results, in order to improve the quality of the results reported by the laboratory. Quality control is a measure of precision, or how well the measurement system reproduces the same result over time and under varying operating conditions. Laboratory quality control material is usually run at the beginning of each shift, after an instrument is serviced, when reagent lots are changed, after calibration, and whenever patient results seem inappropriate. Quality control material should approximate the same matrix as patient specimens, taking into account properties such as viscosity, turbidity, composition, and color. It should be simple to use, with minimal vial to vial variability, because variability could be misinterpreted as systematic error in the method or instrument. It should be stable for long periods of time, and available in large enough quantities for a single batch to last at least one year. Liquid controls are more convenient than lyophilized controls because they do not have to be reconstituted minimizing pipetting error.
In statistics, a spurious relationship (not to be confused with spurious correlation) is a mathematical relationship in which two events or variables have no direct causal connection, yet it may be wrongly inferred that they do, due to either coincidence or the presence of a certain third, unseen factor (referred to as a "common response variable", "confounding factor", or "lurking variable"). Suppose there is found to be a correlation between A and B. Aside from coincidence, there are three possible relationships: Where A is present, B is observed. (A causes B.) Where B is present, A is observed. (B causes A.) OR Where C is present, both A and B are observed. (C causes both A and B.) In the last case there is a spurious relationship between A and B. In a regression model where A is regressed on B but C is actually the true causal factor for A, this misleading choice of independent variable (B instead of C) is called specification error. Because correlation can arise from the presence of a lurking variable rather than from direct causation, it is often said that "Correlation does not imply causation". A spurious relationship should not be confused with a spurious regression, which refers to a regression that shows significant results due to the presence of a unit root in both variables.
The Theil index is a statistic primarily used to measure economic inequality and other economic phenomena, though it has also been used to measure racial segregation. The basic Theil index TT is the same as redundancy in information theory which is the maximum possible entropy of the data minus the observed entropy. It is a special case of the generalized entropy index. It can be viewed as a measure of redundancy, lack of diversity, isolation, segregation, inequality, non-randomness, and compressibility. It was proposed by econometrician Henri Theil at the Erasmus University Rotterdam.
In statistics, the Rao Blackwell theorem, sometimes referred to as the Rao Blackwell Kolmogorov theorem, is a result which characterizes the transformation of an arbitrarily crude estimator into an estimator that is optimal by the mean-squared-error criterion or any of a variety of similar criteria. The Rao Blackwell theorem states that if g(X) is any kind of estimator of a parameter  , then the conditional expectation of g(X) given T(X), where T is a sufficient statistic, is typically a better estimator of  , and is never worse. Sometimes one can very easily construct a very crude estimator g(X), and then evaluate that conditional expected value to get an estimator that is in various senses optimal. The theorem is named after Calyampudi Radhakrishna Rao and David Blackwell. The process of transforming an estimator using the Rao Blackwell theorem is sometimes called Rao Blackwellization. The transformed estimator is called the Rao Blackwell estimator.
A well-behaved statistic is a term sometimes used in the theory of statistics to describe part of a procedure. This usage is broadly similar to the use of well-behaved in more general mathematics. It is essentially an assumption about the formulation of an estimation procedure (which entails the specification of an estimator or statistic) that is used to avoid giving extensive details about what conditions need to hold. In particular it means that the statistic is not an unusual one in the context being studied. Due to this, the meaning attributed to well-behaved statistic may vary from context to context. The present article is mainly concerned with the context of data mining procedures applied to statistical inference and, in particular, to the group of computationally intensive procedure that have been called algorithmic inference.
Stein's lemma, named in honor of Charles Stein, is a theorem of probability theory that is of interest primarily because of its applications to statistical inference   in particular, to James Stein estimation and empirical Bayes methods   and its applications to portfolio choice theory. The theorem gives a formula for the covariance of one random variable with the value of a function of another, when the two random variables are jointly normally distributed.
In probability theory and statistics, the half-logistic distribution is a continuous probability distribution the distribution of the absolute value of a random variable following the logistic distribution. That is, for  where Y is a logistic random variable, X is a half-logistic random variable.
In statistics, Wombling is any of a number of techniques used for identifying zones of rapid change, typically in some quantity as it varies across some geographical or Euclidean space. It is named for statistician William H. Womble. The technique may be applied to gene frequency in a population of organisms, and to evolution of language.
In mathematics, a Bregman divergence or Bregman distance is similar to a metric, but does not satisfy the triangle inequality nor symmetry. There are three ways in which Bregman divergences are important. Firstly, they generalize squared Euclidean distance to a class of distances that all share similar properties. Secondly, they bear a strong connection to exponential families of distributions; as has been shown by (Banerjee et al. 2005), there is a bijection between regular exponential families and regular Bregman divergences. Finally, Bregman divergences appear in a natural way as regret functions in problems involving optimization over convex sets (Harremoe s 2015). Bregman divergences are named after Lev M. Bregman, who introduced the concept in 1967. More recently researchers in geometric algorithms have shown that many important algorithms can be generalized from Euclidean metrics to distances defined by Bregman divergence (Banerjee et al. 2005; Nielsen and Nock 2006; Boissonnat et al. 2010).
In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. Instrumental variable methods allow consistent estimation when the explanatory variables (covariates) are correlated with the error terms of a regression relationship. Such correlation may occur when the dependent variable causes at least one of the covariates ("reverse" causation), when there are relevant explanatory variables which are omitted from the model, or when the covariates are subject to measurement error. In this situation, ordinary linear regression generally produces biased and inconsistent estimates. However, if an instrument is available, consistent estimates may still be obtained. An instrument is a variable that does not itself belong in the explanatory equation and is correlated with the endogenous explanatory variables, conditional on the other covariates. In linear models, there are two main requirements for using an IV: The instrument must be correlated with the endogenous explanatory variables, conditional on the other covariates. The instrument cannot be correlated with the error term in the explanatory equation (conditional on the other covariates), that is, the instrument cannot suffer from the same problem as the original predicting variable.
The Nash Sutcliffe model efficiency coefficient is used to assess the predictive power of hydrological models. It is defined as:  where Qo is the mean of observed discharges, and Qm is modeled discharge. Qot is observed discharge at time t. Nash Sutcliffe efficiency can range from    to 1. An efficiency of 1 (E = 1) corresponds to a perfect match of modeled discharge to the observed data. An efficiency of 0 (E = 0) indicates that the model predictions are as accurate as the mean of the observed data, whereas an efficiency less than zero (E < 0) occurs when the observed mean is a better predictor than the model or, in other words, when the residual variance (described by the numerator in the expression above), is larger than the data variance (described by the denominator). Essentially, the closer the model efficiency is to 1, the more accurate the model is. The efficiency coefficient is sensitive to extreme values and might yield sub-optimal results when the dataset contains large outliers in it. Nash Sutcliffe efficiency can be used to quantitatively describe the accuracy of model outputs other than discharge. This method can be used to describe the predictive accuracy of other models as long as there is observed data to compare the model results to. For example, Nash Sutcliffe efficiency has been reported in scientific literature for model simulations of discharge, and water quality constituents such as sediment, nitrogen, and phosphorus loading.
In probability theory, de Finetti's theorem states that exchangeable observations are conditionally independent given some latent variable to which an epistemic probability distribution would then be assigned. It is named in honor of Bruno de Finetti. For the special case of an exchangeable sequence of Bernoulli random variables it states that such a sequence is a "mixture" of sequences of independent and identically distributed (i.i.d.) Bernoulli random variables. While the individual variables of the exchangeable sequence are not themselves i.i.d., only exchangeable, there is an underlying family of i.i.d. random variables. Thus, while observations need not be i.i.d. for a sequence to be exchangeable, there are underlying, generally unobservable, quantities which are i.i.d.   exchangeable sequences are (not necessarily i.i.d.) mixtures of i.i.d. sequences.
In statistics, the generalized linear array model (GLAM) is used for analyzing data sets with array structures. It based on the generalized linear model with the design matrix written as a Kronecker product.
In probability theory and statistics, the mathematical concepts of covariance and correlation are very similar. Both describe the degree to which two random variables or sets of random variables tend to deviate from their expected values in similar ways. If X and Y are two random variables, with means  X and  Y, and standard deviations  X and  Y, respectively, then their covariance and correlation are as follows:  So that  where E is the expected value operator. Notably, correlation is dimensionless while covariance is in units obtained by multiplying the units of the two variables. The covariance of a variable with itself (i.e. ) is called the variance and is more commonly denoted as  the square of the standard deviation. The correlation of a variable with itself is always 1 (except in the degenerate case where the two variances are zero, in which case the correlation does not exist).
In statistics and demography, a cohort is a group of subjects who have shared a particular event together during a particular time span (e.g., people born in Europe between 1918 and 1939; survivors of an aircrash; truck drivers who smoked between age 30 and 40). Cohorts may be tracked over extended periods in a cohort study. The cohort can be modified by censoring, i.e. excluding certain individuals from statistical calculations relating to time periods (e.g., after death) when their data would contaminate the conclusions. The term cohort can also be used where membership of a group is defined by some factor other than a time-based one: for example, where a study covers workers in many buildings, a cohort might consist of the people who work in a given building. Demography often contrasts cohort perspectives and period perspectives. For instance, the total cohort fertility rate is an index of the average completed family size for cohorts of women, but since it can only be known for women who have finished child-bearing, it cannot be measured for currently fertile women. It can be calculated as the sum of the cohort's age-specific fertility rates that obtain as it ages through time. In contrast, the total period fertility rate uses current age-specific fertility rates to calculate the completed family size for a notional woman were she to experience these fertility rates through her life.
Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers. Therefore, it also can be interpreted as an outlier detection method. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981.They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations. A basic assumption is that the data consists of "inliers", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and "outliers" which are data that do not fit the model. The outliers can come, e.g., from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.
In psychometrics, the Kuder Richardson Formula 20 (KR-20) first published in 1937 is a measure of internal consistency reliability for measures with dichotomous choices. It is analogous to Cronbach's  , except Cronbach's   is also used for non-dichotomous (continuous) measures. It is often claimed that a high KR-20 coefficient (e.g., > 0.90) indicates a homogeneous test. However, like Cronbach's  , homogeneity (that is, unidimensionality) is actually an assumption, not a conclusion, of reliability coefficients. It is possible, for example, to have a high KR-20 with a multidimensional scale, especially with a large number of items. Values can range from 0.00 to 1.00 (sometimes expressed as 0 to 100), with high values indicating that the examination is likely to correlate with alternate forms (a desirable characteristic). The KR-20 may be affected by difficulty of the test, the spread in scores and the length of the examination. In the case when scores are not tau-equivalent (for example when there is not homogeneous but rather examination items of increasing difficulty) then the KR-20 is an indication of the lower bound of internal consistency (reliability). The formula for KR-20 for a test with K test items numbered i=1 to K is  where pi is the proportion of correct responses to test item i, qi is the proportion of incorrect responses to test item i (so that pi + qi = 1), and the variance for the denominator is  where n is the total sample size. If it is important to use unbiased operators then the sum of squares should be divided by degrees of freedom (n   1) and the probabilities are multiplied by  Since Cronbach's   was published in 1951, there has been no known advantage to KR-20 over Cronbach. KR-20 is seen as a derivative of the Cronbach formula, with the advantage to Cronbach that it can handle both dichotomous and continuous variables. The KR-20 formula can't be used when multiple-choice questions involve partial credit, and it requires detailed item analysis.
The control variates method is a variance reduction technique used in Monte Carlo methods. It exploits information about the errors in estimates of known quantities to reduce the error of an estimate of an unknown quantity.
In statistics, an exact (significance) test is a test where all assumptions, upon which the derivation of the distribution of the test statistic is based, are met as opposed to an approximate test (in which the approximation may be made as close as desired by making the sample size big enough). This will result in a significance test that will have a false rejection rate always equal to the significance level of the test. For example an exact test at significance level 5% will in the long run reject true null hypotheses exactly 5% of the time. Parametric tests, such as those described in exact statistics, are exact tests when the parametric assumptions are fully met, but in practice the use of the term exact (significance) test is reserved for those tests that do not rest on parametric assumptions   non-parametric tests. However, in practice most implementations of non-parametric test software use asymptotical algorithms for obtaining the significance value, which makes the implementation of the test non-exact. So when the result of a statistical analysis is said to be an  exact test  or an  exact p-value , it ought to imply that the test is defined without parametric assumptions and evaluated without using approximate algorithms. In principle however it could also mean that a parametric test has been employed in a situation where all parametric assumptions are fully met, but it is in most cases impossible to prove this completely in a real world situation. Exceptions when it is certain that parametric tests are exact include tests based on the binomial or Poisson distributions. Sometimes permutation test is used as a synonym for exact test, but although all permutation tests are exact tests, not all exact tests are permutation tests.
In statistics, the maximal information coefficient (MIC) is a measure of the strength of the linear or non-linear association between two variables X and Y. The MIC belongs to the maximal information-based nonparametric exploration (MINE) class of statistics. In a simulation study, MIC outperformed some selected low power tests, however concerns have been raised regarding reduced statistical power in detecting some associations in settings with low sample size when compared to powerful methods such as distance correlation and HHG. Comparisons with these methods, in which MIC was outperformed, were made in  and. It is claimed that MIC approximately satisfies a property called equitability which is illustrated by selected simulation studies. It was later proved that no non-trivial coefficient can exactly satisfy the equitability property as defined by Reshef et al., although this result has been challenged. Some criticisms of MIC are addressed by Reshef et al. in further studies published on arXiv.
In probability theory, the Chinese restaurant process is a discrete-time stochastic process, analogous to seating customers at tables in a Chinese restaurant. Imagine a Chinese restaurant with an infinite number of circular tables, each with infinite capacity. Customer 1 is seated at an unoccupied table with probability 1. At time n + 1, a new customer chooses uniformly at random to sit at one of the following n + 1 places: directly to the left of one of the n customers already sitting at an occupied table, or at a new, unoccupied table. At time n, the value of the process is a partition of the set of n customers, where the tables are the blocks of the partition. Mathematicians are interested in the probability distribution of this random partition. David J. Aldous attributes the restaurant analogy to Jim Pitman and Lester Dubins in his 1983 book.
In finance, the Vasicek model is a mathematical model describing the evolution of interest rates. It is a type of one-factor short rate model as it describes interest rate movements as driven by only one source of market risk. The model can be used in the valuation of interest rate derivatives, and has also been adapted for credit markets, although it has the disadvantage of allowing negative interest rates. It was introduced in 1977 by Oldr ich Vas i c ek and can be also seen as a stochastic investment model.
In epidemiology and demography, age adjustment, also called age standardization, is a technique used to allow populations to be compared when the age profiles of the populations are quite different.
In statistics, path analysis is used to describe the directed dependencies among a set of variables. This includes models equivalent to any form of multiple regression analysis, factor analysis, canonical correlation analysis, discriminant analysis, as well as more general families of models in the multivariate analysis of variance and covariance analyses (MANOVA, ANOVA, ANCOVA). In addition to being thought of as a form of multiple regression focusing on causality, path analysis can be viewed as a special case of structural equation modeling (SEM)   one in which only single indicators are employed for each of the variables in the causal model. That is, path analysis is SEM with a structural model, but no measurement model. Other terms used to refer to path analysis include causal modeling, analysis of covariance structures, and latent variable models.
The low birth-weight paradox is an apparently paradoxical observation relating to the birth weights and mortality rate of children born to tobacco smoking mothers. Low birth-weight children born to smoking mothers have a lower infant mortality rate than the low birth weight children of non-smokers. It is an example of Simpson's paradox.
Forecast Skill (or skill score, forecast skill, prediction skill) is a generic term referring to the accuracy and/or degree of association of prediction to an observation or estimate of the actual value of the predictand (ie, what is being predicted). The term 'forecast skill' can be used both quantitatively and qualitatively. In the former case, skill could be equal to a statistic describing forecast performance, such as the correlation of the forecast with observations. In the latter case, it could either refer to forecast performance according to a single metric or to the overall forecast performance based on multiple metrics. Forecast skill for single-value forecasts is commonly represented in terms of metrics such as correlation, root mean squared error, mean absolute error, relative mean absolute error, bias, and the Brier Score, among others. Probabilistic forecast skill scores may use metrics such as the Ranked Probabilistic Skill Score (RPSS) or the Continuous RPSS (CRPSS), among others. Categorical skill metrics such as the False Alarm Ratio (FAR), the Probability of Detection (POD), the Critical Success Index (CSI), and Equitable Threat Score (ETC) are also relevant for some forecasting applications. Skill is often, but not exclusively, expressed as the relative representation that compares the forecast performance of a particular forecast prediction to that of a reference, benchmark prediction a formulation called a 'Skill Score'. Forecast skill metric and score calculations should be made over a large enough sample of forecast-observation pairs to be statistically robust. A sample of predictions for a single predictand (eg, temperature at one location, or a single stock value) typically includes forecasts made on a number of different dates. A sample could also pool forecast-observation pairs across space, for a prediction made on a single date, as in the forecast of a weather event that is verified at many locations. An example of a skill calculation which uses the error metric 'Mean Squared Error (MSE)' and the associated skill score is given in the table below. In this case, a perfect forecast results in a forecast skill metric of zero, and skill score value of 1.0. A forecast with equal skill to the reference forecast would have a skill score of 0.0, and a forecast which is less skillful than the reference forecast would have unbounded negative skill score values. A broad range of forecast metrics can be found in published and online resources (such as the Australian Bureau of Meteorology's longstanding web pages on verification, http://www.cawcr.gov.au/projects/verification/) A popular textbook and reference that discusses forecast skill is Statistical Methods in the Atmospheric Sciences.
In estimation theory, the extended Kalman filter (EKF) is the nonlinear version of the Kalman filter which linearizes about an estimate of the current mean and covariance. In the case of well defined transition models, the EKF has been considered the de facto standard in the theory of nonlinear state estimation, navigation systems and GPS.
The Wigner semicircle distribution, named after the physicist Eugene Wigner, is the probability distribution supported on the interval [ R, R] the graph of whose probability density function f is a semicircle of radius R centered at (0, 0) and then suitably normalized (so that it is really a semi-ellipse):  for  R   x   R, and f(x) = 0 if R < |x|. This distribution arises as the limiting distribution of eigenvalues of many random symmetric matrices as the size of the matrix approaches infinity. It is a scaled beta distribution, more precisely, if Y is beta distributed with parameters   =   = 3/2, then X = 2RY   R has the above Wigner semicircle distribution.
Banburismus was a cryptanalytic process developed by Alan Turing at Bletchley Park in England during the Second World War. It was used by Bletchley Park's Hut 8 to help break German Kriegsmarine (naval) messages enciphered on Enigma machines. The process used sequential conditional probability to infer information about the likely settings of the Enigma machine. It gave rise to Turing's invention of the ban as a measure of the weight of evidence in favour of a hypothesis. This concept was later applied in Turingery and all the other methods used for breaking the Lorenz cipher. The aim of Banburismus was to reduce the time required of the electromechanical Bombe machines by identifying the most likely right-hand and middle wheels of the Enigma. Hut 8 performed the procedure continuously for two years, stopping only in 1943 when sufficient bombe time became readily available. Banburismus was a development of the "clock method" invented by the Polish cryptanalyst Jerzy Ro z ycki. Hugh Alexander was regarded as the best of the Banburists. He and I. J. Good considered the process more an intellectual game than a job. It was "not easy enough to be trivial, but not difficult enough to cause a nervous breakdown".
In probability theory, the central limit theorem states conditions under which the average of a sufficiently large number of independent random variables, each with finite mean and variance, will be approximately normally distributed. Directional statistics is the subdiscipline of statistics that deals with directions (unit vectors in Rn), axes (lines through the origin in Rn) or rotations in Rn. The means and variances of directional quantities are all finite, so that the central limit theorem may be applied to the particular case of directional statistics. This article will deal only with unit vectors in 2-dimensional space (R2) but the method described can be extended to the general case.
In mathematics, unimodality means possessing a unique mode. More generally, unimodality means there is only a single highest value, somehow defined, of some mathematical object.
An opinion poll, sometimes simply referred to as a poll, is a survey of public opinion from a particular sample. Opinion polls are usually designed to represent the opinions of a population by conducting a series of questions and then extrapolating generalities in ratio or within confidence intervals.
Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved. Compared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.
In probability theory, the inverse Gaussian distribution (also known as the Wald distribution) is a two-parameter family of continuous probability distributions with support on (0, ). Its probability density function is given by  for x > 0, where  is the mean and  is the shape parameter. As   tends to infinity, the inverse Gaussian distribution becomes more like a normal (Gaussian) distribution. The inverse Gaussian distribution has several properties analogous to a Gaussian distribution. The name can be misleading: it is an "inverse" only in that, while the Gaussian describes a Brownian Motion's level at a fixed time, the inverse Gaussian describes the distribution of the time a Brownian Motion with positive drift takes to reach a fixed positive level. Its cumulant generating function (logarithm of the characteristic function) is the inverse of the cumulant generating function of a Gaussian random variable. To indicate that a random variable X is inverse Gaussian-distributed with mean   and shape parameter   we write
Falconer's formula is used in twin studies to determine the genetic heritability of a trait based on the difference between twin correlations. The formula is Hb2 = 2(rmz - rdz), where Hb2 is the broad sense heritability, rmz is the (monozygotic, MZ) identical twin correlation, and rdz is the (dizygotic, DZ) fraternal twin correlation. The correlation of same sex MZ twins is always higher than the DZ twin correlation with various sexes and thus all gender differences are evaluated as heritable. To avoid this error, only genetic studies comparing MZ twins with the same sex DZ twins are valid. Or correlation between A = hb2 (additive genetics) and C (common environment) must be included in derivation rmz = A + C + 2 Corr(A,C) rdz = 1 2A + C + 2 Corr(1 2A,C) and can not be neglected (also as covariance in the statistical model). It is also possible to do narrow-sense heritability estimates by using sibling/adoptee correlations: hn2 = rsib - rad, where hn2 is the narrow sense heritability, rsib is the sibling correlation, and rad is the adoptee correlation.  
In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.
In probability theory, a Markov kernel (or stochastic kernel) is a map that plays the role, in the general theory of Markov processes, that the transition matrix does in the theory of Markov processes with a finite state space.
In statistics, the Siegel Tukey test, named after Sidney Siegel and John Tukey, is a non-parametric test which may be applied to data measured at least on an ordinal scale. It tests for differences in scale between two groups. The test is used to determine if one of two groups of data tends to have more widely dispersed values than the other. In other words, the test determines whether one of the two groups tends to move, sometimes to the right, sometimes to the left, but away from the center (of the ordinal scale). The test was published in 1960 by Sidney Siegel and John Wilder Tukey in the Journal of the American Statistical Association, in the article "A Nonparametric Sum of Ranks Procedure for Relative Spread in Unpaired Samples."
Sampling is the use of a subset of the population to represent the whole population or to inform about (social) processes that are meaningful beyond the particular cases, individuals or sites studied. Probability sampling, or random sampling, is a sampling technique in which the probability of getting any particular sample may be calculated. Nonprobability sampling does not meet this criterion and, as any methodological decision, should adjust to the research question that one envisages to answer. Nonprobability sampling techniques cannot be used to infer from the sample to the general population in statistical terms and thus answer "how many"-related research questions. Thus, one cannot say the same on the basis of a nonprobability sample than on the basis of a probability sample. The grounds for drawing generalizations (e.g., propose new theory, propose policy) from studies based on nonprobability samples are based on the notion of "theoretical saturation" and "analytical generalization" (Yin, 2014) instead of on statistical generalization. Researchers working with the notion of purposive sampling assert that while probability methods are suitable for large-scale studies concerned with representativeness, non-probability approaches are more suitable for in-depth qualitative research in which the focus is often to understand complex social phenomena (e.g., Marshall 1996; Small 2009). One of the advantages of nonprobability sampling is its lower cost compared to probability sampling. Moreover, the in-depth analysis of a small-N purposive sample or a case study enables the "discovery" and identification of patterns and causal mechanisms that do not draw time and context-free assumptions. From the point of view of the quantitative and statistical way of doing research, though, these assertions raise some questions  how can one understand a complex social phenomenon by drawing only the most convenient expressions of that phenomenon into consideration  What assumption about homogeneity in the world must one make to justify such assertions  Alas, the consideration that research can only be based in statistical inference focuses on the problems of bias linked to nonprobability sampling and acknowledges only one situation in which a non-probability sample can be appropriate  if one is interested only in the specific cases studied (for example, if one is interested in the Battle of Gettysburg), one does not need to draw a probability sample from similar cases (Lucas 2014a). Still, some use nonprobability sampling. Examples of nonprobability sampling include: Convenience, haphazard or accidental sampling - members of the population are chosen based on their relative ease of access. To sample friends, co-workers, or shoppers at a single mall, are all examples of convenience sampling. Such samples are biased because researchers may unconsciously approach some kinds of respondents and avoid others (Lucas 2014a), and respondents who volunteer for a study may differ in unknown but important ways from others (Wiederman 1999). Snowball sampling - The first respondent refers an acquaintance. The friend also refers a friend, and so on. Such samples are biased because they give people with more social connections an unknown but higher chance of selection (Berg 2006), but lead to higher response rates. Judgmental sampling or purposive sampling - The researcher chooses the sample based on who they think would be appropriate for the study. This is used primarily when there is a limited number of people that have expertise in the area being researched, or when the interest of the research is on a specific field or a small group. Different types of purposive sampling include: Deviant case - The researcher obtains cases that substantially differ from the dominant pattern (a special type of purposive sample). The case is selected in order to obtain information on unusual cases that can be specially problematic or specially good. Case study - The research is limited to one group, often with a similar characteristic or of small size. Ad hoc quotas - A quota is established (e.g. 65% women) and researchers are free to choose any respondent they wish as long as the quota is met. Nonprobability sampling should not intend to meet the same type of results neither to be assesed with the quality criteria of probabilistic sampling (Steinke, 2004). Studies intended to use probability sampling sometimes end up using nonprobability samples because of characteristics of the sampling method. For example, using a sample of people in the paid labor force to analyze the effect of education on earnings is to use a non-probability sample of persons who could be in the paid labor force. Because the education people obtain could determine their likelihood of being in the paid labor force, technically the sample in the paid labor force is a nonprobability sample for the question at issue. In such cases results are biased. The statistical model one uses can also render the data a non-probability sample. For example, Lucas (2014b) notes that several published studies that use multilevel modeling have been based on samples that are probability samples in general, but nonprobability samples for one or more of the levels of analysis in the study. Evidence indicates that in such cases the bias is poorly behaved, such that inferences from such analyses are unjustified. These problems occur in the academic literature, but they may be more common in non-academic research. For example, in public opinion polling by private companies (or other organizations unable to require response), the sample can be self-selected rather than random. This often introduces an important type of error: self-selection bias. This error sometimes makes it unlikely that the sample will accurately represent the broader population. More important, this error makes it impossible to establish that the sample represents the broader population. Volunteering for the sample may be determined by characteristics such as submissiveness or availability. The samples in such surveys should be treated as non-probability samples of the population, and the validity of the findings based on them is unknown and cannot be established.
In probability, statistics, economics, and actuarial science, the Benini distribution is a continuous probability distribution that is a statistical size distribution often applied to model incomes, severity of claims or losses in actuarial applications, and other economic data. It's tail behavior decays faster than a power law, but not as fast as an exponential. This distribution was introduced by Rodolfo Benini in 1905. Somewhat later than Benini's original work, the distribution has been independently discovered or discussed by a number of authors.  
In probability theory, a Le vy process, named after the French mathematician Paul Le vy, is a stochastic process with independent, stationary increments: it represents the motion of a point whose successive displacements are random and independent, and statistically identical over different time intervals of the same length. A Le vy process may thus be viewed as the continuous-time analog of a random walk. The most well known examples of Le vy processes are Brownian motion and the Poisson process. Aside from Brownian motion with drift, all other proper Le vy processes have discontinuous paths.
Data collection is the process of gathering and measuring information on targeted variables in an established systematic fashion, which then enables one to answer relevant questions and evaluate outcomes. The data collection component of research is common to all fields of study including physical and social sciences, humanities and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that then translates to rich data analysis and allows the building of a convincing and credible answer to questions that have been posed.
In mathematics, a Gaussian function, often simply referred to as a Gaussian, is a function of the form:  for arbitrary real constants a, b and c. It is named after the mathematician Carl Friedrich Gauss. The graph of a Gaussian is a characteristic symmetric "bell curve" shape. The parameter a is the height of the curve's peak, b is the position of the center of the peak and c (the standard deviation, sometimes called the Gaussian RMS width) controls the width of the "bell". Gaussian functions are widely used in statistics where they describe the normal distributions, in signal processing where they serve to define Gaussian filters, in image processing where two-dimensional Gaussians are used for Gaussian blurs, and in mathematics where they are used to solve heat equations and diffusion equations and to define the Weierstrass transform.
A jump process is a type of stochastic process that has discrete movements, called jumps, rather than small continuous movements. A general mathematical framework relating discrete-time processes to continuous time ones is the continuous-time random walk. In physics, jump processes result in diffusion. On a microscopic level, they are described by jump diffusion models. In finance, various stochastic models are used to model the price movements of financial instruments; for example the Black Scholes model for pricing options assumes that the underlying instrument follows a traditional diffusion process, with small, continuous, random movements. John Carrington Cox and Stephen Ross proposed that prices actually follow a 'jump process'. The Cox Ross Rubinstein binomial options pricing model formalizes this approach. This is a more intuitive view of financial markets, with allowance for larger moves in asset prices caused by sudden world events. Robert C. Merton extended this approach to a hybrid model known as jump diffusion, which states that the prices have large jumps followed by small continuous movements.
Item tree analysis (ITA) is a data analytical method which allows constructing a hierarchical structure on the items of a questionnaire or test from observed response patterns. Assume that we have a questionnaire with m items and that subjects can answer positive (1) or negative (0) to each of these items, i.e. the items are dichotomous. If n subjects answer the items this results in a binary data matrix D with m columns and n rows. Typical examples of this data format are test items which can be solved (1) or failed (0) by subjects. Other typical examples are questionnaires where the items are statements to which subjects can agree (1) or disagree (0). Depending on the content of the items it is possible that the response of a subject to an item j determines her or his responses to other items. It is, for example, possible that each subject who agrees to item j will also agree to item i. In this case we say that item j implies item i (short ). The goal of an ITA is to uncover such deterministic implications from the data set D.
The basic tenet of a meta-analysis is that there is a common truth behind all conceptually similar scientific studies, but which has been measured with a certain error within individual studies. The aim in meta-analysis then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. In essence, all existing methods yield a weighted average from the results of the individual studies and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies. Meta-analysis can be thought of as "conducting research about previous research." Meta-analysis can only proceed if we are able to identify a common statistical measure that is shared among studies, called the effect size, which has a standard error so that we can proceed with computing a weighted average of that common measure. Such weighting usually takes into consideration the sample sizes of the individual studies, although it can also include other factors, such as study quality. A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices many of which can affect its results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews.  
Data visualization or data visualisation is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data, meaning "information that has been abstracted in some schematic form, including attributes or variables for the units of information". A primary goal of data visualization is to communicate information clearly and efficiently to users via the statistical graphics, plots, information graphics charts selected. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message. Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable and usable. Users may have particular analytical tasks, such as making comparisons or understanding causality, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look-up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables. Data visualization is both an art and a science. It is viewed as a branch of descriptive statistics by some, but also as a grounded theory development tool by others. The rate at which data is generated has increased, driven by an increasingly information-bonomy. Data created by internet activity and an expanding number of sensors in the environment, such as satellites and teras, are referred to as "Big Data". Processing, analyzing and communicating this data present a variety of ethical and analytical challenges for data visualization. The field of data science and practitioners called data scientists have emerged to help address this challenge.  
In probability and statistics, the logarithmic distribution (also known as the logarithmic series distribution or the log-series distribution) is a discrete probability distribution derived from the Maclaurin series expansion  From this we obtain the identity  This leads directly to the probability mass function of a Log(p)-distributed random variable:  for k   1, and where 0 < p < 1. Because of the identity above, the distribution is properly normalized. The cumulative distribution function is  where B is the incomplete beta function. A Poisson compounded with Log(p)-distributed random variables has a negative binomial distribution. In other words, if N is a random variable with a Poisson distribution, and Xi, i = 1, 2, 3, ... is an infinite sequence of independent identically distributed random variables each having a Log(p) distribution, then  has a negative binomial distribution. In this way, the negative binomial distribution is seen to be a compound Poisson distribution. R. A. Fisher described the logarithmic distribution in a paper that used it to model relative species abundance. The probability mass function   of this distribution satisfies the recurrence relation
Demography (from prefix demo- from Ancient Greek        de mos, meaning "the people", and -graphy from        grapho , implies "writing, description or measurement") is the statistical study of populations, especially human beings. As a very general science, it can analyze any kind of dynamic living population, i.e., one that changes over time or space (see population dynamics). Demography encompasses the study of the size, structure, and distribution of these populations, and spatial or temporal changes in them in response to birth, migration, ageing, and death. Based on the demographic research of the earth, earth's population up to the year 2050 and 2100 can be estimated by demographers. Demographics are quantifiable characteristics of a given population. Demographic analysis can cover whole societies, or groups defined by criteria such as education, nationality, religion and ethnicity. Educational institutions usually treat demography as a field of sociology, though there are a number of independent demography departments. Formal demography limits its object of study to the measurement of population processes, while the broader field of social demography or population studies also analyzes the relationships between economic, social, cultural and biological processes influencing a population.
For a matrix whose elements are stochastic, see Random matrix In mathematics, a stochastic matrix (also termed probability matrix, transition matrix, substitution matrix, or Markov matrix) is a matrix used to describe the transitions of a Markov chain. Each of its entries is a nonnegative real number representing a probability. It has found use in probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. There are several different definitions and types of stochastic matrices: A right stochastic matrix is a real square matrix, with each row summing to 1. A left stochastic matrix is a real square matrix, with each column summing to 1. A doubly stochastic matrix is a square matrix of nonnegative real numbers with each row and column summing to 1. In the same vein, one may define stochastic vector (also called probability vector) as a vector whose elements are nonnegative real numbers which sum to 1. Thus, each row of a right stochastic matrix (or column of a left stochastic matrix) is a stochastic vector. A common convention in English language mathematics literature is to use row vectors of probabilities and right stochastic matrices rather than column vectors of probabilities and left stochastic matrices; this article follows that convention.
In combinatorics, a Graeco-Latin square or Euler square or orthogonal Latin squares of order n over two sets S and T, each consisting of n symbols, is an n n arrangement of cells, each cell containing an ordered pair (s,t), where s is in S and t is in T, such that every row and every column contains each element of S and each element of T exactly once, and that no two cells contain the same ordered pair. Orthogonal Latin squares  The arrangement of the s-coordinates by themselves (which may be thought of as Latin characters) and of the t-coordinates (the Greek characters) each forms a Latin square. A Graeco-Latin square can therefore be decomposed into two "orthogonal" Latin squares. Orthogonality here means that every pair (s, t) from the Cartesian product S T occurs exactly once.
A methodological advisor or consultant provides methodological and statistical advice and guidance to clients interested in making decisions regarding the design of studies, the collection and analysis of data, and the presentation and dissemination of research findings. Trained in both methods and statistics and communication skills, advisors may work in academia, industry, or the public sector.
In mathematical physics and probability and statistics, the Gaussian q-distribution is a family of probability distributions that includes, as limiting cases, the uniform distribution and the normal (Gaussian) distribution. It was introduced by Diaz and Teruel, is a q-analogue of the Gaussian or normal distribution. The distribution is symmetric about zero and is bounded, except for the limiting case of the normal distribution. The limiting uniform distribution is on the range -1 to +1.
The shifted Gompertz distribution is the distribution of the largest of two independent random variables one of which has an exponential distribution with parameter b and the other has a Gumbel distribution with parameters  and b. In its original formulation the distribution was expressed referring to the Gompertz distribution instead of the Gumbel distribution but, since the Gompertz distribution is a reverted Gumbel distribution, the labelling can be considered as accurate. It has been used as a model of adoption of innovations. It was proposed by Bemmaor (1994). Some of its statistical properties have been studied further by Jime nez and Jodra  (2009). It has been used to predict the growth and decline of social networks and on-line services and shown to be superior to the Bass model and Weibull distribution (see the work by Christian Bauckhage and co-authors).
Quality control, or QC for short, is a process by which entities review the quality of all factors involved in production. ISO 9000 defines quality control as "A part of quality management focused on fulfilling quality requirements". This approach places an emphasis on three aspects: Elements such as controls, job management, defined and well managed processes, performance and integrity criteria, and identification of records Competence, such as knowledge, skills, experience, and qualifications Soft elements, such as personnel, integrity, confidence, organizational culture, motivation, team spirit, and quality relationships. Controls include product inspection, where every product is examined visually, and often using a stereo microscope for fine detail before the product is sold into the external market. Inspectors will be provided with lists and descriptions of unacceptable product defects such as cracks or surface blemishes for example. The quality of the outputs is at risk if any of these three aspects is deficient in any way. Quality control emphasizes testing of products to uncover defects and reporting to management who make the decision to allow or deny product release, whereas quality assurance attempts to improve and stabilize production (and associated processes) to avoid, or at least minimize, issues which led to the defect(s) in the first place. For contract work, particularly work awarded by government agencies, quality control issues are among the top reasons for not renewing a contract.
Tikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, and with multiple independent discoveries, it is also variously known as the Tikhonov Miller method, the Phillips Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg Marquardt algorithm for non-linear least-squares problems. Suppose that for a known matrix  and vector , we wish to find a vector  such that . The standard approach is ordinary least squares linear regression. However if no  satisfies the equation or more than one  does -- that is the solution is not unique -- the problem is said not to be well posed. In such cases, ordinary least squares estimation leads to an overdetermined (over-fitted), or more often an underdetermined (under-fitted) system of equations. Most real-world phenomena have the effect of low-pass filters in the forward direction where  maps  to . Therefore in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of  that is in the null-space of , rather than allowing for a model to be used as a prior for . Ordinary least squares seeks to minimize the sum of squared residuals, which can be compactly written as  where  is the Euclidean norm. In order to give preference to a particular solution with desirable properties, a regularization term can be included in this minimization:  for some suitably chosen Tikhonov matrix, . In many cases, this matrix is chosen as a multiple of the identity matrix (), giving preference to solutions with smaller norms; this is known as L2 regularization. In other cases, lowpass operators (e.g., a difference operator or a weighted Fourier operator) may be used to enforce smoothness if the underlying vector is believed to be mostly continuous. This regularization improves the conditioning of the problem, thus enabling a direct numerical solution. An explicit solution, denoted by , is given by:  The effect of regularization may be varied via the scale of matrix . For  this reduces to the unregularized least squares solution provided that (ATA) 1 exists. L2 regularization is used in many contexts aside from linear regression, such as classification with logistic regression or support vector machines, and matrix factorization.
In probability and statistics, the parabolic fractal distribution is a type of discrete probability distribution in which the logarithm of the frequency or size of entities in a population is a quadratic polynomial of the logarithm of the rank (with the largest example having rank 1). This can markedly improve the fit over a simple power-law relationship (see references below). In the Laherre re/Deheuvels paper below, examples include galaxy sizes (ordered by luminosity), towns (in the USA, France, and world), spoken languages (by number of speakers) in the world, and oil fields in the world (by size). They also mention utility for this distribution in fitting seismic events (no example). The authors assert the advantage of this distribution is that it can be fitted using the largest known examples of the population being modeled, which are often readily available and complete, then the fitted parameters found can be used to compute the size of the entire population. So, for example, the populations of the hundred largest cities on the planet can be sorted and fitted, and the parameters found used to extrapolate to the smallest villages, to estimate the population of the planet. Another example is estimating total world oil reserves using the largest fields. In a number of applications, there is a so-called King effect where the top-ranked item(s) have a significantly greater frequency or size than the model predicts on the basis of the other items. The Laherre re/Deheuvels paper shows the example of Paris, when sorting the sizes of towns in France. When the paper was written Paris was the largest city with about ten million inhabitants, but the next largest town had only about 1.5 million. Towns in France excluding Paris closely follow a parabolic distribution, well enough that the 56 largest gave a very good estimate of the population of the country. But that distribution would predict the largest city to have about two million inhabitants, not 10 million. The King Effect is named after the notion that a King must defeat all rivals for the throne and takes their wealth, estates and power, thereby creating a buffer between himself and the next-richest of his subjects. That specific effect (intentionally created) may apply to corporate sizes, where the largest businesses use their wealth to buy up smaller rivals. Absent intent, the King Effect may occur as a result of some persistent growth advantage due to scale, or to some unique advantage. Larger cities are more efficient connectors of people, talent and other resources. Unique advantages might include being a port city, or a Capital city where law is made, or a center of activity where physical proximity increases opportunity and creates a feedback loop. An example is the motion picture industry; where actors, writers and other workers move to where the most studios are, and new studios are founded in the same place because that is where the most talent resides. To test for the King Effect, the distribution must be fitted excluding the 'k' top-ranked items, but without assigning new rank numbers to the remaining members of the population. For example, in France the ranks are (as of 2010): 1. Paris, 12.09M 2. Lyon, 2.12M 3. Marseille, 1.72M 4. Toulouse, 1.20M 5. Lille, 1.15M A fitting algorithm would process pairs {(1,12.09), (2,2.12), (3,1.72), (4,1.20), (5,1.15)} and find the parameters for the best parabolic fit through those points. To test for the King Effect we just exclude the first pair (or first 'k' pairs), and find parabolic parameters that fit the remainder of the points. So for France we would fit the four points {(2,2.12), (3,1.72), (4,1.20), (5,1.15)}. Then we can use those parameters to estimate the size of cities ranked [1,k] and determine if they are King Effect members or normal members. By comparison, Zipf's law fits a line through the points (also using the log of the rank and log of the value). A parabola (with one more parameter) will fit better, but users should note that far from the vertex the parabola is also nearly linear. Thus, although it is a judgment call for the statistician, if the fitted parameters put the vertex far from the points fitted, or if the parabolic curve is not a significantly better fit than a line, those may be symptomatic of overfitting (aka over-parameterization). The line (with two parameters instead of three) is probably the better generalization. More parameters always fit better, but at the cost of adding unexplained parameters or unwarranted assumptions (such as the assumption that a slight parabolic curve is a more appropriate model than a line). Alternatively, it is possible to force the fitted parabola to have its vertex at the rank 1 position. In that case, it is not certain the parabola will fit better (have less error) than a straight line; and the choice might be made between the two based on which has the least error.
Barnardisation is a method of disclosure control for tables of counts that involves randomly adding or subtracting 1 from some cells in the table. It is named after Professor George Alfred Barnard (1915 2002), a professor of mathematics at the University of Essex. In the United Kingdom, barnardisation is sometimes employed by public agencies in order to enable them to provide information for statistical purposes without infringing the information privacy rights of the individuals to whom the information relates. The question whether barnardisation may fall short of the complete anonymisation of data and the status of barnardised data under the complex provisions of the Data Protection Act 1998 were considered by the House of Lords in the case of Common Services Agency v Scottish Information Commissioner [2008] 1 WLR 1550, the above case is also reported at All ER 2008 (4) 851.
In time series analysis, Bartlett's method (also known as the method of averaged periodograms), is used for estimating power spectra. It provides a way to reduce the variance of the periodogram in exchange for a reduction of resolution, compared to standard periodograms. A final estimate of the spectrum at a given frequency is obtained by averaging the estimates from the periodograms (at the same frequency) derived from a non-overlapping portions of the original series. The method is used in physics, engineering, and applied mathematics. Common applications of Bartlett's method are frequency response measurements and general spectrum analysis. The method is named after M. S. Bartlett who first proposed it.
In probability theory and statistics, the raised cosine distribution is a continuous probability distribution supported on the interval . The probability density function (PDF) is  for  and zero otherwise. The cumulative distribution function (CDF) is  for  and zero for  and unity for . The moments of the raised cosine distribution are somewhat complicated, but are considerably simplified for the standard raised cosine distribution. The standard raised cosine distribution is just the raised cosine distribution with  and . Because the standard raised cosine distribution is an even function, the odd moments are zero. The even moments are given by:  where  is a generalized hypergeometric function.
In probability theory and statistics, the index of dispersion, dispersion index, coefficient of dispersion, relative variance, or variance-to-mean ratio (VMR), like the coefficient of variation, is a normalized measure of the dispersion of a probability distribution: it is a measure used to quantify whether a set of observed occurrences are clustered or dispersed compared to a standard statistical model. It is defined as the ratio of the variance  to the mean ,  It is also known as the Fano factor, though this term is sometimes reserved for windowed data (the mean and variance are computed over a subpopulation), where the index of dispersion is used in the special case where the window is infinite. Windowing data is frequently done: the VMR is frequently computed over various intervals in time or small regions in space, which may be called "windows", and the resulting statistic called the Fano factor. It is only defined when the mean  is non-zero, and is generally only used for positive statistics, such as count data or time between events, or where the underlying distribution is assumed to be the exponential distribution or Poisson distribution.
The F-distribution, also known as Snedecor's F distribution or the Fisher Snedecor distribution (after Ronald Fisher and George W. Snedecor) is, in probability theory and statistics, a continuous probability distribution. The F-distribution arises frequently as the null distribution of a test statistic, most notably in the analysis of variance; see F-test.
In statistics, the score, score function, efficient score or informant indicates how sensitively a likelihood function  depends on its parameter . Explicitly, the score for  is the gradient of the log-likelihood with respect to . The score plays an important role in several aspects of inference. For example:  in formulating a test statistic for a locally most powerful test; in approximating the error in a maximum likelihood estimate; in demonstrating the asymptotic sufficiency of a maximum likelihood estimate; in the formulation of confidence intervals; in demonstrations of the Crame r Rao inequality.  The score function also plays an important role in computational statistics, as it can play a part in the computation of maximum likelihood estimates.
In statistics, Cochran's C test, named after William G. Cochran, is a one-sided upper limit variance outlier test. The C test is used to decide if a single estimate of a variance (or a standard deviation) is significantly larger than a group of variances (or standard deviations) with which the single estimate is supposed to be comparable. The C test is discussed in many text books  and has been recommended by IUPAC  and ISO. Cochran's C test should not be confused with Cochran's Q test, which applies to the analysis of two-way randomized block designs. The C test assumes a balanced design, i.e. the considered full data set should consist of individual data series that all have equal size. The C test further assumes that each individual data series is normally distributed. Although primarily an outlier test, the C test is also in use as a simple alternative for regular homoscedasticity tests such as Bartlett's test, Levene's test and the Brown Forsythe test to check a statistical data set for homogeneity of variances. An even simpler way to check homoscedasticity is provided by Hartley's Fmax test, but Hartley's Fmax test has the disadvantage that it only accounts for the minimum and the maximum of the variance range, while the C test accounts for all variances within the range.
Naive Bayes classifiers are a popular statistical technique of e-mail filtering. They typically use bag of words features to identify spam e-mail, an approach commonly used in text classification. Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam. Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering, with roots in the 1990s.
In probability theory and statistics, the exponential distribution (a.k.a. negative exponential distribution) is the probability distribution that describes the time between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate. It is a particular case of the gamma distribution. It is the continuous analogue of the geometric distribution, and it has the key property of being memoryless. In addition to being used for the analysis of Poisson processes, it is found in various other contexts. The exponential distribution is not the same as the class of exponential families of distributions, which is a large class of probability distributions that includes the exponential distribution as one of its members, but also includes the normal distribution, binomial distribution, gamma distribution, Poisson, and many others.
Averaged one-dependence estimators (AODE) is a probabilistic classification learning technique. It was developed to address the attribute-independence problem of the popular naive Bayes classifier. It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation.
Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.
In probability theory and statistics, the generalized multivariate log-gamma (G-MVLG) distribution is a multivariate distribution introduced by Demirhan and Hamurkaroglu in 2011. The G-MVLG is a flexible distribution. Skewness and kurtosis are well controlled by the parameters of the distribution. This enables one to control dispersion of the distribution. Because of this property, the distribution is effectively used as a joint prior distribution in Bayesian analysis, especially when the likelihood is not from the location-scale family of distributions such as normal distribution.  
In statistics and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it describes certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (a stochastic an imperfectly predictable term); thus the model is in the form of a stochastic difference equation. It is a special case of the more general ARMA model of time series, which has a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one stochastic difference equation.
Recursive partitioning is a statistical method for multivariable analysis. Recursive partitioning creates a decision tree that strives to correctly classify members of the population by splitting it into sub-populations based on several dichotomous independent variables. The process is termed recursive because each sub-population may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached. Recursive partitioning methods have been developed since the 1980s. Well known methods of recursive partitioning include Ross Quinlan's ID3 algorithm and its successors, C4.5 and C5.0 and Classification and Regression Trees. Ensemble learning methods such as Random Forests help to overcome a common criticism of these methods - their vulnerability to overfitting of the data - by employing different algorithms and combining their output in some way. This article focuses on recursive partitioning for medical diagnostic tests, but the technique has far wider applications. See decision tree. As compared to regression analysis, which creates a formula that health care providers can use to calculate the probability that a patient has a disease, recursive partition creates a rule such as 'If a patient has finding x, y, or z they probably have disease q'. A variation is 'Cox linear recursive partitioning'.
In various science/engineering applications, such as independent component analysis, image analysis, genetic analysis, speech recognition, manifold learning, and time delay estimation it is useful to estimate the differential entropy of a system or process, given some observations. The simplest and most common approach uses histogram-based estimation, but other approaches have been developed and used, each with their own benefits and drawbacks. The main factor in choosing a method is often a trade-off between the bias and the variance of the estimate although the nature of the (suspected) distribution of the data may also be a factor.
Machine learning is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions. Machine learning is closely related to and often overlaps with computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is infeasible. Example applications include spam filtering, optical character recognition (OCR), search engines and computer vision. Machine learning is sometimes conflated with data mining, where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning. Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.
In probability theory, a Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state and not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable.
Random assignment or random placement is an experimental technique for assigning human participants or animal subjects to different groups in an experiment (e.g., a treatment group versus a control group) using randomization, such as by a chance procedure (e.g., flipping a coin) or a random number generator. This ensures that each participant or subject has an equal chance of being placed in any group. Random assignment of participants helps to ensure that any differences between and within the groups are not systematic at the outset of the experiment. Thus, any differences between groups recorded at the end of the experiment can be more confidently attributed to the experimental procedures or treatment. Random assignment, blinding, and controlling are key aspects of the design of experiments, because they help ensure that the results are not spurious or deceptive via confounding. This is why randomized controlled trials are vital in clinical research, especially ones that can be double-blinded and placebo-controlled. Mathematically, there are distinctions between randomization, pseudorandomization, and quasirandomization, as well as between random number generators and pseudorandom number generators. How much these differences matter in experiments (such as clinical trials) is a matter of trial design and statistical rigor, which affect evidence grading. Studies done without randomization include quasi-experiments and open-label trials; those done with pseudo- or quasirandomization are usually given nearly the same weight as those with true randomization but are viewed with a bit more caution.
The Gauss Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required. Non-linear least squares problems arise for instance in non-linear regression, where parameters in a model are sought such that the model is in good agreement with available observations. The method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton.
A stochastic differential equation (SDE) is a differential equation in which one or more of the terms is a stochastic process, resulting in a solution which is also a stochastic process. SDEs are used to model various phenomena such as unstable stock prices or physical systems subject to thermal fluctuations. Typically, SDEs contain a variable which represents random white noise calculated as the derivative of Brownian motion or the Wiener process. However, it should be noted that other types of random behaviour are possible, such as jump processes.  
In population genetics, F-statistics (also known as fixation indices) describe the statistically expected level of heterozygosity in a population; more specifically the expected degree of (usually) a reduction in heterozygosity when compared to Hardy Weinberg expectation. F-statistics can also be thought of as a measure of the correlation between genes drawn at different levels of a (hierarchically) subdivided population. This correlation is influenced by several evolutionary processes, such as mutation, migration, inbreeding, natural selection, or the Wahlund effect, but it was originally designed to measure the amount of allelic fixation owing to genetic drift. The concept of F-statistics was developed during the 1920s by the American geneticist Sewall Wright, who was interested in inbreeding in cattle. However, because complete dominance causes the phenotypes of homozygote dominants and heterozygotes to be the same, it was not until the advent of molecular genetics from the 1960s onwards that heterozygosity in populations could be measured. F can be used to define effective population size.
Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education. The terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as "the application of computer science to statistics", and 'computational statistics' as "aiming at the design of algorithm for implementing statistical methods on computers, including the ones unthinkable before the computer age (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems" [sic]. The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.
In statistics, Bayesian multivariate linear regression is a Bayesian approach to multivariate linear regression, i.e. linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator.
In statistics, multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.). Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, softmax regression, multinomial logit, maximum entropy (MaxEnt) classifier, conditional maximum entropy model.
Aumann's agreement theorem says that two people acting rationally (in a certain precise sense) and with common knowledge of each other's beliefs cannot agree to disagree. More specifically, if two people are genuine Bayesian rationalists with common priors, and if they each have common knowledge of their individual posteriors, then their posteriors must be equal. This theorem holds even if the people's individual posteriors are based on different observed information about the world. Simply knowing that another agent observed some information and came to their respective conclusion will force each to revise their beliefs, resulting eventually in total agreement on the correct posterior. Thus, two rational Bayesian agents with the same priors and who know each other's posteriors will have to agree. A question arises whether such an agreement can be reached in a reasonable time and, from a mathematical perspective, whether this can be done efficiently. Scott Aaronson has shown that this is indeed the case. Of course, the assumption of common priors is a rather strong one and may not hold in practice. However, Robin Hanson has presented an argument that Bayesians who agree about the processes that gave rise to their priors (e.g., genetic and environmental influences) should, if they adhere to a certain pre-rationality condition, have common priors. Studying the same issue from a different perspective, a research paper by Ziv Hellman considers what happens if priors are not common. The paper presents a way to measure how distant priors are from being common. If this distance is   then, under common knowledge, disagreement on events is always bounded from above by  . When   goes to zero, Aumann's original agreement theorem is recapitulated. In a 2013 paper, Joseph Halpern and Willemien Kets argued that "players can agree to disagree in the presence of ambiguity, even if there is a common prior, but that allowing for ambiguity is more restrictive than assuming heterogeneous priors."
The Treynor ratio (sometimes called the reward-to-volatility ratio or Treynor measure), named after Jack L. Treynor, is a measurement of the returns earned in excess of that which could have been earned on an investment that has no diversifiable risk (e.g., Treasury bills or a completely diversified portfolio), per each unit of market risk assumed. The Treynor ratio relates excess return over the risk-free rate to the additional risk taken; however, systematic risk is used instead of total risk. The higher the Treynor ratio, the better the performance of the portfolio under analysis.
The two envelopes problem, also known as the exchange paradox, is a brain teaser, puzzle, or paradox in logic, probability, and recreational mathematics. It is of special interest in decision theory, and for the Bayesian interpretation of probability theory. Historically, it arose as a variant of the necktie paradox. The problem typically is introduced by formulating a hypothetical challenge of the following type: Of two indistinguishable envelopes, each containing money, one contains twice as much as the other. The subject may pick one envelope and keep the money it contains. Having chosen an envelope at will, but before inspecting it, the subject gets the chance to take the other envelope instead. What is the optimal rational strategy for maximising the amount of money to be gained  There is no point at all in switching envelopes as the situation is symmetric. However, the story now introduces the so-called switching argument that shows that it is more beneficial to switch. The problem is to show what is wrong with this argument.
In combinatorial mathematics, a block design is a set together with a family of subsets (repeated subsets are allowed at times) whose members are chosen to satisfy some set of properties that are deemed useful for a particular application. These applications come from many areas, including experimental design, finite geometry, software testing, cryptography, and algebraic geometry. Many variations have been examined, but the most intensely studied are the balanced incomplete block designs (BIBDs or 2-designs) which historically were related to statistical issues in the design of experiments. A block design in which all the blocks have the same size is called uniform. The designs discussed in this article are all uniform. Pairwise balanced designs (PBDs) are examples of block designs that are not necessarily uniform.
Economic statistics is a topic in applied statistics that concerns the collection, processing, compilation, dissemination, and analysis of economic data. It is also common to call the data themselves 'economic statistics', but for this usage see economic data. The data of concern to economic statistics may include those of an economy of region, country, or group of countries. Economic statistics may also refer to a subtopic of official statistics for data produced by official organizations (e.g. national statistical services, intergovernmental organizations such as United Nations, European Union or OECD, central banks, ministries, etc.). Analyses within economic statistics both make use of and provide the empirical data needed in economic research, whether descriptive or econometric. They are a key input for decision making as to economic policy. The subject includes statistical analysis of topics and problems in microeconomics, macroeconomics, business, finance, forecasting, data quality, and policy evaluation. It also includes such considerations as what data to collect in order to quantify some particular aspect of an economy and of how best to collect in any given instance.
In applied probability, a Markov additive process (MAP) is a bivariate Markov process where the future states depends only on one of the variables.
The Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy Lorentz distribution, Lorentz(ian) function, or Breit Wigner distribution. The Cauchy distribution is often used in statistics as the canonical example of a "pathological" distribution since both its mean and its variance are undefined. (But see the section Explanation of undefined moments below.) The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. The Cauchy distribution has no moment generating function. The Cauchy distribution  is the distribution of the X-intercept of a ray issuing from  with a uniformly distributed angle. Its importance in physics is the result of it being the solution to the differential equation describing forced resonance. In mathematics, it is closely related to the Poisson kernel, which is the fundamental solution for the Laplace equation in the upper half-plane. In spectroscopy, it is the description of the shape of spectral lines which are subject to homogeneous broadening in which all atoms interact in the same way with the frequency range contained in the line shape. Many mechanisms cause homogeneous broadening, most notably collision broadening. It is one of the few distributions that is stable and has a probability density function that can be expressed analytically, the others being the normal distribution and the Le vy distribution.
Statistical genetics is a scientific field concerned with the development and application of statistical methods for drawing inferences from genetic data. The term is most commonly used in the context of human genetics. Research in statistical genetics generally falls into one of three areas: population genetics - Study of evolutionary processes affecting genetic variation between organisms genetic epidemiology - Studying effects of genes on diseases quantitative genetics - Studying the effects of genes on 'normal' phenotypes Statistical geneticists tend to collaborate closely with geneticists, molecular biologists, clinicians and bioinformaticians. Statistical genetics is a type of computational biology.
In statistics, the Cunningham function or Pearson Cunningham function  m,n(x) is a generalisation of a special function introduced by Pearson (1906) and studied in the form here by Cunningham (1908). It can be defined in terms of the confluent hypergeometric function U, by  The function was studied by Cunningham in the context of a multivariate generalisation of the Edgeworth expansion for approximating a probability density function based on its (joint) moments. In a more general context, the function is related to the solution of the constant-coefficient diffusion equation, in one or more dimensions. The function  m,n(x) is a solution of the differential equation for X:  The special function studied by Pearson is given, in his notation by,
When two probability distributions overlap, statistical interference exists. Knowledge of the distributions can be used to determine the likelihood that one parameter exceeds another, and by how much. This technique can be used for dimensioning of mechanical parts, determining when an applied load exceeds the strength of a structure, and in many other situations. This type of analysis can also be used to estimate the probability of failure or the frequency of failure.
A portmanteau test is a type of statistical hypothesis test in which the null hypothesis is well specified, but the alternative hypothesis is more loosely specified. Tests constructed in this context can have the property of being at least moderately powerful against a wide range of departures from the null hypothesis. Thus, in applied statistics, a portmanteau test provides a reasonable way of proceeding as a general check of a model's match to a dataset where there are many different ways in which the model may depart from the underlying data generating process. Use of such tests avoids having to be very specific about the particular type of departure being tested.
In statistics, m-separation is a measure of disconnectedness in ancestral graphs and a generalization of d-separation for directed acyclic graphs. It is the opposite of m-connectedness. Suppose G is an ancestral graph. For given source and target nodes s and t and a set Z of nodes in G\{s, t}, m-connectedness can be defined as follows. Consider a path from s to t. An intermediate node on the path is called a collider if both edges on the path touching it are directed toward the node. The path is said to m-connect the nodes s and t, given Z, if and only if: every non-collider on the path is outside Z, and for each collider c on the path, either c is in Z or there is a directed path from c to an element of Z. If s and t cannot be m-connected by any path satisfying the above conditions, then the nodes are said to be m-separated. The definition can be extended to node sets S and T. Specifically, S and T are m-connected if each node in S can be m-connected to any node in T, and are m-separated otherwise.
Mathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state   the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.
Biclustering, block clustering , co-clustering, or two-mode clustering   is a data mining technique which allows simultaneous clustering of the rows and columns of a matrix. The term was first introduced by Mirkin, although the technique was originally introduced much earlier (i.e., by J.A. Hartigan). Given a set of  rows in  columns (i.e., an  matrix), the biclustering algorithm generates biclusters   a subset of rows which exhibit similar behavior across a subset of columns, or vice versa.
In Bayesian probability, the Jeffreys prior, named after Sir Harold Jeffreys, is a non-informative (objective) prior distribution for a parameter space; it is proportional to the square root of the determinant of the Fisher information:  It has the key feature that it is invariant under reparameterization of the parameter vector . This makes it of special interest for use with scale parameters.  
In probability theory and statistics, the Dirichlet-multinomial distribution is a family of discrete multivariate probability distributions on a finite support of non-negative integers. It is also called the Dirichlet compound multinomial distribution (DCM) or multivariate Po lya distribution (after George Po lya). It is a compound probability distribution, where a probability vector p is drawn from a Dirichlet distribution with parameter vector , and an observation drawn from a multinomial distribution with probability vector p and number of trials N. The compounding corresponds to a Polya urn scheme. It is frequently encountered in Bayesian statistics, empirical Bayes methods and classical statistics as an overdispersed multinomial distribution. It reduces to the Categorical distribution as a special case when n = 1. It also approximates the multinomial distribution arbitrarily well for large  . The Dirichlet-multinomial is a multivariate extension of the Beta-binomial distribution, as the multinomial and Dirichlet distributions are multivariate versions of the binomial distribution and beta distributions, respectively.
Chou's invariance theorem, named after Kuo-Chen Chou, is a result deployed in bioinformatics and cheminformatics related to multivariate statistics. Where a distance that would, in standard statistical theory, be defined as a Mahalanobis distance cannot be defined in this way because the relevant covariance matrix is singular, a replacement would be to reduce the dimension of the multivariate space until the relevant covariance matrix is invertible. This can be achievable by simply omitting one or more of the original coordinates until a space of full rank is reached. Chou's invariance theorem says that it does not matter which of the coordinates are selected for removal, as the same values of distance would be calculated as a final result.
In statistics, multivariate analysis of variance (MANOVA) is a procedure for comparing multivariate sample means. As a multivariate procedure, it is used when there are two or more dependent variables, and is typically followed by significance tests involving individual dependent variables separately. It helps to answer  Do changes in the independent variable(s) have significant effects on the dependent variables  What are the relationships among the dependent variables  What are the relationships among the independent variables 
In probability theory, a branch of mathematics, a diffusion process is a solution to a stochastic differential equation. It is a continuous-time Markov process with almost surely continuous sample paths. Brownian motion, reflected Brownian motion and Ornstein Uhlenbeck processes are examples of diffusion processes. A sample path of a diffusion process models the trajectory of a particle embedded in a flowing fluid and subjected to random displacements due to collisions with molecules, which is called Brownian motion. The position of the particle is then random; its probability density function as a function of space and time is governed by an advection-diffusion equation.
In probability theory, a logit-normal distribution is a probability distribution of a random variable whose logit has a normal distribution. If Y is a random variable with a normal distribution, and P is the logistic function, then X = P(Y) has a logit-normal distribution; likewise, if X is logit-normally distributed, then Y = logit(X)= log (X/(1-X)) is normally distributed. It is also known as the logistic normal distribution, which often refers to a multinomial logit version (e.g.). A variable might be modeled as logit-normal if it is a proportion, which is bounded by zero and one, and where values of zero and one never occur.
The Durbin Wu Hausman test (also called Hausman specification test) is a statistical hypothesis test in econometrics named after James Durbin, De-Min Wu, and Jerry A. Hausman. The test evaluates the consistency of an estimator when compared to an alternative, less efficient, estimator which is already known to be consistent. It helps one evaluate if a statistical model corresponds to the data.
A separation test is a statistical procedure for early-phase research, to decide whether to pursue further research. It is designed to avoid the prevalent situation in early-phase research, when a statistically underpowered test gives a negative result.
Writer invariant, also called authorial invariant or author's invariant, is a property of a text which is invariant of its author, that is, it will be similar in all texts of a given author and different in texts of different authors. It can be used to find plagiarism or discover who is real author of anonymously published text. Writer invariant is also an author's pattern of writing a letter in handwritten text recognition. While it is generally recognised that writer invariants exist, it is not agreed what properties of a text should be used. Among the first ones used was distribution of word lengths; other proposed invariants include average sentence length, average word length, noun, verb or adjective usage frequency, vocabulary richness, and frequency of function words, or specific function words. Of these, average sentence lengths can be very similar in works of different authors or vary significantly even within a single work; average word lengths likewise turn out to be very similar in works of different authors. Analysis of function words shows promise because they are used by authors unconsciously.
In mathematics   specifically, in the theory of stochastic processes   Doob's martingale convergence theorems are a collection of results on the long-time limits of supermartingales, named after the American mathematician Joseph L. Doob.
In sociology and statistics research, snowball sampling (or chain sampling, chain-referral sampling, referral sampling) is a non-probability sampling technique where existing study subjects recruit future subjects from among their acquaintances. Thus the sample group is said to grow like a rolling snowball (similarly to breadth-first search (BFS) in computer science). As the sample builds up, enough data are gathered to be useful for research. This sampling technique is often used in hidden populations which are difficult for researchers to access; example populations would be drug users or sex workers. As sample members are not selected from a sampling frame, snowball samples, analogously to BFS samples, are subject to numerous biases. For example, people who have many friends are more likely to be recruited into the sample. It was widely believed that it was impossible to make unbiased estimates from snowball samples, but a variation of snowball sampling called respondent-driven sampling has been shown to allow researchers to make asymptotically unbiased estimates from snowball samples under certain conditions. Snowball sampling and respondent-driven sampling also allows researchers to make estimates about the social network connecting the hidden population.
The Baraba si Albert (BA) model is an algorithm for generating random scale-free networks using a preferential attachment mechanism. Scale-free networks are widely observed in natural and human-made systems, including the Internet, the world wide web, citation networks, and some social networks. The algorithm is named for its inventors Albert-La szlo  Baraba si and Re ka Albert.
In signal processing, the multitaper method is a technique developed by David J. Thomson to estimate the power spectrum SX of a stationary ergodic finite-variance random process X, given a finite contiguous realization of X as data. It is one of a number of approaches to spectral density estimation.
The following terms are used by electrical engineers in statistical signal processing studies instead of typical statistician's terms.  In other engineering fields, particularly mechanical engineering, uncertainty analysis examines systematic and random components of variations in measurements associated with physical experiments.
In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.
Clustering high-dimensional data is the cluster analysis of data with anywhere from a few dozen to many thousands of dimensions. Such high-dimensional data spaces are often encountered in areas such as medicine, where DNA microarray technology can produce a large number of measurements at once, and the clustering of text documents, where, if a word-frequency vector is used, the number of dimensions equals the size of the vocabulary.
In the theory of probability and statistics, a Bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, "success" and "failure", in which the probability of success is the same every time the experiment is conducted. It is named after Jacob Bernoulli, a Swiss mathematician of the 17th century. The mathematical formalization of the Bernoulli trial is known as the Bernoulli process. This article offers an elementary introduction to the concept, whereas the article on the Bernoulli process offers a more advanced treatment. Since a Bernoulli trial has only two possible outcomes, it can be framed as some "yes or no" question. For example: Is the top card of a shuffled deck an ace  Was the newborn child a girl  Therefore, success and failure are merely labels for the two outcomes, and should not be construed literally. The term "success" in this sense consists in the result meeting specified conditions, not in any moral judgement. More generally, given any probability space, for any event (set of outcomes), one can define a Bernoulli trial, corresponding to whether the event occurred or not (event or complementary event). Examples of Bernoulli trials include: Flipping a coin. In this context, obverse ("heads") conventionally denotes success and reverse ("tails") denotes failure. A fair coin has the probability of success 0.5 by definition. In this case there are exactly two outcomes. Rolling a die, where a six is "success" and everything else a "failure". In this case there are six outcomes, and the event is a six; the complementary event "not a six" corresponds to the other five outcomes. In conducting a political opinion poll, choosing a voter at random to ascertain whether that voter will vote "yes" in an upcoming referendum.  ^ Papoulis, A. (1984). "Bernoulli Trials". Probability, Random Variables, and Stochastic Processes (2nd ed.). New York: McGraw-Hill. pp. 57 63.  ^ James Victor Uspensky: Introduction to Mathematical Probability, McGraw-Hill, New York 1937, page 45
In information geometry, the Fisher information metric is a particular Riemannian metric which can be defined on a smooth statistical manifold, i.e., a smooth manifold whose points are probability measures defined on a common probability space. It can be used to calculate the informational difference between measurements. The metric is interesting in several respects. First, it can be understood to be the infinitesimal form of the relative entropy (i.e., the Kullback Leibler divergence); specifically, it is the Hessian of the divergence. Alternately, it can be understood as the metric induced by the flat space Euclidean metric, after appropriate changes of variable. When extended to complex projective Hilbert space, it becomes the Fubini Study metric; when written in terms of mixed states, it is the quantum Bures metric. Considered purely as a matrix, it is known as the Fisher information matrix. Considered as a measurement technique, where it is used to estimate hidden parameters in terms of observed random variables, it is known as the observed information.
Pignistic probability, in decision theory, is a probability that a rational person will assign to an option when required to make a decision. A person may have, at one level certain beliefs or a lack of knowledge, or uncertainty, about the options and their actual likelihoods. However, when it is necessary to make a decision (such as deciding whether to place a bet), the behaviour of the rational person would suggest that the person has assigned a set of regular probabilities to the options. These are the pignistic probabilities. The term was coined by Philippe Smets, and stems from the Latin pignus, a bet. He contrasts the pignistic level, where one might take action, with the credal level, where one interprets the state of the world: The transferable belief model is based on the assumption that beliefs manifest themselves at two mental levels: the  credal  level where beliefs are entertained and the  pignistic  level where beliefs are used to make decisions (from  credo  I believe and  pignus  a bet, both in Latin). Usually these two levels are not distinguished and probability functions are used to quantify beliefs at both levels. The justification for the use of probability functions is usually linked to  rational  behavior to be held by an ideal agent involved in some decision contexts. A pignistic probability transform will calculate these pignistic probabilities from a structure that describes belief structures.
Financial models with long-tailed distributions and volatility clustering have been introduced to overcome problems with the realism of classical financial models. These classical models of financial time series typically assume homoskedasticity and normality cannot explain stylized phenomena such as skewness, heavy tails, and volatility clustering of the empirical asset returns in finance. In 1963, Benoit Mandelbrot first used the stable (or -stable) distribution to model the empirical distributions which have the skewness and heavy-tail property. Since -stable distributions have infinite -th moments for all , the tempered stable processes have been proposed for overcoming this limitation of the stable distribution. On the other hand, GARCH models have been developed to explain the volatility clustering. In the GARCH model, the innovation (or residual) distributions are assumed to be a standard normal distribution, despite the fact that this assumption is often rejected empirically. For this reason, GARCH models with non-normal innovation distribution have been developed. Many financial models with stable and tempered stable distributions together with volatility clustering have been developed and applied to risk management, option pricing, and portfolio selection.
In probability and statistics, the log-logistic distribution (known as the Fisk distribution in economics) is a continuous probability distribution for a non-negative random variable. It is used in survival analysis as a parametric model for events whose rate increases initially and decreases later, for example mortality rate from cancer following diagnosis or treatment. It has also been used in hydrology to model stream flow and precipitation, and in economics as a simple model of the distribution of wealth or income. The log-logistic distribution is the probability distribution of a random variable whose logarithm has a logistic distribution. It is similar in shape to the log-normal distribution but has heavier tails. Unlike the log-normal, its cumulative distribution function can be written in closed form.
Mathematical and theoretical biology is an interdisciplinary scientific research field with a range of applications in biology, biotechnology, and medicine. The field is also called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side. Mathematical biology aims at the mathematical representation, treatment and modeling of biological processes, using a variety of applied mathematical techniques and tools. It has both theoretical and practical applications in biological, biomedical and biotechnology research. For example, in cell biology, protein interactions are often represented as "cartoon" models, which, although easy to visualize, do not accurately describe the systems studied. This requires precise mathematical models. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. Mathematical biology may deploy calculus, probability theory, statistics, linear algebra, abstract algebra, graph theory, combinatorics, algebraic geometry, topology, dynamical systems, differential equations and coding theory. Some mathematical areas, such as certain methodologies in statistics, were developed as tools during the conduct of research into mathematical biology.
Confusion of the inverse, also called the conditional probability fallacy or the inverse fallacy, is a logical fallacy whereupon a conditional probability is equivocated with its inverse: That is, given two events A and B, the probability of A happening given that B has happened is assumed to be about the same as the probability of B given A. More formally, P(A|B) is assumed to be approximately equal to P(B|A).
The principle of maximum entropy states that, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one with largest entropy. Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. Of those, the one with maximal information entropy is the proper distribution, according to this principle.
In experiments in which additional factors are not likely to interact with any of the other factors, a saturated array can be used. In a saturated array, a controllable factor is substituted for the interaction of two or more by-products. Using a saturated array, a two-factor test matrix could be used to test three factors. Using the saturated array allows three factors to be tested in four tests rather than in eight, as would be required by a standard orthogonal array.
Double mass analysis  is a commonly used data analysis approach for investigating the behaviour of records made of hydrological or meteorological data at a number of locations. It is used to determine whether there is a need for corrections to the data - to account for changes in data collection procedures or other local conditions. Such changes may result from a variety of things including changes in instrumentation, changes in observation procedures, or changes in gauge location or surrounding conditions. Double mass analysis for checking consistency of a hydrological or meteorological record is considered to be an essential tool before taking it for analysis purpose. This method is based on the hypothesis that each item of the recorded data of a population is consistent. An example of a double mass analysis is a "double mass plot", or "double mass curve". For this, points and/or a joining line are plotted where the x- and y- coordinates are determined by the running totals of the values observed at two stations. If both stations are affected to the same extent by the same trends then a double mass curve should follow a straight line. A break in the slope of the curve would indicate that conditions have changed at one location but not at another. This technique is based on the principle that when each recorded data comes from the same parent population, they are consistent.
The survival function, also known as a survivor function or reliability function, is a property of any random variable that maps a set of events, usually associated with mortality or failure of some system, onto time. It captures the probability that the system will survive beyond a specified time. The term reliability function is common in engineering while the term survival function is used in a broader range of applications, including human mortality. Another name for the survival function is the complementary cumulative distribution function.
In statistics, the mean signed difference, deviation, or error (MSD or MSE) is a sample statistic that summarises how well an estimator  matches the quantity  that it is supposed to estimate. It is one of a number of statistics that can be used to assess an estimation procedure, and it would often be used in conjunction with a sample version of the mean square error.  
In probability theory and statistics, variance measures how far a set of numbers are spread out. A variance of zero indicates that all the values are identical. Variance is always non-negative: a small variance indicates that the data points tend to be very close to the mean (expected value) and hence to each other, while a high variance indicates that the data points are very spread out around the mean and from each other. An equivalent measure is the square root of the variance, called the standard deviation. The standard deviation has the same dimension as the data, and hence is comparable to deviations from the mean. As standard deviation is often represented with the symbol   (lowercase sigma), so variance is often represented with the symbol  2 (sigma squared). There are two distinct concepts that are both called "variance". One variance is a characteristic of a set of observations. The other is part of a theoretical probability distribution and is defined by an equation. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below. The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance. The variance is one of several descriptors of a probability distribution. In particular, the variance is one of the moments of a distribution. In that context, it forms part of a systematic approach to distinguishing between probability distributions. While other such approaches have been developed, those based on moments are advantageous in terms of mathematical and computational simplicity.
The sample mean or empirical mean and the sample covariance are statistics computed from a collection (the sample) of data on one or more random variables. The sample mean and sample covariance are estimators of the population mean and population covariance, where the term population refers to the set from which the sample was taken. The sample mean is a vector each of whose elements is the sample mean of one of the random variables   that is, each of whose elements is the arithmetic average of the observed values of one of the variables. The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables. If only one variable has had values observed, then the sample mean is a single number (the arithmetic average of the observed values of that variable) and the sample covariance matrix is also simply a single value (a 1x1 matrix containing a single number, the sample variance of the observed values of that variable). Due to their ease of calculation and other desirable characteristics, the sample mean and sample covariance are widely used in statistics and applications to numerically represent the location and dispersion, respectively, of a distribution.
In Bayesian statistics, a credible interval is an interval in the domain of a posterior probability distribution or predictive distribution used for interval estimation. The generalisation to multivariate problems is the credible region. Credible intervals are analogous to confidence intervals in frequentist statistics, although they differ on a philosophical basis; Bayesian intervals treat their bounds as fixed and the estimated parameter as a random variable, whereas frequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value. For example, in an experiment that determines the uncertainty distribution of parameter , if the probability that  lies between 35 and 45 is 0.95, then  is a 95% credible interval.
In statistics, probability theory, and information theory, a statistical distance quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points. A distance between populations can be interpreted as measuring the distance between two probability distributions and hence they are essentially measures of distances between probability measures. Where statistical distance measures relate to the differences between random variables, these may have statistical dependence, and hence these distances are not directly related to measures of distances between probability measures. Again, a measure of distance between random variables may relate to the extent of dependence between them, rather than to their individual values. Statistical distance measures are mostly not metrics and they need not be symmetric. Some types of distance measures are referred to as (statistical) divergences.
Validity is the extent to which a concept, conclusion or measurement is well-founded and corresponds accurately to the real world. The word "valid" is derived from the Latin validus, meaning strong. The validity of a measurement tool (for example, a test in education) is considered to be the degree to which the tool measures what it claims to measure; in this case, the validity is an equivalent to accuracy. In psychometrics, validity has a particular application known as test validity: "the degree to which evidence and theory support the interpretations of test scores" ("as entailed by proposed uses of tests"). It is generally accepted that the concept of scientific validity addresses the nature of reality and as such is an epistemological and philosophical issue as well as a question of measurement. The use of the term in logic is narrower, relating to the truth of inferences made from premises. Validity is important because it can help determine what types of tests to use, and help to make sure researchers are using methods that are not only ethical, and cost-effective, but also a method that truly measures the idea or construct in question.
Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s. As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include statistical relational learning and reasoning (emphasizing the importance of reasoning) and first-order probabilistic languages (emphasizing the key properties of the languages with which models are represented).  
Feller's coin-tossing constants are a set of numerical constants which describe asymptotic probabilities that in n independent tosses of a fair coin, no run of k consecutive heads (or, equally, tails) appears. William Feller showed that if this probability is written as p(n,k) then  where  k is the smallest positive real root of  and  
In probability theory, a probability distribution is infinitely divisible if it can be expressed as the probability distribution of the sum of an arbitrary number of independent and identically distributed random variables. The characteristic function of any infinitely divisible distribution is then called an infinitely divisible characteristic function. More rigorously, the probability distribution F is infinitely divisible if, for every positive integer n, there exist n independent identically distributed random variables Xn1, ..., Xnn whose sum Sn = Xn1 + ... + Xnn has the distribution F. The concept of infinite divisibility of probability distributions was introduced in 1929 by Bruno de Finetti. This type of decomposition of a distribution is used in probability and statistics to find families of probability distributions that might be natural choices for certain models or applications. Infinitely divisible distributions play an important role in probability theory in the context of limit theorems.
Lucia de Berk (born September 22, 1961 in The Hague, Netherlands), often called Lucia de B., is a Dutch licensed paediatric nurse, who was the subject of a miscarriage of justice. In 2003, she was sentenced to life imprisonment (for which no parole is possible under Dutch law) for four murders and three attempted murders of patients in her care. In 2004, after an appeal, she was convicted of seven murders and three attempts. Her conviction was controversial in the media and amongst scientists, and was questioned by investigative reporter Peter R. de Vries. In October 2008, the case was reopened by the Supreme Court of the Netherlands, as new facts had been uncovered that undermined the previous verdicts. De Berk was freed, and her case was re-tried; she was exonerated in April 2010.
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.
In statistics, a probability plot is a graphical technique for comparing two data sets, either two sets of empirical observations, one empirical set against a theoretical set, or (more rarely) two theoretical sets against each other. It commonly means one of: P P plot, "Probability-Probability" or "Percent-Percent" plot; Q Q plot, "Quantile-Quantile" plot, which is more commonly used. Special cases include the  Normal probability plot, a Q Q plot against the standard normal distribution;  The term "probability plot" may be used to refer to both of these types of plot, or the term "probability plot" may be used to refer specifically to a P-P plot.
This page contains examples of Markov chains in action.
In statistics, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model given data. Likelihood functions play a key role in statistical inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, "likelihood" is often used as a synonym for "probability." In statistics, a distinction is made depending on the roles of outcomes vs. parameters. Probability is used before data are available to describe possible future outcomes given a fixed value for the parameter (or parameter vector). Likelihood is used after data are available to describe a function of a parameter (or parameter vector) for a given outcome.
In statistics, the Phillips Perron test (named after Peter C. B. Phillips and Pierre Perron) is a unit root test. That is, it is used in time series analysis to test the null hypothesis that a time series is integrated of order 1. It builds on the Dickey Fuller test of the null hypothesis  in , where  is the first difference operator. Like the augmented Dickey Fuller test, the Phillips Perron test addresses the issue that the process generating data for  might have a higher order of autocorrelation than is admitted in the test equation making  endogenous and thus invalidating the Dickey Fuller t-test. Whilst the augmented Dickey Fuller test addresses this issue by introducing lags of  as regressors in the test equation, the Phillips Perron test makes a non-parametric correction to the t-test statistic. The test is robust with respect to unspecified autocorrelation and heteroscedasticity in the disturbance process of the test equation. Davidson and MacKinnon (2004) report that the Phillips Perron test performs worse in finite samples than the augmented Dickey Fuller test.
Test equating traditionally refers to the statistical process of determining comparable scores on different forms of an exam. It can be accomplished using either classical test theory or item response theory. In item response theory, equating is the process of equating the units and origins of two scales on which the abilities of students have been estimated from results on different tests. The process is analogous to equating degrees Fahrenheit with degrees Celsius by converting measurements from one scale to the other. The determination of comparable scores is a by-product of equating that results from equating the scales obtained from test results.
In econometrics, the method of simulated moments (MSM) (also called simulated method of moments) is a structural estimation technique introduced by Daniel McFadden. It extends the generalized method of moments to cases where theoretical moment functions cannot be evaluated directly, such as when moment functions involve high-dimensional integrals. MSM's earliest and principal applications have been to research in industrial organization, after its development by Ariel Pakes, David Pollard, and others, though applications in consumption are emerging.
In estimation theory and decision theory, a Bayes estimator or a Bayes action is an estimator or decision rule that minimizes the posterior expected value of a loss function (i.e., the posterior expected loss). Equivalently, it maximizes the posterior expectation of a utility function. An alternative way of formulating an estimator within Bayesian statistics is maximum a posteriori estimation.
In probability theory, Le Cam's theorem, named after Lucien le Cam (1924   2000), states the following. Suppose: X1, ..., Xn are independent random variables, each with a Bernoulli distribution (i.e., equal to either 0 or 1), not necessarily identically distributed. Pr(Xi = 1) = pi for i = 1, 2, 3, ...   (i.e.  follows a Poisson binomial distribution) Then  In other words, the sum has approximately a Poisson distribution and the above inequality bounds the approximation error in terms of the total variation distance. By setting pi =  n/n, we see that this generalizes the usual Poisson limit theorem. When  is a large a better bound is possible:  It is also possible to weaken the independence requirement.
A Savitzky Golay filter is a digital filter that can be applied to a set of digital data points for the purpose of smoothing the data, that is, to increase the signal-to-noise ratio without greatly distorting the signal. This is achieved, in a process known as convolution, by fitting successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares. When the data points are equally spaced, an analytical solution to the least-squares equations can be found, in the form of a single set of "convolution coefficients" that can be applied to all data sub-sets, to give estimates of the smoothed signal, (or derivatives of the smoothed signal) at the central point of each sub-set. The method, based on established mathematical procedures, was popularized by Abraham Savitzky and Marcel J. E. Golay who published tables of convolution coefficients for various polynomials and sub-set sizes in 1964. Some errors in the tables have been corrected. The method has been extended for the treatment of 2- and 3-dimensional data. Savitzky and Golay's paper is one of the most widely cited papers in the journal Analytical Chemistry and is classed by that journal as one of its "10 seminal papers" saying "it can be argued that the dawn of the computer-controlled analytical instrument can be traced to this article".
In probability theory and statistics, a probability mass function (pmf) is a function that gives the probability that a discrete random variable is exactly equal to some value. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete. A probability mass function differs from a probability density function (pdf) in that the latter is associated with continuous rather than discrete random variables; the values of the latter are not probabilities as such: a pdf must be integrated over an interval to yield a probability.
Named for the Dutch mathematician Bartel Leendert van der Waerden, the Van der Waerden test is a statistical test that k population distribution functions are equal. The Van Der Waerden test converts the ranks from a standard Kruskal-Wallis one-way analysis of variance to quantiles of the standard normal distribution (details given below). These are called normal scores and the test is computed from these normal scores. The k population version of the test is an extension of the test for two populations published by Van der Waerden (1952,1953).
A mixed model is a statistical model containing both fixed effects and random effects. These models are useful in a wide variety of disciplines in the physical, biological and social sciences. They are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study), or where measurements are made on clusters of related statistical units. Because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures ANOVA.
In statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values. It is also known as the look-elsewhere effect. Errors in inference, including confidence intervals that fail to include their corresponding population parameters or hypothesis tests that incorrectly reject the null hypothesis, are more likely to occur when one considers the set as a whole. Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared. These techniques generally require a higher significance threshold for individual comparisons, so as to compensate for the number of inferences being made.
The Friedman test is a non-parametric statistical test developed by Milton Friedman. Similar to the parametric repeated measures ANOVA, it is used to detect differences in treatments across multiple test attempts. The procedure involves ranking each row (or block) together, then considering the values of ranks by columns. Applicable to complete block designs, it is thus a special case of the Durbin test. Classic examples of use are: n wine judges each rate k different wines. Are any wines ranked consistently higher or lower than the others  n wines are each rated by k different judges. Are the judges' ratings consistent with each other  n welders each use k welding torches, and the ensuing welds were rated on quality. Do any of the torches produce consistently better or worse welds  The Friedman test is used for one-way repeated measures analysis of variance by ranks. In its use of ranks it is similar to the Kruskal Wallis one-way analysis of variance by ranks. Friedman test is widely supported by many statistical software packages.
Clinical trials are experiments done in clinical research. Such prospective biomedical or behavioral research studies on human participants are designed to answer specific questions about biomedical or behavioral interventions, including new treatments (such as novel vaccines, drugs, dietary choices, dietary supplements, and medical devices) and known interventions that warrant further study and comparison. Clinical trials generate data on safety and efficacy. They are conducted only after they have received health authority/ethics committee approval in the country where approval of the therapy is sought. These authorities are responsible for vetting the risk/benefit ratio of the trial - their approval does not mean that the therapy is 'safe' or effective, only that the trial may be conducted. Depending on product type and development stage, investigators initially enroll volunteers and/or patients into small pilot studies, and subsequently conduct progressively larger scale comparative studies. Clinical trials can vary in size and cost, and they can involve a single research center or multiple centers, in one country or in multiple countries. Clinical study design aims to ensure the scientific validity and reproducibility of the results. Trials can be quite costly, depending on a number of factors. The sponsor may be a governmental organization or a pharmaceutical, biotechnology or medical device company. Certain functions necessary to the trial, such as monitoring and lab work, may be managed by an outsourced partner, such as a contract research organization or a central laboratory. Only 10% of all drugs started in human clinical trials become an approved drug.
In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its "theoretical value". The error (or disturbance) of an observed value is the deviation of the observed value from the (unobservable) true value of a quantity of interest (for example, a population mean), and the residual of an observed value is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where it leads to the concept of studentized residuals.
Tukey's range test, also known as the Tukey's test, Tukey method, Tukey's honest significance test, Tukey's HSD (honest significant difference) test, or the Tukey Kramer method, is a single-step multiple comparison procedure and statistical test. It can be used on raw data or in conjunction with an ANOVA (Post-hoc analysis) to find means that are significantly different from each other. Named after John Tukey, it compares all possible pairs of means, and is based on a studentized range distribution (q) (this distribution is similar to the distribution of t from the t-test. See below). The Tukey HSD tests should not be confused with the Tukey Mean Difference tests (also known as the Bland Altman test). Tukey's test compares the means of every treatment to the means of every other treatment; that is, it applies simultaneously to the set of all pairwise comparisons  and identifies any difference between two means that is greater than the expected standard error. The confidence coefficient for the set, when all sample sizes are equal, is exactly 1    . For unequal sample sizes, the confidence coefficient is greater than 1    . In other words, the Tukey method is conservative when there are unequal sample sizes.
In statistics and data analysis, a raw score is an original datum that has not been transformed. This may include, for example, the original result obtained by a student on a test (i.e., the number of correctly answered items) as opposed to that score after transformation to a standard score or percentile rank or the like. Often the conversion must be made to a standard score before the data can be used. For example, an open ended survey question will yield raw data that cannot be used for statistical purposes as it is; however a multiple choice question will yield raw data that is either easy to convert to a standard score, or even can be used as it is.
The logit (/ lo d  t/ LOH-jit) function is the inverse of the sigmoidal "logistic" function or logistic transform used in mathematics, especially in statistics. When the function's parameter represents a probability p, the logit function gives the log-odds, or the logarithm of the odds p/(1   p).
In mathematical optimization, the ordered subset expectation maximization (OSEM) method is an iterative method that is used in computed tomography. In applications in medical imaging, the OSEM method is used for positron emission tomography, for single photon emission computed tomography, and for X-ray computed tomography. The OSEM method is related to the expectation maximization (EM) method of statistics. The OSEM method is also related to methods of filtered back projection.
The Nicholson Bailey model was developed in the 1930s to describe the population dynamics of a coupled host-parasitoid system.a It is named after Alexander John Nicholson and Victor Albert Bailey. Host-parasite and prey-predator systems can also be represented with the Nicholson-Bailey model. The model is closely related to the Lotka Volterra model, which describes the dynamics of antagonistic populations (preys and predators) using differential equations. The model uses (discrete time) difference equations to describe the population growth of host-parasite populations. The model assumes that parasitoids search for hosts at random, and that both parasitoids and hosts are assumed to be distributed in a non-contiguous ("clumped") fashion in the environment. In its original form, the model does not allow for stable coexistence. Subsequent refinements of the model, notably adding density dependence on several terms, allowed this coexistence to happen. A credible, simple alternative to the Lotka Volterra predator-prey model and its common prey dependent generalizations (like Nicholson Bailey) is the ratio-dependent or Arditi Ginzburg model. The two are the extremes of the spectrum of predator interference models. According to the authors of the alternative view, the data show that true interactions in nature are so far from the Lotka Volterra extreme on the interference spectrum that the model can simply be discounted as wrong. They are much closer to the ratio dependent extreme, so if a simple model is needed one can use the Arditi Ginzburg model as the first approximation.
In statistics The optimistic knowledge gradient is a new approximation policy proposed by Xi Chen, Qihang Lin and Dengyong Zhou in 2013. This policy is created to solve the challenge of computationally intractable of large size of optimal computing budget allocation problem in binary/multi-class crowd labeling where each label from the crowd has a certain cost.
The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with true experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation. In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is reflected in a variable called the predictor. The change in the predictor is generally hypothesized to result in a change in the second variable, hence called the outcome variable. Experimental design involves not only the selection of suitable predictors and outcomes, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources. Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the predictor, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity. Correctly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making.
Quantum is a software package and programming language for statistical survey data validation and manipulation and tabulation. Originally developed by Quantime to run on Unix systems, it was incorporated into SPSS Inc.'s SPSS MR product line [1] after its acquisition of Quantime on September 1997.
In probability theory, the martingale representation theorem states that a random variable that is measurable with respect to the filtration generated by a Brownian motion can be written in terms of an Ito  integral with respect to this Brownian motion. The theorem only asserts the existence of the representation and does not help to find it explicitly; it is possible in many cases to determine the form of the representation using Malliavin calculus. Similar theorems also exist for martingales on filtrations induced by jump processes, for example, by Markov chains.
Panel (data) analysis is a statistical method, widely used in social science, epidemiology, and econometrics, which deals with two and "n"-dimensional (in and by the - cross sectional/times series time) panel data. The data are usually collected over time and over the same individuals and then a regression is run over these two dimensions. Multidimensional analysis is an econometric method in which data are collected over more than two dimensions (typically, time, individuals, and some third dimension). A common panel data regression model looks like , where y is the dependent variable, x is the independent variable, a and b are coefficients, i and t are indices for individuals and time. The error  is very important in this analysis. Assumptions about the error term determine whether we speak of fixed effects or random effects. In a fixed effects model,  is assumed to vary non-stochastically over  or  making the fixed effects model analogous to a dummy variable model in one dimension. In a random effects model,  is assumed to vary stochastically over  or  requiring special treatment of the error variance matrix. Panel data analysis has three more-or-less independent approaches: independently pooled panels; random effects models; fixed effects models or first differenced models. The selection between these methods depends upon the objective of our analysis, and the problems concerning the exogeneity of the explanatory variables.
The quadrant count ratio (QCR) is a measure of the association between two quantitative variables. The QCR is not commonly used in the practice of statistics; rather, it is a useful tool in statistics education because it can be used as an intermediate step in the development of Pearson's correlation coefficient.
In time series analysis (or forecasting)   as conducted in statistics, signal processing, and many other fields   the innovation is the difference between the observed value of a variable at time t and the optimal forecast of that value based on information available prior to time t. If the forecasting method is working correctly, successive innovations are uncorrelated with each other, i.e., constitute a white noise time series. Thus it can be said that the innovation time series is obtained from the measurement time series by a process of 'whitening', or removing the predictable component. The use of the term innovation in the sense described here is due to Hendrik Bode and Claude Shannon (1950) in their discussion of the Wiener filter problem, although the notion was already implicit in the work of Kolmogorov.
A preferential attachment process is any of a class of processes in which some quantity, typically some form of wealth or credit, is distributed among a number of individuals or objects according to how much they already have, so that those who are already wealthy receive more than those who are not. "Preferential attachment" is only the most recent of many names that have been given to such processes. They are also referred to under the names "Yule process", "cumulative advantage", "the rich get richer", and, less correctly, the "Matthew effect". They are also related to Gibrat's law. The principal reason for scientific interest in preferential attachment is that it can, under suitable circumstances, generate power law distributions.
In statistics, canonical-correlation analysis (CCA) is a way of making sense of cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym) of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of the Xi and Yj which have maximum correlation with each other. T. R. Knapp notes "virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables." The method was first introduced by Harold Hotelling in 1936. It is important to be familiar with basic linear algebra, and transposition in order to use canonical-correlation analysis.
In statistics, a likelihood ratio test is a statistical test used to compare the goodness of fit of two models, one of which (the null model) is a special case of the other (the alternative model). The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio, or equivalently its logarithm, can then be used to compute a p-value, or compared to a critical value to decide whether to reject the null model in favour of the alternative model. When the logarithm of the likelihood ratio is used, the statistic is known as a log-likelihood ratio statistic, and the probability distribution of this test statistic, assuming that the null model is true, can be approximated using Wilks  theorem. In the case of distinguishing between two models, each of which has no unknown parameters, use of the likelihood ratio test can be justified by the Neyman Pearson lemma, which demonstrates that such a test has the highest power among all competitors.
Cover's Theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation. The proof is easy. A deterministic mapping may be used. Indeed, suppose there are  samples. Lift them onto the vertices of the simplex in the  dimensional real space. Every partition of the samples into two sets is separable by a linear separator. QED.  A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.
Risk is the potential of gaining or losing something of value. Values (such as physical health, social status, emotional well-being or financial wealth) can be gained or lost when taking risk resulting from a given action or inaction, foreseen or unforeseen. Risk can also be defined as the intentional interaction with uncertainty. Uncertainty is a potential, unpredictable, and uncontrollable outcome; risk is a consequence of action taken in spite of uncertainty. Risk perception is the subjective judgment people make about the severity and probability of a risk, and may vary person to person. Any human endeavor carries some risk, but some are much riskier than others.
In the theory of probability and statistics, the Dvoretzky Kiefer Wolfowitz inequality predicts how close an empirically determined distribution function will be to the distribution function from which the empirical samples are drawn. It is named after Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz, who in 1956 proved the inequality with an unspecified multiplicative constant C in front of the exponent on the right-hand side. In 1990, Pascal Massart proved the inequality with the sharp constant C = 2,  confirming a conjecture due to Birnbaum and McCarty.  
A statistical hypothesis is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables. A statistical hypothesis test is a method of statistical inference. Commonly, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets. The comparison is deemed statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability the significance level. Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance. The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors (type 1 & type 2), and by specifying parametric limits on e.g. how much type 1 error will be permitted. An alternative framework for statistical hypothesis testing is to specify a set of statistical models, one for each candidate hypothesis, and then use model selection techniques to choose the most appropriate model. The most common selection techniques are based on either Akaike information criterion or Bayes factor. Statistical hypothesis testing is sometimes called confirmatory data analysis. It can be contrasted with exploratory data analysis, which may not have pre-specified hypotheses.
Injury prevention is an effort to prevent or reduce the severity of bodily injuries caused by external mechanisms, such as accidents, before they occur. Injury prevention is a component of safety and public health, and its goal is to improve the health of the population by preventing injuries and hence improving quality of life. Among laypersons, the term "accidental injury" is often used. However, "accidental" implies the causes of injuries are random in nature. Researchers use the term "unintentional injury" to refer to injuries that are nonvolitional but preventable. Within the field of public health, efforts are also made to prevent or reduce "intentional injury." Data from the U.S. Centers for Disease Control, for example, show unintentional injuries are the leading cause of death from early childhood until middle adulthood. During these years, unintentional injuries account for more deaths than the next nine leading causes of death combined. Injury prevention strategies cover a variety of approaches, many of which are classified as falling under the  3 E s  of injury prevention: education, engineering modifications, and enforcement/enactment. Some organizations, such as Safe Kids Worldwide, have expanded the list to six E s adding: evaluation, economic incentives and empowerment.
In statistics, sometimes the covariance matrix of a multivariate random variable is not known but has to be estimated. Estimation of covariance matrices then deals with the question of how to approximate the actual covariance matrix on the basis of a sample from the multivariate distribution. Simple cases, where observations are complete, can be dealt with by using the sample covariance matrix. The sample covariance matrix (SCM) is an unbiased and efficient estimator of the covariance matrix if the space of covariance matrices is viewed as an extrinsic convex cone in Rp p; however, measured using the intrinsic geometry of positive-definite matrices, the SCM is a biased and inefficient estimator. In addition, if the random variable has normal distribution, the sample covariance matrix has Wishart distribution and a slightly differently scaled version of it is the maximum likelihood estimate. Cases involving missing data require deeper considerations. Another issue is the robustness to outliers, to which sample covariance matrices are highly sensitive. Statistical analyses of multivariate data often involve exploratory studies of the way in which the variables change in relation to one another and this may be followed up by explicit statistical models involving the covariance matrix of the variables. Thus the estimation of covariance matrices directly from observational data plays two roles:  to provide initial estimates that can be used to study the inter-relationships; to provide sample estimates that can be used for model checking.  Estimates of covariance matrices are required at the initial stages of principal component analysis and factor analysis, and are also involved in versions of regression analysis that treat the dependent variables in a data-set, jointly with the independent variable as the outcome of a random sample.
In statistics, Ward's method is a criterion applied in hierarchical cluster analysis. Ward's minimum variance method inaccurate, see talk is a special case of the objective function approach originally presented by Joe H. Ward, Jr. Ward suggested a general agglomerative hierarchical clustering procedure, where the criterion for choosing the pair of clusters to merge at each step is based on the optimal value of an objective function. This objective function could be "any function that reflects the investigator's purpose." Many of the standard clustering procedures are contained in this very general class. To illustrate the procedure, Ward used the example where the objective function is the error sum of squares, and this example is known as Ward's method or more precisely Ward's minimum variance method.
In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. There are point and interval estimators. The point estimators yield single-valued results, although this includes the possibility of single vector-valued results and results that can be expressed as a single function. This is in contrast to an interval estimator, where the result would be a range of plausible values (or vectors or functions). Estimation theory is concerned with the properties of estimators; that is, with defining properties that can be used to compare different estimators (different rules for creating estimates) for the same quantity, based on the same data. Such properties can be used to determine the best rules to use under given circumstances. However, in robust statistics, statistical theory goes on to consider the balance between having good properties, if tightly defined assumptions hold, and having less good properties that hold under wider conditions.
Double counting is a fallacy in which, when counting events or occurrences in probability or in other areas, a solution counts events two or more times, resulting in an erroneous number of events or occurrences which is higher than the true result. This results in the calculated sum of probabilities for all possible outcomes to be higher than 100%, which is impossible. For example, what is the probability of seeing at least one 5 when throwing a pair of dice  An erroneous argument goes as follows: The first die shows a 5 with probability 1/6; the second die shows a 5 with probability 1/6; therefore the probability of seeing a 5 is 1/6 + 1/6 = 1/3 = 12/36. However, the correct answer is 11/36, because the erroneous argument has double-counted the event where both dice show 5s. In mathematical terms, the previous example calculated the probability of P(A or B) as P(A)+P(B). However, by the inclusion-exclusion principle, P(A or B) = P(A) + P(B) - P(A and B). The principle is used to compensate for double counting by subtracting those objects which were double counted.
Cricket is a sport that generates a large number of statistics. Statistics are recorded for each player during a match, and aggregated over a career. At the professional level, statistics for Test cricket, one-day internationals, and first-class cricket are recorded separately. However, since Test matches are a form of first-class cricket, a player's first-class statistics will include his Test match statistics   but not vice versa. Nowadays records are also maintained for List A and Twenty20 limited over matches. These matches are normally limited over games played domestically at the national level by leading Test nations. Since one-day internationals are a form of List A limited over matches, a player's List A statistics will include his ODI match statistics   but not vice versa.
In statistics, a binomial proportion confidence interval is a confidence interval for a proportion in a statistical population. It uses the proportion estimated in a statistical sample and allows for sampling error. There are several formulas for a binomial confidence interval, but all of them rely on the assumption of a binomial distribution. In general, a binomial distribution applies when an experiment is repeated a fixed number of times, each trial of the experiment has two possible outcomes (labeled arbitrarily success and failure), the probability of success is the same for each trial, and the trials are statistically independent. A simple example of a binomial distribution is the set of various possible outcomes, and their probabilities, for the number of heads observed when a (not necessarily fair) coin is flipped ten times. The observed binomial proportion is the fraction of the flips which turn out to be heads. Given this observed proportion, the confidence interval for the true proportion innate in that coin is a range of possible proportions which may contain the true proportion. A 95% confidence interval for the proportion, for instance, will contain the true proportion 95% of the times that the procedure for constructing the confidence interval is employed. Note that this does not mean that a calculated 95% confidence interval will contain the true proportion with 95% probability. Instead, one should interpret it as follows: the process of drawing a random sample and calculating an accompanying 95% confidence interval will generate a confidence interval that contains the true proportion in 95% of all cases. There are several ways to compute a confidence interval for a binomial proportion. The normal approximation interval is the simplest formula, and the one introduced in most basic Statistics classes and textbooks. This formula, however, is based on an approximation that does not always work well. Several competing formulas are available that perform better, especially for situations with a small sample size and a proportion very close to zero or one. The choice of interval will depend on how important it is to use a simple and easy-to-explain interval versus the desire for better accuracy.
The Q-statistic is a test statistic output by either the Box-Pierce test or, in a modified version which provides better small sample properties, by the Ljung-Box test. It follows the chi-squared distribution. See also Portmanteau test. The q statistic or studentized range statistic is a statistic used for multiple significance testing across a number of means: see Tukey Kramer method.
In probability and statistics, the Hellinger distance (also called Bhattacharyya distance as this was originally introduced by Anil Kumar Bhattacharya) is used to quantify the similarity between two probability distributions. It is a type of f-divergence. The Hellinger distance is defined in terms of the Hellinger integral, which was introduced by Ernst Hellinger in 1909.
In statistics and probability theory, a point process is a type of random process for which any one realisation consists of a set of isolated points either in time or geographical space, or in even more general spaces. For example, the occurrence of lightning strikes might be considered as a point process in both time and geographical space if each is recorded according to its location in time and space. Point processes are well studied objects in probability theory and the subject of powerful tools in statistics for modeling and analyzing spatial data, which is of interest in such diverse disciplines as forestry, plant ecology, epidemiology, geography, seismology, materials science, astronomy, telecommunications, computational neuroscience, economics and others. Point processes on the real line form an important special case that is particularly amenable to study, because the points are ordered in a natural way, and the whole point process can be described completely by the (random) intervals between the points. These point processes are frequently used as models for random events in time, such as the arrival of customers in a queue (queueing theory), of impulses in a neuron (computational neuroscience), particles in a Geiger counter, location of radio stations in a telecommunication network or of searches on the world-wide web.
Seasonal adjustment is a statistical method for removing the seasonal component of a time series that exhibits a seasonal pattern. It is usually done when wanting to analyse the trend of a time series independently of the seasonal components. It is normal to report seasonally adjusted data for unemployment rates to reveal the underlying trends in labor markets. Many economic phenomena have seasonal cycles, such as agricultural production and consumer consumption, e.g. greater consumption leading up to Christmas. It is necessary to adjust for this component in order to understand what underlying trends are in the economy and so official statistics are often adjusted to remove seasonal components.
Clinical study design is the formulation of trials and experiments, as well as observational studies in medical, clinical and other types of research (e.g., epidemiological) involving human beings. The goal of a clinical study is to assess the safety, efficacy, and / or the mechanism of action of an investigational medicinal product, or new drug or device that is in development, but potentially not yet approved by a health authority (e.g. FDA). Some of the considerations here are shared under the more general topic of design of experiments but there can be others, in particular related to patient confidentiality and ethics.
The following outline is provided as an overview of and topical guide to regression analysis, which involves any of several statistical techniques for learning about the relationship between one or more dependent variables (Y) and one or more independent variables (X).
The Ellsberg paradox is a paradox in decision theory in which people's choices violate the postulates of subjective expected utility. It is generally taken to be evidence for ambiguity aversion. The paradox was popularized by Daniel Ellsberg, although a version of it was noted considerably earlier by John Maynard Keynes. The basic idea is that people overwhelmingly prefer taking on risk in situations where they know specific odds rather than an alternative risk scenario in which the odds are completely ambiguous they will always choose a known probability of winning over an unknown probability of winning even if the known probability is low and the unknown probability could be a guarantee of winning. That is, given a choice of risks to take (such as bets), people "prefer the devil they know" rather than assuming a risk where odds are difficult or impossible to calculate. Ellsberg actually proposed two separate thought experiments, the proposed choices which contradict subjective expected utility. The 2-color problem involves bets on two urns, both of which contain balls of two different colors. The 3-color problem, described below, involves bets on a single urn, which contains balls of three different colors.
The Pickands Balkema de Haan theorem is often called the second theorem in extreme value theory. It gives the asymptotic tail distribution of a random variable X, when the true distribution F of X is unknown. Unlike the first theorem (the Fisher Tippett Gnedenko theorem) in extreme value theory, the interest here is the values above a threshold.
A climate ensemble involves slightly different models of the climate system. There are at least four different types, to be described below. For the equivalent in numerical weather prediction, see ensemble forecasting.
In statistics, autoregressive fractionally integrated moving average models are time series models that generalize ARIMA (autoregressive integrated moving average) models by allowing non-integer values of the differencing parameter. These models are useful in modeling time series with long memory that is, in which deviations from the long-run mean decay more slowly than an exponential decay. The acronyms "ARFIMA" or "FARIMA" are often used, although it is also conventional to simply extend the "ARIMA(p,d,q)" notation for models, by simply allowing the order of differencing, d, to take fractional values.  
In statistics, line-intercept sampling (LIS) is a method of sampling elements in a region whereby an element is sampled if a chosen line segment, called a  transect , intersects the element. Line intercept sampling has proven to be a reliable, versatile, and easy to implement method to analyze an area containing various objects of interest. It has recently also been applied to estimating variances during particulate material sampling.
In queueing theory, a discipline within the mathematical theory of probability, a BCMP network is a class of queueing network for which a product-form equilibrium distribution exists. It is named after the authors of the paper where the network was first described: Baskett, Chandy, Muntz and Palacios. The theorem is a significant extension to a Jackson network allowing virtually arbitrary customer routing and service time distributions, subject to particular service disciplines. The paper is well known, and the theorem was described in 1990 as "one of the seminal achievements in queueing theory in the last 20 years" by J. Michael Harrison and Ruth J. Williams.
In decision theory, the expected value of sample information (EVSI) is the expected increase in utility that you could obtain from gaining access to a sample of additional observations before making a decision. The additional information obtained from the sample may allow you to make a more informed, and thus better, decision, thus resulting in an increase in expected utility. EVSI attempts to estimate what this improvement would be before seeing actual sample data; hence, EVSI is a form of what is known as preposterior analysis.
Selective recruitment is an observed effect in traffic safety. When safety belt laws are passed, belt wearing rates increase, but casualties decline by smaller percentages than estimated in a simple calculation. This is because those converted from non-use to use are not  recruited  random members of the driving population. Instead, users differ from non-users in many ways that influence safety. Two effects are: 1. When non-wearers crash, they have more severe crashes. 2. Non-wearers are more likely to crash
Generalized Procrustes analysis (GPA) is a method of statistical analysis that can be used to compare the shapes of objects, or the results of surveys, interviews, or panels. It was developed for analysing the results of free-choice profiling, a survey technique which allows respondents (such as sensory panelists) to describe a range of products in their own words or language. GPA is one way to make sense of free-choice profiling data; other ways can be multiple factor analysis (MFA), or the STATIS method. The method was first published by J. C. Gower in 1975. Generalized Procrustes analysis estimates the scaling factor applied to respondent scale usage, thus it generates a weighting factor that is used to compensate for individual scale usage differences. Unlike measures such as a principal component analysis, since GPA uses individual level data, a measure of variance is utilized in the analysis. The Procrustes distance provides a metric to minimize in order to superimpose a pair of shape instances annotated by landmark points. GPA applies the Procrustes analysis method to superimpose a population of shapes instead of only two shape instances. The algorithm outline is the following: arbitrarily choose a reference shape (typically by selecting it among the available instances) superimpose all instances to current reference shape compute the mean shape of the current set of superimposed shapes if the Procrustes distance between the mean shape and the reference is above a threshold, set reference to mean shape and continue to step 2.
A stochastic grammar (statistical grammar) is a grammar framework with a probabilistic notion of grammaticality: Stochastic context-free grammar Statistical parsing Data-oriented parsing Hidden Markov model Estimation theory Statistical natural language processing uses stochastic, probabilistic and statistical methods, especially to resolve difficulties that arise because longer sentences are highly ambiguous when processed with realistic grammars, yielding thousands or millions of possible analyses. Methods for disambiguation often involve the use of corpora and Markov models. "A probabilistic model consists of a non-probabilistic model plus some numerical quantities; it is not true that probabilistic models are inherently simpler or less structural than non-probabilistic models." The technology for statistical NLP comes mainly from machine learning and data mining, both of which are fields of artificial intelligence that involve learning from data.  
In statistics the trimean (TM), or Tukey's trimean, is a measure of a probability distribution's location defined as a weighted average of the distribution's median and its two quartiles:  This is equivalent to the average of the median and the midhinge:  The foundations of the trimean were part of Arthur Bowley's teachings, and later popularized by statistician John Tukey in his 1977 book which has given its name to a set of techniques called Exploratory data analysis. Like the median and the midhinge, but unlike the sample mean, it is a statistically resistant L-estimator with a breakdown point of 25%. This beneficial property has been described as follows:  An advantage of the trimean as a measure of the center (of a distribution) is that it combines the median's emphasis on center values with the midhinge's attention to the extremes.
Biplots are a type of exploratory graph used in statistics, a generalization of the simple two-variable scatterplot. A biplot allows information on both samples and variables of a data matrix to be displayed graphically. Samples are displayed as points while variables are displayed either as vectors, linear axes or nonlinear trajectories. In the case of categorical variables, category level points may be used to represent the levels of a categorical variable. A generalised biplot displays information on both continuous and categorical variables.
Stanine (STAndard NINE) is a method of scaling test scores on a nine-point standard scale with a mean of five and a standard deviation of two. Some web sources attribute stanines to the U.S. Army Air Forces during World War II. Psychometric legend has it that a 1-9 scale was used because of the compactness of recording the score as a single digit but Thorndike claims that by reducing scores to just nine values, stanines "reduce the tendency to try to interpret small score differences (p. 131)". The earliest known use of stanines was by the U.S. Army Air Forces in 1943.
In Bayesian statistics, a strong prior is a preceding assumption, theory, concept or idea upon which, after taking account of new information, a current assumption, theory, concept or idea is founded. The term is used to contrast the case of a weak or uninformative prior probability. A strong prior would be a type of informative prior in which the information contained in the prior distribution dominates the information contained in the data being analysed. The Bayesian analysis combines the information contained in the prior with that extracted from the data to produce the posterior distribution which, in the case of a "strong prior", would be little changed from the prior distribution.
In statistics, the Mann Whitney U test (also called the Mann Whitney Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon Mann Whitney test) is a nonparametric test of the null hypothesis that two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other. Unlike the t-test it does not require the assumption of normal distributions. It is nearly as efficient as the t-test on normal distributions.
Structural equation modeling (SEM) refers to a diverse set of mathematical models, computer algorithms, and statistical methods that fit networks of constructs to data. SEM includes confirmatory factor analysis, path analysis, partial least squares path analysis, LISREL and latent growth modeling. The term should not be confused with Structural Modeling in economics. Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables.  The links between constructs of a structural equation model may be estimated with independent regression equations or through more involved approaches such as those employed in LISREL. Use of SEM is commonly justified in the social sciences because of its ability to impute relationships between unobserved constructs (latent variables) from observable variables. To provide a simple example, the concept of human intelligence cannot be measured directly as one could measure height or weight. Instead, psychologists develop theories of intelligence and write measurement instruments with items (questions) designed to measure intelligence according to their theory. They would then use SEM to test their theory using data gathered from people who took their intelligence test. With SEM, "intelligence" would be the latent variable and the test items would be the observed variables. A simplistic model suggesting that intelligence (as measured by four questions) can predict academic performance (as measured by SAT, ACT, and high school GPA) is shown below. In SEM diagrams, latent variables are commonly shown as ovals and observed variables as rectangles. The below diagram shows how error (e) influences each intelligence question and the SAT, ACT, and GPA scores, but does not influence the latent variables. SEM provides numerical estimates for each of the parameters (arrows) in the model to indicate the strength of the relationships. Thus, in addition to testing the overall theory, SEM therefore allows the researcher to diagnose which observed variables are good indicators of the latent variables. Various methods in structural equation modeling have been used in the sciences, business, education, and other fields. Use of SEM methods in analysis is controversial because SEM methods generally lack widely accepted goodness-of-fit statistics and most SEM software offers little latitude for error analysis. This puts SEM at a disadvantage with respect to systems of regression equation methods, though the latter are limited in their ability to fit unobserved 'latent' constructs.
In objective video quality assessment, the outliers ratio (OR) is a measure of the performance of an objective video quality metric. It is the ratio of "false" scores given by the objective metric to the total number of scores. The "false" scores are the scores that lie outside the interval  where MOS is the mean opinion score and   is the standard deviation of the MOS.
In statistics, the Ha jek Le Cam convolution theorem states that any regular estimator in a parametric model is asymptotically equivalent to a sum of two independent random variables, one of which is normal with asymptotic variance equal to the inverse of Fisher information, and the other having arbitrary distribution. The obvious corollary from this theorem is that the  best  among regular estimators are those with the second component identically equal to zero. Such estimators are called efficient and are known to always exist for regular parametric models. The theorem is named after Jaroslav Ha jek and Lucien Le Cam.
Clustering is the problem of partitioning data points into groups based on their similarity. Correlation clustering provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.
Benford's law, also called the first-digit law, is a phenomenological law about the frequency distribution of leading digits in many (but not all) real-life sets of numerical data. The law states that in many naturally occurring collections of numbers, the leading significant digit is likely to be small. For example, in sets which obey the law, the number 1 appears as the most significant digit about 30% of the time, while 9 appears as the most significant digit less than 5% of the time. By contrast, if the digits were distributed uniformly, they would each occur about 11.1% of the time. Benford's law also makes (different) predictions about the distribution of second digits, third digits, digit combinations, and so on. It has been shown that this result applies to a wide variety of data sets, including electricity bills, street addresses, stock prices, house prices, population numbers, death rates, lengths of rivers, physical and mathematical constants, and processes described by power laws (which are very common in nature). It tends to be most accurate when values are distributed across multiple orders of magnitude. The graph here shows Benford's law for base 10. There is a generalization of the law to numbers expressed in other bases (for example, base 16), and also a generalization from leading 1 digit to leading n digits. It is named after physicist Frank Benford, who stated it in 1938, although it had been previously stated by Simon Newcomb in 1881.
In probability theory, especially in mathematical statistics, a location-scale family is a family of probability distributions parametrized by a location parameter and a non-negative scale parameter. For any random variable  whose probability distribution function belongs to such a family, the distribution function of  also belongs to the family (where  means "equal in distribution" that is, "has the same distribution as"). Moreover, if  and  are two random variables whose distribution functions are members of the family, and  has zero mean and unit variance, then  can be written as  , where  and  are the mean and standard deviation of . In other words, a class  of probability distributions is a location-scale family if for all cumulative distribution functions  and any real numbers  and , the distribution function  is also a member of . In decision theory, if all alternative distributions available to a decision-maker are in the same location-scale family, and the first two moments are finite, then a two-moment decision model can apply, and decision-making can be framed in terms of the means and the variances of the distributions.
In statistics, the Hannan Quinn information criterion (HQC) is a criterion for model selection. It is an alternative to Akaike information criterion (AIC) and Bayesian information criterion (BIC). It is given as  where  is the log-likelihood, k is the number of parameters, and n is the number of observations. Burnham & Anderson (2002, p. 287) say that HQC, "while often cited, seems to have seen little use in practice". They also note that HQC, like BIC, but unlike AIC, is not an estimator of Kullback Leibler divergence. Claeskens & Hjort (2008, ch. 4) note that HQC, like BIC, but unlike AIC, is not asymptotically efficient, and further point out that whatever method is being used for fine-tuning the criterion will be more important in practice than the term log log n, since this latter number is small even for very large n.
In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly choose points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals. There are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, importance sampling, Sequential Monte Carlo (a.k.a. particle filter), and mean field particle methods.
In probability theory, a continuous-time Markov chain (CTMC or continuous-time Markov process) is a mathematical model which takes values in some finite state space and for which the time spent in each state takes non-negative real values and has an exponential distribution. It is a continuous-time stochastic process with the Markov property which means that future behaviour of the model (both remaining time in current state and next state) depends only on the current state of the model and not on historical behaviour. The model is a continuous-time version of the Markov chain model, named because the output from such a process is a sequence (or chain) of states.
In statistics, a central composite design is an experimental design, useful in response surface methodology, for building a second order (quadratic) model for the response variable without needing to use a complete three-level factorial experiment. After the designed experiment is performed, linear regression is used, sometimes iteratively, to obtain results. Coded variables are often used when constructing this design.  
In statistics, the empirical distribution function is the distribution function associated with the empirical measure of the sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. The empirical distribution function estimates the cumulative distribution function underlying of the points in the sample and converges with probability 1 according to the Glivenko Cantelli theorem. A number of results exist to quantify the rate of convergence of the empirical distribution function to the underlying cumulative distribution function.
In statistics, a contingency table is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables. They are heavily used in survey research, business intelligence, engineering and scientific research. They provide a basic picture of the interrelation between two variables and can help find interactions between them. The term contingency table was first used by Karl Pearson in "On the Theory of Contingency and Its Relation to Association and Normal Correlation", part of the Drapers' Company Research Memoirs Biometric Series I published in 1904. A crucial problem of multivariate statistics is finding (direct-)dependence structure underlying the variables contained in high-dimensional contingency tables. If some of the conditional independences are revealed, then even the storage of the data can be done in a smarter way (see Lauritzen (2002)). In order to do this one can use information theory concepts, which gain the information only from the distribution of probability, which can be expressed easily from the contingency table by the relative frequencies.
Covariance intersection is an algorithm for combining two or more estimates of state variables in a Kalman filter when the correlation between them is unknown.
In mathematics, rejection sampling is a basic technique used to generate observations from a distribution. It is also commonly called the acceptance-rejection method or "accept-reject algorithm" and is a type of Monte Carlo method. The method works for any distribution in  with a density. Rejection sampling is based on the observation that to sample a random variable one can perform a uniformly random sampling of the 2D cartesian graph, and keep the samples in the region under the graph of its density function. Note that this property can be extended to N-dimension functions.  
In medicine and psychology, clinical significance is the practical importance of a treatment effect - whether it has a real genuine, palpable, noticeable effect on daily life.
OpenBUGS is a computer software for the Bayesian analysis of complex statistical models using Markov chain Monte Carlo (MCMC) methods. OpenBUGS is the open source variant of WinBUGS (Bayesian inference Using Gibbs Sampling). It runs under Windows and Linux, as well as from inside the R statistical package. Versions from v3.0.7 onwards have been designed to be at least as efficient and reliable as WinBUGS over a range of test applications.
In statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values. It is also known as the look-elsewhere effect. Errors in inference, including confidence intervals that fail to include their corresponding population parameters or hypothesis tests that incorrectly reject the null hypothesis, are more likely to occur when one considers the set as a whole. Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared. These techniques generally require a higher significance threshold for individual comparisons, so as to compensate for the number of inferences being made.
In probability theory, Chebyshev's inequality (also spelled as Tchebysheff's inequality, Russian:                       ) guarantees that in any probability distribution, "nearly all" values are close to the mean   the precise statement being that no more than 1/k2 of the distribution's values can be more than k standard deviations away from the mean (or equivalently, at least 1 1/k2 of the distribution's values are within k standard deviations of the mean). The rule is often called Chebyshev's theorem, about the range of standard deviations around the mean, in statistics. The inequality has great utility because it can be applied to completely arbitrary distributions (unknown except for mean and variance). For example, it can be used to prove the weak law of large numbers. In practical usage, in contrast to the 68-95-99.7% rule, which applies to normal distributions, under Chebyshev's inequality a minimum of just 75% of values must lie within two standard deviations of the mean and 89% within three standard deviations. The term Chebyshev's inequality may also refer to Markov's inequality, especially in the context of analysis.
The classical definition or interpretation of probability is identified with the works of Jacob Bernoulli and Pierre-Simon Laplace. As stated in Laplace's The orie analytique des probabilite s, The probability of an event is the ratio of the number of cases favorable to it, to the number of all cases possible when nothing leads us to expect that any one of these cases should occur more than any other, which renders them, for us, equally possible. This definition is essentially a consequence of the principle of indifference. If elementary events are assigned equal probabilities, then the probability of a disjunction of elementary events is just the number of events in the disjunction divided by the total number of elementary events. The classical definition of probability was called into question by several writers of the nineteenth century, including John Venn and George Boole. The frequentist definition of probability became widely accepted as a result of their criticism, and especially through the works of R.A. Fisher. The classical definition enjoyed a revival of sorts due to the general interest in Bayesian probability, because Bayesian methods require a prior probability distribution and the principle of indifference offers one source of such a distribution. Classical probability can offer prior probabilities that reflect ignorance which often seems appropriate before an experiment is conducted.
In statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of actually existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all possible hands in a game of poker). A common aim of statistical analysis is to produce information about some chosen population. In statistical inference, a subset of the population (a statistical sample) is chosen to represent the population in a statistical analysis. If a sample is chosen properly, characteristics of the entire population that the sample is drawn from can be estimated from corresponding characteristics of the sample.
The order in probability notation is used in probability theory and statistical theory in direct parallel to the big-O notation that is standard in mathematics. Where the big-O notation deals with the convergence of sequences or sets of ordinary numbers, the order in probability notation deals with convergence of sets of random variables, where convergence is in the sense of convergence in probability.
A longitudinal survey is a correlational research study that involves repeated observations of the same variables over long periods of time, often many decades. It is often a type of observational study, although they can also be structured as longitudinal randomized experiments. Longitudinal studies are often used in psychology, to study developmental trends across the life span, and in sociology, to study life events throughout lifetimes or generations. The reason for this is that unlike cross-sectional studies, in which different individuals with the same characteristics are compared, longitudinal studies track the same people and so the differences observed in those people are less likely to be the result of cultural differences across generations. Longitudinal studies thus make observing changes more accurate and are applied in various other fields. In medicine, the design is used to uncover predictors of certain diseases. In advertising, the design is used to identify the changes that advertising has produced in the attitudes and behaviors of those within the target audience who have seen the advertising campaign. When longitudinal studies are observational, in the sense that they observe the state of the world without manipulating it, it has been argued that they may have less power to detect causal relationships than experiments. However, because of the repeated observation at the individual level, they have more power than cross-sectional observational studies, by virtue of being able to exclude time-invariant unobserved individual differences and also of observing the temporal order of events. Some of the disadvantages of longitudinal study include the fact that they take a lot of time and are very expensive. Therefore, they are not very convenient. Longitudinal studies allow social scientists to distinguish short from longterm phenomena, such as poverty. If the poverty rate is 10% at a point in time, this may mean that 10% of the population are always poor or that the whole population experiences poverty for 10% of the time. It is impossible to conclude which of these possibilities is the case by using one-off cross-sectional studies. Types of longitudinal studies include panel studies and cohort studies. Cohort studies sample a cohort, defined as a group experiencing some event (typically birth) in a selected time period, performing a cross-section at intervals through time. Panel studies also use cross-sectional data and compare the same group of individuals at intervals through time, but the sample is not necessarily a cohort, as it can be a group of people that do not share a common event. Therefore, a cohort study can be considered a panel study, but a panel study is not always a cohort study. A retrospective study is a longitudinal study that looks back in time. For instance, a researcher may look up the medical records of previous years to look for a trend.
Dap is a statistics and graphics program, that performs data management, analysis, and graphical visualization tasks which are commonly required in statistical consulting practice. Dap was written to be a free replacement for SAS, but users are assumed to have a basic familiarity with the C programming language in order to permit greater flexibility. Unlike R it has been designed to be used on large data sets.
In mathematics, the Ornstein Uhlenbeck process (named after Leonard Ornstein and George Eugene Uhlenbeck), is a stochastic process that, roughly speaking, describes the velocity of a massive Brownian particle under the influence of friction. The process is stationary, Gaussian, and Markovian, and is the only nontrivial process that satisfies these three conditions, up to allowing linear transformations of the space and time variables. Over time, the process tends to drift towards its long-term mean: such a process is called mean-reverting. The process can be considered to be a modification of the random walk in continuous time, or Wiener process, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the centre. The Ornstein Uhlenbeck process can also be considered as the continuous-time analogue of the discrete-time AR(1) process.
An epidemic model is a simplified means of describing the transmission of communicable disease through individuals.
The use of evidence under Bayes' theorem relates to the likelihood of finding evidence in relation to the accused, where Bayes' theorem concerns the probability of an event and its inverse. Specifically, it compares the probability of finding particular evidence if the accused were guilty, versus if they were not guilty. An example would be the probability of finding a person's hair at the scene, if guilty, versus if just passing through the scene. Another issue would be finding a person's DNA where they lived, regardless of committing a crime there.
Mauchly's sphericity test is a statistical test used to validate a repeated measures analysis of variance (ANOVA).
In statistical mechanics, the correlation function is a measure of the order in a system, as characterized by a mathematical correlation function. Correlation functions describe how microscopic variables, such as spin and density, at different positions are related. More specifically, the correlation functions quantifies how microscopic variables co-vary with one another on average across space and time. A classic example of such spatial correlations is in ferro- and antiferromagnetic materials, where the spins prefer to align parallel and antiparallel with their nearest neighbors, respectively. The spatial correlation between spins in such materials is shown in the figure to the right.
In statistics, the standard score is the signed number of standard deviations an observation or datum is above the mean. A positive standard score indicates a datum above the mean, while a negative standard score indicates a datum below the mean. It is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This conversion process is called standardizing or normalizing (however, "normalizing" can refer to many types of ratios; see normalization (statistics) for more). Standard scores are also called z-values, z-scores, normal scores, and standardized variables; the use of "Z" is because the normal distribution is also known as the "Z distribution". They are most frequently used to compare a sample to a standard normal deviate, though they can be defined without assumptions of normality. The z-score is only defined if one knows the population parameters; if one only has a sample set, then the analogous computation with sample mean and sample standard deviation yields the Student's t-statistic.
In statistics, the use of Bayes factors is a Bayesian alternative to classical hypothesis testing. Bayesian model comparison is a method of model selection based on Bayes factors.
In health care, a clinical trial is a comparison test of a medication or other medical treatment (such as a medical device), versus a placebo (inactive look-alike), other medications or devices, or the standard medical treatment for a patient's condition. To be ethical, researchers must obtain the full and voluntary informed consent of participating human subjects. If the subject is unable to consent for him/herself, researchers can seek consent from the subject's legally authorized representative. For a minor child this is typically a parent or guardian since as under the age of 18 cannot legally give consent to participate in a clinical trial.
MedCalc is a statistical software package designed for the biomedical sciences. It has an integrated spreadsheet for data input and can import files in several formats (Excel, SPSS, CSV, ...). MedCalc includes basic parametric and non-parametric statistical procedures and graphs such as descriptive statistics, ANOVA, Mann Whitney test, Wilcoxon test,  2 test, correlation, linear as well as non-linear regression, logistic regression, etc.  Survival analysis includes Cox regression (Proportional hazards model) and Kaplan Meier survival analysis. Procedures for method evaluation and method comparison include ROC curve analysis, Bland Altman plot, as well as Deming and Passing Bablok regression. The software also includes meta-analysis and sample size calculations. The first DOS version of MedCalc was released in April 1993 and the first version for Windows was available in November 1996. On 7 March 2007, version 9.3 obtained the Certified for Windows Vista logo.  Version 15.2 introduced a user-interface in English, Chinese (simplified and traditional), French, German, Italian, Japanese, Korean, Polish, Portuguese (Brazilian), Russian and Spanish.
The Lexis ratio is used in statistics as a measure which seeks to evaluate differences between the statistical properties of random mechanisms where the outcome is two-valued   for example "success" or "failure", "win" or "lose". The idea is that the probability of success might vary between different sets of trials in different situations. The measure compares the between-set variance of the sample proportions (evaluated for each set) with what the variance should be if there were no difference between in the true proportions of success across the different sets. Thus the measure is used to evaluate how data compares to a fixed-probability-of-success Bernoulli distribution. The term "Lexis ratio" is sometimes referred to as L or Q, where  Where  is the (weighted) sample variance derived from the observed proportions of success in sets in "Lexis trials" and  is the variance computed from the expected Bernoulli distribution on the basis of the overall average proportion of success. Trials where L falls significantly above or below 1 are known as supernormal and subnormal, respectively.
Trend estimation is a statistical technique to aid interpretation of data. When a series of measurements of a process are treated as a time series, trend estimation can be used to make and justify statements about tendencies in the data, by relating the measurements to the times at which they occurred. This model can then be used to describe the behaviour of the observed data. In particular, it may be useful to determine if measurements exhibit an increasing or decreasing trend which is statistically distinguished from random behaviour. Some examples are determining the trend of the daily average temperatures at a given location from winter to summer, and determining the trend in a global temperature series over the last 100 years. In the latter case, issues of homogeneity are important (for example, about whether the series is equally reliable throughout its length).
In probability theory, a Cox process, also known as a doubly stochastic Poisson process or mixed Poisson process, is a stochastic process which is a generalization of a Poisson process where the time-dependent intensity  (t) is itself a stochastic process. The process is named after the statistician David Cox, who first published the model in 1955. Cox processes are used to generate simulations of spike trains (the sequence of action potentials generated by a neuron), and also in financial mathematics where they produce a "useful framework for modeling prices of financial instruments in which credit risk is a significant factor."
GraphPad InStat is a commercial scientific statistics software published by GraphPad Software, Inc., a privately owned California corporation. InStat is available for both Windows and Macintosh computers.
This is a list of software to create any kind of information graphics: either includes the ability to create one or more infographics from a provided data set either it is provided specifically for information visualization  
Complete spatial randomness (CSR) describes a point process whereby point events occur within a given study area in a completely random fashion. It is synonymous with a homogeneous spatial Poisson process. Such a process is modeled using only one parameter , i.e. the density of points within the defined area. The term complete spatial randomness is commonly used in Applied Statistics in the context of examining certain point patterns, whereas in most other statistical contexts it is referred to the concept of a spatial Poisson process.
In statistics, a misleading graph, also known as a distorted graph, is a graph that misrepresents data, constituting a misuse of statistics and with the result that an incorrect conclusion may be derived from it. Graphs may be misleading through being excessively complex or poorly constructed. Even when constructed to accurately display the characteristics of their data, graphs can be subject to different interpretation. Misleading graphs may be created intentionally to hinder the proper interpretation of data or accidentally due to unfamiliarity with graphing software, misinterpretation of data, or because data cannot be accurately conveyed. Misleading graphs are often used in false advertising. One of the first authors to write about misleading graphs was Darrell Huff, publisher of the 1954 book How to Lie with Statistics. The field of data visualization describes ways to present information that avoids creating misleading graphs.
In archaeology, seriation is a relative dating method in which assemblages or artifacts from numerous sites, in the same culture, are placed in chronological order. Where absolute dating methods, such as carbon dating, cannot be applied, archaeologists have to use relative dating methods to date archaeological finds and features. Seriation is a standard method of dating in archaeology. It can be used to date stone tools, pottery fragments, and other artifacts. In Europe, it has been used frequently to reconstruct the chronological sequence of graves in a cemetery (e.g. J rgensen 1992; Mu ssemeier, Nieveler et al. 2003).
The Markov condition (sometimes called Markov assumption) for a Bayesian network states that any node in a Bayesian network is conditionally independent of its nondescendents, given its parents. A node is conditionally independent of the entire network, given its Markov blanket. The related causal Markov condition is that a phenomenon is independent of its noneffects, given its direct causes. In the event that the structure of a Bayesian network accurately depicts causality, the two conditions are equivalent. However, a network may accurately embody the Markov condition without depicting causality, in which case it should not be assumed to embody the causal Markov condition.
In queueing theory, a discipline within the mathematical theory of probability, the Gordon Newell theorem is an extension of Jackson's theorem from open queueing networks to closed queueing networks of exponential servers where customers cannot leave the network. Jackson's theorem cannot be applied to closed networks because the queue length at a node in the closed network is limited by the population of the network. The Gordon Newell theorem calculates the open network solution and then eliminates the infeasible states by renormalizing the probabilities. Calculation of the normalizing constant makes the treatment more awkward as the whole state space must be enumerated. Buzen's algorithm or mean value analysis can be used to calculate the normalizing constant more efficiently.
In statistics and applications of statistics, normalization can have a range of meanings. In the simplest cases, normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted values into alignment. In the case of normalization of scores in educational assessment, there may be an intention to align distributions to a normal distribution. A different approach to normalization of probability distributions is quantile normalization, where the quantiles of the different measures are brought into alignment. In another usage in statistics, normalization refers to the creation of shifted and scaled versions of statistics, where the intention is that these normalized values allow the comparison of corresponding normalized values for different datasets in a way that eliminates the effects of certain gross influences, as in an anomaly time series. Some types of normalization involve only a rescaling, to arrive at values relative to some size variable. In terms of levels of measurement, such ratios only make sense for ratio measurements (where ratios of measurements are meaningful), not interval measurements (where only distances are meaningful, but not ratios). In theoretical statistics, parametric normalization can often lead to pivotal quantities   functions whose sampling distribution does not depend on the parameters   and to ancillary statistics   pivotal quantities that can be computed from observations, without knowing parameters.
In economics, discrete choice models, or qualitative choice models, describe, explain, and predict choices between two or more discrete alternatives, such as entering or not entering the labor market, or choosing between modes of transport. Such choices contrast with standard consumption models in which the quantity of each good consumed is assumed to be a continuous variable. In the continuous case, calculus methods (e.g. first-order conditions) can be used to determine the optimum amount chosen, and demand can be modeled empirically using regression analysis. On the other hand, discrete choice analysis examines situations in which the potential outcomes are discrete, such that the optimum is not characterized by standard first-order conditions. Thus, instead of examining  how much  as in problems with continuous choice variables, discrete choice analysis examines  which one.  However, discrete choice analysis can also be used to examine the chosen quantity when only a few distinct quantities must be chosen from, such as the number of vehicles a household chooses to own  and the number of minutes of telecommunications service a customer decides to purchase. Techniques such as logistic regression and probit regression can be used for empirical analysis of discrete choice. Discrete choice models theoretically or empirically model choices made by people among a finite set of alternatives. The models have been used to examine, e.g., the choice of which car to buy, where to go to college, which mode of transport (car, bus, rail) to take to work among numerous other applications. Discrete choice models are also used to examine choices by organizations, such as firms or government agencies. In the discussion below, the decision-making unit is assumed to be a person, though the concepts are applicable more generally. Daniel McFadden won the Nobel prize in 2000 for his pioneering work in developing the theoretical basis for discrete choice. Discrete choice models statistically relate the choice made by each person to the attributes of the person and the attributes of the alternatives available to the person. For example, the choice of which car a person buys is statistically related to the person s income and age as well as to price, fuel efficiency, size, and other attributes of each available car. The models estimate the probability that a person chooses a particular alternative. The models are often used to forecast how people s choices will change under changes in demographics and/or attributes of the alternatives. Discrete choice models specify the probability that an individual chooses an option among a set of alternatives. The probabilistic description of discrete choice behavior is used not to reflect individual behavior that is viewed as intrinsically probabilistic. Rather, it is the lack of information that leads us to describe choice in a probabilistic fashion. In practice, we cannot know all factors affecting individual choice decisions as their determinants are partially observed or imperfectly measured. Therefore, discrete choice models rely on stochastic assumptions and specifications to account for unobserved factors related to a) choice alternatives, b) taste variation over people (interpersonal heterogeneity) and over time (intra-individual choice dynamics), and c) heterogeneous choice sets. The different formulations have been summarized and classified into groups of models.
Dynamic topic models are generative models that can be used to analyze the evolution of (unobserved) topics of a collection of documents over time. This family of models was proposed by David Blei and John Lafferty and is an extension to Latent Dirichlet Allocation (LDA) that can handle sequential documents. In LDA, both the order the words appear in a document and the order the documents appear in the corpus are oblivious to the model. Whereas words are still assumed to be exchangeable, in a dynamic topic model the order of the documents plays a fundamental role. More precisely, the documents are grouped by time slice (e.g.: years) and it is assumed that the documents of each group come from a set of topics that evolved from the set of the previous slice.
In statistical decision theory, an admissible decision rule is a rule for making a decision such that there is not any other rule that is always "better" than it. Generally speaking, in most decision problems the set of admissible rules is large, even infinite, so this is not a sufficient criterion to pin down a single rule, but as will be seen there are some good reasons to favor admissible rules; compare Pareto efficiency.
In mathematics, the  ukaszyk Karmowski metric is a function defining a distance between two random variables or two random vectors. This function is not a metric as it does not satisfy the identity of indiscernibles condition of the metric, that is for two identical arguments its value is greater than zero. The concept is named after Szymon  ukaszyk and Wojciech Karmowski.
A field experiment applies the scientific method to experimentally examine an intervention in the real world (or as many experimentalists like to say, naturally occurring environments) rather than in the laboratory. Field experiments, like lab experiments, generally randomize subjects (or other sampling units) into treatment and control groups and compare outcomes between these groups. Field experiments are so named in order to draw a contrast with laboratory experiments, which enforce scientific control by testing a hypothesis in the artificial and highly controlled setting of a laboratory. Often used in the social sciences, and especially in economic analyses of education and health interventions, field experiments have the advantage that outcomes are observed in a natural setting rather than in a contrived laboratory environment. For this reason, field experiments are sometimes seen as having higher external validity than laboratory experiments. However, like natural experiments, field experiments suffer from the possibility of contamination: experimental conditions can be controlled with more precision and certainty in the lab. Yet some phenomena (e.g., voter turnout in an election) cannot be easily studied in a laboratory. Examples include: Clinical trials of pharmaceuticals are one example of field experiments. Economists have used field experiments to analyze discrimination, health care programs, charitable fundraising, education, information aggregation in markets, and microfinance programs. Engineers often conduct field tests of prototype products to validate earlier laboratory tests and to obtain broader feedback.
In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable. In the Bayesian setting, the term MMSE more specifically refers to estimation with quadratic cost function. In such case, the MMSE estimator is given by the posterior mean of the parameter to be estimated. Since the posterior mean is cumbersome to calculate, the form of the MMSE estimator is usually constrained to be within a certain class of functions. Linear MMSE estimators are a popular choice since they are easy to use, calculate, and very versatile. It has given rise to many popular estimators such as the Wiener-Kolmogorov filter and Kalman filter.
Information geometry is a branch of mathematics that applies the techniques of differential geometry to the field of probability theory. This is done by taking probability distributions for a statistical model as the points of a Riemannian manifold, forming a statistical manifold. The Fisher information metric provides the Riemannian metric. Information geometry reached maturity through the work of Shun'ichi Amari and other Japanese mathematicians in the 1980s. Amari and Nagaoka's book, Methods of Information Geometry, is cited by most works of the relatively young field due to its broad coverage of significant developments attained using the methods of information geometry up to the year 2000. Many of these developments were previously only available in Japanese-language publications.
A quasi-experiment is an empirical study used to estimate the causal impact of an intervention on its target population. Quasi-experimental research shares similarities with the traditional experimental design or randomized controlled trial, but they specifically lack the element of random assignment to treatment or control. Instead, quasi-experimental designs typically allow the researcher to control the assignment to the treatment condition, but using some criterion other than random assignment (e.g., an eligibility cutoff mark). In some cases, the researcher may have control over assignment to treatment. Quasi-experiments are subject to concerns regarding internal validity, because the treatment and control groups may not be comparable at baseline. With random assignment, study participants have the same chance of being assigned to the intervention group or the comparison group. As a result, differences between groups on both observed and unobserved characteristics would be due to chance, rather than to a systematic factor related to treatment (e.g., illness severity). Randomization itself does not guarantee that groups will be equivalent at baseline. Any change in characteristics post-intervention is likely attributable to the intervention. With quasi-experimental studies, it may not be possible to convincingly demonstrate a causal link between the treatment condition and observed outcomes. This is particularly true if there are confounding variables that cannot be controlled or accounted for.
Combinatorial design theory is the part of combinatorial mathematics that deals with the existence, construction and properties of systems of finite sets whose arrangements satisfy generalized concepts of balance and/or symmetry. These concepts are not made precise so that a wide range of objects can be thought of as being under the same umbrella. At times this might involve the numerical sizes of set intersections as in block designs, while at other times it could involve the spatial arrangement of entries in an array as in Sudoku grids. Combinatorial design theory can be applied to the area of design of experiments. Some of the basic theory of combinatorial designs originated in the statistician Ronald Fisher's work on the design of biological experiments. Modern applications are also found in a wide gamut of areas including; Finite geometry, tournament scheduling, lotteries, mathematical biology, algorithm design and analysis, networking, group testing and cryptography.
Cohen's kappa coefficient is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since   takes into account the agreement occurring by chance.
In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0. The traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall: . The general formula for positive real   is: . The formula in terms of Type I and type II errors: . Two other commonly used F measures are the  measure, which weights recall higher than precision, and the  measure, which puts more emphasis on precision than recall. The F-measure was derived so that  "measures the effectiveness of retrieval with respect to a user who attaches   times as much importance to recall as precision". It is based on Van Rijsbergen's effectiveness measure . Their relationship is  where .
Per capita is a Latin prepositional phrase: per (preposition, taking the accusative case, meaning  by, by means of ) and capita (accusative plural of the noun caput,  head ). The phrase thus means  by heads  or  for each head , i.e., per individual/person. The term is used in a wide variety of social sciences and statistical research contexts, including government statistics, economic indicators, and built environment studies. It is commonly and usually used in the field of statistics in place of saying  for each person  or  per person . It is also used in wills to indicate that each of the named beneficiaries should receive, by devise or bequest, equal shares of the estate. This is in contrast to a per stirpes division, in which each branch (Latin stirps, plural stirpes) of the inheriting family inherits an equal share of the estate.
Business statistics is the science of good decision making in the face of uncertainty and is used in many disciplines such as financial analysis, econometrics, auditing, production and operations including services improvement, and marketing research. These sources feature regular repetitive publication of series of data. This makes the topic of time series especially important for business statistics. It is also a branch of applied statistics working mostly on data collected as a by-product of doing business or by government agencies. It provides knowledge and skills to interpret and use statistical techniques in a variety of business applications. A typical business statistics course is intended for business majors, and covers statistical study, descriptive statistics (collection, description, analysis, and summary of data), probability, and the binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation.
In many areas of information science, finding predictive relationships from data is a very important task. Initial discovery of relationships is usually done with a training set while a test set and validation set are used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. Test and training sets are used in intelligent systems, machine learning, genetic programming and statistics.
The convolution of probability distributions arises in probability theory and statistics as the operation in terms of probability distributions that corresponds to the addition of independent random variables and, by extension, to forming linear combinations of random variables. The operation here is a special case of convolution in the context of probability distributions.
In the mathematical study of stochastic processes, a Harris chain is a Markov chain where the chain returns to a particular part of the state space an unbounded number of times. Harris chains are regenerative processes and are named after Theodore Harris.
A time series is a sequence of data points made: 1) over a continuous time interval 2) out of successive measurements across that interval 3) using equal spacing between every two consecutive measurements 4) with each time unit within the time interval having at most one data point Examples of time series are ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average. Non-Examples: The height measurements of a group of people where each height is recorded over a period of time and each person has only one record in the data set. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). Yet a data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate. Time series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, intelligent transport and trajectory forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements. Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called "time series analysis", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Time series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.) Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).
In statistics, the coverage probability of a confidence interval is the proportion of the time that the interval contains the true value of interest. For example, suppose our interest is in the mean number of months that people with a particular type of cancer remain in remission following successful treatment with chemotherapy. The confidence interval aims to contain the unknown mean remission duration with a given probability. This is the "confidence level" or "confidence coefficient" of the constructed interval which is effectively the "nominal coverage probability" of the procedure for constructing confidence intervals. The "nominal coverage probability" is often set at 0.95. The coverage probability is the actual probability that the interval contains the true mean remission duration in this example. If all assumptions used in deriving a confidence interval are met, the nominal coverage probability will equal the coverage probability (termed "true" or "actual" coverage probability for emphasis). If any assumptions are not met, the actual coverage probability could either be less than or greater than the nominal coverage probability. When the actual coverage probability is greater than the nominal coverage probability, the interval is termed "conservative", if it is less than the nominal coverage probability, the interval is termed "anti-conservative", or "permissive." A discrepancy between the coverage probability and the nominal coverage probability frequently occurs when approximating a discrete distribution with a continuous one. The construction of binomial confidence intervals is a classic example where coverage probabilities rarely equal nominal levels. For the binomial case, several techniques for constructing intervals have been created. The Wilson or Score confidence interval is one well known construction based on the normal distribution. Other constructions include the Wald, exact, Agresti-Coull, and likelihood intervals. While the Wilson interval may not be the most conservative estimate, it produces average coverage probabilities that are equal to nominal levels while still producing a comparatively narrow confidence interval. The "probability" in coverage probability is interpreted with respect to a set of hypothetical repetitions of the entire data collection and analysis procedure. In these hypothetical repetitions, independent data sets following the same probability distribution as the actual data are considered, and a confidence interval is computed from each of these data sets; see Neyman construction.
Demography (from prefix demo- from Ancient Greek        de mos, meaning "the people", and -graphy from        grapho , implies "writing, description or measurement") is the statistical study of populations, especially human beings. As a very general science, it can analyze any kind of dynamic living population, i.e., one that changes over time or space (see population dynamics). Demography encompasses the study of the size, structure, and distribution of these populations, and spatial or temporal changes in them in response to birth, migration, ageing, and death. Based on the demographic research of the earth, earth's population up to the year 2050 and 2100 can be estimated by demographers. Demographics are quantifiable characteristics of a given population. Demographic analysis can cover whole societies, or groups defined by criteria such as education, nationality, religion and ethnicity. Educational institutions usually treat demography as a field of sociology, though there are a number of independent demography departments. Formal demography limits its object of study to the measurement of population processes, while the broader field of social demography or population studies also analyzes the relationships between economic, social, cultural and biological processes influencing a population.
In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a conditional distribution, which gives the probabilities contingent upon the values of the other variables. The term marginal variable is used to refer to those variables in the subset of variables being retained. These terms are dubbed "marginal" because they used to be found by summing values in a table along rows or columns, and writing the sum in the margins of the table. The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out. The context here is that the theoretical studies being undertaken, or the data analysis being done, involves a wider set of random variables but that attention is being limited to a reduced number of those variables. In many applications an analysis may start with a given collection of random variables, then first extend the set by defining new ones (such as the sum of the original random variables) and finally reduce the number by placing interest in the marginal distribution of a subset (such as the sum). Several different analyses may be done, each treating a different subset of variables as the marginal variables.
In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of  successes in  draws, without replacement, from a finite population of size  that contains exactly  successes, wherein each draw is either a success or a failure. In contrast, the binomial distribution describes the probability of  successes in  draws with replacement. In statistics, the hypergeometric test uses the hypergeometric distribution to calculate the statistical significance of having drawn a specific  successes (out of  total draws) from the aforementioned population. The test is often used to identify which sub-populations are over- or under-represented in a sample. This test has a wide range of applications. For example, a marketing group could use the test to understand their customer base by testing a set of known customers for over-representation of various demographic subgroups (e.g., women, people under 30).
Stress majorization is an optimization strategy used in multidimensional scaling (MDS) where, for a set of n m-dimensional data items, a configuration X of n points in r(<<m)-dimensional space is sought that minimizes the so-called stress function . Usually r is 2 or 3, i.e. the (r x n) matrix X lists points in 2- or 3-dimensional Euclidean space so that the result may be visualised (i.e. an MDS plot). The function  is a cost or loss function that measures the squared differences between ideal (-dimensional) distances and actual distances in r-dimensional space. It is defined as:  where  is a weight for the measurement between a pair of points ,  is the euclidean distance between  and  and  is the ideal distance between the points (their separation) in the -dimensional data space. Note that  can be used to specify a degree of confidence in the similarity between points (e.g. 0 can be specified if there is no information for a particular pair). A configuration  which minimizes  gives a plot in which points that are close together correspond to points that are also close together in the original -dimensional data space. There are many ways that  could be minimized. For example, Kruskal recommended an iterative steepest descent approach. However, a significantly better (in terms of guarantees on, and rate of, convergence) method for minimizing stress was introduced by Jan de Leeuw. De Leeuw's iterative majorization method at each step minimizes a simple convex function which both bounds  from above and touches the surface of  at a point , called the supporting point. In convex analysis such a function is called a majorizing function. This iterative majorization process is also referred to as the SMACOF algorithm ("Scaling by majorizing a convex function").
The Armitage Doll model is a statistical model of carcinogenesis, proposed in 1954 by Peter Armitage and Richard Doll, which suggested that a sequence of multiple distinct genetic events preceded the onset of cancer. The original paper has recently been reprinted with a set of commentary articles.
In statistics, the Bhattacharyya distance measures the similarity of two discrete or continuous probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations. Both measures are named after Anil Kumar Bhattacharya, a statistician who worked in the 1930s at the Indian Statistical Institute. The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same. Therefore, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, however, the Bhattacharyya distance would grow depending on the difference between the standard deviations.
Inference is the act or process of deriving logical conclusions from premises known or assumed to be true. The laws of valid inference are studied in the field of logic. Alternatively, inference is defined as the non-logical, but rational means, through observation of patterns of facts, to indirectly see new meanings and contexts for understanding. Of particular use to this application of inference are anomalies and symbols. Inference, in this sense, does not draw conclusions but opens new paths for inquiry. (See second set of examples.) In this definition of inference, there are two types of inference: inductive inference and deductive inference. Unlike the definition of inference in the first paragraph above, meaning of word meanings are not tested but meaningful relationships are articulated. Human inference (i.e. how humans draw conclusions) is traditionally studied within the field of cognitive psychology; artificial intelligence researchers develop automated inference systems to emulate human inference. Statistical inference uses mathematics to draw conclusions in the presence of uncertainty. This generalizes deterministic reasoning, with the absence of uncertainty as a special case. Statistical inference uses quantitative or qualitative (categorical) data which may be subject to random variation.
In queueing theory, a discipline within the mathematical theory of probability, an M/M/1 queue represents the queue length in a system having a single server, where arrivals are determined by a Poisson process and job service times have an exponential distribution. The model name is written in Kendall's notation. The model is the most elementary of queueing models and an attractive object of study as closed-form expressions can be obtained for many metrics of interest in this model. An extension of this model with more than one server is the M/M/c queue.
Prognostics is an engineering discipline focused on predicting the time at which a system or a component will no longer perform its intended function. This lack of performance is most often a failure beyond which the system can no longer be used to meet desired performance. The predicted time then becomes the remaining useful life (RUL), which is an important concept in decision making for contingency mitigation. Prognostics predicts the future performance of a component by assessing the extent of deviation or degradation of a system from its expected normal operating conditions. The science of prognostics is based on the analysis of failure modes, detection of early signs of wear and aging, and fault conditions. An effective prognostics solution is implemented when there is sound knowledge of the failure mechanisms that are likely to cause the degradations leading to eventual failures in the system. It is therefore necessary to have initial information on the possible failures (including the site, mode, cause and mechanism) in a product. Such knowledge is important to identify the system parameters that are to be monitored. Potential uses for prognostics is in condition-based maintenance. The discipline that links studies of failure mechanisms to system lifecycle management is often referred to as prognostics and health management (PHM), sometimes also system health management (SHM) or   in transportation applications   vehicle health management (VHM) or engine health management (EHM). Technical approaches to building models in prognostics can be categorized broadly into data-driven approaches, model-based approaches, and hybrid approaches.
Spatial analysis or spatial statistics includes any of the formal techniques which study entities using their topological, geometric, or geographic properties. Spatial analysis includes a variety of techniques, many still in their early development, using different analytic approaches and applied in fields as diverse as astronomy, with its studies of the placement of galaxies in the cosmos, to chip fabrication engineering, with its use of "place and route" algorithms to build complex wiring structures. In a more restricted sense, spatial analysis is the technique applied to structures at the human scale, most notably in the analysis of geographic data. Complex issues arise in spatial analysis, many of which are neither clearly defined nor completely resolved, but form the basis for current research. The most fundamental of these is the problem of defining the spatial location of the entities being studied. Classification of the techniques of spatial analysis is difficult because of the large number of different fields of research involved, the different fundamental approaches which can be chosen, and the many forms the data can take.
Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements. For example, it is desired to estimate the proportion of a population of voters who will vote for a particular candidate. That proportion is the parameter sought; the estimate is based on a small random sample of voters. Or, for example, in radar the goal is to estimate the range of objects (airplanes, boats, etc.) by analyzing the two-way transit timing of received echoes of transmitted pulses. Since the reflected pulses are unavoidably embedded in electrical noise, their measured values are randomly distributed, so that the transit time must be estimated. In estimation theory, two approaches are generally considered.  The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector. For example, in electrical communication theory, the measurements which contain information regarding the parameters of interest are often associated with a noisy signal. Without randomness, or noise, the problem would be deterministic and estimation would not be needed.
In statistics, a Tsallis distribution is a probability distribution derived from the maximization of the Tsallis entropy under appropriate constraints. There are several different families of Tsallis distributions, yet different sources may reference an individual family as "the Tsallis distribution". The q-Gaussian is a generalization of the Gaussian in the same way that Tsallis entropy is a generalization of standard Boltzmann Gibbs entropy or Shannon entropy. Similarly, if the domain of the variable is constrained to be positive in the maximum entropy procedure, the q-exponential distribution is derived. The Tsallis distributions have been applied to problems in the fields of statistical mechanics, geology, anatomy, astronomy, economics, finance, and machine learning. The distributions are often used for their heavy tails. Note that Tsallis distributions are obtained as Box-Cox transformation over usual distributions, with deformation parameter . This deformation transforms exponentials into q-exponentials.
In the statistical theory of the design of experiments, blocking is the arranging of experimental units in groups (blocks) that are similar to one another.
A scatter plot (also called a scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are color-coded you can increase the number of displayed variables to three. The data is displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.
In psychometrics, item response theory (IRT), also known as latent trait theory, strong true score theory, or modern mental test theory, is a paradigm for the design, analysis, and scoring of tests, questionnaires, and similar instruments measuring abilities, attitudes, or other variables. Unlike simpler alternatives for creating scales evaluating questionnaire responses it does not assume that each item is equally difficult. This distinguishes IRT from, for instance, the assumption in Likert scaling that "All items are assumed to be replications of each other or in other words items are considered to be parallel instruments" (p. 197). By contrast, item response theory treats the difficulty of each item (the ICCs) as information to be incorporated in scaling items. ICC stands for item characteristic curve. It is based on the application of related mathematical models to testing data. Because it is generally regarded as superior to classical test theory, it is the preferred method for developing scales in the United States, especially when optimal decisions are demanded, as in so-called high-stakes tests, e.g., the Graduate Record Examination (GRE) and Graduate Management Admission Test (GMAT). The name item response theory is due to the focus of the theory on the item, as opposed to the test-level focus of classical test theory. Thus IRT models the response of each examinee of a given ability to each item in the test. The term item is generic: covering all kinds of informative item. They might be multiple choice questions that have incorrect and correct responses, but are also commonly statements on questionnaires that allow respondents to indicate level of agreement (a rating or Likert scale), or patient symptoms scored as present/absent, or diagnostic information in complex systems. IRT is based on the idea that the probability of a correct/keyed response to an item is a mathematical function of person and item parameters. The person parameter is construed as (usually) a single latent trait or dimension. Examples include general intelligence or the strength of an attitude. Parameters on which items are characterized include their difficulty (known as "location" for their location on the difficulty range), discrimination (slope or correlation) representing how steeply the rate of success of individuals varies with their ability, and a pseudoguessing parameter, characterising the (lower) asymptote at which even the least able persons will score due to guessing (for instance, 25% for pure chance on a multiple choice item with four possible responses).
In psychometrics, an anchor test is a common set of test items administered in combination with two or more alternative forms of the test with the aim of establishing the equivalence of the test scores on the alternative forms. The purpose of the anchor test is to provide a baseline for an equating analysis between different forms of a test. Anchor test is one type of psychological assessment tool to measure an individual's knowledge or cognitive ability by testing the same areas in different ways. In psychometrics, to develop assessment tools that are reliable for testing certain skills and abilities are what most Psychometricists are interested in. Anchor tests are not intended to test the subject's ability to take tests, interpret questions, or understand a concept that is unrelated to the test questions. Instead, it eliminates the incongruency between what the test is designed to assess and what it actually assesses. Subjects will be tested on the same knowledge and skills in multiple ways in an anchor test. Compared with traditional tests in both education and psychology, anchor tests are intended to find out what an individual is able to do rather than what an individual is unable to do. A study examined that higher anchor test to total test correlation leads to better equating then implies that an anchor test with items of medium difficulty may lead to better equating.
In mathematics, in the area of combinatorial designs, an orthogonal array is a "table" (array) whose entries come from a fixed finite set of symbols (typically, {1,2,...,n}), arranged in such a way that there is an integer t so that for every selection of t columns of the table, all ordered t-tuples of the symbols, formed by taking the entries in each row restricted to these columns, appear the same number of times. The number t is called the strength of the orthogonal array. Here is a simple example of an orthogonal array with symbol set {1,2}:  Notice that the four ordered pairs (2-tuples) formed by the rows restricted to the first and third columns, namely (1,1), (2,1), (1,2) and (2,2) are all the possible ordered pairs of the two element set and each appears exactly once. The second and third columns would give, (1,1), (2,1), (2,2) and (1,2); again, all possible ordered pairs each appearing once. The same statement would hold had the first and second columns been used. This is thus an orthogonal array of strength two. Orthogonal arrays generalize the idea of mutually orthogonal latin squares in a tabular form. These arrays have many connections to other combinatorial designs and have applications in the statistical design of experiments, coding theory, cryptography and various types of software testing.
A bar chart or bar graph is a chart that presents grouped data with rectangular bars with lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column bar chart. A bar graph is a chart that uses either horizontal or vertical bars to show comparisons among categories. One axis of the chart shows the specific categories being compared, and the other axis represents a discrete value. Some bar graphs present bars clustered in groups of more than one (grouped bar graphs), and others show the bars divided into subparts to show cumulative effect (stacked bar graphs).
In finance, the Sharpe ratio (also known as the Sharpe index, the Sharpe measure, and the reward-to-variability ratio) is a way to examine the performance of an investment by adjusting for its risk. The ratio measures the excess return (or risk premium) per unit of deviation in an investment asset or a trading strategy, typically referred to as risk (and is a deviation risk measure), named after William F. Sharpe.
The leftover hash lemma is a lemma in cryptography first stated by Russell Impagliazzo, Leonid Levin, and Michael Luby. Imagine that you have a secret key  that has  uniform random bits, and you would like to use this secret key to encrypt a message. Unfortunately, you were a bit careless with the key, and know that an adversary was able to learn about  bits of that key, but you do not know which. Can you still use your key, or do you have to throw it away and choose a new key  The leftover hash lemma tells us that we can produce a key of about  bits, over which the adversary has almost no knowledge. Since the adversary knows all but  bits, this is almost optimal. More precisely, the leftover hash lemma tells us that we can extract a length asymptotic to  (the min-entropy of ) bits from a random variable  that are almost uniformly distributed. In other words, an adversary who has some partial knowledge about , will have almost no knowledge about the extracted value. That is why this is also called privacy amplification (see privacy amplification section in the article Quantum key distribution). Randomness extractors achieve the same result, but use (normally) less randomness. Let  be a random variable over  and let . Let  be a 2-universal hash function. If  then for  uniform over  and independent of , we have  where  is uniform over  and independent of .  is the Min-entropy of , which measures the amount of randomness  has. The min-entropy is always less than or equal to the Shannon entropy. Note that  is the probability of correctly guessing . (The best guess is to guess the most probable value.) Therefore, the min-entropy measures how difficult it is to guess .  is a statistical distance between  and .
In combinatorics and in experimental design, a Latin square is an n   n array filled with n different symbols, each occurring exactly once in each row and exactly once in each column. Here is an example: The name "Latin square" was inspired by mathematical papers by Leonhard Euler, who used Latin characters as symbols. Other symbols can be used instead of Latin letters: in the above example, the alphabetic sequence A, B, C can be replaced by the integer sequence 1, 2, 3.
In statistics and epidemiology, relative risk or risk ratio (RR) is the ratio of the probability of an event occurring (for example, developing a disease, being injured) in an exposed group to the probability of the event occurring in a comparison, non-exposed group. Relative risk includes two important features: (i) a comparison of risk between two "exposures" puts risks in context, and (ii) "exposure" is ensured by having proper denominators for each group representing the exposure   Consider an example where the probability of developing lung cancer among smokers was 20% and among non-smokers 1%. This situation is expressed in the 2   2 table to the right. Here, a = 20, b = 80, c = 1, and d = 99. Then the relative risk of cancer associated with smoking would be  Smokers would be twenty times as likely as non-smokers to develop lung cancer. The alternative term risk ratio is sometimes used because it is the ratio of the risk in the exposed divided by the risk in the unexposed. Relative risk contrasts with the actual or absolute risk, and may be confused with it in the media or elsewhere.
Probabilistic forecasting summarizes what is known about, or opinions about, future events. In contrast to single-valued forecasts (such as forecasting that the maximum temperature at given site on a given day will be 23 degrees Celsius, or that the result in a given football match will be a no-score draw), probabilistic forecasts assign a probability to each of a number of different outcomes, and the complete set of probabilities represents a probability forecast. Thus, probabilistic forecasting is a type of probabilistic classification. Weather forecasting represents a service in which probability forecasts are sometimes published for public consumption, although it may also be used by weather forecasters as the basis of a simpler type of forecast. For example forecasters may combine their own experience together with computer-generated probability forecasts to construct a forecast of the type "we expect heavy rainfall". Sports betting is another field of application where probabilistic forecasting can play a role. The pre-race odds published for a horse race can be considered to correspond to a summary of bettors' opinions about the likely outcome of a race, although this needs to be tempered with caution as bookmakers' profits needs to be taken into account. In sports betting, probability forecasts may not be published as such, but may underlie bookmakers' activities in setting pay-off rates, etc..
In multilinear algebra, the tensor rank decomposition or canonical polyadic decomposition (CPD) may be regarded as a generalization of the matrix singular value decomposition (SVD) to tensors, which has found application in statistics, signal processing, psychometrics, linguistics and chemometrics. It was introduced by Hitchcock in 1927 and later rediscovered several times, notably in psychometrics. For this reason, the tensor rank decomposition is sometimes historically referred to as PARAFAC or CANDECOMP.
In Bayesian probability theory, if the posterior distributions p( |x) are in the same family as the prior probability distribution p( ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian. This means that the Gaussian distribution is a conjugate prior for the likelihood that is also Gaussian. The concept, as well as the term "conjugate prior", were introduced by Howard Raiffa and Robert Schlaifer in their work on Bayesian decision theory. A similar concept had been discovered independently by George Alfred Barnard. Consider the general problem of inferring a distribution for a parameter   given some datum or data x. From Bayes' theorem, the posterior distribution is equal to the product of the likelihood function  and prior , normalized (divided) by the probability of the data :  Let the likelihood function be considered fixed; the likelihood function is usually well-determined from a statement of the data-generating process. It is clear that different choices of the prior distribution p( ) may make the integral more or less difficult to calculate, and the product p(x| )   p( ) may take one algebraic form or another. For certain choices of the prior, the posterior has the same algebraic form as the prior (generally with different parameter values). Such a choice is a conjugate prior. A conjugate prior is an algebraic convenience, giving a closed-form expression for the posterior; otherwise a difficult numerical integration may be necessary. Further, conjugate priors may give intuition, by more transparently showing how a likelihood function updates a prior distribution. All members of the exponential family have conjugate priors.
Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors. It is usually defined as follows:  where At is the actual value and Ft is the forecast value. The absolute difference between At and Ft is divided by half the sum of absolute values of the actual value At and the forecast value Ft. The value of this calculation is summed for every fitted point t and divided again by the number of fitted points n. The earliest reference to similar formula appears to be Armstrong (1985, p. 348) where it is called "adjusted MAPE" and is defined without the absolute values in denominator. It has been later discussed, modified and re-proposed by Flores (1986). The final version above is due to Makridakis (1993). Armstrong's original definition is as follows:  The problem is that it can be negative (if ) or even infinite (if ). Therefore the currently accepted version of SMAPE assumes the absolute values in the denominator. In contrast to the mean absolute percentage error, SMAPE has both a lower bound and an upper bound. Indeed, the formula above provides a result between 0% and 200%. However a percentage error between 0% and 100% is much easier to interpret. That is the reason why the formula below is often used in practice (i.e. no factor 0.5 in denominator):  One supposed problem with SMAPE is that it is not symmetric since over- and under-forecasts are not treated equally. Let's consider the following example by applying the second SMAPE formula: Over-forecasting: At = 100 and Ft = 110 give SMAPE = 4.76% Under-forecasting: At = 100 and Ft = 90 give SMAPE = 5.26%. However, one should only expect this type of symmetry for measures which are entirely difference-based and not relative (such as mean squared error and mean absolute deviation). There is a third version of SMAPE, which allows to measure the direction of the bias in the data by generating a positive and a negative error on line item level. Furthermore it is better protected against outliers and the bias effect mentioned in the previous paragraph than the two other formulas. The formula is:  A limitation to SMAPE is that if the actual value or forecast value is 0, the value of error will boom up to the upper-limit of error. (200% for the first formula and 100% for the second formula). Provided the data are strictly positive, a better measure of relative accuracy can be obtained based on the log of the accuracy ratio: log(Ft / At) This measure is easier to analyse statistically, and has valuable symmetry and unbiasedness properties. When used in constructing forecasting models the resulting prediction corresponds to the geometric mean (Tofallis, 2015).
In statistics, an expectation maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.
Cluster sampling is a sampling technique used when "natural" but relatively heterogeneous groupings are evident in a statistical population. It is often used in marketing research. In this technique, the total population is divided into these groups (or clusters) and a simple random sample of the groups is selected. The elements in each cluster are then sampled. If all elements in each cluster are sampled, then this is referred to as a "one-stage" cluster design. If a simple random subsample of elements is selected within each of these groups, this is referred to as a "two-stage" design. A common motivation for cluster sampling is to reduce the total number of interviews and costs given the desired accuracy. Assuming a fixed sample size, the technique gives more accurate results when most of the variation in the population is within the groups, not between them.  
Square root biased sampling is a sampling method proposed by William H. Press, a computer scientist and computational biologist, for use in airport screenings. It is the mathematically optimal compromise between simple random sampling and strong profiling that most quickly finds a rare malfeasor, given fixed screening resources. Using this method, if a group is  times as likely as the average to be a security risk, then persons from that group will be  times as likely to undergo additional screening. For example, if someone from a profiled group is nine times more likely than the average person to be a security risk, then when using square root biased sampling, people from the profiled group would be screened three times more often than the average person.
The Dagum distribution is a continuous probability distribution defined over positive real numbers. It is named after Camilo Dagum, who proposed it in a series of papers in the 1970s. The Dagum distribution arose from several variants of a new model on the size distribution of personal income and is mostly associated with the study of income distribution. There is both a three-parameter specification (Type I) and a four-parameter specification (Type II) of the Dagum distribution; a summary of the genesis of this distribution can be found in "A Guide to the Dagum Distributions". A general source on statistical size distributions often cited in work using the Dagum distribution is Statistical Size Distributions in Economics and Actuarial Sciences.  
Data dredging (also data fishing, data snooping, equation fitting and p-hacking) is the use of data mining to uncover patterns in data that can be presented as statistically significant, without first devising a specific hypothesis as to the underlying causality. The process of data mining involves automatically testing huge numbers of hypotheses about a single data set by exhaustively searching for combinations of variables that might show a correlation. Conventional tests of statistical significance are based on the probability that an observation arose by chance, and necessarily accept some risk of mistaken test results, called the significance. When large numbers of tests are performed, some produce false results, hence 5% of randomly chosen hypotheses turn out to be significant at the 5% level, 1% turn out to be significant at the 1% significance level, and so on, by chance alone. When enough hypotheses are tested, it is virtually certain that some falsely appear statistically significant, since almost every data set with any degree of randomness is likely to contain some spurious correlations. If they are not cautious, researchers using data mining techniques can be easily misled by these apparently significant results. The multiple comparisons hazard is common in data dredging. Moreover, subgroups are sometimes explored without alerting the reader to the number of questions at issue, which can lead to misinformed conclusions.
In applied statistics, regression-Kriging (RK) is a spatial prediction technique that combines a regression of the dependent variable on auxiliary variables (such as parameters derived from digital elevation modelling, remote sensing/imagery, and thematic maps) with Kriging of the regression residuals. It is mathematically equivalent to the interpolation method variously called universal Kriging and Kriging with external drift, where auxiliary predictors are used directly to solve the Kriging weights.
In probability theory and directional statistics, a circular uniform distribution is a probability distribution on the unit circle whose density is uniform for all angles.
Kuiper's test is used in statistics to test that whether a given distribution, or family of distributions, is contradicted by evidence from a sample of data. It is named after Dutch mathematician Nicolaas Kuiper. Kuiper's test is closely related to the better-known Kolmogorov Smirnov test (or K-S test as it is often called). As with the K-S test, the discrepancy statistics D+ and D  represent the absolute sizes of the most positive and most negative differences between the two cumulative distribution functions that are being compared. The trick with Kuiper's test is to use the quantity D+ + D  as the test statistic. This small change makes Kuiper's test as sensitive in the tails as at the median and also makes it invariant under cyclic transformations of the independent variable. The Anderson Darling test is another test that provides equal sensitivity at the tails as the median, but it does not provide the cyclic invariance. This invariance under cyclic transformations makes Kuiper's test invaluable when testing for cyclic variations by time of year or day of the week or time of day, and more generally for testing the fit of, and differences between, circular probability distributions.
Incidence in epidemiology is a measure of the probability of occurrence of a given medical condition in a population within a specified period of time. Although sometimes loosely expressed simply as the number of new cases during some time period, it is better expressed as a proportion or a rate with a denominator. Incidence proportion (also known as cumulative incidence) is the number of new cases within a specified time period divided by the size of the population initially at risk. For example, if a population initially contains 1,000 non-diseased persons and 28 develop a condition over two years of observation, the incidence proportion is 28 cases per 1,000 persons per two years, i.e. 2.8%.
Directional statistics (also circular statistics or spherical statistics) is the subdiscipline of statistics that deals with directions (unit vectors in Rn), axes (lines through the origin in Rn) or rotations in Rn. More generally, directional statistics deals with observations on compact Riemannian manifolds.  The fact that 0 degrees and 360 degrees are identical angles, so that for example 180 degrees is not a sensible mean of 2 degrees and 358 degrees, provides one illustration that special statistical methods are required for the analysis of some types of data (in this case, angular data). Other examples of data that may be regarded as directional include statistics involving temporal periods (e.g. time of day, week, month, year, etc.), compass directions, dihedral angles in molecules, orientations, rotations and so on.
In statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a "false positive"), while a type II error is the failure to reject a false null hypothesis (a "false negative"). More simply stated, a type I error is detecting an effect that is not present, while a type II error is failing to detect an effect that is present. The terms "type I error" and "type II error" are often used interchangeably with the general notion of false positives and false negatives in binary classification, such as medical testing, but narrowly speaking refer specifically to statistical hypothesis testing in the Neyman Pearson framework, as discussed in this article.
In mathematics, the Schuette Nesbitt formula is a generalization of the inclusion exclusion principle. It is named after Donald R. Schuette and Cecil J. Nesbitt. The probabilistic version of the Schuette Nesbitt formula has practical applications in actuarial science, where it is used to calculate the net single premium for life annuities and life insurances based on the general symmetric status.
In statistics, a ranklet is an orientation-selective non-parametric feature which is based on the computation of Mann Whitney Wilcoxon (MWW) rank-sum test statistics. Ranklets achieve similar response to Haar wavelets as they share the same pattern of orientation-selectivity, multi-scale nature and a suitable notion of completeness. Rank-based (non-parametric) features have become popular in the field of image processing for their robustness in detecting outliers and invariance to monotonic transformations such as brightness, contrast changes and gamma correction. The MWW is a combination of Wilcoxon rank-sum test and Mann Whitney U-test. It is a non-parametric alternative to the t-test used to test the hypothesis for the comparison of two independent distributions. It assesses whether two samples of observations, usually referred as Treatment T and Control C, come from the same distribution but do not have to be normally distributed. The Wilcoxon rank-sum statistics Ws is determined as:  Subsequently, let MW be the Mann Whitney statistics defined by:  where m is the number of Treatment values. A ranklet R is defined as the normalization of MW in the range [ 1, +1]:  where a positive value means that the Treatment region is brighter than the Control region, and a negative value otherwise.
In the statistical analysis of time series, a stochastic process is trend stationary if an underlying trend (function solely of time) can be removed, leaving a stationary process.
Clustering is the assignment of objects into groups (called clusters) so that objects from the same cluster are more similar to each other than objects from different clusters. Often similarity is assessed according to a distance measure. Clustering is a common technique for statistical data analysis, which is used in many fields, including machine learning, data mining, pattern recognition, image analysis and bioinformatics. Consensus clustering has emerged as an important elaboration of the classical clustering problem. Consensus clustering, also called aggregation of clustering (or partitions), refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Consensus clustering is thus the problem of reconciling clustering information about the same data set coming from different sources or from different runs of the same algorithm. When cast as an optimization problem, consensus clustering is known as median partition, and has been shown to be NP-complete. Consensus clustering for unsupervised learning is analogous to ensemble learning in supervised learning.
In decision theory, a score function, or scoring rule, measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes. The set of possible outcomes can be either binary or categorical in nature, and the probabilities assigned to this set of outcomes must sum to one (where each individual probability is in the range of 0 to 1). A score can be thought of as either a measure of the "calibration" of a set of probabilistic predictions, or as a "cost function" or "loss function". If a cost is levied in proportion to a proper scoring rule, the minimal expected cost corresponds to reporting the true set of probabilities. Proper scoring rules are used in meteorology, finance, and pattern classification where a forecaster or algorithm will attempt to minimize the average score to yield refined, calibrated probabilities (i.e. accurate probabilities). Various scoring rules have also been used to assess the predictive accuracy of forecast models for association football.
In statistics, the Gauss Markov theorem, named after Carl Friedrich Gauss and Andrey Markov, states that in a linear regression model in which the errors have expectation zero and are uncorrelated and have equal variances, the best linear unbiased estimator (BLUE) of the coefficients is given by the ordinary least squares (OLS) estimator. Here "best" means giving the lowest variance of the estimate, as compared to other unbiased, linear estimators. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance). The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the James Stein estimator (which also drops linearity) or ridge regression.
In probability theory, a product-form solution is a particularly efficient form of solution for determining some metric of a system with distinct sub-components, where the metric for the collection of components can be written as a product of the metric across the different components. Using capital Pi notation a product-form solution has algebraic form  where B is some constant. Solutions of this form are of interest as they are computationally inexpensive to evaluate for large values of n. Such solutions in queueing networks are important for finding performance metrics in models of multiprogrammed and time-shared computer systems.
LOESS and LOWESS (locally weighted scatterplot smoothing) are two strongly related non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model. "LOESS" is a later generalization of LOWESS; although it is not a true initialism, it may be understood as standing for "LOcal regrESSion". LOESS and LOWESS thus build on "classical" methods, such as linear and nonlinear least squares regression. They address situations in which the classical procedures do not perform well or cannot be effectively applied without undue labor. LOESS combines much of the simplicity of linear least squares regression with the flexibility of nonlinear regression. It does this by fitting simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point. In fact, one of the chief attractions of this method is that the data analyst is not required to specify a global function of any form to fit a model to the data, only to fit segments of the data. The trade-off for these features is increased computation. Because it is so computationally intensive, LOESS would have been practically impossible to use in the era when least squares regression was being developed. Most other modern methods for process modeling are similar to LOESS in this respect. These methods have been consciously designed to use our current computational ability to the fullest possible advantage to achieve goals not easily achieved by traditional approaches. A smooth curve through a set of data points obtained with this statistical technique is called a Loess Curve, particularly when each smoothed value is given by a weighted quadratic least squares regression over the span of values of the y-axis scattergram criterion variable. When each smoothed value is given by a weighted linear least squares regression over the span, this is known as a Lowess curve; however, some authorities treat Lowess and Loess as synonyms.
In probability and statistics, the reciprocal distribution is a continuous probability distribution. It is characterised by its probability density function, within the support of the distribution, being proportional to the reciprocal of the variable. The reciprocal distribution is an example of an inverse distribution, and the reciprocal (inverse) of a random variable with a reciprocal distribution itself has a reciprocal distribution.  
Resentful demoralization is an issue in controlled experiments in which those in the control group become resentful of not receiving the experimental treatment. Alternatively, the experimental group could be resentful of the control group, if the experimental group perceive its treatment as inferior. They may become angry, depressed, uncooperative, or non-compliant. This may lead to significant systematic differences in the outcome of the control group, obscuring the results of the study and threatening their validity.
Tukey's range test, also known as the Tukey's test, Tukey method, Tukey's honest significance test, Tukey's HSD (honest significant difference) test, or the Tukey Kramer method, is a single-step multiple comparison procedure and statistical test. It can be used on raw data or in conjunction with an ANOVA (Post-hoc analysis) to find means that are significantly different from each other. Named after John Tukey, it compares all possible pairs of means, and is based on a studentized range distribution (q) (this distribution is similar to the distribution of t from the t-test. See below). The Tukey HSD tests should not be confused with the Tukey Mean Difference tests (also known as the Bland Altman test). Tukey's test compares the means of every treatment to the means of every other treatment; that is, it applies simultaneously to the set of all pairwise comparisons  and identifies any difference between two means that is greater than the expected standard error. The confidence coefficient for the set, when all sample sizes are equal, is exactly 1    . For unequal sample sizes, the confidence coefficient is greater than 1    . In other words, the Tukey method is conservative when there are unequal sample sizes.
In probability theory and statistics, the chi-squared distribution (also chi-square or  2-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. It is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, e.g., in hypothesis testing or in construction of confidence intervals. When it is being distinguished from the more general noncentral chi-squared distribution, this distribution is sometimes called the central chi-squared distribution. The chi-squared distribution is used in the common chi-squared tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a sample standard deviation. Many other statistical tests also use this distribution, like Friedman's analysis of variance by ranks.
Stochastic approximation methods are a family of iterative stochastic optimization algorithms that attempt to find zeroes or extrema of functions which cannot be computed directly, but only estimated via noisy observations. Mathematically, this refers to solving:  where the objective is to find the parameter , which minimizes  for some unknown random variable, . Denoting  as the dimension of the parameter , we can assume that while the domain  is known, the objective function, , cannot be computed exactly, but instead approximated via simulation. This can be intuitively explained as follows.  is the original function we want to minimize. However, due to noise,  can not be evaluated exactly. This situation is modeled by the function , where  represents the noise and is a random variable. Since  is a random variable, so is the value of . The objective is then to minimize , but through evaluating . A reasonable way to do this is to minimize the expectation of , i.e., . The first, and prototypical, algorithms of this kind are the Robbins-Monro and Kiefer-Wolfowitz algorithms introduced respectively in 1951 and 1952.
In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.
In probability theory and statistics, the noncentral beta distribution is a continuous probability distribution that is a generalization of the (central) beta distribution. The noncentral beta distribution (Type I) is the distribution of the ratio  where  is a noncentral chi-squared random variable with degrees of freedom m and noncentrality parameter , and  is a central chi-squared random variable with degrees of freedom n, independent of . In this case,  A Type II noncentral beta distribution is the distribution of the ratio  where the noncentral chi-squared variable is in the denominator only. If  follows the type II distribution, then  follows a type I distribution.
In statistics, the Champernowne distribution is a symmetric, continuous probability distribution, describing random variables that take both positive and negative values. It is a generalization of the logistic distribution that was introduced by D. G. Champernowne. Champernowne developed the distribution to describe the logarithm of income.
In probability theory, the total variation distance is a distance measure for probability distributions. It is an example of a statistical distance metric, and is sometimes just called "the" statistical distance.
In economics and finance, a Taleb distribution is a returns profile that appears at times deceptively low-risk with steady returns, but experiences periodically catastrophic drawdowns. The term was coined by journalist Martin Wolf and economist John Kay, and is named after Nassim Taleb, based on ideas outlined in his Fooled by Randomness. According to Taleb in Silent Risk, it should be called "payoff" not a "distribution". It does not describe a statistical probability distribution, and does not have an associated mathematical formula. The term is meant to refer to an investment returns profile in which there is a high probability of a small gain, and a small probability of a very large loss, which more than outweighs the gains. In these situations the expected value is very much less than zero, but this fact is camouflaged by the appearance of low risk and steady returns. It is a combination of kurtosis risk and skewness risk: overall returns are dominated by extreme events (kurtosis), which are to the downside (skew). More detailed and formal discussion of the bets on small probability events is in the academic essay by Taleb, called "Why Did the Crisis of 2008 Happen " and in the 2004 paper in the Journal of Behavioral Finance called "Why Do We Prefer Asymmetric Payoffs " in which he writes "agents risking other people s capital would have the incentive to camouflage the properties by showing a steady income. Intuitively, hedge funds are paid on an annual basis while disasters happen every four or five years, for example. The fund manager does not repay his incentive fee."
Constant false alarm rate (CFAR) detection refers to a common form of adaptive algorithm used in radar systems to detect target returns against a background of noise, clutter and interference.
A population pyramid, also called an age pyramid or age picture diagram, is a graphical illustration that shows the distribution of various age groups in a population (typically that of a country or region of the world), which forms the shape of a pyramid when the population is growing. It is also used in ecology to determine the overall age distribution of a population; an indication of the reproductive capabilities and likelihood of the continuation of a species. It typically consists of two back-to-back histograms, with the population plotted on the X-axis and age on the Y-axis, one showing the number of males and one showing females in a particular population in five-year age groups (also called cohorts). Males are conventionally shown on the left and females on the right, and they may be measured by raw number or as a percentage of the total population. Population pyramids are often viewed as the most effective way to graphically depict the age and sex distribution of a population, partly because of the very clear image these pyramids present. A great deal of information about the population broken down by age and sex can be read from a population pyramid, and this can shed light on the extent of development and other aspects of the population. A population pyramid also tells how many people of each age range live in the area. There tends to be more females than males in the older age groups, due to females' longer life expectancy.
In statistical quality control, the individual/moving-range chart is a type of control chart used to monitor variables data from a business or industrial process for which it is impractical to use rational subgroups. The chart is necessary in the following situations: Where automation allows inspection of each unit, so rational subgrouping has less benefit. Where production is slow so that waiting for enough samples to make a rational subgroup unacceptably delays monitoring For processes that produce homogeneous batches (e.g., chemical) where repeat measurements vary primarily because of measurement error The "chart" actually consists of a pair of charts: one, the individuals chart, displays the individual measured values; the other, the moving range chart, displays the difference from one point to the next. As with other control charts, these two charts enable the user to monitor a process for shifts in the process that alter the mean or variance of the measured statistic.
In statistics, a fixed effects model is a statistical model that represents the observed quantities in terms of explanatory variables that are treated as if the quantities were non-random. This is in contrast to random effects models and mixed models in which either all or some of the explanatory variables are treated as if they arise from random causes. Contrast this to the biostatistics definitions, as biostatisticians use "fixed" and "random" effects to respectively refer to the population-average and subject-specific effects (and where the latter are generally assumed to be unknown, latent variables). Often the same structure of model, which is usually a linear regression model, can be treated as any of the three types depending on the analyst's viewpoint, although there may be a natural choice in any given situation. In panel data analysis, the term fixed effects estimator (also known as the within estimator) is used to refer to an estimator for the coefficients in the regression model. If we assume fixed effects, we impose time independent effects for each entity that are possibly correlated with the regressors.
Berkson's paradox also known as Berkson's bias or Berkson's fallacy is a result in conditional probability and statistics which is counterintuitive for some people, and hence a veridical paradox. It is a complicating factor arising in statistical tests of proportions. Specifically, it arises when there is an ascertainment bias inherent in a study design. The effect is related to the explaining away phenomenon in Bayesian networks. It is often described in the fields of medical statistics or biostatistics, as in the original description of the problem by Joseph Berkson.
Risk perception is the subjective judgement that people make about the characteristics and severity of a risk. The phrase is most commonly used in reference to natural hazards and threats to the environment or health, such as nuclear power. Several theories have been proposed to explain why different people make different estimates of the dangerousness of risks. Three major families of theory have been developed: psychology approaches (heuristics and cognitive), anthropology/sociology approaches (cultural theory) and interdisciplinary approaches (social amplification of risk framework).
Regression dilution, also known as regression attenuation, is the biasing of the regression slope towards zero (or the underestimation of its absolute value), caused by errors in the independent variable. Consider fitting a straight line for the relationship of an outcome variable y to a predictor variable x, and estimating the slope of the line. Statistical variability, measurement error or random noise in the y variable cause uncertainty in the estimated slope, but not bias: on average, the procedure calculates the right slope. However, variability, measurement error or random noise in the x variable causes bias in the estimated slope (as well as imprecision). The greater the variance in the x measurement, the closer the estimated slope must approach zero instead of the true value. It may seem counter-intuitive that noise in the predictor variable x induces a bias, but noise in the outcome variable y does not. Recall that linear regression is not symmetric: the line of best fit for predicting y from x (the usual linear regression) is not the same as the line of best fit for predicting x from y.
In probability theory, the rectified Gaussian distribution is a modification of the Gaussian distribution when its negative elements are reset to 0 (analogous to an electronic rectifier). It is essentially a mixture of a discrete distribution (constant 0) and a continuous distribution (a truncated Gaussian distribution with interval ).
In statistical data analysis the total sum of squares (TSS or SST) is a quantity that appears as part of a standard way of presenting results of such analyses. It is defined as being the sum, over all observations, of the squared differences of each observation from the overall mean. In statistical linear models, (particularly in standard regression models), the TSS is the sum of the squares of the difference of the dependent variable and its mean:  where  is the mean. For wide classes of linear models, the total sum of squares equals the explained sum of squares plus the residual sum of squares. For a proof of this in the multivariate OLS case, see partitioning in the general OLS model. In analysis of variance (ANOVA) the total sum of squares is the sum of the so-called "within-samples" sum of squares and "between-samples" sum of squares, i.e., partitioning of the sum of squares. In multivariate analysis of variance (MANOVA) the following equation applies  where T is the total sum of squares and products (SSP) matrix, W is the within-samples SSP matrix and B is the between-samples SSP matrix. Similar terminology may also be used in linear discriminant analysis, where W and B are respectively referred to as the within-groups and between-groups SSP matrices.
In statistics, a unit of observation is the unit described by the data that one analyzes. For example, in a study of the demand for money, the unit of observation might be chosen as the individual, with different observations (data points) for a given point in time differing as to which individual they refer to; or the unit of observation might be the country, with different observations differing only in regard to the country they refer to. A study may have a differing unit of observation and unit of analysis: for example, in community research, the research design may collect data at the individual level of observation but the level of analysis might be at the neighborhood level, drawing conclusions on neighborhood characteristics from data collected from individuals. Together, the unit of observation and the level of analysis define the population of a research enterprise.
The Kirkwood superposition approximation was introduced in 1935 by John G. Kirkwood as a means of representing a discrete probability distribution. The Kirkwood approximation for a discrete probability density function  is given by  where  is the product of probabilities over all subsets of variables of size i in variable set . This kind of formula has been considered by Watanabe (1960) and, according to Watanabe, also by Robert Fano. For the three-variable case, it reduces to simply  The Kirkwood approximation does not generally produce a valid probability distribution (the normalization condition is violated). Watanabe claims that for this reason informational expressions of this type are not meaningful, and indeed there has been very little written about the properties of this measure. The Kirkwood approximation is the probabilistic counterpart of the interaction information. Judea Pearl (1988  3.2.4) indicates that an expression of this type can be exact in the case of a decomposable model, that is, a probability distribution that admits a graph structure whose cliques form a tree. In such cases, the numerator contains the product of the intra-clique joint distributions and the denominator contains the product of the clique intersection distributions.
In probability theory, a balance equation is an equation that describes the probability flux associated with a Markov chain in and out of states or set of states.
A Brownian tree, whose name is derived from Robert Brown via Brownian motion, is a form of computer art that was briefly popular in the 1990s, when home computers started to have sufficient power to simulate Brownian motion. Brownian trees are mathematical models of dendritic structures associated with the physical process known as diffusion-limited aggregation. A Brownian tree is built with these steps: first, a "seed" is placed somewhere on the screen. Then, a particle is placed in a random position of the screen, and moved randomly until it bumps against the seed. The particle is left there, and another particle is placed in a random position and moved until it bumps against the seed or any previous particle, and so on.  The resulting tree can have many different shapes, depending on principally three factors: the seed position the initial particle position (anywhere on the screen, from a circle surrounding the seed, from the top of the screen, etc.) the moving algorithm (usually random, but for example a particle can be deleted if it goes too far from the seed, etc.) Particle color can change between iterations, giving interesting effects. At the time of their popularity (helped by a Scientific American article in the Computer Recreations section, December 1988), a common computer took hours, and even days, to generate a small tree. Today's computers can generate trees with tens of thousands of particles in minutes or seconds. These trees can also be grown easily in an electrodeposition cell, and are the direct result of diffusion-limited aggregation.
The term cohort effect is used in social science to describe variations in the characteristics of an area of study (such as the incidence of a characteristic or the age at onset) over time among individuals who are defined by some shared temporal experience or common life experience, such as year of birth, or year of exposure to radiation. Cohort effects are important to epidemiologists searching for patterns in illnesses. Certain illnesses may be socially affected via the anticipation phenomenon, and cohort effects can be an indicator of this sort of phenomenon. Cohort effects are important to resource dependency and economics theorists when these groups affect structures of influence within their larger organizations. Cohorts in organizations are often defined by entry or birth date, and retain some common characteristic (size, cohesiveness, competition) that can affect the organization. For example, cohort effects are critical issues in school enrollment. In order to determine whether a cohort effect is present, a researcher may conduct a cohort study.
Multivariate analysis (MVA) is based on the statistical principle of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. In design and analysis, the technique is used to perform trade studies across multiple dimensions while taking into account the effects of all variables on the responses of interest. Uses for multivariate analysis include: design for capability (also known as capability-based design) inverse design, where any variable can be treated as an independent variable Analysis of Alternatives (AoA), the selection of concepts to fulfil a customer need analysis of concepts with respect to changing scenarios identification of critical design-drivers and correlations across hierarchical levels. Multivariate analysis can be complicated by the desire to include physics-based analysis to calculate the effects of variables for a hierarchical "system-of-systems". Often, studies that wish to use multivariate analysis are stalled by the dimensionality of the problem. These concerns are often eased through the use of surrogate models, highly accurate approximations of the physics-based code. Since surrogate models take the form of an equation, they can be evaluated very quickly. This becomes an enabler for large-scale MVA studies: while a Monte Carlo simulation across the design space is difficult with physics-based codes, it becomes trivial when evaluating surrogate models, which often take the form of response-surface equations.
Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs  that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.
StatXact is a statistical software package for analyzing data using exact statistics. It calculates exact p-values and confidence intervals for contingency tables and non-parametric procedures. It is marketed by Cytel Inc.
In data mining, anomaly detection (or outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions. In particular in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then testing the likelihood of a test instance to be generated by the learnt model.
How to Lie with Statistics is a book written by Darrell Huff in 1954 presenting an introduction to statistics for the general reader. Huff was a journalist who wrote many "how to" articles as a freelancer, but was not a statistician. The book is a brief, breezy, illustrated volume outlining errors when it comes to the interpretation of statistics, and how these errors may create incorrect conclusions. In the 1960s and 1970s, it became a standard textbook introduction to the subject of statistics for many college students. It has become one of the best-selling statistics books in history, with over one and a half million copies sold in the English-language edition. It has also been widely translated. Themes of the book include "Correlation does not imply causation" and "Using random sampling". It also shows how statistical graphs can be used to distort reality, for example by truncating the bottom of a line or bar chart, so that differences seem larger than they are, or by representing one-dimensional quantities on a pictogram by two- or three-dimensional objects to compare their sizes, so that the reader forgets that the images do not scale the same way the quantities do. The original edition contained humorous illustrations by artist Irving Geis. In a UK edition these were replaced with cartoons by Mel Calman.
In mathematics and statistics, Skorokhod's representation theorem is a result that shows that a weakly convergent sequence of probability measures whose limit measure is sufficiently well-behaved can be represented as the distribution/law of a pointwise convergent sequence of random variables defined on a common probability space. It is named for the Ukrainian mathematician A.V. Skorokhod.
Econometric models are statistical models used in econometrics. An econometric model specifies the statistical relationship that is believed to hold between the various economic quantities pertaining to a particular economic phenomenon under study. An econometric model can be derived from a deterministic economic model by allowing for uncertainty, or from an economic model which itself is stochastic. However, it is also possible to use econometric models that are not tied to any specific economic theory. A simple example of an econometric model is one that assumes that monthly spending by consumers is linearly dependent on consumers' income in the previous month. Then the model will consist of the equation  where Ct is consumer spending in month t, Yt-1 is income during the previous month, and et is an error term measuring the extent to which the model cannot fully explain consumption. Then one objective of the econometrician is to obtain estimates of the parameters a and b; these estimated parameter values, when used in the model's equation, enable predictions for future values of consumption to be made contingent on the prior month's income.
In statistics, a parametric model or parametric family or finite-dimensional model is a family of distributions that can be described using a finite number of parameters. These parameters are usually collected together to form a single k-dimensional parameter vector   = ( 1,  2, ...,  k). Parametric models are contrasted with the semi-parametric, semi-nonparametric, and non-parametric models, all of which consist of an infinite set of  parameters  for description. The distinction between these four classes is as follows: in a  parametric  model all the parameters are in finite-dimensional parameter spaces; a model is  non-parametric  if all the parameters are in infinite-dimensional parameter spaces; a  semi-parametric  model contains finite-dimensional parameters of interest and infinite-dimensional nuisance parameters; a  semi-nonparametric  model has both finite-dimensional and infinite-dimensional unknown parameters of interest. Some statisticians believe that the concepts  parametric ,  non-parametric , and  semi-parametric  are ambiguous. It can also be noted that the set of all probability measures has cardinality of continuum, and therefore it is possible to parametrize any model at all by a single number in (0,1) interval. This difficulty can be avoided by considering only  smooth  parametric models.
The normal probability plot is a graphical technique to identify substantive departures from normality. This includes identifying outliers, skewness, kurtosis, a need for transformations, and mixtures. Normal probability plots are made of raw data, residuals from model fits, and estimated parameters.  In a normal probability plot (also called a "normal plot"), the sorted data are plotted vs. values selected to make the resulting image look close to a straight line if the data are approximately normally distributed. Deviations from a straight line suggest departures from normality. The plotting can be manually performed by using a special graph paper, called normal probability paper. With modern computers normal plots are commonly made with software. The normal probability plot is a special case of the Q Q probability plot for a normal distribution. The theoretical quantiles are generally chosen to approximate either the mean or the median of the corresponding order statistics.
Cliodynamics is an area of research focused on mathematical modeling of historical dynamics.
A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases, and then decreases back to zero. It can typically be visualized as a "brief oscillation" like one might see recorded by a seismograph or heart monitor. Generally, wavelets are purposefully crafted to have specific properties that make them useful for signal processing. Wavelets can be combined, using a "reverse, shift, multiply and integrate" technique called convolution, with portions of a known signal to extract information from the unknown signal.  For example, a wavelet could be created to have a frequency of Middle C and a short duration of roughly a 32nd note. If this wavelet was to be convolved with a signal created from the recording of a song, then the resulting signal would be useful for determining when the Middle C note was being played in the song. Mathematically, the wavelet will correlate with the signal if the unknown signal contains information of similar frequency. This concept of correlation is at the core of many practical applications of wavelet theory. As a mathematical tool, wavelets can be used to extract information from many different kinds of data, including   but certainly not limited to   audio signals and images. Sets of wavelets are generally needed to analyze data fully. A set of "complementary" wavelets will decompose data without gaps or overlap so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet based compression/decompression algorithms where it is desirable to recover the original information with minimal loss. In formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square integrable functions.
Psephology /s  f l d i/ (from Greek psephos       , 'pebble', which the Greeks used as ballots) is a branch of political science which deals with the study and scientific analysis of elections. Psephology uses historical precinct voting data, public opinion polls, campaign finance information and similar statistical data. The term was coined in the United Kingdom in 1952 by the historian R. B. McCallum to describe the scientific analysis of past elections.
In mathematics and statistics, an asymptotic distribution is a distribution that is in a sense the "limiting" distribution of a sequence of distributions. One of the main uses of the idea of an asymptotic distribution is in providing approximations to the cumulative distribution functions of statistical estimators.
A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. Because of the central limit theorem, many test statistics are approximately normally distributed for large samples. For each significance level, the Z-test has a single critical value (for example, 1.96 for 5% two tailed) which makes it more convenient than the Student's t-test which has separate critical values for each sample size. Therefore, many statistical tests can be conveniently performed as approximate Z-tests if the sample size is large or the population variance known. If the population variance is unknown (and therefore has to be estimated from the sample itself) and the sample size is not large (n < 30), the Student's t-test may be more appropriate. If T is a statistic that is approximately normally distributed under the null hypothesis, the next step in performing a Z-test is to estimate the expected value   of T under the null hypothesis, and then obtain an estimate s of the standard deviation of T. After that the standard score Z = (T    ) / s is calculated, from which one-tailed and two-tailed p-values can be calculated as  ( Z) (for upper-tailed tests),  (Z) (for lower-tailed tests) and 2 ( |Z|) (for two-tailed tests) where   is the standard normal cumulative distribution function.  
In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) which are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are generally presented as systems of interconnected "neurons" which exchange messages between each other. The connections have numeric weights that can be tuned based on experience, making neural nets adaptive to inputs and capable of learning. For example, a neural network for handwriting recognition is defined by a set of input neurons which may be activated by the pixels of an input image. After being weighted and transformed by a function (determined by the network's designer), the activations of these neurons are then passed on to other neurons. This process is repeated until finally, the output neuron that determines which character was read is activated. Like other machine learning methods    systems that learn from data    neural networks have been used to solve a wide variety of tasks, like computer vision and speech recognition, that are hard to solve using ordinary rule-based programming.
Simultaneous equation models are a form of statistical model in the form of a set of linear simultaneous equations. They are often used in econometrics.
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.
In bioinformatics, neighbor joining is a bottom-up (agglomerative) clustering method for the creation of phylogenetic trees, created by Naruya Saitou and Masatoshi Nei in 1987. Usually used for trees based on DNA or protein sequence data, the algorithm requires knowledge of the distance between each pair of taxa (e.g., species or sequences) to form the tree.
In statistics, the restricted (or residual, or reduced) maximum likelihood (REML) approach is a particular form of maximum likelihood estimation which does not base estimates on a maximum likelihood fit of all the information, but instead uses a likelihood function calculated from a transformed set of data, so that nuisance parameters have no effect. In the case of variance component estimation, the original data set is replaced by a set of contrasts calculated from the data, and the likelihood function is calculated from the probability distribution of these contrasts, according to the model for the complete data set. In particular, REML is used as a method for fitting linear mixed models. In contrast to the earlier maximum likelihood estimation, REML can produce unbiased estimates of variance and covariance parameters. The idea underlying REML estimation was put forward by M. S. Bartlett in 1937. The first description of the approach applied to estimating components of variance in unbalanced data was by Desmond Patterson and Robin Thompson of the University of Edinburgh in 1971, although they did not use the term REML. A review of the early literature was given by Harville. REML estimation is available in a number of general-purpose statistical software packages, including Genstat (the REML directive), SAS (the MIXED procedure), SPSS (the MIXED command), Stata (the mixed command), JMP (statistical software), and R (especially the lme4 and older nlme packages), as well as in more specialist packages such as MLwiN, HLM, ASReml, Statistical Parametric Mapping and CropStat.
In statistics, the interdecile range is the difference between the first and the ninth deciles (10% and 90%). The interdecile range is a measure of statistical dispersion of the values in a set of data, similar to the range and the interquartile range, and can be computed from the (non-parametric) seven-number summary. Despite its simplicity, for estimating the standard deviation of a normal distribution, the scaled interdecile range gives a reasonably efficient estimator. More precisely, a more efficient estimator is given by instead taking the 7% trimmed range (the difference between the 7th and 93rd percentiles) and dividing by 3 (corresponding to 86% of the data of a normal distribution falling within 1.5 standard deviations of the mean) yields an estimate of about 65% efficiency. Analogous measures of location are given by the median, midhinge, and trimean (or statistics based on nearby points).
The Anderson Darling test is a statistical test of whether a given sample of data is drawn from a given probability distribution. In its basic form, the test assumes that there are no parameters to be estimated in the distribution being tested, in which case the test and its set of critical values is distribution-free. However, the test is most often used in contexts where a family of distributions is being tested, in which case the parameters of that family need to be estimated and account must be taken of this in adjusting either the test-statistic or its critical values. When applied to testing if a normal distribution adequately describes a set of data, it is one of the most powerful statistical tools for detecting most departures from normality. K-sample Anderson Darling tests are available for testing whether several collections of observations can be modelled as coming from a single population, where the distribution function does not have to be specified. In addition to its use as a test of fit for distributions, it can be used in parameter estimation as the basis for a form of minimum distance estimation procedure. The test is named after Theodore Wilbur Anderson (born 1918) and Donald A. Darling, who invented it in 1952.
In probability theory and statistics, the Van Houtum distribution is a discrete probability distribution named after prof. Geert-Jan van Houtum. It can be characterized by saying that all values of a finite set of possible values are equally probable, except for the smallest and largest element of this set. Since the Van Houtum distribution is a generalization of the discrete uniform distribution, i.e. it is uniform except possibly at its boundaries, it is sometimes also referred to as quasi-uniform. It is regularly the case that the only available information concerning some discrete random variable are its first two moments. The Van Houtum distribution can be used to fit a distribution with finite support on these moments. A simple example of the Van Houtum distribution arises when throwing a loaded dice which has been tampered with to land on a 6 twice as often as on a 1. The possible values of the sample space are 1, 2, 3, 4, 5 and 6. Each time the die is thrown, the probability of throwing a 2, 3, 4 or 5 is 1/6; the probability of a 1 is 1/9 and the probability of throwing a 6 is 2/9.
A unit root is a feature of processes that evolve through time that can cause problems in statistical inference involving time series models. A linear stochastic process has a unit root if 1 is a root of the process's characteristic equation. Such a process is non-stationary. If the other roots of the characteristic equation lie inside the unit circle that is, have a modulus (absolute value) less than one then the first difference of the process will be stationary.
The Ehrenfest model (or dog-flea model) of diffusion was proposed by Tatiana and Paul Ehrenfest to explain the second law of thermodynamics. The model considers N particles in two containers. Particles independently change container at a rate  . If X(t) = i is defined to be the number of particles in one container at time t, then it is a birth-death process with transition rates  for i = 1, 2, ..., N  for i = 0, 1, ..., N   1 and equilibrium distribution . Mark Kac proved in 1947 that if the initial system state is not equilibrium, then the entropy, given by  is monotonically increasing (H-theorem). This is a consequence of the convergence to the equilibrium distribution.
In population genetics an idealised population is one that can be described using a number of simplifying assumptions. Models of idealised populations are either used to make a general point, or they are fit to data on real populations for which the assumptions may not hold true. For example, coalescent theory is used to fit data to models of idealised populations. The most common idealized population in population genetics is the Fisher-Wright population after R.A. Fisher and Sewall Wright. Wright-Fisher populations have constant size, and their members can mate and reproduce with any other member. Another example is a Moran model, which has overlapping generations, rather than the non-overlapping generations of the Wright-Fisher model. The complexities of real populations can cause their behavior to match an idealised population with an effective population size that is very different from the census population size of the real population. For sexual diploids, idealized populations will have genotype frequencies related to the allele frequencies according to Hardy-Weinberg equilibrium.
Indicators of spatial association are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.
The Birnbaum Saunders distribution, also known as the fatigue life distribution, is a probability distribution used extensively in reliability applications to model failure times. There are several alternative formulations of this distribution in the literature. It is named after Z. W. Birnbaum and S. C. Saunders.
A cost-of-living index is a theoretical price index that measures relative cost of living over time or regions. It is an index that measures differences in the price of goods and services, and allows for substitutions with other items as prices vary. There are many different methodologies that have been developed to approximate cost-of-living indexes. A Konu s index is a type of cost-of-living index that uses an expenditure function such as one used in assessing expected compensating variation. The expected indirect utility is equated in both periods.
In pattern recognition and information retrieval with binary classification, precision (also called positive predictive value) is the fraction of retrieved instances that are relevant, while recall (also known as sensitivity) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of relevance. Suppose a computer program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is "how useful the search results are", and recall is "how complete the results are". In statistics, if the null hypothesis is that all and only the relevant items are retrieved, absence of type I and type II errors corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative). The above pattern recognition example contained 7   4 = 3 type I errors and 9   4 = 5 type II errors. Precision can be seen as a measure of exactness or quality, whereas recall is a measure of completeness or quantity. In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant, while high recall means that an algorithm returned most of the relevant results.
In science, a null result is a result without the expected content: that is, the proposed result is absent. It is an experimental outcome which does not show an otherwise expected effect. This does not imply a result of zero or nothing, simply a result that does not support the hypothesis. The term is a translation of the scientific Latin nullus resultarum, meaning "no consequence". In statistical hypothesis testing, a null result occurs when an experimental result is not significantly different from what is to be expected under the null hypothesis; its probability (under the null hypothesis) does not exceed the significance level, i.e., the threshold set prior to testing for rejection of the null hypothesis. The significance level varies, but is often set at p-value 0.05 (5%). As an example in physics, the results of the Michelson Morley experiment were of this type, as it did not detect the expected velocity relative to the postulated luminiferous aether. This experiment's famous failed detection, commonly referred to as the null result, contributed to the development of special relativity. Note that the experiment did appear to measure a non-zero "drift", but the value was far too small to account for the theoretically expected results; it is generally thought to be inside the noise level of the experiment.
In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable. Bayes' theorem calculates the renormalized pointwise product of the prior and the likelihood function, to produce the posterior probability distribution, which is the conditional distribution of the uncertain quantity given the data. Similarly, the prior probability of a random event or an uncertain proposition is the unconditional probability that is assigned before any relevant evidence is taken into account. Priors can be created using a number of methods. A prior can be determined from past information, such as previous experiments. A prior can be elicited from the purely subjective assessment of an experienced expert. An uninformative prior can be created to reflect a balance among outcomes when no information is available. Priors can also be chosen according to some principle, such as symmetry or maximizing entropy given constraints; examples are the Jeffreys prior or Bernardo's reference prior. When a family of conjugate priors exists, choosing a prior from that family simplifies calculation of the posterior distribution. Parameters of prior distributions are a kind of hyperparameter. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: p is a parameter of the underlying system (Bernoulli distribution), and   and   are parameters of the prior distribution (beta distribution); hence hyperparameters. Hyperparameters themselves may have hyperprior distributions expressing beliefs about their values. A Bayesian model with more than one level of prior this like is called a hierarchical Bayes model.
In statistics, the White test is a statistical test that establishes whether the residual variance of a variable in a regression model is constant: that is for homoskedasticity. This test, and an estimator for heteroskedasticity-consistent standard errors, were proposed by Halbert White in 1980. These methods have become extremely widely used, making this paper one of the most cited articles in economics. In cases where the White test statistic is statistically significant, heteroskedasticity may not necessarily be the cause; instead the problem could be a specification error. In other words, the White test can be a test of heteroskedasticity or specification error or both. If no cross product terms are introduced in the White test procedure, then this is a pure test of pure heteroskedasticity. If cross products are introduced in the model, then it is a test of both heteroskedasticity and specification bias.
In statistics, resampling is any of a variety of methods for doing one of the following: Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping) Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests) Validating models by using random subsets (bootstrapping, cross validation) Common resampling techniques include bootstrapping, jackknifing and permutation tests.
Taylor's law (also known as Taylor s power law) is an empirical law in ecology that relates the variance of the number of individuals of a species per unit area of habitat to the corresponding mean by a power law relationship.
Nonparametric statistics are statistics not based on parameterized families of probability distributions. They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed. The difference between parametric models and non-parametric models is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data. Note that the non-parametric model does not have any parameters: parameters are determined by the training data, not the model.  
Model Output Statistics (MOS) is a multiple linear regression technique in which predicands, often near-surface quantities, such as 2-meter air temperature, horizontal visibility, and wind direction, speed and gusts, are related statistically to one or more predictors. The predictors are typically forecasts from a numerical weather prediction (NWP) model, climatic data, and, if applicable, recent surface observations. Thus, output from NWP models can be transformed by the MOS technique into sensible weather parameters that are familiar to the "person on the street".
In statistics, response surface methodology (RSM) explores the relationships between several explanatory variables and one or more response variables. The method was introduced by G. E. P. Box and K. B. Wilson in 1951. The main idea of RSM is to use a sequence of designed experiments to obtain an optimal response. Box and Wilson suggest using a second-degree polynomial model to do this. They acknowledge that this model is only an approximation, but use it because such a model is easy to estimate and apply, even when little is known about the process.
Repeatability or test retest reliability is the variation in measurements taken by a single person or instrument on the same item, under the same conditions, and in a short period of time. A less-than-perfect test retest reliability causes test retest variability. Such variability can be caused by, for example, intra-individual variability and intra-observer variability. A measurement may be said to be repeatable when this variation is smaller than a pre-determined acceptance criteria. Test retest variability is practically used, for example, in medical monitoring of conditions. In these situations, there is often a predetermined "critical difference", and for differences in monitored values that are smaller than this critical difference, the possibility of pre-test variability as a sole cause of the difference may be considered in addition to, for examples, changes in diseases or treatments.
In queueing theory, a discipline within the mathematical theory of probability, the Pollaczek Khinchine formula states a relationship between the queue length and service time distribution Laplace transforms for an M/G/1 queue (where jobs arrive according to a Poisson process and have general service time distribution). The term is also used to refer to the relationships between the mean queue length and mean waiting/service time in such a model. The formula was first published by Felix Pollaczek in 1930 and recast in probabilistic terms by Aleksandr Khinchin two years later. In ruin theory the formula can be used to compute the probability of ultimate ruin (probability of an insurance company going bankrupt).
Composite bar charts are bar charts where each bar displays multiple data points stacked in a single row or column. This may, for instance, take the form of uniform height bars charting a time series with internal stacked colors indicating the percentage participation of a sub-type of data. Another example would be a time series displaying total numbers, with internal colors indicating participation in the total by sub-types.
In signal processing, time frequency analysis comprises those techniques that study a signal in both the time and frequency domains simultaneously, using various time frequency representations. Rather than viewing a 1-dimensional signal (a function, real or complex-valued, whose domain is the real line) and some transform (another function whose domain is the real line, obtained from the original via some transform), time frequency analysis studies a two-dimensional signal   a function whose domain is the two-dimensional real plane, obtained from the signal via a time frequency transform.  The mathematical motivation for this study is that functions and their transform representation are often tightly connected, and they can be understood better by studying them jointly, as a two-dimensional object, rather than separately. A simple example is that the 4-fold periodicity of the Fourier transform   and the fact that two-fold Fourier transform reverses direction   can be interpreted by considering the Fourier transform as a 90  rotation in the associated time frequency plane: 4 such rotations yield the identity, and 2 such rotations simply reverse direction (reflection through the origin). The practical motivation for time frequency analysis is that classical Fourier analysis assumes that signals are infinite in time or periodic, while many signals in practice are of short duration, and change substantially over their duration. For example, traditional musical instruments do not produce infinite duration sinusoids, but instead begin with an attack, then gradually decay. This is poorly represented by traditional methods, which motivates time frequency analysis. One of the most basic forms of time frequency analysis is the short-time Fourier transform (STFT), but more sophisticated techniques have been developed, notably wavelets.  
The negentropy, also negative entropy, syntropy, extropy, ectropy or entaxy, of a living system is the entropy that it exports to keep its own entropy low; it lies at the intersection of entropy and life. The concept and phrase "negative entropy" was introduced by Erwin Schro dinger in his 1944 popular-science book What is Life  Later, Le on Brillouin shortened the phrase to negentropy, to express it in a more "positive" way: a living system imports negentropy and stores it. In 1974, Albert Szent-Gyo rgyi proposed replacing the term negentropy with syntropy. That term may have originated in the 1940s with the Italian mathematician Luigi Fantappie , who tried to construct a unified theory of biology and physics. Buckminster Fuller tried to popularize this usage, but negentropy remains common. In a note to What is Life  Schro dinger explained his use of this phrase. Indeed, negentropy has been used by biologists as the basis for purpose or direction in life, namely cooperative or moral instincts. In 2009, Mahulikar & Herwig redefined negentropy of a dynamically ordered sub-system as the specific entropy deficit of the ordered sub-system relative to its surrounding chaos. Thus, negentropy has SI units of (J kg-1 K-1) when defined based on specific entropy per unit mass, and (K 1) when defined based on specific entropy per unit energy. This definition enabled: i) scale-invariant thermodynamic representation of dynamic order existence, ii) formulation of physical principles exclusively for dynamic order existence and evolution, and iii) mathematical interpretation of Schro dinger's negentropy debt.
Error bars are a graphical representation of the variability of data and are used on graphs to indicate the error, or uncertainty in a reported measurement. They give a general idea of how precise a measurement is, or conversely, how far from the reported value the true (error free) value might be. Error bars often represent one standard deviation of uncertainty, one standard error, or a certain confidence interval (e.g., a 95% interval). These quantities are not the same and so the measure selected should be stated explicitly in the graph or supporting text. Error bars can be used to compare visually two quantities if various other conditions hold. This can determine whether differences are statistically significant. Error bars can also suggest goodness of fit of a given function, i.e., how well the function describes the data. Scientific papers in the experimental sciences are expected to include error bars on all graphs, though the practice differs somewhat between sciences, and each journal will have its own house style. It has also been shown that error bars can be used as a direct manipulation interface for controlling probabilistic algorithms for approximate computation. Error bars can also be expressed in a plus-minus sign ( ), plus the upper limit of the error and minus the lower limit of the error.
In applied probability theory, the Simon model is a class of stochastic models that results in a power-law distribution function. It was proposed by Herbert A. Simon to account for the wide range of empirical distributions following a power-law. It models the dynamics of a system of elements with associated counters (e.g., words and their frequencies in texts, or nodes in a network and their connectivity ). In this model the dynamics of the system is based on constant growth via addition of new elements (new instances of words) as well as incrementing the counters (new occurrences of a word) at a rate proportional to their current values.
In econometrics, the Park test is a test for heteroscedasticity. The test is based on the method proposed by Rolla Edward Park for estimating linear regression parameters in the presence of heteroscedastic error terms.
The Unscrambler  X is a commercial software product for multivariate data analysis, used for calibration of multivariate data which is often in the application of analytical data such as near infrared spectroscopy and Raman spectroscopy, and development of predictive models for use in real-time spectroscopic analysis of materials. The software was originally developed in 1986 by Harald Martens and later by CAMO Software.
A geometric Brownian motion (GBM) (also known as exponential Brownian motion) is a continuous-time stochastic process in which the logarithm of the randomly varying quantity follows a Brownian motion (also called a Wiener process) with drift. It is an important example of stochastic processes satisfying a stochastic differential equation (SDE); in particular, it is used in mathematical finance to model stock prices in the Black Scholes model.
A streamgraph, or stream graph, is a type of stacked area graph which is displaced around a central axis, resulting in a flowing, organic shape. Streamgraphs were popularized by Lee Byron and their use in a February 2008 New York Times article on movie box office revenues.
In statistics, unit-weighted regression is a simplified and robust version (Wainer & Thissen, 1976) of multiple regression analysis where only the intercept term is estimated. That is, it fits a model  where each of the  are binary variables, perhaps multiplied with an arbitrary weight. Contrast this with the more common multiple regression model, where each predictor has its own estimated coefficient:  In the social sciences, unit-weighted regression is sometimes used for classification purposes, i.e. to predict a yes-no answer where  indicates "no",  "yes". It is easier to interpret than multiple linear regression (known as linear discriminant analysis in the classification case).
A counting process is a stochastic process {N(t), t   0} with values that are positive, integer, and increasing: N(t)   0. N(t) is an integer. If s   t then N(s)   N(t). If s < t, then N(t)   N(s) is the number of events occurred during the interval [s, t ]. Examples of counting processes include Poisson processes and Renewal processes. Because of the third property, a counting process is increasing and hence a submartingale. Then by Doob-Meyer, it can be written as  with a martingale M(t) and a predictable increasing process A(t). The martingale M(t) is called the martingale associated with the counting process N(t) and the predictable process A(t) is called the cumulative intensity of the counting process N(t). Counting processes deal with the number of various outcomes in a system over time. An example of a counting process is the number of occurrences of "heads" over some number of coin tosses. If a process has the Markov property, it is said to be a Markov counting process.
An experiment is a procedure carried out to verify, refute, or validate a hypothesis. Experiments provide insight into cause-and-effect by demonstrating what outcome occurs when a particular factor is manipulated. Experiments vary greatly in goal and scale, but always rely on repeatable procedure and logical analysis of the results. There also exist natural experimental studies. A child may carry out basic experiments to understand gravity, while teams of scientists may take years of systematic investigation to advance their understanding of a phenomenon. Experiments and other types of hands-on activities are very important to student learning in the science classroom. Experiments can raise test scores and help a student become more engaged and interested in the material they are learning, especially when used over time. Experiments can vary from personal and informal natural comparisons (e.g. tasting a range of chocolates to find a favorite), to highly controlled (e.g. tests requiring complex apparatus overseen by many scientists that hope to discover information about subatomic particles). Uses of experiments vary considerably between the natural and human sciences. Experiments typically include controls, which are designed to minimize the effects of variables other than the single independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. Ideally, all variables in an experiment are controlled (accounted for by the control measurements) and none are uncontrolled. In such an experiment, if all controls work as expected, it is possible to conclude that the experiment works as intended, and that results are due to the effect of the tested variable.
Inverse probability weighting is a statistical technique for calculating statistics standardized to a population different from that in which the data was collected. Study designs with a disparate sampling population and population of target inference (target population) are common in application. There may be prohibitive factors barring researchers from directly sampling from the target population such as cost, time, or ethical concerns. A solution to this problem is to use an alternate design strategy, e.g. stratified sampling. Weighting, when correctly applied, can potentially improve the efficiency and reduce the bias of unweighted estimators. One very early weighted estimator is the Horvitz Thompson estimator of the mean. When the sampling probability is known, from which the sampling population is drawn from the target population, then the inverse of this probability is used to weight the observations. This approach has been generalized to many aspects of statistics under various frameworks. In particular, there are weighted likelihoods, weighted estimating equations, and weighted probability densities from which a majority of statistics are derived. These applications codified the theory of other statistics and estimators such as marginal structural models, the standardized mortality ratio, and the EM algorithm for coarsened or aggregate data. Inverse probability weighting is also used to account for missing data when subjects with missing data cannot be included in the primary analysis. With an estimate of the inclusion probability, or the probability that the factor would be measured in another measurement, inverse probability weighting can be used to inflate the weight for subjects who are underrepresented due to a large degree of missing data.  
This article gives two concrete illustrations of the central limit theorem. Both involve the sum of independent and identically-distributed random variables and show how the probability distribution of the sum approaches the normal distribution as the number of terms in the sum increases. The first illustration involves a continuous probability distribution, for which the random variables have a probability density function. The second illustration, for which most of the computation can be done by hand, involves a discrete probability distribution, which is characterized by a probability mass function. A free full-featured interactive simulation that allows the user to set up various distributions and adjust the sampling parameters is available through the External links section at the bottom of this page.
See also Wigner distribution (disambiguation). The Wigner quasiprobability distribution (also called the Wigner function or the Wigner Ville distribution after Eugene Wigner and Jean-Andre  Ville) is a quasiprobability distribution. It was introduced by Eugene Wigner in 1932 to study quantum corrections to classical statistical mechanics. The goal was to link the wavefunction that appears in Schro dinger's equation to a probability distribution in phase space. It is a generating function for all spatial autocorrelation functions of a given quantum-mechanical wavefunction  (x). Thus, it maps on the quantum density matrix in the map between real phase-space functions and Hermitian operators introduced by Hermann Weyl in 1927, in a context related to representation theory in mathematics (cf. Weyl quantization in physics). In effect, it is the Wigner Weyl transform of the density matrix, so the realization of that operator in phase space. It was later rederived by Jean Ville in 1948 as a quadratic (in signal) representation of the local time-frequency energy of a signal, effectively a spectrogram. In 1949, Jose  Enrique Moyal, who had derived it independently, recognized it as the quantum moment-generating functional, and thus as the basis of an elegant encoding of all quantum expectation values, and hence quantum mechanics, in phase space (cf. phase space formulation). It has applications in statistical mechanics, quantum chemistry, quantum optics, classical optics and signal analysis in diverse fields such as electrical engineering, seismology, time frequency analysis for music signals, spectrograms in biology and speech processing, and engine design.
In statistics, fractional factorial designs are experimental designs consisting of a carefully chosen subset (fraction) of the experimental runs of a full factorial design. The subset is chosen so as to exploit the sparsity-of-effects principle to expose information about the most important features of the problem studied, while using a fraction of the effort of a full factorial design in terms of experimental runs and resources.  
The Information ratio is a measure of the risk-adjusted return of a financial security (or asset or portfolio). It is also known as Appraisal ratio and is defined as expected active return divided by tracking error, where active return is the difference between the return of the security and the return of a selected benchmark index, and tracking error is the standard deviation of the active return; i.e., the information ratio  is: , where  is the portfolio return,  is the benchmark return,  is the expected value of the active return, and  is the standard deviation of the active return, which is an alternate definition of the aforementioned tracking error. Note in this case,  is defined as excess return, not the risk-adjusted excess return or Jensen's alpha calculated using regression analysis. Some analysts, however, do use Jensen's alpha for the numerator and a regression-adjusted tracking error for the denominator (this version of the information ratio is often described as the appraisal ratio to differentiate it from the more common definition). The information ratio is often used to gauge the skill of managers of mutual funds, hedge funds, etc. In this case, it measures the active return of the manager's portfolio divided by the amount of risk that the manager takes relative to the benchmark. The higher the information ratio, the higher the active return of the portfolio, given the amount of risk taken, and the better the manager. Top-quartile investment managers typically achieve annualized information ratios of about one-half. There are both ex ante expected and ex post observed information ratios. Generally, the information ratio compares the returns of the manager's portfolio with those of a benchmark such as the yield on three-month Treasury bills or an equity index such as the S&P 500. The information ratio is often annualized. While it is then common for the numerator to be calculated as the arithmetic difference between the annualized portfolio return and the annualized benchmark return, this is an approximation because the annualization of an arithmetic difference between terms is not the arithmetic difference of the annualized terms. Since the denominator is here taken to be the annualized standard deviation of the arithmetic difference of these series, which is a standard measure of annualized risk, and since the ratio of annualized terms is the annualization of their ratio, the annualized information ratio provides the annualized risk-adjusted active return of the portfolio relative to the benchmark. The information ratio is similar to the Sharpe ratio but, whereas the Sharpe ratio is the 'excess' return of an asset over the return of a risk free asset divided by the variability or standard deviation of returns, the information ratio is the 'active' return to the most relevant benchmark index divided by the standard deviation of the 'active' return or tracking error. Some hedge funds use Information ratio as a metric for calculating a performance fee. One of the main criticisms of the Information Ratio is that it considers arithmetic returns and ignores leverage. This can lead to the Information Ratio calculated for a manager being negative when the manager produces alpha to the benchmark and vice versa. A better measure of the alpha produced by the manager is the Geometric Information Ratio.
In mathematics, the logarithmic mean is a function of two non-negative numbers which is equal to their difference divided by the logarithm of their quotient. In symbols:  for the positive numbers . This calculation is applicable in engineering problems involving heat and mass transfer.
In probability theory, Raikov s theorem, named after Dmitry Raikov, states that if the sum of two independent non-negative random variables X and Y has a Poisson distribution, then both X and Y themselves must have the Poisson distribution. It says the same thing about the Poisson distribution that Crame r's theorem says about the normal distribution. It can readily be shown by mathematical induction that the same is true of the sum of more than two independent random variables.
Imprecise probability generalizes probability theory to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique probability distribution may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately. Imprecision is useful for dealing with expert elicitation, because: People have a limited ability to determine their own subjective probabilities and might find that they can only provide an interval. As an interval is compatible with a range of opinions, the analysis ought to be more convincing to a range of different people.
A first-in-man study is a clinical trial where a medical procedure, previously developed and assessed through in vitro or animal testing, or through mathematical modelling is tested on human subjects for the first time. Sometimes, this is called a Phase 0 study. Such studies present particular risks to the human subjects as was signally the case in the TGN1412 trial in 2006. Consequently, there are ethical issues as to whether such trials should be performed on healthy volunteers, who have nothing to gain beyond a fee, or on patients who have a chance of enjoying a health benefit.
In statistics and regression analysis, moderation occurs when the relationship between two variables depends on a third variable. The third variable is referred to as the moderator variable or simply the moderator. The effect of a moderating variable is characterized statistically as an interaction; that is, a categorical (e.g., sex, race, class) or quantitative (e.g., level of reward) variable that affects the direction and/or strength of the relation between dependent and independent variables. Specifically within a correlational analysis framework, a moderator is a third variable that affects the zero-order correlation between two other variables, or the value of the slope of the dependent variable on the independent variable. In analysis of variance (ANOVA) terms, a basic moderator effect can be represented as an interaction between a focal independent variable and a factor that specifies the appropriate conditions for its operation.
In probability theory and statistics, the coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. It is often expressed as a percentage, and is defined as the ratio of the standard deviation  to the mean  (or its absolute value, ). The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay. It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R.
In applied mathematics, Topological data analysis (TDA) is an approach to the analysis of datasets using techniques from topology. Extraction of information from datasets that are high-dimensional, incomplete and noisy is in general challenging. TDA provides a general framework to analyze such data in a manner that is insensitive to the particular metric and provides dimension reduction and robustness to noise. Beyond, it inherits functorality, one of the keys to the modern mathematics, from its topological nature, which makes it adaptive to new tools from mathematics. The initial motivation is to study the shape of data. TDA has combined algebraic topology and other tools from pure mathematics to give mathematically strict and quantitative study of "shape". The main tool is persistent homology, a modified concept of homology group. Nowadays, this area has been proven to be successful in practice. It has been applied to many types of data input, and different data resource and numerous fields. Moreover, its mathematical foundation is also of theoretical importance to mathematics itself. Its unique features make it a promising bridge between topology and geometry.
In statistics, Bayesian vector autoregression (BVAR) uses Bayesian methods to estimate a vector autoregression (VAR). In that respect, the difference with standard VAR models lies in the fact that the model parameters are treated as random variables, and prior probabilities are assigned to them. Vector autoregressions are flexible statistical models that typically include many free parameters. Given the limited length of standard macroeconomic datasets, Bayesian methods have become an increasingly popular way of dealing with this problem of over-parameterization. The general idea is to use informative priors to shrink the unrestricted model towards a parsimonious nai ve benchmark, thereby reducing parameter uncertainty and improving forecast accuracy (see  for a survey). A typical example is the shrinkage prior proposed by Robert Litterman, and subsequently developed by other researchers at University of Minnesota, which is known in the BVAR literature as the "Minnesota prior". The informativeness of the prior can be set by treating it as an additional parameter, based on a hierarchical interpretation of the model. Recent research has shown that Bayesian vector autoregression is an appropriate tool for modelling large data sets.
Minimax (sometimes MinMax or MM) is a decision rule used in decision theory, game theory, statistics and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.
In the mathematics of probability, a stochastic process is a random function. In practical applications, the domain over which the function is defined as a time interval (time series) or a region of space (random field). Familiar examples of time series include stock market and exchange rate fluctuations, signals such as speech, audio and video; medical data such as a patient's EKG, EEG, blood pressure or temperature; and random movement such as Brownian motion or random walks. Examples of random fields include static images, random topographies (landscapes), or composition variations of an inhomogeneous material.
Deep sampling is a variation of statistical sampling in which precision is sacrificed for insight. Small numbers of samples are taken, with each sample containing much information. The samples are taken approximately uniformly over the resource of interest, such as time or space. It is useful for identifying large hidden problems.
A percentage point (pp) is the unit for the arithmetic difference of two percentages. For example, going from 40% to 44% is a 4 percentage point increase. In the literature, the percentage point unit is usually either written out, or abbreviated as pp, p.p. or %. Consider the following hypothetical example: In 1980, 40 percent of the population smoked, and in 1990 only 30 percent smoked. One can thus say that from 1980 to 1990, the prevalence of smoking decreased by 10 percentage points although smoking did not decrease by 10 percent (actually it decreased by 25 percent)   percentages indicate ratios, not differences. Percentage point differences are one way to express a risk or probability. Consider a drug that cures a given disease in 70 percent of all cases, while without the drug, the disease heals spontaneously in only 50 percent of cases. The drug reduces absolute risk by 20 percentage points. Alternatives may be more meaningful to consumers of statistics, such as the reciprocal, also known as the number needed to treat (NNT). In this case, the reciprocal transform of the percentage point difference would be 1/(20%) = 1/0.20 = 5. Thus if 5 patients are treated with the drug, one could expect to heal one more case of the disease than would have occurred in the absence of the drug. For measurements with percentage as unit, like growth, yield, or ejection fraction, the standard deviation will have percentage points as unit. Mistakenly using percentage as the unit for the standard deviation is confusing since percentage is also used as a unit for the relative standard deviation, i.e. the standard deviation divided by the average value (Coefficient of Variation).
Sally Clark (August 1964   15 March 2007) was a British solicitor who, in November 1999, became the victim of a miscarriage of justice when she was found guilty of the murder of two of her sons. Although the conviction was overturned and she was freed from prison in 2003, the experience caused her to develop serious psychiatric problems and she died in her home in March 2007 from alcohol poisoning. Clark's first son died suddenly within a few weeks of his birth in September 1996, and in December 1998 her second died in a similar manner. A month later, she was arrested and subsequently tried for the murder of both children. The prosecution case relied on significantly flawed statistical evidence presented by paediatrician Professor Sir Roy Meadow, who testified that the chance of two children from an affluent family suffering sudden infant death syndrome was 1 in 73 million. He had arrived at this figure erroneously by squaring 1 in 8500, as being the likelihood of a cot death in similar circumstances. The Royal Statistical Society later issued a statement arguing that there was "no statistical basis" for Meadow's claim, and expressing its concern at the "misuse of statistics in the courts". Clark was convicted in November 1999. The convictions were upheld at appeal in October 2000, but overturned in a second appeal in January 2003, after it emerged that Dr Alan Williams, the prosecution forensic pathologist who examined both of her babies, had incompetently failed to disclose microbiological reports that suggested the second of her sons had died of natural causes. She was released from prison having served more than three years of her sentence. The journalist Geoffrey Wansell called Clark's experience "one of the great miscarriages of justice in modern British legal history". As a result of her case, the Attorney-General ordered a review of hundreds of other cases, and two other women had their convictions overturned.
In quality control, multi-vari charts are a visual way of presenting variability through a series of charts. The content and format of the charts has evolved over time.
Geometric data analysis can refer to geometric aspects of image analysis, pattern analysis and shape analysis or the approach of multivariate statistics that treats arbitrary data sets as clouds of points in n-dimensional space. This includes topological data analysis, cluster analysis, inductive data analysis, correspondence analysis, multiple correspondence analysis, principal components analysis and fr:Iconographie des corre lations.
In probability theory and statistics, the Gumbel distribution is used to model the distribution of the maximum (or the minimum) of a number of samples of various distributions. This distribution might be used to represent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years. It is useful in predicting the chance that an extreme earthquake, flood or other natural disaster will occur. The potential applicability of the Gumbel distribution to represent the distribution of maxima relates to extreme value theory which indicates that it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type. The Gumbel distribution is a particular case of the generalized extreme value distribution (also known as the Fisher-Tippett distribution). It is also known as the log-Weibull distribution and the double exponential distribution (a term that is alternatively sometimes used to refer to the Laplace distribution). It is related to the Gompertz distribution: when its density is first reflected about the origin and then restricted to the positive half line, a Gompertz function is obtained. In the latent variable formulation of the multinomial logit model   common in discrete choice theory   the errors of the latent variables follow a Gumbel distribution. This is useful because the difference of two Gumbel-distributed random variables has a logistic distribution. The Gumbel distribution is named after Emil Julius Gumbel (1891 1966), based on his original papers describing the distribution.
A geometric stable distribution or geo-stable distribution is a type of leptokurtic probability distribution. Geometric stable distributions were introduced in Klebanov, L. B., Maniya, G. M., and Melamed, I. A. (1985). A problem of Zolotarev and analogs of infinitely divisible and stable distributions in a scheme for summing a random number of random variables. These distributions are analogues for stable distributions for the case when the number of summands is random, independent of the distribution of summand, and having geometric distribution. The geometric stable distribution may be symmetric or asymmetric. A symmetric geometric stable distribution is also referred to as a Linnik distribution. The Laplace distribution and asymmetric Laplace distribution are special cases of the geometric stable distribution. The Laplace distribution is also a special case of a Linnik distribution. The Mittag Leffler distribution is also a special case of a geometric stable distribution. The geometric stable distribution has applications in finance theory.
Boolean analysis was introduced by Flament (1976). The goal of a Boolean analysis is to detect deterministic dependencies between the items of a questionnaire or similar data-structures in observed response patterns. These deterministic dependencies have the form of logical formulas connecting the items. Assume, for example, that a questionnaire contains items i, j, and k. Examples of such deterministic dependencies are then i   j, i   j   k, and i   j   k. Since the basic work of Flament (1976) a number of different methods for Boolean analysis have been developed. See, for example, Buggenhaut and Degreef (1987), Duquenne (1987), item tree analysis Leeuwe (1974), Schrepp (1999), or Theuns (1998). These methods share the goal to derive deterministic dependencies between the items of a questionnaire from data, but differ in the algorithms to reach this goal. Boolean analysis is an explorative method to detect deterministic dependencies between items. The detected dependencies must be confirmed in subsequent research. Methods of Boolean analysis do not assume that the detected dependencies describe the data completely. There may be other probabilistic dependencies as well. Thus, a Boolean analysis tries to detect interesting deterministic structures in the data, but has not the goal to uncover all structural aspects in the data set. Therefore, it makes sense to use other methods, like for example latent class analysis, together with a Boolean analysis.
In Bayesian statistics, a maximum a posterior probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to Fisher's method of maximum likelihood (ML), but employs an augmented optimization objective which incorporates a prior distribution over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of ML estimation.
In statistics, Yates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. In some cases, Yates' correction may adjust too far, and so its current use is limited.
In signal processing, a matched filter is obtained by correlating a known signal, or template, with an unknown signal to detect the presence of the template in the unknown signal. This is equivalent to convolving the unknown signal with a conjugated time-reversed version of the template. The matched filter is the optimal linear filter for maximizing the signal to noise ratio (SNR) in the presence of additive stochastic noise. Matched filters are commonly used in radar, in which a known signal is sent out, and the reflected signal is examined for common elements of the out-going signal. Pulse compression is an example of matched filtering. It is so called because impulse response is matched to input pulse signals. Two-dimensional matched filters are commonly used in image processing, e.g., to improve SNR for X-ray. Matched filtering is a demodulation technique with LTI (linear time invariant) filters to maximize SNR. It was originally also known as a North filter.
In statistics, standardized coefficients or beta coefficients are the estimates resulting from a regression analysis that have been standardized so that the variances of dependent and independent variables are 1. Therefore, standardized coefficients refer to how many standard deviations a dependent variable will change, per standard deviation increase in the predictor variable. For univariate regression, the absolute value of the standardized coefficient equals the correlation coefficient. Standardization of the coefficient is usually done to answer the question of which of the independent variables have a greater effect on the dependent variable in a multiple regression analysis, when the variables are measured in different units of measurement (for example, income measured in dollars and family size measured in number of individuals). Some statistical software packages like PSPP, SPSS and SYSTAT label the standardized regression coefficients as "Beta" while the unstandardized coefficients are labeled "B". Others, like DAP/SAS label them "Standardized Coefficient". Sometimes the unstandardized variables are also labeled as "b". A regression carried out on original (unstandardized) variables produces unstandardized coefficients. A regression carried out on standardized variables produces standardized coefficients. Values for standardized and unstandardized coefficients can also be derived subsequent to either type of analysis. Before solving a multiple regression problem, all variables (independent and dependent) can be standardized. Each variable can be standardized by subtracting its mean from each of its values and then dividing these new values by the standard deviation of the variable. Standardizing all variables in a multiple regression yields standardized regression coefficients that show the change in the dependent variable measured in standard deviations. Advantages Standard coefficients' advocates note that the coefficients ignore the independent variable's scale of units, which makes comparisons easy. Disadvantages Critics voice concerns that such a standardization can be misleading. Since standardizing a variable removes the unit of measurement from its value, a standardized coefficient for a given relationship only represents its strength relative to the variation in the distributions. This invites bias due to sampling error when one standardizes variables using means and standard deviations based on small samples. Furthermore, a change of one standard deviation in one variable is only equivalent to a change of one standard deviation in another predictor insofar as the shapes of the two variables' distributions resemble one another. The meaning of a standard deviation may vary markedly between non-normal distributions (e.g., when skewed or otherwise asymmetrical). This underscores the importance of normality assumptions in parametric statistics, and poses an additional problem when interpreting standardized coefficient estimates that even nonparametric regression does not solve when dealing with non-normal distributions.
In multivariate analysis, ordination or gradient analysis is a method complementary to data clustering, and used mainly in exploratory data analysis (rather than in hypothesis testing). Ordination orders objects that are characterized by values on multiple variables (i.e., multivariate objects) so that similar objects are near each other and dissimilar objects are farther from each other. These relationships between the objects, on each of several axes (one for each variable), are then characterized numerically and/or graphically. Many ordination techniques exist, including principal components analysis (PCA), non-metric multidimensional scaling (NMDS), correspondence analysis (CA) and its derivatives (detrended CA (DCA), canonical CA (CCA)), Bray Curtis ordination, and redundancy analysis (RDA), among others.
The Mantel test, named after Nathan Mantel, is a statistical test of the correlation between two matrices. The matrices must be of the same rank; in most applications, they are matrices of interrelations between the same vectors of objects. The test was first published by Nathan Mantel, a biostatistician at the National Institutes of Health, in 1967. Accounts of it can be found in advanced statistics books (e.g., Sokal & Rohlf 1995).
In mathematics, an information source is a sequence of random variables ranging over a finite alphabet  , having a stationary distribution. The uncertainty, or entropy rate, of an information source is defined as  where  is the sequence of random variables defining the information source, and  is the conditional information entropy of the sequence of random variables. Equivalently, one has
Stochastic calculus is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. It is used to model systems that behave randomly. The best-known stochastic process to which stochastic calculus is applied is the Wiener process (named in honor of Norbert Wiener), which is used for modeling Brownian motion as described by Louis Bachelier in 1900 and by Albert Einstein in 1905 and other physical diffusion processes in space of particles subject to random forces. Since the 1970s, the Wiener process has been widely applied in financial mathematics and economics to model the evolution in time of stock prices and bond interest rates. The main flavours of stochastic calculus are the Ito  calculus and its variational relative the Malliavin calculus. For technical reasons the Ito  integral is the most useful for general classes of processes but the related Stratonovich integral is frequently useful in problem formulation (particularly in engineering disciplines.) The Stratonovich integral can readily be expressed in terms of the Ito  integral. The main benefit of the Stratonovich integral is that it obeys the usual chain rule and therefore does not require Ito 's lemma. This enables problems to be expressed in a coordinate system invariant form, which is invaluable when developing stochastic calculus on manifolds other than Rn. The dominated convergence theorem does not hold for the Stratonovich integral, consequently it is very difficult to prove results without re-expressing the integrals in Ito  form.
Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning. They are typically used in complex statistical models consisting of observed variables (usually termed "data") as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model. As is typical in Bayesian inference, the parameters and latent variables are grouped together as "unobserved variables". Variational Bayesian methods are primarily used for two purposes: To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables. To derive a lower bound for the marginal likelihood (sometimes called the "evidence") of the observed data (i.e. the marginal probability of the data given the model, with marginalization performed over unobserved variables). This is typically used for performing model selection, the general idea being that a higher marginal likelihood for a given model indicates a better fit of the data by that model and hence a greater probability that the model in question was the one that generated the data. (See also the Bayes factor article.) In the former purpose (that of approximating a posterior probability), variational Bayes is an alternative to Monte Carlo sampling methods   particularly, Markov chain Monte Carlo methods such as Gibbs sampling   for taking a fully Bayesian approach to statistical inference over complex distributions that are difficult to directly evaluate or sample from. In particular, whereas Monte Carlo techniques provide a numerical approximation to the exact posterior using a set of samples, Variational Bayes provides a locally-optimal, exact analytical solution to an approximation of the posterior. Variational Bayes can be seen as an extension of the EM (expectation-maximization) algorithm from maximum a posteriori estimation (MAP estimation) of the single most probable value of each parameter to fully Bayesian estimation which computes (an approximation to) the entire posterior distribution of the parameters and latent variables. As in EM, it finds a set of optimal parameter values, and it has the same alternating structure as does EM, based on a set of interlocked (mutually dependent) equations that cannot be solved analytically. For many applications, variational Bayes produces solutions of comparable accuracy to Gibbs sampling at greater speed. However, deriving the set of equations used to iteratively update the parameters often requires a large amount of work compared with deriving the comparable Gibbs sampling equations. This is the case even for many models that are conceptually quite simple, as is demonstrated below in the case of a basic non-hierarchical model with only two parameters and no latent variables.
Combinatorial meta-analysis (CMA) is the study of the behaviour of statistical properties of combinations of studies from a meta-analytic dataset (typically in social science research). In an article that develops the notion of "gravity" in the context of meta-analysis, Dr. Travis Gee proposed that the jackknife methods applied to meta-analysis in that article could be extended to examine all possible combinations of studies (where practical) or random subsets of studies (where the combinatorics of the situation made it computationally infeasible).
Cumulative incidence or incidence proportion is a measure of frequency, as in epidemiology, where it is a measure of disease frequency during a period of time. Where the period of time considered is an entire lifetime, the incidence proportion is called lifetime risk. Cumulative incidence is defined as the probability that a particular event, such as occurrence of a particular disease, has occurred before a given time. It is equivalent to the incidence, calculated using a period of time during which all of the individuals in the population are considered to be at risk for the outcome. It is sometimes also referred to as the incidence proportion. Cumulative incidence is calculated by the number of new cases during a period divided by the number of subjects at risk in the population at the beginning of the study. It may also be calculated by the incidence rate multiplied by duration:
In computer science, computational learning theory (or just learning theory) is a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms.
In probability theory and statistics, the bivariate von Mises distribution is a probability distribution describing values on a torus. It may be thought of as an analogue on the torus of the bivariate normal distribution. The distribution belongs to the field of directional statistics. The general bivariate von Mises distribution was first proposed by Kanti Mardia in 1975. One of its variants is today used in the field of bioinformatics to formulate a probabilistic model of protein structure in atomic detail.
An experiment is a procedure carried out to verify, refute, or validate a hypothesis. Experiments provide insight into cause-and-effect by demonstrating what outcome occurs when a particular factor is manipulated. Experiments vary greatly in goal and scale, but always rely on repeatable procedure and logical analysis of the results. There also exist natural experimental studies. A child may carry out basic experiments to understand gravity, while teams of scientists may take years of systematic investigation to advance their understanding of a phenomenon. Experiments and other types of hands-on activities are very important to student learning in the science classroom. Experiments can raise test scores and help a student become more engaged and interested in the material they are learning, especially when used over time. Experiments can vary from personal and informal natural comparisons (e.g. tasting a range of chocolates to find a favorite), to highly controlled (e.g. tests requiring complex apparatus overseen by many scientists that hope to discover information about subatomic particles). Uses of experiments vary considerably between the natural and human sciences. Experiments typically include controls, which are designed to minimize the effects of variables other than the single independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. Ideally, all variables in an experiment are controlled (accounted for by the control measurements) and none are uncontrolled. In such an experiment, if all controls work as expected, it is possible to conclude that the experiment works as intended, and that results are due to the effect of the tested variable.
In statistics, a sum of squares due to lack of fit, or more tersely a lack-of-fit sum of squares, is one of the components of a partition of the sum of squares in an analysis of variance, used in the numerator in an F-test of the null hypothesis that says that a proposed model fits well.
In statistics, the concept of the shape of the distribution refers to the shape of a probability distribution and it most often arises in questions of finding an appropriate distribution to use to model the statistical properties of a population, given a sample from that population. The shape of a distribution may be considered either descriptively, using terms such as "J-shaped", or numerically, using quantitative measures such as skewness and kurtosis. Considerations of the shape of a distribution arise in statistical data analysis, where simple quantitative descriptive statistics and plotting techniques such as histograms can lead on to the selection of a particular family of distributions for modelling purposes.
De Moivre's Law is a survival model applied in actuarial science, named for Abraham de Moivre. It is a simple law of mortality based on a linear survival function.
Grouped data is a statistical term used in data analysis. A raw dataset can be organized by constructing a table showing the frequency distribution of the variable (whose values are given in the raw dataset). Such a frequency table is often referred to as grouped data.
In statistics, Wilks's lambda distribution (named for Samuel S. Wilks), is a probability distribution used in multivariate hypothesis testing, especially with regard to the likelihood-ratio test and multivariate analysis of variance (MANOVA).  
Vapnik Chervonenkis theory (also known as VC theory) was developed during 1960 1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view. VC theory is related to statistical learning theory and to empirical processes. Richard M. Dudley and Vladimir Vapnik, among others, have applied VC-theory to empirical processes.
In probability theory, the law (or formula) of total probability is a fundamental rule relating marginal probabilities to conditional probabilities. It expresses the total probability of an outcome which can be realized via several distinct events - hence the name.
TinkerPlots is exploratory data analysis and modeling software designed for use by students in grades 4 through university. It was designed by Clifford Konold and Craig Miller at the University of Massachusetts Amherst and is currently published by the Learn Troop. It runs on Windows XP or later and Mac OS 10.4 or later. The program allows users to enter their own data, to import them from other applications or the Web, or to generate them using a sampling engine. The program also comes with 50 multivariate data sets. Using TinkerPlots, students can make a large variety of graphs, including those specified for middle school in Common Core State Standards for Mathematics But rather than making these graphs directly using commands, students construct them by progressively organizing cases using basic operations including  stack,   order,  and  separate.  Responding to these operations, case icons animate into different screen positions. The interface was based on observations of people organizing  data cards  on a table to make graphs to answer specific questions  Innovations of TinkerPlots include using a superimposed color gradient to detect covariation in two numeric attributes and a  hat plot,  a reformulated and generalized version of the box plot. The latest version is 2.3.2. This version does not have substantively different features from versions 2.1 and 2.2, but has a number of bug fixes.
Youden's J statistic (also called Youden's index) is a single statistic that captures the performance of a diagnostic test.
In statistics, a full factorial experiment is an experiment whose design consists of two or more factors, each with discrete possible values or "levels", and whose experimental units take on all possible combinations of these levels across all such factors. A full factorial design may also be called a fully crossed design. Such an experiment allows the investigator to study the effect of each factor on the response variable, as well as the effects of interactions between factors on the response variable. For the vast majority of factorial experiments, each factor has only two levels. For example, with two factors each taking two levels, a factorial experiment would have four treatment combinations in total, and is usually called a 2 2 factorial design. If the number of combinations in a full factorial design is too high to be logistically feasible, a fractional factorial design may be done, in which some of the possible combinations (usually at least half) are omitted.
The three-point estimation technique is used in management and information systems applications for the construction of an approximate probability distribution representing the outcome of future events, based on very limited information. While the distribution used for the approximation might be a normal distribution, this is not always so and, for example a triangular distribution might be used, depending on the application., In three-point estimation, three figures are produced initially for every distribution that is required, based on prior experience or best-guesses: a = the best-case estimate m = the most likely estimate b = the worst-case estimate These are then combined to yield either a full probability distribution, for later combination with distributions obtained similarly for other variables, or summary descriptors of the distribution, such as the mean, standard deviation or percentage points of the distribution. The accuracy attributed to the results derived can be no better than the accuracy inherent in the 3 initial points, and there are clear dangers in using an assumed form for an underlying distribution that itself has little basis.
In statistics, the coefficient of determination, denoted R2 or r2 and pronounced R squared, is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable. It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model (pp. 187, 287). There are several definitions of R2 that are only sometimes equivalent. One class of such cases includes that of simple linear regression where r2 is used instead of R2. In this case, if an intercept is included, then r2 is simply the square of the sample correlation coefficient (i.e., r) between the outcomes and their predicted values. If additional explanators are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination ranges from 0 to 1. Important cases where the computational definition of R2 can yield negative values, depending on the definition used, arise where the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data, and where linear regression is conducted without including an intercept. Additionally, negative values of R2 may occur when fitting non-linear functions to data. In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion.
Recursive Bayesian estimation, also known as a Bayes filter, is a general probabilistic approach for estimating an unknown probability density function recursively over time using incoming measurements and a mathematical process model. The Bayes filter should not be confused with Bayes spam filtering, which is also often referred to as Bayesian filtering. In this article, filtering is the process of sequentially estimating the states of a dynamic system (see Sequential Bayesian filtering below). In Bayes spam filtering, the term filter denotes the separation of spam and non-spam content.
Quota sampling is a method for selecting survey participants that is a non-probabilistic version of stratified sampling.
In statistics, and particularly in econometrics, the reduced form of a system of equations is the result of solving the system for the endogenous variables. This gives the latter as a function of the exogenous variables, if any. In econometrics, "structural form" models begin from deductive theories of the economy, while "reduced form" models begin by identifying particular relationships between variables. Let Y and X be random vectors. Y is the vector of the variables to be explained (endogeneous variables) by a statistical model and X is the vector of explanatory (exogeneous) variables. In addition let  be a vector of error terms. Then the general expression of a structural form is , where f is a function, possibly from vectors to vectors in the case of a multiple-equation model. The reduced form of this model is given by , with g a function.
In probability theory, fractional Brownian motion (fBm), also called a fractal Brownian motion, is a generalization of Brownian motion. Unlike classical Brownian motion, the increments of fBm need not be independent. fBm is a continuous-time Gaussian process BH(t) on [0, T], which starts at zero, has expectation zero for all t in [0, T], and has the following covariance function:  where H is a real number in (0, 1), called the Hurst index or Hurst parameter associated with the fractional Brownian motion. The Hurst exponent describes the raggedness of the resultant motion, with a higher value leading to a smoother motion. It was introduced by Mandelbrot & van Ness (1968). The value of H determines what kind of process the fBm is: if H = 1/2 then the process is in fact a Brownian motion or Wiener process; if H > 1/2 then the increments of the process are positively correlated; if H < 1/2 then the increments of the process are negatively correlated. The increment process, X(t) = BH(t+1)   BH(t), is known as fractional Gaussian noise. There is also a generalization of fractional Brownian motion: n-th order fractional Brownian motion, abbreviated as n-fBm. n-fBm is a Gaussian, self-similar, non-stationary process whose increments of order n are stationary. For n = 1, n-fBm is classical fBm. Like the Brownian motion that it generalizes, fractional Brownian motion is named after 19th century biologist Robert Brown; fractional Gaussian noise is named after mathematician Carl Friedrich Gauss.
In mathematics, the Kolmogorov continuity theorem is a theorem that guarantees that a stochastic process that satisfies certain constraints on the moments of its increments will be continuous (or, more precisely, have a "continuous version"). It is credited to the Soviet mathematician Andrey Nikolaevich Kolmogorov.
A case-control study is a type of observational study in which two existing groups differing in outcome are identified and compared on the basis of some supposed causal attribute. Case-control studies are often used to identify factors that may contribute to a medical condition by comparing subjects who have that condition/disease (the "cases") with patients who do not have the condition/disease but are otherwise similar (the "controls"). They require fewer resources but provide less evidence for causal inference than a randomized controlled trial.
The runs test (also called Wald Wolfowitz test after Abraham Wald and Jacob Wolfowitz) is a non-parametric statistical test that checks a randomness hypothesis for a two-valued data sequence. More precisely, it can be used to test the hypothesis that the elements of the sequence are mutually independent. A "run" of a sequence is a maximal non-empty segment of the sequence consisting of adjacent equal elements. For example, the 22-element-long sequence "++++   +++  ++++++    " consists of 6 runs, 3 of which consist of "+" and the others of " ". The run test is based on the null hypothesis that each element in the sequence is independently drawn from the same distribution. Under the null hypothesis, the number of runs in a sequence of N elements is a random variable whose conditional distribution given the observation of N+ positive values and N  negative values (N = N+ + N ) is approximately normal, with: mean  variance  These parameters do not assume that the positive and negative elements have equal probabilities of occurring, but only assume that the elements are independent and identically distributed. If the number of runs is significantly higher or lower than expected, the hypothesis of statistical independence of the elements may be rejected. Runs tests can be used to test: the randomness of a distribution, by taking the data in the given order and marking with + the data greater than the median, and with   the data less than the median; (Numbers equalling the median are omitted.) whether a function fits well to a data set, by marking the data exceeding the function value with + and the other data with  . For this use, the runs test, which takes into account the signs but not the distances, is complementary to the chi square test, which takes into account the distances but not the signs. The Kolmogorov Smirnov test has been shown to be more powerful than the Wald-Wolfowitz test for detecting differences between distributions that differ solely in their location. However, the reverse is true if the distributions differ in variance and have at the most only a small difference in location.[5] The Wald-Wolfowitz runs test has been extended for use with several samples [6] [7].
In statistics, Tukey's test of additivity, named for John Tukey, is an approach used in two-way ANOVA (regression analysis involving two qualitative factors) to assess whether the factor variables are additively related to the expected value of the response variable. It can be applied when there are no replicated values in the data set, a situation in which it is impossible to directly estimate a fully general non-additive regression structure and still have information left to estimate the error variance. The test statistic proposed by Tukey has one degree of freedom under the null hypothesis, hence this is often called "Tukey's one-degree-of-freedom test."
In statistics and probability theory, a median is the number separating the higher half of a data sample, a population, or a probability distribution, from the lower half. The median of a finite list of numbers can be found by arranging all the observations from lowest value to highest value and picking the middle one (e.g., the median of {3, 3, 5, 9, 11} is 5). If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values   (the median of {3, 5, 7, 9} is (5 + 7) / 2 = 6), which corresponds to interpreting the median as the fully trimmed mid-range. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large result. A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions. In a sample of data, or a finite population, there may be no member of the sample whose value is identical to the median (in the case of an even sample size); if there is such a member, there may be more than one so that the median may not uniquely identify a sample member. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid. At most, half the population have values strictly less than the median, and, at most, half have values strictly greater than the median. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if a < b < c, then the median of the list {a, b, c} is b, and, if a < b < c < d, then the median of the list {a, b, c, d} is the mean of b and c; i.e., it is (b + c)/2. The median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors. In terms of notation, some authors represent the median of a variable x either as x  or as  1/2 sometimes also M. There is no widely accepted standard notation for the median, so the use of these or other symbols for the median needs to be explicitly defined when they are introduced. The median is the 2nd quartile, 5th decile, and 50th percentile.  
In statistics the Maxwell Boltzmann distribution is a particular probability distribution named after James Clerk Maxwell and Ludwig Boltzmann. It was first defined and used in physics (in particular in statistical mechanics) for describing particle speeds in idealized gases where the particles move freely inside a stationary container without interacting with one another, except for very brief collisions in which they exchange energy and momentum with each other or with their thermal environment. Particle in this context refers to gaseous particles (atoms or molecules), and the system of particles is assumed to have reached thermodynamic equilibrium. While the distribution was first derived by Maxwell in 1860 on heuristic grounds, Boltzmann later carried out significant investigations into the physical origins of this distribution. A particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another. The distribution depends on the temperature of the system and the mass of the particle. The Maxwell Boltzmann distribution applies to the classical ideal gas, which is an idealization of real gases. In real gases, there are various effects (e.g., van der Waals interactions, vortical flow, relativistic speed limits, and quantum exchange interactions) that can make their speed distribution different from the Maxwell Boltzmann form. However, rarefied gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. Thus, it forms the basis of the Kinetic theory of gases, which provides a simplified explanation of many fundamental gaseous properties, including pressure and diffusion.
In mathematics, a stopped process is a stochastic process that is forced to assume the same value after a prescribed (possibly random) time.
In statistics, sequential analysis or sequential hypothesis testing is statistical analysis where the sample size is not fixed in advance. Instead data is evaluated as it is collected, and further sampling is stopped in accordance with a pre-defined stopping rule as soon as significant results are observed. Thus a conclusion may sometimes be reached at a much earlier stage than would be possible with more classical hypothesis testing or estimation, at consequently lower financial and/or human cost.
In mathematics, the Gaussian isoperimetric inequality, proved by Boris Tsirelson and Vladimir Sudakov and independently by Christer Borell, states that among all sets of given Gaussian measure in the n-dimensional Euclidean space, half-spaces have the minimal Gaussian boundary measure.
Bayesian refers to methods in probability and statistics named after Thomas Bayes (c. 1702 61), in particular methods related to statistical inference: Bayesian probability or degree-of-belief interpretation of probability, as opposed to frequency or proportion or propensity interpretations Bayes' theorem on conditional probability Bayesian inference These methods include: Bayes estimator Bayes factor Bayesian average Bayesian spam filtering Bayesian game Bayesian information criterion Bayesian multivariate linear regression Bayesian linear regression, a special case  Bayesian network Empirical Bayes method Naive Bayes classifier Bayesian econometrics Bayesian experimental design Bayesian inference in phylogeny Bayesian search theory Bayesian vector autoregression Bayesian also refers to the application of this probability theory to the functioning of the brain: Bayesian approaches to brain function Bayesian methods have been also applied to the interpretation of quantum mechanics: Quantum Bayesianism
The Helmert Wolf blocking (HWB) is a least squares solution method for a sparse canonical block-angular (CBA) system of linear equations. Helmert (1843 1917) reported on the use of such systems for geodesy in 1880. Wolf (1910 1994) published his direct semianalytic solution based on ordinary Gaussian elimination in matrix form  in 1978.
In statistics, the Nemenyi test is a post-hoc test intended to find the groups of data that differ after a statistical test of multiple comparisons (such as the Friedman test) has rejected the null hypothesis that the performance of the comparisons on the groups of data is similar. The test makes pair-wise tests of performance. The test is named after Peter Nemenyi. The test is sometimes referred to as the "Nemenyi Damico Wolfe Dunn test".
In probability and statistics, a circular distribution or polar distribution is a probability distribution of a random variable whose values are angles, usually taken to be in the range [0, 2 ). A circular distribution is often a continuous probability distribution, and hence has a probability density, but such distributions can also be discrete, in which case they are called circular lattice distributions. Circular distributions can be used even when the variables concerned are not explicitly angles: the main consideration is that there is not usually any real distinction between events occurring at the lower or upper end of the range, and the division of the range could notionally be made at any point.
In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. Missing data can occur because of nonresponse: no information is provided for several items or no information is provided for a whole unit. Some items are more sensitive for nonresponse than others, for example items about private subjects such as income. Dropout is a type of missingness that occurs mostly when studying development over time. In this type of study the measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing. Sometimes missing values are caused by the researcher for example, when data collection is done improperly or mistakes are made in data entry. Data often are missing in research in economics, sociology, and political science because governments choose not to, or fail to, report critical statistics.
In statistics, a paired difference test is a type of location test that is used when comparing two sets of measurements to assess whether their population means differ. A paired difference test uses additional information about the sample that is not present in an ordinary unpaired testing situation, either to increase the statistical power, or to reduce the effects of confounders. Specific methods for carrying out paired difference tests are, for normally distributed difference t-test (where the population standard deviation of difference is not known) and the paired Z-test (where the population standard deviation of the difference is known), and for differences that may not be normally distributed the Wilcoxon signed-rank test. In addition to tests that deal with non-normality, there is also a test that is robust to the common violation of homogeneity of variance across samples (an underlying assumption of these tests): this is Welch's t-test, which makes use of unpooled variance and results in unusual degrees of freedom (e.g. df' = 4.088 rather than df = 4). The most familiar example of a paired difference test occurs when subjects are measured before and after a treatment. Such a "repeated measures" test compares these measurements within subjects, rather than across subjects, and will generally have greater power than an unpaired test.
In mathematics, the Gauss Kuzmin distribution is a discrete probability distribution that arises as the limit probability distribution of the coefficients in the continued fraction expansion of a random variable uniformly distributed in (0, 1). The distribution is named after Carl Friedrich Gauss, who derived it around 1800, and Rodion Kuzmin, who gave a bound on the rate of convergence in 1929. It is given by the probability mass function  
Klecka's tau ( ) is a statistic which is used to test whether a given classification analysis improves one's classification to groups over a random allocation to the various groups under consideration. The maximum value of   is 1.0 indicating no errors in the prediction. A value of zero indicates no improvement over a random assignment. The distribution of   is not presently known and it is used as a descriptive rather than as an analytic statistic.
In probability theory, an additive Markov chain is a Markov chain with an additive conditional probability function. Here the process is a discrete-time Markov chain of order m and the transition probability to a state at the next time is a sum of functions, each depending on the next state and one of the m previous states.
Least absolute deviations (LAD), also known as least absolute errors (LAE), least absolute value (LAV), least absolute residual (LAR), sum of absolute deviations, or the L1 norm condition, is a statistical optimality criterion and the statistical optimization technique that relies on it. Similar to the popular least squares technique, it attempts to find a function which closely approximates a set of data. In the simple case of a set of (x,y) data, the approximation function is a simple "trend line" in two-dimensional Cartesian coordinates. The method minimizes the sum of absolute errors (SAE) (the sum of the absolute values of the vertical "residuals" between points generated by the function and corresponding points in the data). The least absolute deviations estimate also arises as the maximum likelihood estimate if the errors have a Laplace distribution.
In probability theory and statistics, a stochastic order quantifies the concept of one random variable being "bigger" than another. These are usually partial orders, so that one random variable  may be neither stochastically greater than, less than nor equal to another random variable . Many different orders exist, which have different applications.
In probability theory, the law of total variance or variance decomposition formula, also known as Eve's law, states that if X and Y are random variables on the same probability space, and the variance of Y is finite, then  Some writers on probability call this the "conditional variance formula". In language perhaps better known to statisticians than to probabilists, the two terms are the "unexplained" and the "explained" components of the variance respectively (cf. fraction of variance unexplained, explained variation). In actuarial science, specifically credibility theory, the first component is called the expected value of the process variance (EVPV) and the second is called the variance of the hypothetical means (VHM). There is a general variance decomposition formula for c   2 components (see below). For example, with two conditioning random variables:  which follows from the law of total conditional variance:  Note that the conditional expected value E( Y | X ) is a random variable in its own right, whose value depends on the value of X. Notice that the conditional expected value of Y given the event X = x is a function of x (this is where adherence to the conventional and rigidly case-sensitive notation of probability theory becomes important!). If we write E( Y | X = x ) = g(x) then the random variable E( Y | X ) is just g(X). Similar comments apply to the conditional variance. One special case, (similar to the Law of total expectation) states that if  is a partition of the whole outcome space, i.e. these events are mutually exclusive and exhaustive, then  In this formula, the first component is the expectation of the conditional variance; the other two rows are the variance of the conditional expectation.
In statistics, listwise deletion is a method for handling missing data. In this method, an entire record is excluded from analysis if any single value is missing.
In linguistics, Heaps' law (also called Herdan's law) is an empirical law which describes the number of distinct words in a document (or set of documents) as a function of the document length (so called type-token relation). It can be formulated as  where VR is the number of distinct words in an instance text of size n. K and   are free parameters determined empirically. With English text corpora, typically K is between 10 and 100, and   is between 0.4 and 0.6. The law is frequently attributed to Harold Stanley Heaps, but was originally discovered by Gustav Herdan (1960). Under mild assumptions, the Herdan Heaps law is asymptotically equivalent to Zipf's law concerning the frequencies of individual words within a text. This is a consequence of the fact that the type-token relation (in general) of a homogenous text can be derived from the distribution of its types. Heaps' law means that as more instance text is gathered, there will be diminishing returns in terms of discovery of the full vocabulary from which the distinct terms are drawn. It is interesting to note that Heaps' law also applies to situations in which the "vocabulary" is just some set of distinct types which are attributes of some collection of objects. For example, the objects could be people, and the types could be country of origin of the person. If persons are selected randomly (that is, we are not selecting based on country of origin), then Heaps' law says we will quickly have representatives from most countries (in proportion to their population) but it will become increasingly difficult to cover the entire set of countries by continuing this method of sampling.
In probability theory, the arcsine distribution is the probability distribution whose cumulative distribution function is  for 0   x   1, and whose probability density function is  on (0, 1). The standard arcsine distribution is a special case of the beta distribution with   =   = 1/2. That is, if  is the standard arcsine distribution then  The arcsine distribution appears in the Le vy arcsine law; in the Erdo s arcsine law; as the Jeffreys prior for the probability of success of a Bernoulli trial.
In mathematics, the term ergodic is used to describe a dynamical system which, broadly speaking, has the same behavior averaged over time as averaged over the space of all the system's states (phase space). In physics the term is used to imply that a system satisfies the ergodic hypothesis of thermodynamics. In statistics, the term describes a random process for which the time average of one sequence of events is the same as the ensemble average. In other words, for a Markov chain, as one increases the steps, there exists a positive probability measure at step  that is independent of the probability distribution at initial step 0 (Feller, 1971, p. 271).
In statistics, simple linear regression is the least squares estimator of a linear regression model with a single explanatory variable. In other words, simple linear regression fits a straight line through the set of n points in such a way that makes the sum of squared residuals of the model (that is, vertical distances between the points of the data set and the fitted line) as small as possible. The adjective simple refers to the fact that the outcome variable is related to a single predictor. The slope of the fitted line is equal to the correlation between y and x corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that it passes through the center of mass (x, y) of the data points. Other regression methods besides the simple ordinary least squares (OLS) also exist. In particular, when one wants to do regression by eye, one usually tends to draw a slightly steeper line, closer to the one produced by the total least squares method. This occurs because it is more natural for one's mind to consider the orthogonal distances from the observations to the regression line, rather than the vertical ones as OLS method does.
In Bayesian statistics, a hyperparameter is a parameter of a prior distribution; the term is used to distinguish them from parameters of the model for the underlying system under analysis. For example, if one is using a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: p is a parameter of the underlying system (Bernoulli distribution), and   and   are parameters of the prior distribution (beta distribution), hence hyperparameters. One may take a single value for a given hyperparameter, or one can iterate and take a probability distribution on the hyperparameter itself, called a hyperprior.
The Herfindahl index (also known as Herfindahl Hirschman Index, or HHI) is a measure of the size of firms in relation to the industry and an indicator of the amount of competition among them. Named after economists Orris C. Herfindahl and Albert O. Hirschman, it is an economic concept widely applied in competition law, antitrust and also technology management. It is defined as the sum of the squares of the market shares of the firms within the industry (sometimes limited to the 50 largest firms), where the market shares are expressed as fractions. The result is proportional to the average market share, weighted by market share. As such, it can range from 0 to 1.0, moving from a huge number of very small firms to a single monopolistic producer. Increases in the Herfindahl index generally indicate a decrease in competition and an increase of market power, whereas decreases indicate the opposite. Alternatively, if whole percentages are used, the index ranges from 0 to 10,000 "points". For example, an index of .25 is the same as 2,500 points. The major benefit of the Herfindahl index in relationship to such measures as the concentration ratio is that it gives more weight to larger firms. The measure is essentially equivalent to the Simpson diversity index, which is a diversity index used in ecology, and to the inverse participation ratio (IPR) in physics.
Zelen's design is an experimental design for randomized clinical trials proposed by Harvard School of Public Health statistician Marvin Zelen (1927-2014). In this design, patients are randomized to either the treatment or control group before giving informed consent. Because the group to which a given patient is assigned is known, consent can be sought conditionally.
In probability theory and statistics, covariance is a measure of how much two random variables change together. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. For example, as a balloon is blown up it gets larger in all dimensions. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. If a sealed balloon is squashed in one dimension then it will expand in the other two. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. A distinction must be made between (1) the covariance of two random variables, which is a population parameter that can be seen as a property of the joint probability distribution, and (2) the sample covariance, which serves as an estimated value of the parameter.
In probability theory and statistics, the Le vy distribution, named after Paul Le vy, is a continuous probability distribution for a non-negative random variable. In spectroscopy, this distribution, with frequency as the dependent variable, is known as a van der Waals profile. It is a special case of the inverse-gamma distribution. It is one of the few distributions that are stable and that have probability density functions that can be expressed analytically, the others being the normal distribution and the Cauchy distribution.
In statistical theory, a pseudolikelihood is an approximation to the joint probability distribution of a collection of random variables. The practical use of this is that it can provide an approximation to the likelihood function of a set of observed data which may either provide a computationally simpler problem for estimation, or may provide a way of obtaining explicit estimates of model parameters. The pseudolikelihood approach was introduced by Julian Besag in the context of analysing data having spatial dependence.
Multistage testing is an algorithm-based approach to administering tests. It is very similar to computer-adaptive testing in that items are interactively selected for each examinee by the algorithm, but rather than selecting individual items, groups of items are selected, building the test in stages. These groups are called testlets or panels. While multistage tests could theoretically be administered by a human, the extensive computations required (often using item response theory) mean that multistage tests are administered by computer. The number of stages or testlets can vary. If the testlets are relatively small, such as five items, ten or more could easily be used in a test. Some multistage tests are designed with the minimum of two stages (one stage would be a conventional fixed-form test). In response to the increasing use of multistage testing, the scholarly journal Applied Measurement in Education published a special edition on the topic in 2006.
In probability theory, two random variables being uncorrelated does not imply their independence. In some contexts, uncorrelatedness implies at least pairwise independence (as when the random variables involved have Bernoulli distributions). It is sometimes mistakenly thought that one context in which uncorrelatedness implies independence is when the random variables involved are normally distributed. However, this is incorrect if the variables are merely marginally normally distributed but not jointly normally distributed. Suppose two random variables X and Y are jointly normally distributed. That is the same as saying that the random vector (X, Y) has a multivariate normal distribution. It means that the joint probability distribution of X and Y is such that each linear combination of X and Y is normally distributed, i.e. for any two constant (i.e., non-random) scalars a and b, the random variable aX + bY is normally distributed. In that case if X and Y are uncorrelated, i.e., their covariance cov(X, Y) is zero, then they are independent. However, it is possible for two random variables X and Y to be so distributed jointly that each one alone is marginally normally distributed, and they are uncorrelated, but they are not independent; examples are given below.
Spectrum continuation analysis (SCA) is a generalization of the concept of Fourier series to non-periodic functions of which only a fragment has been sampled in the time domain. Recall that a Fourier series is only suitable to the analysis of periodic (or finite-domain) functions f(x) with period 2 . It can be expressed as an infinite series of sinusoids:  where  is the amplitude of the individual harmonics. In SCA however, one decomposes the spectrum into optimized discrete frequencies. As a consequence, and as the period of the sampled function is supposed to be infinite or not yet known, each of the discrete periodic functions that compose the sampled function fragment can not be considered to be a multiple of the fundamental frequency:  As such, SCA does not necessarily deliver  periodic functions, as would have been the case in Fourier analysis. For real-valued functions, the SCA series can be written as:  where An and Bn are the series amplitudes. The amplitudes can only be solved if the series of values  is previously optimized for a desired objective function (usually least residuals).  is not necessarily the average value over the sampled interval: one might prefer to include predominant information on the behavior of the offset value in the time domain.  
Monte Carlo molecular modeling is the application of Monte Carlo methods to molecular problems. These problems can also be modeled by the molecular dynamics method. The difference is that this approach relies on equilibrium statistical mechanics rather than molecular dynamics. Instead of trying to reproduce the dynamics of a system, it generates states according to appropriate Boltzmann probabilities. Thus, it is the application of the Metropolis Monte Carlo simulation to molecular systems. It is therefore also a particular subset of the more general Monte Carlo method in statistical physics. It employs a Markov chain procedure in order to determine a new state for a system from a previous one. According to its stochastic nature, this new state is accepted at random. Each trial usually counts as a move. The avoidance of dynamics restricts the method to studies of static quantities only, but the freedom to choose moves makes the method very flexible. These moves must only satisfy a basic condition of balance in order equilibrium be properly described, but detailed balance, a stronger condition, is usually imposed when designing new algorithms. An additional advantage is that some systems, such as the Ising model, lack a dynamical description and are only defined by an energy prescription; for these the Monte Carlo approach is the only one feasible. The great success of this method in statistical mechanics has led to various generalizations such as the method of simulated annealing for optimization, in which a fictitious temperature is introduced and then gradually lowered. A range of software packages have been developed specifically for the use of the Metropolis Monte Carlo method on molecular simulations. These include: BOSS MCPro Sire ProtoMS
The propensity theory of probability is one interpretation of the concept of probability. Theorists who adopt this interpretation think of probability as a physical propensity, or disposition, or tendency of a given type of physical situation to yield an outcome of a certain kind, or to yield a long run relative frequency of such an outcome. Propensities are not relative frequencies, but purported causes of the observed stable relative frequencies. Propensities are invoked to explain why repeating a certain kind of experiment will generate a given outcome type at a persistent rate. A central aspect of this explanation is the law of large numbers. This law, which is a consequence of the axioms of probability, says that if (for example) a coin is tossed repeatedly many times, in such a way that its probability of landing heads is the same on each toss, and the outcomes are probabilistically independent, then the relative frequency of heads will (with high probability) be close to the probability of heads on each single toss. This law suggests that stable long-run frequencies are a manifestation of invariant single-case probabilities. Frequentists are unable to take this approach, since relative frequencies do not exist for single tosses of a coin, but only for large ensembles or collectives. Hence, these single-case probabilities are known as propensities or chances. In addition to explaining the emergence of stable relative frequencies, the idea of propensity is motivated by the desire to make sense of single-case probability attributions in quantum mechanics, such as the probability of decay of a particular atom at a particular time. The main challenge facing propensity theories is to say exactly what propensity means. And then, of course, to show that propensity thus defined has the required properties. At present, unfortunately, none of the well-recognised accounts of propensity comes close to meeting this challenge.
A stochastic simulation is a simulation that traces the evolution of variables that can change stochastically (randomly) with certain probabilities. With a stochastic model we create a projection which is based on a set of random values. Outputs are recorded and the projection is repeated with a new set of random values of the variables. These steps are repeated until a sufficient amount of data is gathered. In the end, the distribution of the outputs shows the most probable estimates as well as a frame of expectations regarding what ranges of values the variables are more or less likely to fall in.
Operations research, or operational research in British usage, is a discipline that deals with the application of advanced analytical methods to help make better decisions. Further, the term 'operational analysis' is used in the British (and some British Commonwealth) military, as an intrinsic part of capability development, management and assurance. In particular, operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals (COEIA), which support British defence capability acquisition decision-making. It is often considered to be a sub-field of mathematics. The terms management science and decision science are sometimes used as synonyms. Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost) of some real-world objective. Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.
The Design of Experiments is a 1935 book by the English statistician Ronald Fisher about design of experiments and is considered a foundational work in experimental design. Among other contributions, the book introduced the concept of the null hypothesis in the context of the lady tasting tea experiment. A chapter is devoted to the Latin square.
In linear regression mean response and predicted response are values of the dependent variable calculated from the regression parameters and a given value of the independent variable. The values of these two responses are the same, but their calculated variances are different.
In statistics, Fisher's method, also known as Fisher's combined probability test, is a technique for data fusion or "meta-analysis" (analysis of analyses). It was developed by and named for Ronald Fisher. In its basic form, it is used to combine the results from several independent tests bearing upon the same overall hypothesis (H0).
Discriminative models, also called conditional models, are a class of models used in machine learning for modeling the dependence of an unobserved variable  on an observed variable . Within a probabilistic framework, this is done by modeling the conditional probability distribution , which can be used for predicting  from . Discriminative models, as opposed to generative models, do not allow one to generate samples from the joint distribution of  and . However, for tasks such as classification and regression that do not require the joint distribution, discriminative models can yield superior performance. On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks. In addition, most discriminative models are inherently supervised and cannot easily be extended to unsupervised learning. Application specific details ultimately dictate the suitability of selecting a discriminative versus generative model.  
Chemotaxis (from chemo- + taxis) is the movement of an organism in response to a chemical stimulus. Somatic cells, bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming toward the highest concentration of food molecules, or to flee from poisons (e.g., phenol). In multicellular organisms, chemotaxis is critical to early development (e.g., movement of sperm towards the egg during fertilization) and subsequent phases of development (e.g., migration of neurons or lymphocytes) as well as in normal function. In addition, it has been recognized that mechanisms that allow chemotaxis in animals can be subverted during cancer metastasis. Positive chemotaxis occurs if the movement is toward a higher concentration of the chemical in question; negative chemotaxis if the movement is in the opposite direction. Chemically prompted kinesis (randomly directed or nondirectional) can be called chemokinesis.
Recurrence quantification analysis (RQA) is a method of nonlinear data analysis (cf. chaos theory) for the investigation of dynamical systems. It quantifies the number and duration of recurrences of a dynamical system presented by its phase space trajectory.
The theory of association schemes arose in statistics, in the theory of experimental design for the analysis of variance. In mathematics, association schemes belong to both algebra and combinatorics. Indeed, in algebraic combinatorics, association schemes provide a unified approach to many topics, for example combinatorial designs and coding theory. In algebra, association schemes generalize groups, and the theory of association schemes generalizes the character theory of linear representations of groups.
In probability theory, Lindeberg's condition is a sufficient condition (and under certain conditions also a necessary condition) for the central limit theorem (CLT) to hold for a sequence of independent random variables. Unlike the classical CLT, which requires that the random variables in question have finite mean and variance and be both independent and identically distributed, Lindeberg's CLT only requires that they have finite mean and variance, satisfy Lindeberg's condition, and be independent. It is named after the Finnish mathematician Jarl Waldemar Lindeberg.
In statistics, particularly in analysis of variance and linear regression, a contrast is a linear combination of variables (parameters or statistics) whose coefficients add up to zero, allowing comparison of different treatments.
In statistics, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model given data. Likelihood functions play a key role in statistical inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, "likelihood" is often used as a synonym for "probability." In statistics, a distinction is made depending on the roles of outcomes vs. parameters. Probability is used before data are available to describe possible future outcomes given a fixed value for the parameter (or parameter vector). Likelihood is used after data are available to describe a function of a parameter (or parameter vector) for a given outcome.
In time series analysis, singular spectrum analysis (SSA) is a nonparametric spectral estimation method. It combines elements of classical time series analysis, multivariate statistics, multivariate geometry, dynamical systems and signal processing. Its roots lie in the classical Karhunen (1946) Loe ve (1945, 1978) spectral decomposition of time series and random fields and in the Man e  (1981) Takens (1981) embedding theorem. SSA can be an aid in the decomposition of time series into a sum of components, each having a meaningful interpretation. The name "singular spectrum analysis" relates to the spectrum of eigenvalues in a singular value decomposition of a covariance matrix, and not directly to a frequency domain decomposition.
The Haybittle Peto boundary is a rule for deciding when to stop a clinical trial prematurely. The typical clinical trial compares two groups of patients. One group are given a placebo or conventional treatment, while the other group of patients are given the treatment that is being tested. The investigators running the clinical trial will wish to stop the trial early for ethical reasons if the treatment group clearly shows evidence of benefit. In other words, "when early results proved so promising it was no longer fair to keep patients on the older drugs for comparison, without giving them the opportunity to change." The Haybittle Peto boundary is one such stopping rule, and it states that if an interim analysis shows a probability of equal to, or less than 0.001 that a difference as extreme or more between the treatments is found, given that the null hypothesis is true, then the trial should be stopped early. The final analysis is still evaluated at the normal level of significance (usually 0.05). The main advantage of the Haybittle Peto boundary is that the same threshold is used at every interim analysis, unlike other the O'Brien Fleming boundary, which changes at every analysis. Also, using the Haybittle Peto boundary means that the final analysis is performed using a 0.05 level of significance as normal, which makes it easier for investigators and readers to understand. The main argument against the Haybittle Peto boundary is that some investigators believe that the Haybittle Peto boundary is too conservative and makes it too difficult to stop a trial.
In statistics, the phi coefficient (also referred to as the "mean square contingency coefficient" and denoted by   (or r )) is a measure of association for two binary variables. Introduced by Karl Pearson, this measure is similar to the Pearson correlation coefficient in its interpretation. In fact, a Pearson correlation coefficient estimated for two binary variables will return the phi coefficient. The square of the Phi coefficient is related to the chi-squared statistic for a 2 2 contingency table (see Pearson's chi-squared test)  where n is the total number of observations. Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal. If we have a 2 2 table for two random variables x and y  where n11, n10, n01, n00, are non-negative counts of number of observations that sum to n, the total number of observations. The phi coefficient that describes the association of x and y is  Phi is related to the point-biserial correlation coefficient and Cohen's d and estimates the extent of the relationship between two variables (2 x 2).
In statistical analysis, change detection or change point detection tries to identify times when the probability distribution of a stochastic process or time series changes. In general the problem concerns both detecting whether or not a change has occurred, or whether several changes might have occurred, and identifying the times of any such changes. Specific applications, like step detection and edge detection, may be concerned with changes in the mean, variance, correlation, or spectral density of the process. More generally change detection also includes the detection of anomalous behavior: anomaly detection.
Exponential smoothing is a rule of thumb technique for smoothing time series data, particularly for recursively applying as many as three low-pass filters with exponential window functions. Such techniques have broad application that is not intended to be strictly accurate or reliable for every situation. It is an easily learned and easily applied procedure for approximately calculating or recalling some value, or for making some determination based on prior assumptions by the user, such as seasonality. Like any application of repeated low-pass filtering, the observed phenomenon may be an essentially random process, or it may be an orderly, but noisy, process. Whereas in the simple moving average the past observations are weighted equally, exponential window functions assign exponentially decreasing weights over time. The use of three filters is based on empirical evidence and broad application. Exponential smoothing is commonly applied to smooth data, as many window functions are in signal processing, acting as low-pass filters to remove high frequency noise. This method parrots Poisson's use of recursive exponential window functions in convolutions from the 19th century, as well as Kolmogorov and Zurbenko's use of recursive moving averages from their studies of turbulence in the 1940s. See Kolmogorov-Zurbenko filter for more information. The raw data sequence is often represented by  beginning at time , and the output of the exponential smoothing algorithm is commonly written as , which may be regarded as a best estimate of what the next value of  will be. When the sequence of observations begins at time , the simplest form of exponential smoothing is given by the formulas:  where  is the smoothing factor, and .
In biostatistics, spectrum bias refers to the phenomenon that the performance of a diagnostic test may vary in different clinical settings because each setting has a different mix of patients. Because the performance may be dependent on the mix of patients, performance at one clinic may not be predictive of performance at another clinic. These differences are interpreted as a kind of bias. Mathematically, the spectrum bias is a sampling bias and not a traditional statistical bias; this has led some authors to refer to the phenomenon as spectrum effects, whilst others maintain it is a bias if the true performance of the test differs from that which is 'expected'. Usually the performance of a diagnostic test is measured in terms of its sensitivity and specificity and it is changes in these that are considered when referring to spectrum bias. However, other performance measures such as the likelihood ratios may also be affected by spectrum bias. Generally spectrum bias is considered to have three causes. The first is due to a change in the case-mix of those patients with the target disorder (disease) and this affects the sensitivity. The second is due to a change in the case-mix of those without the target disorder (disease-free) and this affects the specificity. The third is due to a change in the prevalence, and this affects both the sensitivity and specificity. This final cause is not widely appreciated, but there is mounting empirical evidence as well as theoretical arguments which suggest that it does indeed affect a test's performance. Examples where the sensitivity and specificity change between different sub-groups of patients may be found with the carcinoembryonic antigen test and urinary dipstick tests. Diagnostic test performances reported by some studies may be artificially overestimated if it is a case-control design where a healthy population ('fittest of the fit') is compared with a population with advanced disease ('sickest of the sick'); that is two extreme populations are compared, rather than typical healthy and diseased populations. If properly analyzed, recognition of heterogeneity of subgroups can lead to insights about the test's performance in varying populations.
In econometrics, panel data is data observed over two dimensions (typically, time and cross-sections). A panel data set is termed "multidimensional" when the phenomenon is observed over three or more dimensions. An example is a data set containing forecasts produced by multiple individuals (the first dimension) forecasting multiple macroeconomic variables (the second dimension) at multiple time horizons (the third dimension) and for multiple target periods (the fourth dimension).
In statistics, deviance is a quality-of-fit statistic for a model that is often used for statistical hypothesis testing. It is a generalization of the idea of using the sum of squares of residuals in ordinary least squares to cases where model-fitting is achieved by maximum likelihood.
The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical tendency of something to occur or is it a measure of how strongly one believes it will occur, or does it draw on both these elements  In answering such questions, mathematicians interpret the probability values of probability theory. There are two broad categories of probability interpretations which can be called "physical" and "evidential" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or "relative frequency", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer). Evidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom). Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of "frequentist" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the existence and importance of physical probabilities, but also consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference. The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word "frequentist" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, "frequentist probability" is just another name for physical (or objective) probability. Those who promote Bayesian inference view "frequentist statistics" as an approach to statistical inference that recognises only physical probabilities. Also the word "objective", as applied to probability, sometimes means exactly what "physical" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.  It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis.
In mathematics, Schilder's theorem is a result in the large deviations theory of stochastic processes. Roughly speaking, Schilder's theorem gives an estimate for the probability that a (scaled-down) sample path of Brownian motion will stray far from the mean path (which is constant with value 0). This statement is made precise using rate functions. Schilder's theorem is generalized by the Freidlin Wentzell theorem for Ito  diffusions.
In statistics, in the analysis of two-way randomized block designs where the response variable can take only two possible outcomes (coded as 0 and 1), Cochran's Q test is a non-parametric statistical test to verify whether k treatments have identical effects. It is named for William Gemmell Cochran. Cochran's Q test should not be confused with Cochran's C test, which is a variance outlier test. Put in less technical terms, requires that there only be a binary response (success/failure or 1/0) and that there be 2 or more matched groups (groups of the same size). The test assesses whether the proportion of successes is the same between groups. Often used to assess if different observers of the same phenomenon have consistent results amongst themselves (interobserver variability).
A multiresolution analysis (MRA) or multiscale approximation (MSA) is the design method of most of the practically relevant discrete wavelet transforms (DWT) and the justification for the algorithm of the fast wavelet transform (FWT). It was introduced in this context in 1988/89 by Stephane Mallat and Yves Meyer and has predecessors in the microlocal analysis in the theory of differential equations (the ironing method) and the pyramid methods of image processing as introduced in 1981/83 by Peter J. Burt, Edward H. Adelson and James Crowley.
S-PLUS is a commercial implementation of the S programming language sold by TIBCO Software Inc.. It features object-oriented programming capabilities and advanced analytical algorithms.
In mathematics, in the area of statistical analysis, the trispectrum is a statistic used to search for nonlinear interactions. The Fourier transform of the second-order cumulant, i.e., the autocorrelation function, is the traditional power spectrum. The Fourier transform of C4 (t1, t2, t3) (fourth-order cumulant-generating function) is called the trispectrum or trispectral density. The trispectrum T(f1,f2,f3) falls into the category of higher-order spectra, or polyspectra, and provides supplementary information to the power spectrum. The trispectrum is a three-dimensional construct. The symmetries of the trispectrum allow a much reduced support set to be defined, contained within the following verticies, where 1 is the Nyquist frequency. (0,0,0) (1/2,1/2,-1/2) (1/3,1/3,0) (1/2,0,0) (1/4,1/4,1/4). The plane containing the points (1/6,1/6,1/6) (1/4,1/4,0) (1/2,0,0) divides this volume into an inner and an outer region. A stationary signal will have zero strength (statistically) in the outer region. The trispectrum support is divide into regions by the plane identified above, and by the (f1,f2) plane. Each region has different requirements in terms of the bandwidth of signal required for non-zero values. In the same way that the bispectrum identifies contributions to a signal's skewness as a function of frequency triples, the trispectrum identifies contributions to a signal's kurtosis as a function of frequency quadruplets. The trispectrum has been used to investigate the domains of applicability of maximum kurtosis phase estimation used in the deconvolution of seismic data to find layer structure.
WinPepi is a freeware package of statistical programs for epidemiologists, comprising seven programs with over 120 modules. WinPepi is not a complete compendium of statistical routines for epidemiologists but it provides a very wide range of procedures, including those most commonly used and many that are not easy to find elsewhere. This has repeatedly led reviewers to use a "Swiss army knife" analogy. Each program has a comprehensive fully referenced manual. WinPepi had its origins in 1983 in a book of programs for hand-held calculators,. In 1993, this was developed into a set of DOS-based computer programs by Paul M. Gahlinger with the assistance of one of the original authors of calculator programs, Prof. JH Abramson  that came to be called Pepi (an acronym for "Programs for EPIdemiologists") and evolved, after its fourth version in 2001, into WinPepi (Pepi-for-Windows). New expanded versions are issued at frequent intervals. The programs are notable for their user-friendliness. A portal links to programs and manuals. Menus, buttons, on-screen instructions, help screens, pop-up hints, and built-in error traps are also provided. The programs can also be operated from a USB flash drive. WinPepi does not provide data management facilities. With some exceptions, it requires the entry (at the keyboard or by pasting from a spreadsheet or text file) of data that have already been counted or summarized.
Least trimmed squares (LTS), or least trimmed sum of squares, is a robust statistical method that fits a function to a set of data whilst not being unduly affected by the presence of outliers. It is one of a number of methods for robust regression.
In probability theory, a stationary ergodic process is a stochastic process which exhibits both stationarity and ergodicity. In essence this implies that the random process will not change its statistical properties with time and that its statistical properties (such as the theoretical mean and variance of the process) can be deduced from a single, sufficiently long sample (realization) of the process. Stationarity is the property of a random process which guarantees that its statistical properties, such as the mean value, its moments and variance, will not change over time. A stationary process is one whose probability distribution is the same at all times. For more information see stationary process. Several sub-types of stationarity are defined: first-order, second-order, nth-order, wide-sense and strict-sense. For details please see the reference above. An ergodic process is one which conforms to the ergodic theorem. The theorem allows the time average of a conforming process to equal the ensemble average. In practice this means that statistical sampling can be performed at one instant across a group of identical processes or sampled over time on a single process with no change in the measured result. A simple example of a violation of ergodicity is a measured process which is the superposition of two underlying processes, each with its own statistical properties. Although the measured process may be stationary in the long term, it is not appropriate to consider the sampled distribution to be the reflection of a single (ergodic) process: The ensemble average is meaningless. Also see ergodic theory and ergodic process.
The birth death process is a special case of continuous-time Markov process where the state transitions are of only two types: "births", which increase the state variable by one and "deaths", which decrease the state by one. The model's name comes from a common application, the use of such models to represent the current size of a population where the transitions are literal births and deaths. Birth death processes have many applications in demography, queueing theory, performance engineering, epidemiology or in biology. They may be used, for example to study the evolution of bacteria, the number of people with a disease within a population, or the number of customers in line at the supermarket. When a birth occurs, the process goes from state n to n + 1. When a death occurs, the process goes from state n to state n   1. The process is specified by birth rates  and death rates .
Pre-test probability and post-test probability (alternatively spelled pretest and posttest probability) are the subjective probabilities of the presence of a condition (such as a disease) before and after a diagnostic test, respectively. Post-test probability, in turn, can be positive or negative, depending on whether the test falls out as a positive test or a negative test, respectively. In some cases, it is used for the probability of developing the condition of interest in the future. The subjectivity of the probabilities is based on the fact that, in reality, an individual either has the condition or not (with the probability always being 100%), so pre- and post-test probabilities for individuals can rather be regarded as psychological phenomena in the minds of those involved in the diagnostics at hand. Test, in this sense, can refer to any medical test (but usually in the sense of diagnostic tests), and in a broad sense also including questions and even assumptions (such as assuming that the target individual is a female or male). The ability to make a difference between pre- and post-test probabilities of various conditions is a major factor in the indication of medical tests.
The James Stein estimator is a biased estimator of the mean of Gaussian random vectors. It can be shown that the James Stein estimator dominates the "ordinary" least squares approach, i.e., it has lower mean squared error on average. It is the best-known example of Stein's phenomenon. An earlier version of the estimator was developed by Charles Stein in 1956, and is sometimes referred to as Stein's estimator. The result was improved by Willard James and Charles Stein in 1961.  
The margin of error is a statistic expressing the amount of random sampling error in a survey's results. It asserts a likelihood (not a certainty) that the result from a sample is close to the number one would get if the whole population had been queried. The likelihood of a result being "within the margin of error" is itself a probability, commonly 95%, though other values are sometimes used. The larger the margin of error, the less confidence one should have that the poll's reported results are close to the true figures; that is, the figures for the whole population. Margin of error applies whenever a population is incompletely sampled. Margin of error is often used in non-survey contexts to indicate observational error in reporting measured quantities. In astronomy, for example, the convention is to report the margin of error as, for example, 4.2421(16) light-years (the distance to Proxima Centauri), with the number in parentheses indicating the expected range of values in the matching digits preceding; in this case, 4.2421(16) is equivalent to 4.2421   0.0016. The latter notation, with the " ", is more commonly seen in most other science and engineering fields.  
In statistics, Box Behnken designs are experimental designs for response surface methodology, devised by George E. P. Box and Donald Behnken in 1960, to achieve the following goals: Each factor, or independent variable, is placed at one of three equally spaced values, usually coded as -1, 0, +1. (At least three levels are needed for the following goal.) The design should be sufficient to fit a quadratic model, that is, one containing squared terms and products of two factors. The ratio of the number of experimental points to the number of coefficients in the quadratic model should be reasonable (in fact, their designs kept it in the range of 1.5 to 2.6). The estimation variance should more or less depend only on the distance from the centre (this is achieved exactly for the designs with 4 and 7 factors), and should not vary too much inside the smallest (hyper)cube containing the experimental points. (See "rotatability" in "Comparisons of response surface designs".) The design with 7 factors was found first while looking for a design having the desired property concerning estimation variance, and then similar designs were found for other numbers of factors. Each design can be thought of as a combination of a two-level (full or fractional) factorial design with an incomplete block design. In each block, a certain number of factors are put through all combinations for the factorial design, while the other factors are kept at the central values. For instance, the Box Behnken design for 3 factors involves three blocks, in each of which 2 factors are varied through the 4 possible combinations of high and low. It is necessary to include centre points as well (in which all factors are at their central values). In this table, m represents the number of factors which are varied in each of the blocks.  The design for 8 factors was not in the original paper. Taking the 9 factor design, deleting one column and any resulting duplicate rows produces an 81 run design for 8 factors, while giving up some "rotatability" (see above). Designs for other numbers of factors have also been invented (at least up to 21). A design for 16 factors exists having only 256 factorial points. Using Plackett Burmans to construct a 16 factor design (see below) requires only 221 points. Most of these designs can be split into groups (blocks), for each of which the model will have a different constant term, in such a way that the block constants will be uncorrelated with the other coefficients.
The folded normal distribution is a probability distribution related to the normal distribution. Given a normally distributed random variable X with mean   and variance  2, the random variable Y = |X| has a folded normal distribution. Such a case may be encountered if only the magnitude of some variable is recorded, but not its sign. The distribution is called Folded because probability mass to the left of the x = 0 is "folded" over by taking the absolute value. In the physics of heat conduction, the folded normal distribution is a fundamental solution of the heat equation on the upper plane (i.e. a heat kernel). The probability density function (PDF) is given by  for x 0, and 0 everywhere else. An alternative formulation is given by , where cosh is the cosine Hyperbolic function. It follows that the cumulative distribution function (CDF) is given by:  for x 0, where erf() is the error function. This expression reduces to the CDF of the half-normal distribution when   = 0. The mean of the folded distribution is then  or  where  is the normal cumulative distribution function:  The variance then is expressed easily in terms of the mean:  Both the mean ( ) and variance ( 2) of X in the original normal distribution can be interpreted as the location and scale parameters of Y in the folded distribution.
Stata is a general-purpose statistical software package created in 1985 by StataCorp. Most of its users work in research, especially in the fields of economics, sociology, political science, biomedicine and epidemiology. Stata's capabilities include data management, statistical analysis, graphics, simulations, regression, and custom programming. The name Stata is a syllabic abbreviation of the words statistics and data. The correct English pronunciation of Stata "must remain a mystery"; any of "Stay-ta", "Sta-ta" or "Stah-ta" are considered acceptable. There are four major builds of each version of Stata: Stata/MP for multiprocessor computers (including dual-core and multicore processors) Stata/SE for large databases Stata/IC, which is the standard version Small Stata, which is a smaller, student version for educational purchase only
Life expectancy is a statistical measure of the average time an organism is expected to live, based on the year of their birth, their current age and other demographic factors including sex. The most commonly used measure of life expectancy is at birth (LEB), which can be defined in two ways: while cohort LEB is the mean length of life of an actual birth cohort (all individuals born a given year) and can be computed only for cohorts born many decades ago, so that all their members died, period LEB is the mean length of life of a hypothetical cohort assumed to be exposed since birth until death of all their members to the mortality rates observed at a given year. National LEB figures reported by statistical national agencies and international organizations are indeed estimates of period LEB. In the Bronze Age and the Iron Age, LEB was 26 years; the 2010 world LEB was 67.2. For recent years, in Swaziland LEB is about 49, and in Japan, it is about 83. The combination of high infant mortality and deaths in young adulthood from accidents, epidemics, plagues, wars, and childbirth, particularly before modern medicine was widely available, significantly lowers LEB. But for those who survive early hazards, a life expectancy of 60 or 70 would not be uncommon. For example, a society with a LEB of 40 may have few people dying at precisely 40: most will die before 30 or very few after 55. In populations with high infant mortality rates, LEB is highly sensitive to the rate of death in the first few years of life. Because of this sensitivity to infant mortality, LEB can be subjected to gross misinterpretation, leading one to believe that a population with a low LEB will necessarily have a small proportion of older people. For example, in a hypothetical stationary population in which half the population dies before the age of five but everybody else dies at exactly 70 years old, LEB will be about 36, but about 25% of the population will be between the ages of 50 and 70. Another measure, such as life expectancy at age 5 (e5), can be used to exclude the effect of infant mortality to provide a simple measure of overall mortality rates other than in early childhood; in the hypothetical population above, life expectancy at 5 would be another 65. Aggregate population measures, such as the proportion of the population in various age groups, should also be used along individual-based measures like formal life expectancy when analyzing population structure and dynamics. Mathematically, life expectancy is the mean number of years of life remaining at a given age, assuming age-specific mortality rates remain at their most recently measured levels. It is denoted by ,[a] which means the mean number of subsequent years of life for someone now aged , according to a particular mortality experience. Longevity, maximum lifespan, and life expectancy are not synonyms. Life expectancy is defined statistically as the mean number of years remaining for an individual or a group of people at a given age. Longevity refers to the characteristics of the relatively long life span of some members of a population. Maximum lifespan is the age at death for the longest-lived individual of a species. Moreover, because life expectancy is an average, a particular person may die many years before or many years after the "expected" survival. The term "maximum life span" has a quite different meaning and is more related to longevity. Life expectancy is also used in plant or animal ecology; life tables (also known as actuarial tables). The term life expectancy may also be used in the context of manufactured objects, but the related term shelf life is used for consumer products, and the terms "mean time to breakdown" (MTTB) and "mean time between failures" (MTBF) are used in engineering.
Foundations of statistics is the usual name for the epistemological debate in statistics over how one should conduct inductive inference from data. Among the issues considered in statistical inference are the question of Bayesian inference versus frequentist inference, the distinction between Fisher's "significance testing" and Neyman-Pearson "hypothesis testing", and whether the likelihood principle should be followed. Some of these issues have been debated for up to 200 years without resolution. Bandyopadhyay & Forster describe four statistical paradigms: "(1) classical statistics or error statistics, (ii) Bayesian statistics, (iii) likelihood-based statistics, and (iv) the Akaikean-Information Criterion-based statistics". Savage's text Foundations of Statistics has been cited over 12000 times on Google Scholar. It tells the following.  It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis.
In data mining, k-means++ is an algorithm for choosing the initial values (or "seeds") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)
Geary's C is a measure of spatial autocorrelation or an attempt to determine if adjacent observations of the same phenomenon are correlated. Spatial autocorrelation is more complex than autocorrelation because the correlation is multi-dimensional and bi-directional. Geary's C is defined as  where  is the number of spatial units indexed by  and ;  is the variable of interest;  is the mean of ;  is a matrix of spatial weights; and  is the sum of all . The value of Geary's C lies between 0 and 2. 1 means no spatial autocorrelation. Values lower than 1 demonstrate increasing positive spatial autocorrelation, whilst values higher than 1 illustrate increasing negative spatial autocorrelation. Geary's C is inversely related to Moran's I, but it is not identical. Moran's I is a measure of global spatial autocorrelation, while Geary's C is more sensitive to local spatial autocorrelation. Geary's C is also known as Geary's contiguity ratio or simply Geary's ratio. This statistic was developed by Roy C. Geary.
Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences, e.g. in population genetics, ecology, epidemiology, and systems biology.
In statistics, the Bayesian information criterion (BIC) or Schwarz criterion (also SBC, SBIC) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC). When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC. The BIC was developed by Gideon E. Schwarz and published in a 1978 paper, where he gave a Bayesian argument for adopting it.
In statistics, normality tests are used to determine if a data set is well-modeled by a normal distribution and to compute how likely it is for a random variable underlying the data set to be normally distributed. More precisely, the tests are a form of model selection, and can be interpreted several ways, depending on one's interpretations of probability: In descriptive statistics terms, one measures a goodness of fit of a normal model to the data   if the fit is poor then the data are not well modeled in that respect by a normal distribution, without making a judgment on any underlying variable. In frequentist statistics statistical hypothesis testing, data are tested against the null hypothesis that it is normally distributed. In Bayesian statistics, one does not "test normality" per se, but rather computes the likelihood that the data come from a normal distribution with given parameters  ,  (for all  , ), and compares that with the likelihood that the data come from other distributions under consideration, most simply using a Bayes factor (giving the relative likelihood of seeing the data given different models), or more finely taking a prior distribution on possible models and parameters and computing a posterior distribution given the computed likelihoods.
A carpet plot is any of a few different specific types of plot. The more common plot referred to as a carpet plot is one that illustrates the interaction between two or more independent variables and one or more dependent variables in a two-dimensional plot. Besides the ability to incorporate more variables, another feature that distinguishes a carpet plot from an equivalent contour plot or 3D surface plot is that a carpet plot can be used to more accurately interpolate data points. A conventional carpet plot can capture the interaction of up to three independent variables and three dependent variables and still be easily read and interpolated. Carpet plots have common applications within areas such as material science for showing elastic modulus in laminates, and within aeronautics. Another plot sometimes referred to as a carpet plot is the temporal raster plot.
The Watts Strogatz model is a random graph generation model that produces graphs with small-world properties, including short average path lengths and high clustering. It was proposed by Duncan J. Watts and Steven Strogatz in their joint 1998 Nature paper. The model also became known as the (Watts) beta model after Watts used  to formulate it in his popular science book Six Degrees.
In probability theory and statistics, the beta-binomial distribution is a family of discrete probability distributions on a finite support of non-negative integers arising when the probability of success in each of a fixed or known number of Bernoulli trials is either unknown or random. The beta-binomial distribution is the binomial distribution in which the probability of success at each trial is not fixed but random and follows the beta distribution. It is frequently used in Bayesian statistics, empirical Bayes methods and classical statistics as an overdispersed binomial distribution. It reduces to the Bernoulli distribution as a special case when n = 1. For   =   = 1, it is the discrete uniform distribution from 0 to n. It also approximates the binomial distribution arbitrarily well for large   and  . The beta-binomial is a one-dimensional version of the Dirichlet-multinomial distribution, as the binomial and beta distributions are univariate versions of the multinomial and Dirichlet distributions, respectively.
In probability theory and statistics, there are several relationships among probability distributions. These relations can be categorized in the following groups: One distribution is a special case of another with a broader parameter space Transforms (function of a random variable); Combinations (function of several variables); Approximation (limit) relationships; Compound relationships (useful for Bayesian inference); Duality; Conjugate priors.
In statistics, a studentized residual is the quotient resulting from the division of a residual by an estimate of its standard deviation. Typically the standard deviations of residuals in a sample vary greatly from one data point to another even when the errors all have the same standard deviation, particularly in regression analysis; thus it does not make sense to compare residuals at different data points without first studentizing. It is a form of a Student's t-statistic, with the estimate of error varying between points. This is an important technique in the detection of outliers. It is among several named in honor of William Sealey Gosset, who wrote under the pseudonym Student, and dividing by an estimate of scale is called studentizing, in analogy with standardizing and normalizing.
In statistics, Goodman and Kruskal's gamma is a measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level. It makes no adjustment for either table size or ties. Values range from  1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association. This statistic (which is distinct from Goodman and Kruskal's lambda) is named after Leo Goodman and William Kruskal, who proposed it in a series of papers from 1954 to 1972.
In bioinformatics, the root-mean-square deviation of atomic positions (or simply root-mean-square deviation, RMSD) is the measure of the average distance between the atoms (usually the backbone atoms) of superimposed proteins. In the study of globular protein conformations, one customarily measures the similarity in three-dimensional structure by the RMSD of the C  atomic coordinates after optimal rigid body superposition. When a dynamical system fluctuates about some well-defined average position, the RMSD from the average over time can be referred to as the RMSF or root mean square fluctuation. The size of this fluctuation can be measured, for example using Mo ssbauer spectroscopy or nuclear magnetic resonance, and can provide important physical information. The Lindemann index is a method of placing the RMSF in the context of the parameters of the system. A widely used way to compare the structures of biomolecules or solid bodies is to translate and rotate one structure with respect to the other to minimize the RMSD. Coutsias, et al. presented a simple derivation, based on quaternions, for the optimal solid body transformation (rotation-translation) that minimizes the RMSD between two sets of vectors. They proved that the quaternion method is equivalent to the well-known Kabsch algorithm. The solution given by Kabsch is an instance of the solution of the d-dimensional problem, introduced by Hurley and Cattell. The quaternion solution to compute the optimal rotation was published in the appendix of a paper of Petitjean. This quaternion solution and the calculation of the optimal isometry in the d-dimensional case were both extended to infinite sets and to the continuous case in the appendix A of an other paper of Petitjean.
In mathematics and statistics, the arithmetic mean (pronunciation: /  r   m t k  mi n/, stress on third syllable of "arithmetic"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the number of numbers in the collection. The collection is often a set of results of an experiment, or a set of results from a survey. The term "arithmetic mean" is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means, such as the geometric mean and the harmonic mean. In addition to mathematics and statistics, the arithmetic mean is used frequently in fields such as economics, sociology, and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population. While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). Notably, for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not accord with one's notion of "middle", and robust statistics, such as the median, may be a better description of central tendency. In a more obscure usage, any sequence of values that form an arithmetic sequence between two numbers x and y can be called "arithmetic means between x and y."
In statistical learning theory, or sometimes computational learning theory, the VC dimension (for Vapnik Chervonenkis dimension) is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a statistical classification algorithm, defined as the cardinality of the largest set of points that the algorithm can shatter. It is a core concept in Vapnik Chervonenkis theory, and was originally defined by Vladimir Vapnik and Alexey Chervonenkis. Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the thresholding of a high-degree polynomial: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This function may not fit the training set well, because it has a low capacity. This notion of capacity is made rigorous below.
A time frequency representation (TFR) is a view of a signal (taken to be a function of time) represented over both time and frequency. Time frequency analysis means analysis into the time frequency domain provided by a TFR. This is achieved by using a formulation often called "Time Frequency Distribution", abbreviated as TFD. TFRs are often complex-valued fields over time and frequency, where the modulus of the field represents either amplitude or "energy density" (the concentration of the root mean square over time and frequency), and the argument of the field represents phase.
Psychometrics is a field of study concerned with the theory and technique of psychological measurement. One part of the field is concerned with the objective measurement of skills and knowledge, abilities, attitudes, personality traits, and educational achievement. For example, some psychometric researchers have, thus far, concerned themselves with the construction and validation of assessment instruments such as questionnaires, tests, raters' judgments, and personality tests. Another part of the field is concerned with statistical research bearing on measurement theory (e.g., item response theory; intraclass correlation). As a result of these focuses, psychometric research involves two major tasks: (i) the construction of instruments; and (ii) the development of procedures for measurement. Practitioners are described as psychometricians. Psychometricians usually possess a specific qualification, and most are psychologists with advanced graduate training. In addition to traditional academic institutions, many psychometricians work for the government or in human resources departments. Others specialize as learning and development professionals.
In the theory of finite population sampling, Bernoulli sampling is a sampling process where each element of the population that is sampled is subjected to an independent Bernoulli trial which determines whether the element becomes part of the sample during the drawing of a single sample. An essential property of Bernoulli sampling is that all elements of the population have equal probability of being included in the sample during the drawing of a single sample. Bernoulli sampling is therefore a special case of Poisson sampling. In Poisson sampling each element of the population may have a different probability of being included in the sample. In Bernoulli sampling, the probability is equal for all the elements. Because each element of the population is considered separately for the sample, the sample size is not fixed but rather follows a binomial distribution.
The generalised hyperbolic distribution (GH) is a continuous probability distribution defined as the normal variance-mean mixture where the mixing distribution is the generalized inverse Gaussian distribution. Its probability density function (see the box) is given in terms of modified Bessel function of the second kind, denoted by . It was introduced by Ole Barndorff-Nielsen, who studied it in the context of physics of wind-blown sand.  
In astronomy, a correlation function describes the distribution of galaxies in the universe. By default, correlation function refers to the two-point autocorrelation function. For a given distance, the two-point autocorrelation function is a function of one variable (distance) which describes the probability that two galaxies are separated by this particular distance. It can be thought of as a lumpiness factor - the higher the value for some distance scale, the more lumpy the universe is at that distance scale. The following definition (from Peebles 1980) is often cited: Given a random galaxy in a location, the correlation function describes the probability that another galaxy will be found within a given distance. However, it can only be correct in the statistical sense that it is averaged over a large number of galaxies chosen as the first, random galaxy. If just one random galaxy is chosen, then the definition is no longer correct, firstly because it is meaningless to talk of just one "random" galaxy, and secondly because the function will vary wildly depending on which galaxy is chosen, in contradiction with its definition as a function. The spatial correlation function  is related to the Fourier space power spectrum of the galaxy distribution, , as  The n-point autocorrelation functions for n greater than 2 or cross-correlation functions for particular object types are defined similarly to the two-point autocorrelation function. The correlation function is important for theoretical models of physical cosmology because it provides a means of testing models which assume different things about the contents of the universe.
Moving least squares is a method of reconstructing continuous functions from a set of unorganized point samples via the calculation of a weighted least squares measure biased towards the region around the point at which the reconstructed value is requested. In computer graphics, the moving least squares method is useful for reconstructing a surface from a set of points. Often it is used to create a 3D surface from a point cloud through either downsampling or upsampling.
In statistics, the Behrens Fisher distribution, named after Ronald Fisher and W. V. Behrens, is a parameterized family of probability distributions arising from the solution of the Behrens Fisher problem proposed first by Behrens and several years later by Fisher. The Behrens Fisher problem is that of statistical inference concerning the difference between the means of two normally distributed populations when the ratio of their variances is not known (and in particular, it is not known that their variances are equal).
The generalized normal distribution or generalized Gaussian distribution (GGD) is either of two families of parametric continuous probability distributions on the real line. Both families add a shape parameter to the normal distribution. To distinguish the two families, they are referred to below as "version 1" and "version 2". However this is not a standard nomenclature.
A statistical syllogism (or proportional syllogism or direct inference) is a non-deductive syllogism. It argues, using inductive reasoning, from a generalization true for the most part to a particular case.
Algebraic statistics is the use of algebra to advance statistics. Algebra has been useful for experimental design, parameter estimation, and hypothesis testing. Traditionally, algebraic statistics has been associated with the design of experiments and multivariate analysis (especially time series). In recent years, the term "algebraic statistics" has been sometimes restricted, sometimes being used to label the use of algebraic geometry and commutative algebra in statistics.
In statistics, time series data is data collected at regular intervals. When there are patterns that repeat over known, fixed periods of time within the data set it is considered to be seasonality, seasonal variation, periodic variation, or periodic fluctuations. This variation can be either regular or semi-regular. Seasonality may be caused by various factors, such as weather, vacation, and holidays and usually consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series. Seasonality can repeat on a weekly, monthly or quarterly basis, these periods of time are structured and occur in a length of time less than a year. Seasonal fluctuations in a time series can be contrasted with cyclical patterns. The latter occur in a period of time that extends beyond a single year, these fluctuations are usually of at least two year and do not repeat over fixed periods of time. Organizations facing seasonal variations, such as ice-cream vendors, are often interested in knowing their performance relative to the normal seasonal variation. Seasonal variations in the labor market can be attributed to the entrance of school leavers into the job market; as they aim to contribute to the workforce during their vacations, or upon the completion of their schooling. These regular changes are of less interest to those who study employment data than the variations that occur due to the underlying state of the economy. Where their focus is on how unemployment in the workforce has changed, despite the impact of the regular seasonal variations. It is necessary for organizations to identify and measure seasonal variations within their market to help them plan for the future. This can prepare them for the temporary increases or decreases in labor requirements and inventory as demand for their product or service fluctuates over certain periods. This may require training, periodic maintenance, and so forth that can be organized in advance. Apart from these considerations, the organizations need to know if variation they have experienced have been more or less than the expected amount, beyond what the usual seasonal variations account for.  
The Gram Charlier A series (named in honor of J rgen Pedersen Gram and Carl Charlier), and the Edgeworth series (named in honor of Francis Ysidro Edgeworth) are series that approximate a probability distribution in terms of its cumulants. The series are the same; but, the arrangement of terms (and thus the accuracy of truncating the series) differ.
Statistical graphics, also known as graphical techniques, are graphics in the field of statistics used to visualize quantitative data.
In statistics, the quartile coefficient of dispersion is a descriptive statistic which measures dispersion and which is used to make comparisons within and between data sets. The statistic is easily computed using the first (Q1) and third (Q3) quartiles for each data set. The quartile coefficient of dispersion is:  ^ Bonett, D. G. (2006). "Confidence interval for a coefficient of quartile variation". Computational Statistics & Data Analysis 50 (11): 2953 2957. doi:10.1016/j.csda.2005.05.007.
In probability theory and statistics, the standardized moment of a probability distribution is a moment (normally a higher degree central moment) that is normalized. The normalization is typically a division by an expression of the standard deviation which renders the moment invariant to level (or scale) and variability. This has the advantage that such normalized moments differ only in other properties than level an variability facilitating e.g. comparison of shape of different probability distributions.
The mean absolute difference is a measure of statistical dispersion equal to the average absolute difference of two independent values drawn from a probability distribution. A related statistic is the relative mean absolute difference, which is the mean absolute difference divided by the arithmetic mean, and equal to twice the Gini coefficient. The mean absolute difference is also known as the absolute mean difference (not to be confused with the absolute value of the mean signed difference) and the Gini mean absolute difference. The mean absolute difference is sometimes denoted by   or as MD.
This page shows the details for different matrix notations of a vector autoregression process with k variables.
In statistics, McNemar's test is a statistical test used on paired nominal data. It is applied to 2   2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is "marginal homogeneity"). It is named after Quinn McNemar, who introduced it in 1947. An application of the test in genetics is the transmission disequilibrium test for detecting linkage disequilibrium.
A Bland Altman plot (Difference plot) in analytical chemistry and biostatistics is a method of data plotting used in analyzing the agreement between two different assays. It is identical to a Tukey mean-difference plot, the name by which it is known in other fields, but was popularised in medical statistics by J. Martin Bland and Douglas G. Altman.
In probability theory, statistics and econometrics, the Burr Type XII distribution or simply the Burr distribution is a continuous probability distribution for a non-negative random variable. It is also known as the Singh Maddala distribution and is one of a number of different distributions sometimes called the "generalized log-logistic distribution". It is most commonly used to model household income (See: Household income in the U.S. and compare to magenta graph at right). The Burr (Type XII) distribution has probability density function:  and cumulative distribution function:  Note when c=1, the Burr distribution becomes the Pareto Type II distribution. When k=1, the Burr distribution is a special case of the Champernowne distribution, often referred to as the Fisk distribution. The Burr Type XII distribution is a member of a system of continuous distributions introduced by Irving W. Burr (1942), which comprises 12 distributions.
In mathematics, the Stieltjes moment problem, named after Thomas Joannes Stieltjes, seeks necessary and sufficient conditions for a sequence { mn, : n = 0, 1, 2, ... } to be of the form  for some measure  . If such a function   exists, one asks whether it is unique. The essential difference between this and other well-known moment problems is that this is on a half-line [0,  ), whereas in the Hausdorff moment problem one considers a bounded interval [0, 1], and in the Hamburger moment problem one considers the whole line (  ,  ).
The Baraba si Albert (BA) model is an algorithm for generating random scale-free networks using a preferential attachment mechanism. Scale-free networks are widely observed in natural and human-made systems, including the Internet, the world wide web, citation networks, and some social networks. The algorithm is named for its inventors Albert-La szlo  Baraba si and Re ka Albert.
In probability theory, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a gambler at a row of slot machines (sometimes known as "one-armed bandits") has to decide which machines to play, how many times to play each machine and in which order to play them. When played, each machine provides a random reward from a probability distribution specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in "some aspects of the sequential design of experiments". A theorem, the Gittins index published first by John C. Gittins gives an optimal policy for maximizing the expected discounted reward. In practice, multi-armed bandits have been used to model the problem of managing research projects in a large organization, like a science foundation or a pharmaceutical company. Given a fixed budget, the problem is to allocate resources among the competing projects, whose properties are only partially known at the time of allocation, but which may become better understood as time passes. In early versions of the multi-armed bandit problem, the gambler has no initial knowledge about the machines. The crucial tradeoff the gambler faces at each trial is between "exploitation" of the machine that has the highest expected payoff and "exploration" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in reinforcement learning.  
In probability and statistics, a natural exponential family (NEF) is a class of probability distributions that is a special case of an exponential family (EF). Every distribution possessing a moment-generating function is a member of a natural exponential family, and the use of such distributions simplifies the theory and computation of generalized linear models.
In statistics, the multinomial test is the test of the null hypothesis that the parameters of a multinomial distribution equal specified values. It is used for categorical data; see Read and Cressie. We begin with a sample of  items each of which has been observed to fall into one of  categories. We can define  as the observed numbers of items in each cell. Hence . Next, we define a vector of parameters , where :. These are the parameter values under the null hypothesis. The exact probability of the observed configuration  under the null hypothesis is given by  The significance probability for the test is the probability of occurrence of the data set observed, or of a data set less likely than that observed, if the null hypothesis is true. Using an exact test, this is calculated as  where the sum ranges over all outcomes as likely as, or less likely than, that observed. In practice this becomes computationally onerous as  and  increase so it is probably only worth using exact tests for small samples. For larger samples, asymptotic approximations are accurate enough and easier to calculate. One of these approximations is the likelihood ratio. We set up an alternative hypothesis under which each value  is replaced by its maximum likelihood estimate . The exact probability of the observed configuration  under the alternative hypothesis is given by  The natural logarithm of the ratio between these two probabilities multiplied by  is then the statistic for the likelihood ratio test  If the null hypothesis is true, then as  increases, the distribution of  converges to that of chi-squared with  degrees of freedom. However it has long been known (e.g. Lawley 1956) that for finite sample sizes, the moments of  are greater than those of chi-squared, thus inflating the probability of type I errors (false positives). The difference between the moments of chi-squared and those of the test statistic are a function of . Williams (1976) showed that the first moment can be matched as far as  if the test statistic is divided by a factor given by  In the special case where the null hypothesis is that all the values  are equal to  (i.e. it stipulates a uniform distribution), this simplifies to  Subsequently, Smith et al. (1981) derived a dividing factor which matches the first moment as far as . For the case of equal values of , this factor is  The null hypothesis can also be tested by using Pearson's chi-squared test  where  is the expected number of cases in category  under the null hypothesis. This statistic also converges to a chi-squared distribution with  degrees of freedom when the null hypothesis is true but does so from below, as it were, rather than from above as  does, so may be preferable to the uncorrected version of  for small samples.
Latent growth modeling is a statistical technique used in the structural equation modeling (SEM) framework to estimate growth trajectory. It is a longitudinal analysis technique to estimate growth over a period of time. It is widely used in the field of behavioral science, education and social science. It is also called latent growth curve analysis. The latent growth model was derived from theories of SEM. General purpose SEM software, such as OpenMx, lavaan (both open source packages based in R), AMOS, Mplus, LISREL, or EQS among others may be used to estimate the trajectory of growth. Latent Growth Models     represent repeated measures of dependent variables as a function of time and other measures. Such longitudinal data share the features that the same subjects are observed repeatedly over time, and on the same tests (or parallel versions), and at known times. In latent growth modeling, the relative standing of an individual at each time is modeled as a function of an underlying growth process, with the best parameter values for that growth process being fitted to each individual. These models have grown in use in social and behavioral research since it was shown that they can be fitted as a restricted common factor model in the structural equation modeling framework. The methodology can be used to investigate systematic change, or growth, and inter-individual variability in this change. A special topic of interest is the correlation of the growth parameters, the so-called initial status and growth rate, as well as their relation with time varying and time invariant covariates. (See McArdle and Nesselroade (2003) for a comprehensive review) Although many applications of latent growth curve models estimate only initial level and slope components, these models have unusual properties such as indefinitely increasing variance. Models with higher order components, e.g., quadratic, cubic, do not predict ever-increasing variance, but require more than two occasions of measurement. It is also possible to fit models based on growth curves with functional forms, often versions of the generalised logistic growth such as the logistic, exponential or Gompertz functions. Though straightforward to fit with versatile software such as OpenMx, these more complex models cannot be fitted with SEM packages in which path coefficients are restricted to being simple constants or free parameters, and cannot be functions of free parameters and data. Similar questions can also be answered using a multilevel model approach.
Pseudoreplication, as originally defined, is a special case of inadequate specification of random factors where both random and fixed factors are present. The problem of inadequate specification arises when treatments are assigned to units that are subsampled and the treatment F-ratio in an analysis of variance (ANOVA) table is formed with respect to the residual mean square rather than with respect to the among unit mean square. The F-ratio relative to the within unit mean square is vulnerable to the confounding of treatment and unit effects, especially when experimental unit number is small (e.g. four tank units, two tanks treated, two not treated, several subsamples per tank). The problem is eliminated by forming the F-ratio relative to the correct mean square in the ANOVA table (tank by treatment MS in the example above), where this is possible. The problem is addressed by the use of mixed models. Hurlbert reported "pseudoreplication" in 48% of the studies he examined, that used inferential statistics. When time and resources limit the number of experimental units, and unit effects cannot be eliminated statistically by testing over the unit variance, it is important to use other sources of information to evaluate the degree to which an F-ratio is confounded by unit effects.
In statistics, the Bhattacharyya distance measures the similarity of two discrete or continuous probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations. Both measures are named after Anil Kumar Bhattacharya, a statistician who worked in the 1930s at the Indian Statistical Institute. The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance, as the Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same. Therefore, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, however, the Bhattacharyya distance would grow depending on the difference between the standard deviations.
In statistics, the probability integral transform or transformation relates to the result that data values that are modelled as being random variables from any given continuous distribution can be converted to random variables having a uniform distribution. This holds exactly provided that the distribution being used is the true distribution of the random variables; if the distribution is one fitted to the data the result will hold approximately in large samples. The result is sometimes modified or extended so that the result of the transformation is a standard distribution other than the uniform distribution, such as the exponential distribution.
In mathematics, and more specifically in graph theory, a polytree (also known as oriented tree or singly connected network) is a directed acyclic graph whose underlying undirected graph is a tree. In other words, if we replace its directed edges with undirected edges, we obtain an undirected graph that is both connected and acyclic. A polytree is an example of an oriented graph. The term polytree was coined in 1987 by Rebane and Pearl.
In the field of multivariate statistics, kernel principal component analysis (kernel PCA)  is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are done in a reproducing kernel Hilbert space with a non-linear mapping.
In ecology, the species discovery curve or species accumulation curve is a graph recording the cumulative number of species of living things recorded in a particular environment as a function of the cumulative effort expended searching for them (usually measured in person-hours). It is related to, but not identical with, the species-area curve. The species discovery curve will necessarily be increasing, and will normally be negatively accelerated (that is, its rate of increase will slow down). Plotting the curve gives a way of estimating the number of additional species that will be discovered with further effort. This is usually done by fitting some kind of functional form to the curve, either by eye or by using non-linear regression techniques. Commonly used functional forms include the logarithmic function and the negative exponential function. The advantage of the negative exponential function is that it tends to an asymptote which equals the number of species that would be discovered if infinite effort is expended. However, some theoretical approaches imply that the logarithmic curve may be more appropriate, implying that though species discovery will slow down with increasing effort, it will never entirely cease, so there is no asymptote, and if infinite effort was expended, an infinite number of species would be discovered. The first theoretical investigation of the species-discovery process was in a classic paper by Fisher, Corbet and Williams (1943), which was based on a large collection of butterflies made in Malaya. Theoretical statistical work on the problem continues, see for example the recent paper by Chao and Shen (2004). The theory is linked to that of Zipf's law. The same approach is used in many other fields. For example, in ethology, it can be applied to the number of distinct fixed action patterns that will be discovered as a function of cumulative effort studying the behaviour of a species of animal; in molecular genetics it is now being applied to the number of distinct genes that are discovered; and in literary studies, it can be used to estimate the total vocabulary of a writer from the given sample of his or her recorded works (see Efron & Thisted, 1976).
Structured data analysis is the statistical data analysis of structured data. This can arise either in the form of an a priori structure such as multiple-choice questionnaires or in situations with the need to search for structure that fits the given data, either exactly or approximately. This structure can then be used for making comparisons, predictions, manipulations etc.
In probability theory, the Type-1 Gumbel density function is  for  The distribution is mainly used in the analysis of extreme values and in survival analysis (also known as duration analysis or event-history modelling).
The interquartile mean (IQM) (or midmean) is a statistical measure of central tendency based on the truncated mean of the interquartile range. The IQM is very similar to the scoring method used in sports that are evaluated by a panel of judges: discard the lowest and the highest scores; calculate the mean value of the remaining scores.  
The sample mean or empirical mean and the sample covariance are statistics computed from a collection (the sample) of data on one or more random variables. The sample mean and sample covariance are estimators of the population mean and population covariance, where the term population refers to the set from which the sample was taken. The sample mean is a vector each of whose elements is the sample mean of one of the random variables   that is, each of whose elements is the arithmetic average of the observed values of one of the variables. The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables. If only one variable has had values observed, then the sample mean is a single number (the arithmetic average of the observed values of that variable) and the sample covariance matrix is also simply a single value (a 1x1 matrix containing a single number, the sample variance of the observed values of that variable). Due to their ease of calculation and other desirable characteristics, the sample mean and sample covariance are widely used in statistics and applications to numerically represent the location and dispersion, respectively, of a distribution.
The normal-inverse Gaussian distribution (NIG) is a continuous probability distribution that is defined as the normal variance-mean mixture where the mixing density is the inverse Gaussian distribution. The NIG distribution was noted by Blaesild in 1977 as a subclass of the generalised hyperbolic distribution discovered by Ole Barndorff-Nielsen, in the next year Barndorff-Nielsen published the NIG in another paper. It was introduced in the mathematical finance literature in 1997. The parameters of the normal-inverse Gaussian distribution are often used to construct a heaviness and skewness plot called the NIG-triangle.
In science and engineering, a log log graph or log log plot is a two-dimensional graph of numerical data that uses logarithmic scales on both the horizontal and vertical axes. Monomials   relationships of the form    appear as straight lines in a log log graph, with the power and constant term corresponding to slope and intercept of the line, and thus these graphs are very useful for recognizing these relationships and estimating parameters. Any base can be used for the logarithm, though most common are 10, e, and 2.
In statistics and related fields, a similarity measure or similarity function is a real-valued function that quantifies the similarity between two objects. Although no single definition of a similarity measure exists, usually such measures are in some sense the inverse of distance metrics: they take on large values for similar objects and either zero or a negative value for very dissimilar objects. E.g., in the context of cluster analysis, Frey and Dueck suggest defining a similarity measure  where  is the squared Euclidean distance. Cosine similarity is a commonly used similarity measure for real-valued vectors, used in (among other fields) information retrieval to score the similarity of documents in the vector space model. In machine learning, common kernel functions such as the RBF kernel can be viewed as similarity functions.
The Wigner semicircle distribution, named after the physicist Eugene Wigner, is the probability distribution supported on the interval [ R, R] the graph of whose probability density function f is a semicircle of radius R centered at (0, 0) and then suitably normalized (so that it is really a semi-ellipse):  for  R   x   R, and f(x) = 0 if R < |x|. This distribution arises as the limiting distribution of eigenvalues of many random symmetric matrices as the size of the matrix approaches infinity. It is a scaled beta distribution, more precisely, if Y is beta distributed with parameters   =   = 3/2, then X = 2RY   R has the above Wigner semicircle distribution.
In queueing theory, a discipline within the mathematical theory of probability, a Jackson network (sometimes Jacksonian network) is a class of queueing network where the equilibrium distribution is particularly simple to compute as the network has a product-form solution. It was the first significant development in the theory of networks of queues, and generalising and applying the ideas of the theorem to search for similar product-form solutions in other networks has been the subject of much research, including ideas used in the development of the Internet. The networks were first identified by James R. Jackson and his paper was re-printed in the journal Management Science s  Ten Most Influential Titles of Management Sciences First Fifty Years.  Jackson was inspired by the work of Burke and Reich, though Jean Walrand notes "product-form results ... [are] a much less immediate result of the output theorem than Jackson himself appeared to believe in his fundamental paper". An earlier product-form solution was found by R. R. P. Jackson for tandem queues (a finite chain of queues where each customer must visit each queue in order) and cyclic networks (a loop of queues where each customer must visit each queue in order). A Jackson network consists of a number of nodes, where each node represents a queue in which the service rate can be both node-dependent and state-dependent. Jobs travel among the nodes following a fixed routing matrix. All jobs at each node belong to a single "class" and jobs follow the same service-time distribution and the same routing mechanism. Consequently, there is no notion of priority in serving the jobs: all jobs at each node are served on a first-come, first-served basis. Jackson networks where a finite population of jobs travel around a closed network also have a product-form solution described by the Gordon Newell theorem.  
In statistics and mathematics, linear least squares is an approach fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system. Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called linear least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure, and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator. In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis. One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See outline of regression analysis for an outline of the topic.
Sammon mapping or Sammon projection is an algorithm that maps a high-dimensional space to a space of lower dimensionality (see multidimensional scaling) by trying to preserve the structure of inter-point distances in high-dimensional space in the lower-dimension projection. It is particularly suited for use in exploratory data analysis. The method was proposed by John W. Sammon in 1969. It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables as possible in techniques such as principal component analysis, which also makes it more difficult to use for classification applications. Denote the distance between ith and jth objects in the original space by , and the distance between their projections by . Sammon's mapping aims to minimize the following error function, which is often referred to as Sammon's stress or Sammon's error:  The minimization can be performed either by gradient descent, as proposed initially, or by other means, usually involving iterative methods. The number of iterations need to be experimentally determined and convergent solutions are not always guaranteed. Many implementations prefer to use the first Principal Components as a starting configuration. The Sammon mapping has been one of the most successful nonlinear metric multidimensional scaling methods since its advent in 1969, but effort has been focused on algorithm improvement rather than on the form of the stress function. The performance of the Sammon mapping has been improved by extending its stress function using left Bregman divergence  and right Bregman divergence.
The multitrait-multimethod (MTMM) matrix is an approach to examining construct validity developed by Campbell and Fiske (1959). It organizes convergent and discriminant validity evidence for comparison of how a measure relates to other measures.
Statistical process control (SPC) is a method of quality control which uses statistical methods. SPC is applied in order to monitor and control a process. Monitoring and controlling the process ensures that it operates at its full potential. At its full potential, the process can make as much conforming product as possible with a minimum (if not an elimination) of waste (rework or scrap). SPC can be applied to any process where the "conforming product" (product meeting specifications) output can be measured. Key tools used in SPC include control charts; a focus on continuous improvement; and the design of experiments. An example of a process where SPC is applied is manufacturing lines.  
In probability theory and directional statistics, a wrapped probability distribution is a continuous probability distribution that describes data points that lie on a unit n-sphere. In one dimension, a wrapped distribution will consist of points on the unit circle. If   is a random variate in the interval (- , ;) with probability density function p( ), then z = e i   will be a circular variable distributed according to the wrapped distribution pzw(z) and  =arg(z) will be an angular variable in the interval (- ,  ] distributed according to the wrapped distribution pw( ). Any probability density function (pdf)  on the line can be "wrapped" around the circumference of a circle of unit radius. That is, the pdf of the wrapped variable  in some interval of length  is  which is a periodic sum of period . The preferred interval is generally  for which
In mathematics, Bochner's theorem (named for Salomon Bochner) characterizes the Fourier transform of a positive finite Borel measure on the real line. More generally in harmonic analysis, Bochner's theorem asserts that under Fourier transform a continuous positive definite function on a locally compact abelian group corresponds to a finite positive measure on the Pontryagin dual group.
The Mexican paradox is the observation that the Mexican people exhibit a surprisingly low incidence of low birth weight (LBW), contrary to what would be expected from their socioeconomic status (SES). This appears as an outlier in graphs correlating SES with low-birth-weight rates. It has been proposed that resistance to changes in diet is responsible for the positive birth weight association for Mexican-American mothers. Nevertheless, the medical causes of lower rates of low birth weights among birthing Mexican mothers has been called into question. The results of the study showed that the mean birth weight of Mexican-American babies was 3.34 kg (7.37 lbs), while that of non-Hispanic White babies was 3.39 kg (7.48 lbs.). This finding re-emphasized the independence of mean birth weight and LBW. This however did not refute the discrepancies in LBW for Mexicans. The study also showed that the overall preterm birth rate was higher among Mexican Americans (10.6%) than non-Hispanic Whites (9.3%). The overall hypothesis of the authors was that this finding reflected an error in recorded gestational age, described in a strongly bimodal birth-weight distribution at young gestational ages for Mexican-Americans.
The infinite monkey theorem states that a monkey hitting keys at random on a typewriter keyboard for an infinite amount of time will almost surely type a given text, such as the complete works of William Shakespeare. In this context, "almost surely" is a mathematical term with a precise meaning, and the "monkey" is not an actual monkey, but a metaphor for an abstract device that produces an endless random sequence of letters and symbols. One of the earliest instances of the use of the "monkey metaphor" is that of French mathematician E mile Borel in 1913, but the first instance may be even earlier. The relevance of the theorem is questionable the probability of a universe full of monkeys typing a complete work such as Shakespeare's Hamlet is so tiny that the chance of it occurring during a period of time hundreds of thousands of orders of magnitude longer than the age of the universe is extremely low (but technically not zero). Variants of the theorem include multiple and even infinitely many typists, and the target text varies between an entire library and a single sentence. The history of these statements can be traced back to Aristotle's On Generation and Corruption and Cicero's De natura deorum (On the Nature of the Gods), through Blaise Pascal and Jonathan Swift, and finally to modern statements with their iconic simians and typewriters. In the early 20th century, E mile Borel and Arthur Eddington used the theorem to illustrate the timescales implicit in the foundations of statistical mechanics.
In statistics, the Chapman Robbins bound or Hammersley Chapman Robbins bound is a lower bound on the variance of estimators of a deterministic parameter. It is a generalization of the Crame r Rao bound; compared to the Crame r Rao bound, it is both tighter and applicable to a wider range of problems. However, it is usually more difficult to compute. The bound was independently discovered by John Hammersley in 1950, and by Douglas Chapman and Herbert Robbins in 1951.
In statistics, Mood's median test is a special case of Pearson's chi-squared test. It is a nonparametric test that tests the null hypothesis that the medians of the populations from which two or more samples are drawn are identical. The data in each sample are assigned to two groups, one consisting of data whose values are higher than the median value in the two groups combined, and the other consisting of data whose values are at the median or below. A Pearson's chi-squared test is then used to determine whether the observed frequencies in each sample differ from expected frequencies derived from a distribution combining the two groups.
A Newey West estimator is used in statistics and econometrics to provide an estimate of the covariance matrix of the parameters of a regression-type model when this model is applied in situations where the standard assumptions of regression analysis do not apply. It was devised by Whitney K. Newey and Kenneth D. West in 1987, although there are a number of later variants. The estimator is used to try to overcome autocorrelation (also called serial correlation), and heteroskedasticity in the error terms in the models, often for regressions applied to time series data. The problem in autocorrelation, often found in time series data, is that the error terms are correlated over time. This can be demonstrated in , a matrix of sums of squares and cross products that involves  and the rows of . The least squares estimator  is a consistent estimator of . This implies that the least squares residuals  are "point-wise" consistent estimators of their population counterparts . The general approach, then, will be to use  and  to devise an estimator of . This means that as the time between error terms increases, the correlation between the error terms decreases. The estimator thus can be used to improve the ordinary least squares (OLS) regression when the residuals are heteroskedastic and/or autocorrelated.
Statistics is the mathematical science involving the collection, analysis and interpretation of data. A number of specialties have evolved to apply statistical theory and methods to various disciplines. Certain topics have "statistical" in their name but relate to manipulations of probability distributions rather than to statistical analysis. Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in the insurance and finance industries. Astrostatistics is the discipline that applies statistical analysis to the understanding of astronomical data. Biostatistics is a branch of biology that studies biological phenomena and observations by means of statistical analysis, and includes medical statistics. Business analytics is a rapidly developing business process that applies statistical methods to data sets (often very large) to develop new insights and understanding of business performance & opportunities Chemometrics is the science of relating measurements made on a chemical system or process to the state of the system via application of mathematical or statistical methods. Demography is the statistical study of all populations. It can be a very general science that can be applied to any kind of dynamic population, that is, one that changes over time or space. Econometrics is a branch of economics that applies statistical methods to the empirical study of economic theories and relationships. Environmental statistics is the application of statistical methods to environmental science. Weather, climate, air and water quality are included, as are studies of plant and animal populations. Epidemiology is the study of factors affecting the health and illness of populations, and serves as the foundation and logic of interventions made in the interest of public health and preventive medicine. Geostatistics is a branch of geography that deals with the analysis of data from disciplines such as petroleum geology, hydrogeology, hydrology, meteorology, oceanography, geochemistry, geography. Machine Learning Operations research (or Operational Research) is an interdisciplinary branch of applied mathematics and formal science that uses methods such as mathematical modeling, statistics, and algorithms to arrive at optimal or near optimal solutions to complex problems. Population ecology is a sub-field of ecology that deals with the dynamics of species populations and how these populations interact with the environment. Psychometric is the theory and technique of educational and psychological measurement of knowledge, abilities, attitudes, and personality traits. Quality control reviews the factors involved in manufacturing and production; it can make use of statistical sampling of product items to aid decisions in process control or in accepting deliveries. Quantitative psychology is the science of statistically explaining and changing mental processes and behaviors in humans. Reliability Engineering is the study of the ability of a system or component to perform its required functions under stated conditions for a specified period of time Statistical finance, an area of econophysics, is an empirical attempt to shift finance from its normative roots to a positivist framework using exemplars from statistical physics with an emphasis on emergent or collective properties of financial markets. Statistical mechanics is the application of probability theory, which includes mathematical tools for dealing with large populations, to the field of mechanics, which is concerned with the motion of particles or objects when subjected to a force. Statistical physics is one of the fundamental theories of physics, and uses methods of probability theory in solving physical problems. Statistical Signal Processing Statistical thermodynamics is the study of the microscopic behaviors of thermodynamic systems using probability theory and provides a molecular level interpretation of thermodynamic quantities such as work, heat, free energy, and entropy.
In mathematical optimization, statistics, decision theory and machine learning, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (sometimes called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. In statistics, typically a loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century. In the context of economics, for example, this is usually economic cost or regret. In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Crame r in the 1920s. In optimal control the loss is the penalty for failing to achieve a desired value. In financial risk management the function is precisely mapped to a monetary loss.
Text mining, also referred to as text data mining, roughly equivalent to text analytics, refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods. A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
Population genetics is the study of the distribution and change in frequency of alleles within populations, and as such it sits firmly within the field of evolutionary biology. The main processes of evolution are natural selection, genetic drift, gene flow, mutation, and genetic recombination and they form an integral part of the theory that underpins population genetics. Studies in this branch of biology examine such phenomena as adaptation, speciation, population subdivision, and population structure. Population genetics was a vital ingredient in the emergence of the modern evolutionary synthesis. Its primary founders were Sewall Wright, J. B. S. Haldane and Ronald Fisher, who also laid the foundations for the related discipline of quantitative genetics. Traditionally a highly mathematical discipline, modern population genetics encompasses theoretical, lab and field work. Computational approaches, often utilising coalescent theory, have played a central role since the 1980s.
Although the subject of sexual dimorphism is not in itself controversial, the measures by which it is assessed differ widely. Most of the measures are used on the assumption that a random variable is considered so that probability distributions should be taken into account. In this review, a series of sexual dimorphism measures are discussed concerning both their definition and the probability law on which they are based. Most of them are sample functions, or statistics, which account for only partial characteristics, for example the mean or expected value, of the distribution involved. Further, the most widely used measure fails to incorporate an inferential support.
Multilinear principal component analysis (MPCA)    is a mathematical procedure that uses multiple orthogonal transformations to convert a set of multidimensional objects into another set of multidimensional objects of lower dimensions. There is one orthogonal (linear) transformation for each dimension (mode); hence multilinear. This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data as possible, subject to the constraint of mode-wise orthogonality. MPCA is a multilinear extension of principal component analysis (PCA). The major difference is that PCA needs to reshape a multidimensional object into a vector, while MPCA operates directly on multidimensional objects through mode-wise processing. For example, for 100x100 images, PCA operates on vectors of 10000x1 while MPCA operates on vectors of 100x1 in two modes. For the same amount of dimension reduction, PCA needs to estimate 49*(10000/(100*2)-1) times more parameters than MPCA. Thus, MPCA is more efficient and better conditioned in practice. MPCA is a basic algorithm for dimension reduction via multilinear subspace learning. In wider scope, it belongs to tensor-based computation. Its origin can be traced back to the Tucker decomposition in 1960s and it is closely related to higher-order singular value decomposition, (HOSVD) and to the best rank-(R1, R2, ..., RN ) approximation of higher-order tensors.
In statistics, one-way analysis of variance (abbreviated one-way ANOVA) is a technique used to compare means of three or more samples (using the F distribution). This technique can be used only for numerical data. The ANOVA tests the null hypothesis that samples in two or more groups are drawn from populations with the same mean values. To do this, two estimates are made of the population variance. These estimates rely on various assumptions (see below). The ANOVA produces an F-statistic, the ratio of the variance calculated among the means to the variance within the samples. If the group means are drawn from populations with the same mean values, the variance between the group means should be lower than the variance of the samples, following the central limit theorem. A higher ratio therefore implies that the samples were drawn from populations with different mean values. Typically, however, the one-way ANOVA is used to test for differences among at least three groups, since the two-group case can be covered by a t-test (Gosset, 1908). When there are only two means to compare, the t-test and the F-test are equivalent; the relation between ANOVA and t is given by F = t2. An extension of one-way ANOVA is two-way analysis of variance that examines the influence of two different categorical independent variables on one dependent variable.
Survivorship bias, or survival bias, is the logical error of concentrating on the people or things that "survived" some process and inadvertently overlooking those that did not because of their lack of visibility. This can lead to false conclusions in several different ways. The survivors may be actual people, as in a medical study, or could be companies or research subjects or applicants for a job, or anything that must make it past some selection process to be considered further. Survivorship bias can lead to overly optimistic beliefs because failures are ignored, such as when companies that no longer exist are excluded from analyses of financial performance. It can also lead to the false belief that the successes in a group have some special property, rather than just coincidence. For example, if three of the five students with the best college grades went to the same high school, that can lead one to believe that the high school must offer an excellent education. This could be true, but the question cannot be answered without looking at the grades of all the other students from that high school, not just the ones who "survived" the top-five selection process. Survivorship bias is a type of selection bias.
The statistics of random permutations, such as the cycle structure of a random permutation are of fundamental importance in the analysis of algorithms, especially of sorting algorithms, which operate on random permutations. Suppose, for example, that we are using quickselect (a cousin of quicksort) to select a random element of a random permutation. Quickselect will perform a partial sort on the array, as it partitions the array according to the pivot. Hence a permutation will be less disordered after quickselect has been performed. The amount of disorder that remains may be analysed with generating functions. These generating functions depend in a fundamental way on the generating functions of random permutation statistics. Hence it is of vital importance to compute these generating functions. The article on random permutations contains an introduction to random permutations.
In statistics, an efficient estimator is an estimator that estimates the quantity of interest in some  best possible  manner. The notion of  best possible  relies upon the choice of a particular loss function   the function which quantifies the relative degree of undesirability of estimation errors of different magnitudes. The most common choice of the loss function is quadratic, resulting in the mean squared error criterion of optimality.
A soliton distribution is a type of discrete probability distribution that arises in the theory of erasure correcting codes. A paper by Luby introduced two forms of such distributions, the ideal soliton distribution and the robust soliton distribution.
Subjective probability is a measure of the expectation that an event will occur, or that a statement is true. Probabilities are given a value between 0 (the event will definitely not occur) and 1 (the event is absolutely certain to occur). The nearer the probability of an event tends towards 1, the more certain it is that the event will occur. The nearer the probability tends towards 0, the more certain it is that the event will not occur. Cromwell's rule, named by statistician Dennis Lindley, states that the use of prior probabilities of 0 or 1 should be avoided, except when applied to statements that are logically true or false. For instance, Lindley would allow one to say that Pr(2+2 = 4) = 1, where Pr represents the probability. In other words, arithmetically, the number 2 added to the number 2 will certainly equal 4. The reference is to Oliver Cromwell. Cromwell wrote to the synod of the Church of Scotland on August 5, 1650, including a phrase that has become well known and frequently quoted: I beseech you, in the bowels of Christ, think it possible that you may be mistaken. As Lindley puts it, assigning a probability should "leave a little probability for the moon being made of green cheese; it can be as small as 1 in a million, but have it there since otherwise an army of astronauts returning with samples of the said cheese will leave you unmoved." Similarly, in assessing the likelihood that tossing a coin will result in either a head or a tail facing upwards, there is a possibility, albeit remote, that the coin will land on its edge and remain in that position. If the prior probability assigned to a hypothesis is 0 or 1, then, by Bayes' theorem, the posterior probability (probability of the hypothesis, given the evidence) is forced to be 0 or 1 as well; no evidence, no matter how strong, could have any influence. A strengthened version of Cromwell's rule, applying also to statements of arithmetic and logic, alters the first rule of probability, or the convexity rule, 0   P(A)   1, to 0 < P(A) < 1.
In probability theory, Hoeffding's lemma is an inequality that bounds the moment-generating function of any bounded random variable. It is named after the Finnish American mathematical statistician Wassily Hoeffding. The proof of Hoeffding's lemma uses Taylor's theorem and Jensen's inequality. Hoeffding's lemma is itself used in the proof of McDiarmid's inequality.
For a more technical treatment, see Identifiability. In statistics and econometrics, the parameter identification problem is the inability in principle to identify a best estimate of the value(s) of one or more parameters in a regression. This problem can occur in the estimation of multiple-equation econometric models where the equations have variables in common. More generally, the term can be used to refer to any situation where a statistical model will invariably have more than one set of parameters that generate the same distribution of observations, meaning that multiple parametrizations are observationally equivalent.
In statistical quality control, the EWMA chart (or exponentially weighted moving average chart) is a type of control chart used to monitor either variables or attributes-type data using the monitored business or industrial process's entire history of output. While other control charts treat rational subgroups of samples individually, the EWMA chart tracks the exponentially-weighted moving average of all prior sample means. EWMA weights samples in geometrically decreasing order so that the most recent samples are weighted most highly while the most distant samples contribute very little. Although the normal distribution is the basis of the EWMA chart, the chart is also relatively robust in the face of non-normally distributed quality characteristics. There is, however, an adaptation of the chart that accounts for quality characteristics that are better modeled by the Poisson distribution. The chart monitors only the process mean; monitoring the process variability requires the use of some other technique. The EWMA control chart requires a knowledgeable person to select two parameters before setup: The first parameter is  , the weight given to the most recent rational subgroup mean.   must satisfy 0 <     1, but selecting the "right" value is a matter of personal preference and experience. One source recommends 0.05       0.25, while another recommends 0.2       0.3. The second parameter is L, the multiple of the rational subgroup standard deviation that establishes the control limits. L is typically set at 3 to match other control charts, but it may be necessary to reduce L slightly for small values of  . Instead of plotting rational subgroup averages directly, the EWMA chart computes successive observations zi by computing the rational subgroup average, , and then combining that new subgroup average with the running average of all preceding observations, zi - 1, using the specially chosen weight,  , as follows: . The control limits for this chart type are  where T and S are the estimates of the long-term process mean and standard deviation established during control-chart setup and n is the number of samples in the rational subgroup. Note that the limits widen for each successive rational subgroup, approaching . The EWMA chart is sensitive to small shifts in the process mean, but does not match the ability of Shewhart-style charts (namely the  and R and  and s charts) to detect larger shifts. One author recommends superimposing the EWMA chart on top of a suitable Shewhart-style chart with widened control limits in order to detect both small and large shifts in the process mean.
In statistics, completeness is a property of a statistic in relation to a model for a set of observed data. In essence, it is a condition which ensures that the parameters of the probability distribution representing the model can all be estimated on the basis of the statistic: it ensures that the distributions corresponding to different values of the parameters are distinct. It is closely related to the idea of identifiability, but in statistical theory it is often found as a condition imposed on a sufficient statistic from which certain optimality results are derived.
Noncentrality parameters are parameters of families of probability distributions that are related to other "central" families of distributions. Whereas the central distribution describes how a test statistic is distributed when the difference tested is null, noncentral distributions describe the distribution of a test statistic when the null is false. This leads to their use in statistics, especially calculating statistical power. If the noncentrality parameter of a distribution is zero, the distribution is identical to a distribution in the central family. For example, the Student's t-distribution is the central family of distributions for the Noncentral t-distribution family. Noncentrality parameters are used in the following distributions:  Noncentral t-distribution Noncentral chi-squared distribution Noncentral chi-distribution Noncentral F-distribution Noncentral beta distribution  In general, noncentrality parameters occur in distributions that are transformations of a normal distribution. The "central" versions are derived from normal distributions that have a mean of zero; the noncentral versions generalize to arbitrary means. For example, the standard (central) chi-squared distribution is the distribution of a sum of squared independent standard normal distributions, i.e., normal distributions with mean 0, variance 1. The noncentral chi-squared distribution generalizes this to normal distributions with arbitrary mean and variance. Each of these distributions has a single noncentrality parameter. However, there are extended versions of these distributions which have two noncentrality parameters: the doubly noncentral beta distribution, the doubly noncentral F distribution and the doubly noncentral t distribution. These types of distributions occur for distributions that are defined as the quotient of two independent distributions. When both source distributions are central (either with a zero mean or a zero noncentrality parameter, depending on the type of distribution), the result is a central distribution. When one is noncentral, a (singly) noncentral distribution results, while if both are noncentral, the result is a doubly noncentral distribution. As an example, a t-distribution is defined (ignoring constant values) as the quotient of a normal distribution and the square root of an independent chi-squared distribution. Extending this definition to encompass a normal distribution with arbitrary mean produces a noncentral t-distribution, while further extending it to allow a noncentral chi-squared distribution in the denominator while produces a doubly noncentral t-distribution. Note also that there are some "noncentral distributions" that are not usually formulated in terms of a "noncentrality parameter": see noncentral hypergeometric distributions, for example. The noncentrality parameter of the t-distribution may be negative or positive while the noncentral parameters of the other three distributions must be greater than zero.
The law of rare events or Poisson limit theorem gives a Poisson approximation to the binomial distribution, under certain conditions. The theorem was named after Sime on Denis Poisson (1781 1840).
Lindley's paradox is a counterintuitive situation in statistics in which the Bayesian and frequentist approaches to a hypothesis testing problem give different results for certain choices of the prior distribution. The problem of the disagreement between the two approaches was discussed in Harold Jeffreys' 1939 textbook; it became known as Lindley's paradox after Dennis Lindley called the disagreement a paradox in a 1957 paper. Although referred to as a paradox, the differing results from the Bayesian and frequentist approaches can be explained as using them to answer fundamentally different questions, rather than actual disagreement between the two methods.
"Human Genetic Diversity: Lewontin's Fallacy" is a 2003 paper by A. W. F. Edwards. He criticises an argument first made by Richard Lewontin in his 1972 article "The Apportionment of Human Diversity", which argued that division of humanity into races is taxonomically invalid. Edwards' critique is discussed in a number of academic and popular science books, with varying degrees of support.
In mathematics, a sample-continuous process is a stochastic process whose sample paths are almost surely continuous functions.
In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.
A systematic review is a type of literature review that collects and critically analyzes multiple research studies or papers. A review of existing studies is often quicker and cheaper than embarking on a new study. Researchers use methods that are selected before one or more research questions are formulated, and then they aim to find and analyze studies that relate to and answer those questions. Systematic reviews of randomized controlled trials are key in the practice of evidence-based medicine. An understanding of systematic reviews, and how to implement them in practice, is highly recommended for professionals involved in the delivery of health care. Besides health interventions, systematic reviews may examine clinical tests, public health interventions, social interventions, adverse effects, and economic evaluations. Systematic reviews are not limited to medicine and are quite common in all other sciences where data are collected, published in the literature, and an assessment of methodological quality for a precisely defined subject would be helpful.
Segmented regression, also known as piecewise regression or 'broken-stick regression', is a method in regression analysis in which the independent variable is partitioned into intervals and a separate line segment is fit to each interval. Segmented regression analysis can also be performed on multivariate data by partitioning the various independent variables. Segmented regression is useful when the independent variables, clustered into different groups, exhibit different relationships between the variables in these regions. The boundaries between the segments are breakpoints. Segmented linear regression is segmented regression whereby the relations in the intervals are obtained by linear regression.
Truncated regression models arise in many applications of statistics, for example in econometrics, in cases where observations with values in the outcome variable below or above certain thresholds are systematically excluded from the sample. Therefore, whole observations are missing, so that neither the dependent nor the independent variable is known. Truncated regression models are often confused with censored regression models where only the value of the dependent variable is clustered at a lower threshold, an upper threshold, or both, while the value for independent variables is available.
In probability theory, the central limit theorem says that, under certain conditions, the sum of many independent identically-distributed random variables, when scaled appropriately, converges in distribution to a standard normal distribution. The martingale central limit theorem generalizes this result for random variables to martingales, which are stochastic processes where the change in the value of the process from time t to time t + 1 has expectation zero, even conditioned on previous outcomes.
The Durbin Wu Hausman test (also called Hausman specification test) is a statistical hypothesis test in econometrics named after James Durbin, De-Min Wu, and Jerry A. Hausman. The test evaluates the consistency of an estimator when compared to an alternative, less efficient, estimator which is already known to be consistent. It helps one evaluate if a statistical model corresponds to the data.
Stationary distribution may refer to: The limiting distribution in a Markov chain The marginal distribution of a stationary process or stationary time series The set of joint probability distributions of a stationary process or stationary time series In some fields of application, the term stable distribution is used for the equivalent of a stationary (marginal) distribution, although in probability and statistics the term has a rather different meaning: see stable distribution. Crudely stated, all of the above are specific cases of a common general concept. A stationary distribution is a specific entity which is unchanged by the effect of some matrix or operator: it need not be unique. Thus stationary distributions are related to eigenvectors for which the eigenvalue is unity.
Formalized by John Tukey, the Tukey lambda distribution is a continuous probability distribution defined in terms of its quantile function. It is typically used to identify an appropriate distribution (see the comments below) and not used in statistical models directly. The Tukey lambda distribution has a single shape parameter  . As with other probability distributions, the Tukey lambda distribution can be transformed with a location parameter,  , and a scale parameter,  . Since the general form of probability distribution can be expressed in terms of the standard distribution, the subsequent formulas are given for the standard form of the function.
Chronux is an open-source software package developed for the loading, visualization and analysis of a variety of modalities / formats of neurobiological time series data. Usage of this tool enables neuroscientists to perform a variety of analysis on multichannel electrophysiological data such as LFP (local field potentials), EEG, MEG, Neuronal spike times and also on spatiotemporal data such as FMRI and dynamic optical imaging data. The software consists of a set of MATLAB routines interfaced with C libraries that can be used to perform the tasks that constitute a typical study of neurobiological data. These include local regression and smoothing, spike sorting and spectral analysis. The package also includes some GUIs for time series visualization and analysis. Chronux is GNU GPL v2 licensed (and MATLAB is proprietary).
In statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model. In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition). Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations. Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175  C, the median of the data will be between 20 and 25  C but the mean temperature will be between 35.5 and 40  C. In this case, the median better reflects the temperature of a randomly sampled object than the mean; naively interpreting the mean as "a typical sample", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set. Estimators capable of coping with outliers are said to be robust: the median is a robust statistic, while the mean is not.
In demography (the branch of statistics that deals with the study of populations) a Lexis diagram (named after economist and social scientist Wilhelm Lexis) is a two dimensional diagram that is used to represent events (such as births or deaths) that occur to individuals belonging to different cohorts. Calendar time is usually represented on the horizontal axis, while age is represented on the vertical axis. In some textbooks the y-axis is plotted backwards, with age 0 at the top of the page and increasing downwards. However, other arrangements of the axes are also seen. As an example the death of an individual in 2009 at age 80 is represented by the point (2009,80); the cohort of all persons born in 1929 is represented by a diagonal line starting at (1929,0) and continuing through (1930,1) and so on.
In statistics, a location family is a class of probability distributions that is parametrized by a scalar- or vector-valued parameter , which determines the "location" or shift of the distribution. Formally, this means that the probability density functions or probability mass functions in this class have the form  Here,  is called the location parameter. Examples of location parameters include the mean, the median, and the mode. Thus in the one-dimensional case if  is increased, the probability density or mass function shifts rigidly to the right, maintaining its exact shape. A location parameter can also be found in families having more than one parameter, such as location-scale families. In this case, the probability density function or probability mass function will be a special case of the more general form  where  is the location parameter,   represents additional parameters, and  is a function parametrized on the additional parameters.
Bayesian approaches to brain function investigate the capacity of the nervous system to operate in situations of uncertainty in a fashion that is close to the optimal prescribed by Bayesian statistics. This term is used in behavioural sciences and neuroscience and studies associated with this term often strive to explain the brain's cognitive abilities based on statistical principles. It is frequently assumed that the nervous system maintains internal probabilistic models that are updated by neural processing of sensory information using methods approximating those of Bayesian probability.
Graphical models have become powerful frameworks for protein structure prediction, protein protein interaction and free energy calculations for protein structures. Using a graphical model to represent the protein structure allows the solution of many problems including secondary structure prediction, protein protein interactions, protein-drug interaction, and free energy calculations. There are two main approaches to use graphical models in protein structure modeling. The first approach uses discrete variables for representing coordinates or dihedral angles of the protein structure. The variables are originally all continuous values and, to transform them into discrete values, a discretization process is typically applied. The second approach uses continuous variables for the coordinates or dihedral angles.
Static analysis, static projection, and static scoring are terms for simplified analysis wherein the effect of an immediate change to a system is calculated without respect to the longer term response of the system to that change. Such analysis typically produces poor correlation to empirical results. Its opposite, dynamic analysis or dynamic scoring, is an attempt to take into account how the system is likely to respond to the change. One common use of these terms is budget policy in the United States, although it also occurs in many other statistical disputes.
Level of measurement or scale of measure is a classification that describes the nature of information within the numbers assigned to variables. Psychologist Stanley Smith Stevens developed the best known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. Other classifications include those by Chrisman and by Mosteller and Tukey. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines.
Cellular noise is random variability in quantities arising in cellular biology. For example, cells which are genetically identical, even within the same tissue, are often observed to have different expression levels of proteins, different sizes and structures. These apparently random differences can have important biological and medical consequences. Cellular noise was originally, and is still often, examined in the context of gene expression levels   either the concentration or copy number of the products of genes within and between cells. As gene expression levels are responsible for many fundamental properties in cellular biology, including cells' physical appearance, behaviour in response to stimuli, and ability to process information and control internal processes, the presence of noise in gene expression has profound implications for many processes in cellular biology.
The independence from irrelevant alternatives (IIA), also known as binary independence or the independence axiom, is an axiom of decision theory and various social sciences. The term is used with different meanings in different contexts; although they all attempt to provide a rational account of individual behavior or aggregation of individual preferences, the exact formulations differ from context to context. In individual choice theory, IIA sometimes refers to Chernoff's condition or Sen's property   (alpha): if an alternative x is chosen from a set T, and x is also an element of a subset S of T, then x must be chosen from S. That is, eliminating some of the unchosen alternatives shouldn't affect the selection of x as the best option. In social choice theory, Arrow's IIA is one of the conditions in Arrow's impossibility theorem, which states that it is impossible to aggregate individual rank-order preferences ("votes") satisfying IIA in addition to certain other reasonable conditions. Arrow defines IIA thus: The social preferences between alternatives x and y depend only on the individual preferences between x and y. Another expression of the principle: If A is preferred to B out of the choice set {A,B}, introducing a third option X, expanding the choice set to {A,B,X}, must not make B preferable to A. In other words, preferences for A or B should not be changed by the inclusion of X, i.e., X is irrelevant to the choice between A and B. This formulation appears in bargaining theory, theories of individual choice, and voting theory. Some theorists find it too strict an axiom; experiments by Amos Tversky, Daniel Kahneman, and others have shown that human behavior rarely adheres to this axiom. In social choice theory, IIA is also defined as: If A is selected over B out of the choice set {A,B} by a voting rule for given voter preferences of A, B, and an unavailable third alternative X, then if only preferences for X change, the voting rule must not lead to B's being selected over A. In other words, whether A or B is selected should not be affected by a change in the vote for an unavailable X, which is irrelevant to the choice between A and B.
In probability theory, an event is a set of outcomes of an experiment (a subset of the sample space) to which a probability is assigned. A single outcome may be an element of many different events, and different events in an experiment are usually not equally likely, since they may include very different groups of outcomes. An event defines a complementary event, namely the complementary set (the event not occurring), and together these define a Bernoulli trial: did the event occur or not  Typically, when the sample space is finite, any subset of the sample space is an event (i.e. all elements of the power set of the sample space are defined as events). However, this approach does not work well in cases where the sample space is uncountably infinite, most notably when the outcome is a real number. So, when defining a probability space it is possible, and often necessary, to exclude certain subsets of the sample space from being events (see Events in probability spaces, below).
The algebra of random variables provides rules for the symbolic manipulation of random variables, while avoiding delving too deeply into the mathematically sophisticated ideas of probability theory. Its symbolism allows the treatment of sums, products, ratios and general functions of random variables, as well as dealing with operations such as finding the probability distributions and the expectations, variances and covariances of such combinations.
A shifting baseline (also known as sliding baseline) is a type of change to how a system is measured, usually against previous reference points (baselines), which themselves may represent significant changes from an even earlier state of the system. A conceptual metaphor for a shifting baseline is the price of coffee. A cup of coffee may have only cost a $0.05 in the 1950s, but in the 1980s the cost shifted to $1.00 (ignoring inflation). The current (21st century) coffee prices are based on the 1980s model, rather than the 1950s model. The point of reference moved. The concept arose in landscape architect Ian McHarg's famous manifesto "Design With Nature" in which the landscape as we know it is compared to that which ancient men once lived on. The concept was then considered by the fisheries scientist Daniel Pauly in his paper "Anecdotes and the shifting baseline syndrome of fisheries". Pauly developed the concept in reference to fisheries management where fisheries scientists sometimes fail to identify the correct "baseline" population size (e.g. how abundant a fish species population was before human exploitation) and thus work with a shifted baseline. He describes the way that radically depleted fisheries were evaluated by experts who used the state of the fishery at the start of their careers as the baseline, rather than the fishery in its untouched state. Areas that swarmed with a particular species hundreds of years ago, may have experienced long term decline, but it is the level of decades previously that is considered the appropriate reference point for current populations. In this way large declines in ecosystems or species over long periods of time were, and are, masked. There is a loss of perception of change that occurs when each generation redefines what is "natural". Most modern fisheries stock assessments do not ignore historical fishing and account for it by either including the historical catch or use other techniques to reconstruct the depletion level of the population at the start of the period for which adequate data is available. Anecdotes about historical populations levels can be highly unreliable and result in severe mismanagement of the fishery. The concept was further refined and applied to the ecology of kelp forests by Paul Dayton and others from the Scripps Institution of Oceanography. They used a slightly different version of the term in their paper, "Sliding baselines, ghosts, and reduced expectations in kelp forest communities". Both terms refer to a shift over time in the expectation of what a healthy ecosystem baseline looks like.
The philosophy of statistics involves the meaning, justification, utility, use and abuse of statistics and its methodology, and ethical and epistemological issues involved in the consideration of choice and interpretation of data and methods of Statistics. Foundations of statistics involves issues in theoretical statistics, its goals and optimization methods to meet these goals, parametric assumptions or lack thereof considered in nonparametric statistics, model selection for the underlying probability distribution, and interpretation of the meaning of inferences made using statistics, related to the philosophy of probability and the philosophy of science. Discussion of the selection of the goals and the meaning of optimization, in foundations of statistics, are the subject of the philosophy of statistics. Selection of distribution models, and of the means of selection, is the subject of the philosophy of statistics, whereas the mathematics of optimization is the subject of nonparametric statistics. David Cox makes the point that any kind of interpretation of evidence is in fact a statistical model, although it is known through Ian Hacking's work that many are ignorant of this subtlety. Issues arise involving sample size, such as cost and efficiency, are common, such as in polling and pharmaceutical research. Extra-mathematical considerations in the design of experiments and accommodating these issues arise in most actual experiments. The motivation and justification of data analysis and experimental design, as part of the scientific method are considered. Distinctions between induction and logical deduction relevant to inferences from data and evidence arise, such as when frequentist interpretations are compared with degrees of certainty derived from Bayesian inference. However, the difference between induction and ordinary reasoning is not generally appreciated  Leo Breiman exposed the diversity of thinking in his article on 'The Two Cultures', making the point that statistics has several kinds of inference to make, modelling and prediction amongst them. Issues in the philosophy of statistics arise throughout the history of statistics. Causality considerations arise with interpretations of, and definitions of, correlation, and in the theory of measurement. Objectivity in statistics is often confused with truth whereas it is better understood as replicability, which then needs to be definied in the particular case. Theodore Porter develops this as being the path pursued when trust has evaporated, being replaced with criteria. Ethics associated with epistemology and medical applications arise from potential abuse of statistics, such as selection of method or transformations of the data to arrive at different probability conclusions for the same data set. For example, the meaning of applications of a statistical inference to a single person, such as one single cancer patient, when there is no frequentist interpretation for that patient to adopt. Campaigns for statistical literacy must wrestle with the problem that most interesting questions around individual risk are very difficult to determine or interpret, even with the computer power currently available.
Numerical Analysis for Excel (NumXL) is an econometrics and time series analysis add-in for Microsoft Excel. Developed by Spider Financial, NumXL provides a wide variety of statistical and time series analysis techniques, including linear and nonlinear time series modeling, statistical tests and others. Although NumXL is intended as an analytical add-in for Excel, it extends Excel s user-interface (UI) and offers many wizards, menus and toolbars to automate the mundane phases of time series analysis. The features include summary statistics, test of hypothesis, correlogram analysis, modeling, calibration, residuals diagnosis, back-testing and forecast. NumXL users have varied backgrounds in finance, economics, engineering and science. NumXL is used in academic and research institutions and industrial enterprises.
R is a programming language and software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. Polls, surveys of data miners, and studies of scholarly literature databases show that R's popularity has increased substantially in recent years. R is an implementation of the S programming language combined with lexical scoping semantics inspired by Scheme. S was created by John Chambers while at Bell Labs. There are some important differences, but much of the code written for S runs unaltered. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and is currently developed by the R Development Core Team, of which Chambers is a member. R is named partly after the first names of the first two R authors and partly as a play on the name of S. R is a GNU project. The source code for the R software environment is written primarily in C, Fortran, and R. R is freely available under the GNU General Public License, and pre-compiled binary versions are provided for various operating systems. While R has a command line interface, there are several graphical front-ends available.
In mathematics, the Ornstein Uhlenbeck process (named after Leonard Ornstein and George Eugene Uhlenbeck), is a stochastic process that, roughly speaking, describes the velocity of a massive Brownian particle under the influence of friction. The process is stationary, Gaussian, and Markovian, and is the only nontrivial process that satisfies these three conditions, up to allowing linear transformations of the space and time variables. Over time, the process tends to drift towards its long-term mean: such a process is called mean-reverting. The process can be considered to be a modification of the random walk in continuous time, or Wiener process, in which the properties of the process have been changed so that there is a tendency of the walk to move back towards a central location, with a greater attraction when the process is further away from the centre. The Ornstein Uhlenbeck process can also be considered as the continuous-time analogue of the discrete-time AR(1) process.
In statistics, dependence is any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationships involving dependence, though in common usage it most often refers to the extent to which two variables have a linear relationship with each other. Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, and the correlation between the demand for a product and its price. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling; however, statistical dependence is not sufficient to demonstrate the presence of such a causal relationship (i.e., correlation does not imply causation). Formally, dependence refers to any situation in which random variables do not satisfy a mathematical condition of probabilistic independence. In loose usage, correlation can refer to any departure of two or more random variables from independence, but technically it refers to any of several more specialized types of relationship between mean values. There are several correlation coefficients, often denoted   or r, measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may exist even if one is a nonlinear function of the other). Other correlation coefficients have been developed to be more robust than the Pearson correlation   that is, more sensitive to nonlinear relationships. Mutual information can also be applied to measure dependence between two variables.
In probability theory, conditional dependence is a relationship between two or more events that are dependent when a third event occurs. For example, if A and B are two events that individually affect the happening of a third event C, and do not directly affect each other, then initially (when the event C has not occurred)  (A and B are independent) Eventually the event C occurs, and now if event A occurs the probability of occurrence of the event B will decrease (similarly event B occurring first will decrease the probability of occurrence of A in future). Hence, now the two events A and B become conditionally dependent because their probability of occurrence is dependent on either event's occurrence. Intuitively we can say that since A and B both were probable causes of C, given C has occurred, occurrence of either of A or B alone could explain away the happening of C.  In essence probability comes from a person's information content about occurrence of an event. For example, let the event A be 'I have a new car'; event B be 'I have a new watch'; and event C be 'I am happy'. Let us assume that the event C has occurred   meaning 'I am happy'. Now if a third person sees my new watch, he/she will attribute this reason to my happiness. Thus in his/her view the probability of the event A ('I have a new car') to have been the cause of the event C ('I am happy') will decrease as the event C has been explained away by the event B. Conditional dependence is different from conditional independence. In conditional independence two events (which may be dependent or not) become independent given the occurrence of a third event.
In probability theory, the Gillespie algorithm (or occasionally the Doob-Gillespie algorithm) generates a statistically correct trajectory (possible solution) of a stochastic equation. It was created by Joseph L. Doob and others (circa 1945), presented by Dan Gillespie in 1976, and popularized in 1977 in a paper where he uses it to simulate chemical or biochemical systems of reactions efficiently and accurately using limited computational power (see stochastic simulation). As computers have become faster, the algorithm has been used to simulate increasingly complex systems. The algorithm is particularly useful for simulating reactions within cells where the number of reagents typically number in the tens of molecules (or less). Mathematically, it is a variety of a dynamic Monte Carlo method and similar to the kinetic Monte Carlo methods. It is used heavily in computational systems biology.
The Crystal Ball function, named after the Crystal Ball Collaboration (hence the capitalized initial letters), is a probability density function commonly used to model various lossy processes in high-energy physics. It consists of a Gaussian core portion and a power-law low-end tail, below a certain threshold. The function itself and its first derivative are both continuous. The Crystal Ball function is given by:  where , ,   (Skwarnicki 1986) is a normalization factor and , ,  and  are parameters which are fitted with the data. erf is the error function.
Predictive modeling uses statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place. In many cases the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example given an email determining how likely that it is spam. Models can use one or more classifiers in trying to determine the probability of a set of data belonging to another set, say spam or 'ham'. Depending on definitional boundaries, predictive modelling is synonymous with, or largely overlapping with, the field of machine learning, as it is more commonly referred to in academic or research and development contexts. When deployed commercially, predictive modelling is often referred to as predictive analytics.
The shifted log-logistic distribution is a probability distribution also known as the generalized log-logistic or the three-parameter log-logistic distribution. It has also been called the generalized logistic distribution, but this conflicts with other uses of the term: see generalized logistic distribution.
Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.
A tag cloud (word cloud, or weighted list in visual design) is a visual representation of text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color. This format is useful for quickly perceiving the most prominent terms and for locating a term alphabetically to determine its relative prominence. When used as website navigation aids, the terms are hyperlinked to items associated with the tag.
Nonparametric regression is a form of regression analysis in which the predictor does not take a predetermined form but is constructed according to information derived from the data. Nonparametric regression requires larger sample sizes than regression based on parametric models because the data must supply the model structure as well as the model estimates.
Statistical conclusion validity is the degree to which conclusions about the relationship among variables based on the data are correct or  reasonable . This began as being solely about whether the statistical conclusion about the relationship of the variables was correct, but now there is a movement towards moving to  reasonable  conclusions that use: quantitative, statistical, and qualitative data. Fundamentally, two types of errors can occur: type I (finding a difference or correlation when none exists) and type II (finding no difference when one exists). Statistical conclusion validity concerns the qualities of the study that make these types of errors more likely. Statistical conclusion validity involves ensuring the use of adequate sampling procedures, appropriate statistical tests, and reliable measurement procedures.
In ancient history, the concepts of chance and randomness were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. At the same time, most ancient cultures used various methods of divination to attempt to circumvent randomness and fate. The Chinese were perhaps the earliest people to formalize odds and chance 3,000 years ago. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the sixteenth century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of modern calculus had a positive impact on the formal study of randomness. In the 19th century the concept of entropy was introduced in physics. The early part of the twentieth century saw a rapid growth in the formal analysis of randomness, and mathematical foundations for probability were introduced, leading to its axiomatization in 1933. At the same time, the advent of quantum mechanics changed the scientific perspective on determinacy. In the mid to late 20th-century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness. Although randomness had often been viewed as an obstacle and a nuisance for many centuries, in the twentieth century computer scientists began to realize that the deliberate introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases, such randomized algorithms are able to outperform the best deterministic methods.
In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.) This relates directly to the k-median problem which is the problem of finding k centers such that the clusters formed by them are the most compact. Formally, given a set of data points x, the k centers ci are to be chosen so as to minimize the sum of the distances from each x to the nearest ci. The criterion function formulated in this way is sometimes a better criterion than that used in the k-means clustering algorithm, in which the sum of the squared distances is used. The sum of distances is widely used in applications such as facility location. The proposed algorithm uses Lloyd-style iteration which alternates between an expectation (E) and maximization (M) step, making this an Expectation maximization algorithm. In the E step, all objects are assigned to their nearest median. In the M step, the medians are recomputed by using the median in each single dimension.
In statistics, circular analysis is the selection of the details of a data analysis using the data that is being analysed. It is often referred to as double dipping, as one uses the same data twice. Circular analysis unjustifiably inflates the apparent statistical strength of any results reported and, at the most extreme, can lead to the apparently significant result being found in data that consists only of noise. In particular, where an experiment is implemented to study a postulated effect, it is a misuse of statistics to initially reduce the complete dataset by selecting a subset of data in ways that are aligned to the effects being studied. A second misuse occurs where the performance of a fitted model or classification rule is reported as a raw result, without allowing for the effects of model-selection and the tuning of parameters based on the data being analyzed.
In probability theory, the Type-2 Gumbel probability density function is  for . This implies that it is similar to the Weibull distributions, substituting  and . Note, however, that a positive k (as in the Weibull distribution) would yield a negative a, which is not allowed here as it would yield a negative probability density. For  the mean is infinite. For  the variance is infinite. The cumulative distribution function is  The moments  exist for  The special case b = 1 yields the Fre chet distribution Based on The GNU Scientific Library, used under GFDL.  
In statistics, engineering, economics, and medical research, censoring is a condition in which the value of a measurement or observation is only partially known. For example, suppose a study is conducted to measure the impact of a drug on mortality rate. In such a study, it may be known that an individual's age at death is at least 75 years (but may be more). Such a situation could occur if the individual withdrew from the study at age 75, or if the individual is currently alive at the age of 75. Censoring also occurs when a value occurs outside the range of a measuring instrument. For example, a bathroom scale might only measure up to 300 pounds (140 kg). If a 350 lb (160 kg) individual is weighed using the scale, the observer would only know that the individual's weight is at least 300 pounds (140 kg). The problem of censored data, in which the observed value of some variable is partially known, is related to the problem of missing data, where the observed value of some variable is unknown. Censoring should not be confused with the related idea truncation. With censoring, observations result either in knowing the exact value that applies, or in knowing that the value lies within an interval. With truncation, observations never result in values outside a given range: values in the population outside the range are never seen or never recorded if they are seen. Note that in statistics, truncation is not the same as rounding.
A location test is a statistical hypothesis test that compares the location parameter of a statistical population to a given constant, or that compares the location parameters of two statistical populations to each other. Most commonly, the location parameter (or parameters) of interest are expected values, but location tests based on medians or other measures of location are also used.
In probability theory, a branching process is a Markov process that models a population in which each individual in generation n produces some random number of individuals in generation n + 1, according, in the simplest case, to a fixed probability distribution that does not vary from individual to individual. Branching processes are used to model reproduction; for example, the individuals might correspond to bacteria, each of which generates 0, 1, or 2 offspring with some probability in a single time unit. Branching processes can also be used to model other systems with similar dynamics, e.g., the spread of surnames in genealogy or the propagation of neutrons in a nuclear reactor. A central question in the theory of branching processes is the probability of ultimate extinction, where no individuals exist after some finite number of generations. It is not hard to show that, starting with one individual in generation zero, the expected size of generation n equals  n where   is the expected number of children of each individual. If   < 1, then the expected number of individuals goes rapidly to zero, which implies ultimate extinction with probability 1 by Markov's inequality. Alternatively, if   > 1, then the probability of ultimate extinction is less than 1 (but not necessarily zero; consider a process where each individual either dies without issue or has 100 children with equal probability). If   = 1, then ultimate extinction occurs with probability 1 unless each individual always has exactly one child. In theoretical ecology, the parameter   of a branching process is called the basic reproductive rate.
A log-linear model is a mathematical model that takes the form of a function whose logarithm is a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression. That is, it has the general form , in which the fi(X) are quantities that are functions of the variables X, in general a vector of values, while c and the wi stand for the model parameters. The term may specifically be used for: A log-linear plot or graph, which is a type of semi-log plot. Poisson regression for contingency tables, a type of generalized linear model. The specific applications of log-linear models are where the output quantity lies in the range 0 to  , for values of the independent variables X, or more immediately, the transformed quantities fi(X) in the range    to + . This may be contrasted to logistic models, similar to the logistic function, for which the output quantity lies in the range 0 to 1. Thus the contexts where these models are useful or realistic often depends on the range of the values being modelled.
In statistics, bootstrapping can refer to any test or metric that relies on random sampling with replacement. Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Generally, it falls in the broader class of resampling methods. Bootstrapping is the practice of estimating properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed dataset (and of equal size to the observed dataset). It may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.
In statistics, Cook's distance or Cook's D is a commonly used estimate of the influence of a data point when performing least squares regression analysis. In a practical ordinary least squares analysis, Cook's distance can be used in several ways: to indicate influential data points that are particularly worth checking for validity; to indicate regions of the design space where it would be good to be able to obtain more data points. It is named after the American statistician R. Dennis Cook, who introduced the concept in 1977.
Conditional random fields (CRFs) are a class of statistical modelling method often applied in pattern recognition and machine learning, where they are used for structured prediction. Whereas an ordinary classifier predicts a label for a single sample without regard to "neighboring" samples, a CRF can take context into account; e.g., the linear chain CRF popular in natural language processing predicts sequences of labels for sequences of input samples. CRFs are a type of discriminative undirected probabilistic graphical model. It is used to encode known relationships between observations and construct consistent interpretations. It is often used for labeling or parsing of sequential data, such as natural language text or biological sequences and in computer vision. Specifically, CRFs find applications in shallow parsing, named entity recognition, gene finding and peptide critical functional region finding, among other tasks, being an alternative to the related hidden Markov models (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation.
"Lies, damned lies, and statistics" is a phrase describing the persuasive power of numbers, particularly the use of statistics to bolster weak arguments. It is also sometimes colloquially used to doubt statistics used to prove an opponent's point. The term was popularised in United States by Mark Twain (among others), who attributed it to the British Prime Minister Benjamin Disraeli: "There are three kinds of lies: lies, damned lies, and statistics." However, the phrase is not found in any of Disraeli's works and the earliest known appearances were years after his death. Several other people have been listed as originators of the quote, and it is often erroneously attributed to Twain himself.
Clinical study design is the formulation of trials and experiments, as well as observational studies in medical, clinical and other types of research (e.g., epidemiological) involving human beings. The goal of a clinical study is to assess the safety, efficacy, and / or the mechanism of action of an investigational medicinal product, or new drug or device that is in development, but potentially not yet approved by a health authority (e.g. FDA). Some of the considerations here are shared under the more general topic of design of experiments but there can be others, in particular related to patient confidentiality and ethics.
The network probability matrix describes the probability structure of a network based on the historical presence or absence of edges in a network. For example, individuals in a social network are not connected to other individuals with uniform random probability. The probability structure is much more complex. Intuitively, there are some people whom a person will communicate with or be connected more closely than others. For this reason, real-world networks tend to have clusters or cliques of nodes that are more closely related than others (Albert and Barabasi, 2002, Carley [year], Newmann 2003). This can be simulated by varying the probabilities that certain nodes will communicate. The network probability matrix was originally proposed by Ian McCulloh.
Statistical proof is the rational demonstration of degree of certainty for a proposition, hypothesis or theory that is used to convince others subsequent to a statistical test of the supporting evidence and the types of inferences that can be drawn from the test scores. Statistical methods are used to increase the understanding of the facts and the proof demonstrates the validity and logic of inference with explicit reference to a hypothesis, the experimental data, the facts, the test, and the odds. Proof has two essential aims: the first is to convince and the second is to explain the proposition through peer and public review. The burden of proof rests on the demonstrable application of the statistical method, the disclosure of the assumptions, and the relevance that the test has with respect to a genuine understanding of the data relative to the external world. There are adherents to several different statistical philosophies of inference, such as Bayes theorem versus the likelihood function, or positivism versus critical rationalism. These methods of reason have direct bearing on statistical proof and its interpretations in the broader philosophy of science. A common demarcation between science and non-science is the hypothetico-deductive proof of falsification developed by Karl Popper, which is a well-established practice in the tradition of statistics. Other modes of inference, however, may include the inductive and abductive modes of proof. Scientists do not use statistical proof as a means to attain certainty, but to falsify claims and explain theory. Science cannot achieve absolute certainty nor is it a continuous march toward an objective truth as the vernacular as opposed to the scientific meaning of the term "proof" might imply. Statistical proof offers a kind of proof of a theory's falsity and the means to learn heuristically through repeated statistical trials and experimental error. Statistical proof also has applications in legal matters with implications for the legal burden of proof.
SPSS Statistics is a software package used for statistical analysis. Long produced by SPSS Inc., it was acquired by IBM in 2009. The current versions (2015) are officially named IBM SPSS Statistics. Companion products in the same family are used for survey authoring and deployment (IBM SPSS Data Collection), data mining (IBM SPSS Modeler), text analytics, and collaboration and deployment (batch and automated scoring services). The software name originally stood for Statistical Package for the Social Sciences (SPSS), reflecting the original market, although the software is now popular in other fields as well, including the health sciences and marketing.  
In statistics, Poisson hidden Markov models (PHMM) are a special case of hidden Markov models where a Poisson process has a rate which varies in association with changes between the different states of a Markov model. PHMMs are not necessarily Markovian processes themselves because the underlying Markov chain or Markov process cannot be observed and only the Poisson signal is observed.
Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of minimum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). The PLS algorithm is employed in partial least squares path modeling, a method of modeling a "causal" network of latent variables (causes cannot be determined without experimental or quasi-experimental methods, but one typically bases a latent variable model on the prior theoretical assumption that latent variables cause manifestations in their measured indicators). This technique is argued to be a form of structural equation modeling, distinguished from the classical method by being component-based rather than covariance-based. Yet, others dispute that this is the case. Partial least squares was introduced by the Swedish statistician Herman Wold, who then developed it with his son, Svante Wold. An alternative term for PLS (and more correct according to Svante Wold) is projection to latent structures, but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience and anthropology. In contrast, PLS path modeling is most often used in social sciences, econometrics, marketing and strategic management. However, within the realm of psychology, it has received criticism for being an unreliable estimation and testing tool.
In statistics, Hodges  estimator (or the Hodges Le Cam estimator), named for Joseph Hodges, is a famous counter example of an estimator which is "superefficient", i.e. it attains smaller asymptotic variance than regular efficient estimators. The existence of such a counterexample is the reason for the introduction of the notion of regular estimators. Hodges  estimator improves upon a regular estimator at a single point. In general, any superefficient estimator may surpass a regular estimator at most on a set of Lebesgue measure zero.
In statistics, Deming regression, named after W. Edwards Deming, is an errors-in-variables model which tries to find the line of best fit for a two-dimensional dataset. It differs from the simple linear regression in that it accounts for errors in observations on both the x- and the y- axis. It is a special case of total least squares, which allows for any number of predictors and a more complicated error structure. Deming regression is equivalent to the maximum likelihood estimation of an errors-in-variables model in which the errors for the two variables are assumed to be independent and normally distributed, and the ratio of their variances, denoted  , is known. In practice, this ratio might be estimated from related data-sources; however the regression procedure takes no account for possible errors in estimating this ratio. The Deming regression is only slightly more difficult to compute compared to the simple linear regression. Many software packages used in clinical chemistry, such as Analyse-it, EP Evaluator, GraphPad Prism, MedCalc, NCSS (statistical software), OriginPro, R, S-PLUS and StatsDirect offer Deming regression. The model was originally introduced by Adcock (1878) who considered the case   = 1, and then more generally by Kummell (1879) with arbitrary  . However their ideas remained largely unnoticed for more than 50 years, until they were revived by Koopmans (1937) and later propagated even more by Deming (1943). The latter book became so popular in clinical chemistry and related fields that the method was even dubbed Deming regression in those fields.
In probability theory and statistics, a scale parameter is a special kind of numerical parameter of a parametric family of probability distributions. The larger the scale parameter, the more spread out the distribution.  
In econometrics and official statistics, and particularly in banking, the Divisia monetary aggregates index is an index of money supply. It is a particular application of a Divisia index to monetary aggregates.
The following outline is provided as an overview and guide to probability: Probability is a measure of the likeliness that an event will occur. Probability is used to quantify an attitude of mind towards some proposition of whose truth we are not certain. The proposition of interest is usually of the form "A specific event will occur." The attitude of mind is of the form "How certain are we that the event will occur " The certainty we adopt can be described in terms of a numerical measure and this number, between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty), we call probability. Probability theory is used extensively in statistics, mathematics, science and philosophy to draw conclusions about the likelihood of potential events and the underlying mechanics of complex systems.  
In statistics, the coefficient of multiple correlation is a measure of how well a given variable can be predicted using a linear function of a set of other variables. It is the correlation between the variable's values and the best predictions that can be computed linearly from the predictive variables. The coefficient of multiple correlation takes values between 0 and 1; a higher value indicates a better predictability of the dependent variable from the independent variables, with a value of 1 indicating that the predictions are exactly correct and a value of 0 indicating that no linear combination of the independent variables is a better predictor than is the fixed mean of the dependent variable. The coefficient of multiple correlation is computed as the square root of the coefficient of determination, but under the particular assumptions that an intercept is included and that the best possible linear predictors are used, whereas the coefficient of determination is defined for more general cases, including those of nonlinear prediction and those in which the predicted values have not been derived from a model-fitting procedure.
In econometrics and statistics, a top-coded data observation is one for which data points whose values are above an upper bound are censored. Survey data are often topcoded before release to the public to preserve the anonymity of respondents. For example, if a survey answer reported a respondent with self-identified wealth of $79 billion, it would not be anonymous because people would know there is a good chance the respondent was Bill Gates. Top-coding may be also applied to prevent possibly-erroneous outliers from being published. Bottom-coding is analogous, e.g. if amounts below zero are reported as zero. Top-coding occurs for data recorded in groups, e.g. if age ranges are reported in these groups: 0-20, 21-50, 50-99, 100-and-up. Here we only know how many people have ages above 100, not their distribution. Producers of survey data sometimes release the average of the censored amounts to help users impute unbiased estimates of the top group.
In econometrics, Prais Winsten estimation is a procedure meant to take care of the serial correlation of type AR(1) in a linear model. Conceived by Sigbert Prais and Christopher Winsten in 1954, it is a modification of Cochrane Orcutt estimation in the sense that it does not lose the first observation, which leads to more efficiency as a result and makes it a special case of feasible generalized least squares.
The Gompertz Makeham law states that the human death rate is the sum of an age-independent component (the Makeham term, named after William Makeham) and an age-dependent component (the Gompertz function, named after Benjamin Gompertz), which increases exponentially with age. In a protected environment where external causes of death are rare (laboratory conditions, low mortality countries, etc.), the age-independent mortality component is often negligible. In this case the formula simplifies to a Gompertz law of mortality. In 1825, Benjamin Gompertz proposed an exponential increase in death rates with age. The Gompertz Makeham law of mortality describes the age dynamics of human mortality rather accurately in the age window from about 30 to 80 years of age. At more advanced ages, some studies have found that death rates increase more slowly   a phenomenon known as the late-life mortality deceleration   but more recent studies disagree.  The decline in the human mortality rate before the 1950s was mostly due to a decrease in the age-independent (Makeham) mortality component, while the age-dependent (Gompertz) mortality component was surprisingly stable. Since the 1950s, a new mortality trend has started in the form of an unexpected decline in mortality rates at advanced ages and "rectangularization" of the survival curve. The hazard function for the Gompert-Makeham distribution is most often characterised as . The empirical magnitude of the beta-parameter is about .085, implying a doubling of mortality every .69/.085 = 8 years (Denmark, 2006). The quantile function can be expressed in a closed-form expressions using the Lambert W function:  The Gompertz law is the same as a Fisher Tippett distribution for the negative of age, restricted to negative values for the random variable (positive values for age).
The Mittag-Leffler distributions are two families of probability distributions on the half-line . They are parametrized by a real  or . Both are defined with the Mittag-Leffler function, named after Go sta Mittag-Leffler.
In probability theory, the Chernoff bound, named after Herman Chernoff but due to Herman Rubin, gives exponentially decreasing bounds on tail distributions of sums of independent random variables. It is a sharper bound than the known first or second moment based tail bounds such as Markov's inequality or Chebyshev inequality, which only yield power-law bounds on tail decay. However, the Chernoff bound requires that the variates be independent   a condition that neither the Markov nor the Chebyshev inequalities require. It is related to the (historically prior) Bernstein inequalities, and to Hoeffding's inequality.
In statistics, a confidence interval (CI) is a type of interval estimate of a population parameter. It is an observed interval (i.e., it is calculated from the observations), in principle different from sample to sample, that frequently includes the value of an unobservable parameter of interest if the experiment is repeated. How frequently the observed interval contains the parameter is determined by the confidence level or confidence coefficient. More specifically, the meaning of the term "confidence level" is that, if CI are constructed across many separate data analyses of replicated (and possibly different) experiments, the proportion of such intervals that contain the true value of the parameter will match the given confidence level. Whereas two-sided confidence limits form a confidence interval, their one-sided counterparts are referred to as lower/upper confidence bounds (or limits). Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter; however, the interval computed from a particular sample does not necessarily include the true value of the parameter. When we say, "we are 99% confident that the true value of the parameter is in our confidence interval", we express that 99% of the hypothetically observed confidence intervals will hold the true value of the parameter. After any particular sample is taken, the population parameter is either in the interval realized or not; it is not a matter of chance. The desired level of confidence is set by the researcher (not determined by data). If a corresponding hypothesis test is performed, the confidence level is the complement of respective level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. The confidence interval contains the parameter values that, when tested, should not be rejected with the same sample. Greater levels of variance yield larger confidence intervals, and hence less precise estimates of the parameter. Confidence intervals of difference parameters not containing 0 imply that there is a statistically significant difference between the populations. In applied practice, confidence intervals are typically stated at the 95% confidence level. However, when presented graphically, confidence intervals can be shown at several confidence levels, for example 90%, 95% and 99%. Certain factors may affect the confidence interval size including size of sample, level of confidence, and population variability. A larger sample size normally will lead to a better estimate of the population parameter. Confidence intervals were introduced to statistics by Jerzy Neyman in a paper published in 1937.
The term "level of analysis" is used in the social sciences to point to the location, size, or scale of a research target. "Level of analysis" is distinct from the term "unit of observation" in that the former refers to a more or less integrated set of relationships while the latter refers to the distinct unit from which data have been or will be gathered. Together, the unit of observation and the level of analysis help define the population of a research enterprise..
A case-control study is a type of observational study in which two existing groups differing in outcome are identified and compared on the basis of some supposed causal attribute. Case-control studies are often used to identify factors that may contribute to a medical condition by comparing subjects who have that condition/disease (the "cases") with patients who do not have the condition/disease but are otherwise similar (the "controls"). They require fewer resources but provide less evidence for causal inference than a randomized controlled trial.
Linear prediction is a mathematical operation where future values of a discrete-time signal are estimated as a linear function of previous samples. In digital signal processing, linear prediction is often called linear predictive coding (LPC) and can thus be viewed as a subset of filter theory. In system analysis (a subfield of mathematics), linear prediction can be viewed as a part of mathematical modelling or optimization.
The risk adjusted mortality rate (RAMR) is a mortality rate that is adjusted for predicted risk of death. It is usually utilized to observe and/or compare the performance of certain institution(s) or person(s), e.g., hospitals or surgeons. It can be found as: RAMR = (Observed Mortality Rate/Predicted Mortality Rate)* Overall (Weighted) Mortality Rate In medical science, RAMR could be a predictor of mortality that takes into account the predicted risk for a group of patients. For example, for a group of patients first we need to find the observed mortality rates for all the hospitals of interest. Then we can build/construct a model or use an existing model to predict mortality rates for each of the hospitals. It is expected that the number of patients in each hospital will be different and hence we need an overall (weighted) mortality rate for all these hospitals. Once we have the above three rates, then we can utilize the above formula to find the risk adjusted mortality rate which will reflect the actual mortality rate of a particular hospital without being biased from the observed mortality.
In mathematics, Fourier analysis (English pronunciation: / f  rie /) is the study of the way general functions may be represented or approximated by sums of simpler trigonometric functions. Fourier analysis grew from the study of Fourier series, and is named after Joseph Fourier, who showed that representing a function as a sum of trigonometric functions greatly simplifies the study of heat transfer. Today, the subject of Fourier analysis encompasses a vast spectrum of mathematics. In the sciences and engineering, the process of decomposing a function into oscillatory components is often called Fourier analysis, while the operation of rebuilding the function from these pieces is known as Fourier synthesis. For example, determining what component frequencies are present in a musical note would involve computing the Fourier transform of a sampled musical note. One could then re-synthesize the same sound by including the frequency components as revealed in the Fourier analysis. In mathematics, the term Fourier analysis often refers to the study of both operations. The decomposition process itself is called a Fourier transformation. Its output, the Fourier transform, is often given a more specific name, which depends on the domain and other properties of the function being transformed. Moreover, the original concept of Fourier analysis has been extended over time to apply to more and more abstract and general situations, and the general field is often known as harmonic analysis. Each transform used for analysis (see list of Fourier-related transforms) has a corresponding inverse transform that can be used for synthesis.
The factor regression model, or hybrid factor model, is a special multivariate model with the following form.  where,  is the -th  (known) observation.  is the -th sample  (unknown) hidden factors.  is the (unknown) loading matrix of the hidden factors.  is the -th sample  (known) design factors.  is the (unknown) regression coefficients of the design factors.  is a vector of (unknown) constant term or intercept.  is a vector of (unknown) errors, often white Gaussian noise.
Belief propagation, also known as sum-product message passing, is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability. The algorithm was first proposed by Judea Pearl in 1982, who formulated this algorithm on trees, and was later extended to polytrees. It has since been shown to be a useful approximate algorithm on general graphs. If X={Xi} is a set of discrete random variables with a joint mass function p, the marginal distribution of a single Xi is simply the summation of p over all other variables:  However, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 299   6.338   1029 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.
In statistics, Duncan's new multiple range test (MRT) is a multiple comparison procedure developed by David B. Duncan in 1955. Duncan's MRT belongs to the general class of multiple comparison procedures that use the studentized range statistic qr to compare sets of means. David B. Duncan developed this test as a modification of the Student Newman Keuls method that would have greater power. Duncan's MRT is especially protective against false negative (Type II) error at the expense of having a greater risk of making false positive (Type I) errors. Duncan's test is commonly used in agronomy and other agricultural research. The result of the test is a set of subsets of means, where in each subset means have been found not to be significantly different from one another.
The term censoring is used in clinical trials to refer to mathematically removing a patient from the survival curve at the end of their follow-up time. Censoring a patient will reduce the sample size for analyzing after the time of the censorship. Reducing the sample size always reduces reliability, so the more patients are censored and the earlier they are censored the more unreliable the results are. Censoring is a form of missing data problem which is common in survival analysis. In statistics, engineering, economics, and medical research, censoring is a condition in which the value of a measurement or observation is only partially known. Many clinical trials are designed with a minimum follow-up time. This means that the results aren't reported until that amount of the time after the last patient signed up for the trial. Often reports of the preliminary results don't include any minimum follow-up time and include the patients with very short follow-up time which definitely affects the reliability of the result.
In mathematics, univariate refers to an expression, equation, function or polynomial of only one variable. Objects of any of these types but involving more than one variable may be called multivariate. In some cases the distinction between the univariate and multivariate cases is fundamental; for example, the fundamental theorem of algebra and Euclid's algorithm for polynomials are fundamental properties of univariate polynomials that cannot be generalized to multivariate polynomials. The term is commonly used in statistics to distinguish a distribution of one variable from a distribution of several variables, although it can be applied in other ways as well. For example, univariate data are composed of a single scalar component. In time series analysis, the term is applied with a whole time series as the object referred to: thus a univariate time series refers to the set of values over time of a single quantity. Correspondingly, a "multivariate time series" refers to the changing values over time of several quantities. Thus there is a minor conflict of terminology since the values within a univariate time series may be treated using certain types of multivariate statistical analyses and may be represented using multivariate distributions.
Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data. In applying statistics to, e.g., a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as "all people living in a country" or "every atom composing a crystal". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments. When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Two main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena. A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. An hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems. Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.
Internal validity is a property of scientific studies which reflects the extent to which a causal conclusion based on a study is warranted. Such warrant is constituted by the extent to which a study minimizes systematic error (or 'bias').
In statistics and the theory of probability, quantiles are cutpoints dividing the range of a probability distribution into contiguous intervals with equal probabilities, or dividing the observations in a sample in the same way. There are one fewer quantiles than the number of groups created. Thus quartiles are the three cut points that will divide a dataset into four equal-size groups. Common quantiles have special names: for instance quartile, decile (creating 10 groups: see below for more). The groups created are termed halves, thirds, quarters, etc., though sometimes the terms for the quantile are used for the groups created, rather than for the cut points. q-Quantiles are values that partition a finite set of values into q subsets of (nearly) equal sizes. There are q   1 of the q-quantiles, one for each integer k satisfying 0 < k < q. In some cases the value of a quantile may not be uniquely determined, as can be the case for the median (2-quantile) of a uniform probability distribution on a set of even size. Quantiles can also be applied to continuous distributions, providing a way to generalize rank statistics to continuous variables. When the cumulative distribution function of a random variable is known, the q-quantiles are the application of the quantile function (the inverse function of the cumulative distribution function) to the values {1/q, 2/q, ..., (q   1)/q}.
In probability, statistics and related fields, a Poisson point process or Poisson process (also called a Poisson random measure, Poisson random point field or Poisson point field) is a type of random mathematical object that consists of points randomly located on a mathematical space. The process has convenient mathematical properties, which has led to it being frequently defined in Euclidean space and used as a mathematical model for seemingly random processes in numerous disciplines such as astronomy, biology, ecology, geology, physics, image processing, and telecommunications. The Poisson point process is often defined on the real line. For example, in queueing theory  it is used to model random events, such as the arrival of customers at a store or phone calls at an exchange, distributed in time. In the plane, the point process, also known as a spatial Poisson process, may represent scattered objects such as transmitters in a wireless network, particles colliding into a detector, or trees in a forest. In this setting, the process is often used in mathematical models and in the related fields of spatial point processes, stochastic geometry, spatial statistics  and continuum percolation theory. In more abstract spaces, the Poisson point process serves as an object of mathematical study in its own right. In all settings, the Poisson point process has the property that each point is stochastically independent to all the other points in the process, which is why it is sometimes called a purely or completely random process. Despite its wide use as a stochastic model of phenomena representable as points, the inherent nature of the process implies that it does not adequately describe phenomena in which there is sufficiently strong interaction between the points. This has sometimes led to the overuse of the point process in mathematical models, and has inspired other point processes, some of which are constructed via the Poisson point process, that seek to capture this interaction. The process is named after French mathematician Sime on Denis Poisson owing to the fact that if a collection of random points in some space forms a Poisson process, then the number points in a region of finite size is directly related to the Poisson distribution, but Poisson never studied the process, which independently arose in several different settings. The process is defined with a single non-negative mathematical object, which, depending on the context, may be a constant, an integrable function or, in more general settings, a Radon measure. If this object is a constant, then the resulting process is called a homogeneous  or stationary  Poisson point process. Otherwise, the parameter depends on its location in the underlying space, which leads to the inhomogeneous or nonhomogeneous Poisson point process. The word point is often omitted, but there are other Poisson processes of objects, which, instead of points, consist of more complicated mathematical objects such as lines and polygons, and such processes can be based on the Poisson point process.
In statistical signal processing, the goal of spectral density estimation (SDE) is to estimate the spectral density (also known as the power spectral density) of a random signal from a sequence of time samples of the signal. Intuitively speaking, the spectral density characterizes the frequency content of the signal. One purpose of estimating the spectral density is to detect any periodicities in the data, by observing peaks at the frequencies corresponding to these periodicities. SDE should be distinguished from the field of frequency estimation, which assumes that a signal is composed of a limited (usually small) number of generating frequencies plus noise and seeks to find the location and intensity of the generated frequencies. SDE makes no assumption on the number of components and seeks to estimate the whole generating spectrum.
In probability theory and statistics, partial correlation measures the degree of association between two random variables, with the effect of a set of controlling random variables removed.
In statistics, Wold's decomposition or the Wold representation theorem (not to be confused with the Wold theorem that is the discrete-time analog of the Wiener Khinchin theorem) named after Herman Wold, says that every covariance-stationary time series  can be written as the sum of two time series, one deterministic and one stochastic. Formally  where:   is the time series being considered,   is an uncorrelated sequence which is the innovation process to the process    that is, a white noise process that is input to the linear filter .   is the possibly infinite vector of moving average weights (coefficients or parameters)   is a deterministic time series, such as one represented by a sine wave.  Note that the moving average coefficients have these properties: Stable, that is square summable  <  Causal (i.e. there are no terms with j < 0) Minimum delay Constant ( independent of t) It is conventional to define  This theorem can be considered as an existence theorem: any stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model. Imagine creating a process that is a moving average but not satisfying these properties 1 4. For example, the coefficients  could define an acausal and non-minimum delay model. Nevertheless the theorem assures the existence of a causal minimum delay moving average that exactly represents this process. How this all works for the case of causality and the minimum delay property is discussed in Scargle (1981), where an extension of the Wold Decomposition is discussed. The usefulness of the Wold Theorem is that it allows the dynamic evolution of a variable  to be approximated by a linear model. If the innovations  are independent, then the linear model is the only possible representation relating the observed value of  to its past evolution. However, when  is merely an uncorrelated but not independent sequence, then the linear model exists but it is not the only representation of the dynamic dependence of the series. In this latter case, it is possible that the linear model may not be very useful, and there would be a nonlinear model relating the observed value of  to its past evolution. However, in practical time series analysis, it is often the case that only linear predictors are considered, partly on the grounds of simplicity, in which case the Wold decomposition is directly relevant. The Wold representation depends on an infinite number of parameters, although in practice they usually decay rapidly. The autoregressive model is an alternative that may have only a few coefficients if the corresponding moving average has many. These two models can be combined into an autoregressive-moving average (ARMA) model, or an autoregressive-integrated-moving average (ARIMA) model if non-stationarity is involved. See Scargle (1981) and references there.
In the theory of stochastic processes, the Karhunen Loe ve theorem (named after Kari Karhunen and Michel Loe ve), also known as the Kosambi Karhunen Loe ve theorem is a representation of a stochastic process as an infinite linear combination of orthogonal functions, analogous to a Fourier series representation of a function on a bounded interval. Stochastic processes given by infinite series of this form were first considered by Damodar Dharmananda Kosambi. There exist many such expansions of a stochastic process: if the process is indexed over [a, b], any orthonormal basis of L2([a, b]) yields an expansion thereof in that form. The importance of the Karhunen Loe ve theorem is that it yields the best such basis in the sense that it minimizes the total mean squared error. In contrast to a Fourier series where the coefficients are fixed numbers and the expansion basis consists of sinusoidal functions (that is, sine and cosine functions), the coefficients in the Karhunen Loe ve theorem are random variables and the expansion basis depends on the process. In fact, the orthogonal basis functions used in this representation are determined by the covariance function of the process. One can think that the Karhunen Loe ve transform adapts to the process in order to produce the best possible basis for its expansion. In the case of a centered stochastic process {Xt}t   [a, b] (centered means E[Xt] = 0 for all t   [a, b]) satisfying a technical continuity condition, Xt admits a decomposition  where Zk are pairwise uncorrelated random variables and the functions ek are continuous real-valued functions on [a, b] that are pairwise orthogonal in L2([a, b]). It is therefore sometimes said that the expansion is bi-orthogonal since the random coefficients Zk are orthogonal in the probability space while the deterministic functions ek are orthogonal in the time domain. The general case of a process Xt that is not centered can be brought back to the case of a centered process by considering Xt   E[Xt] which is a centered process. Moreover, if the process is Gaussian, then the random variables Zk are Gaussian and stochastically independent. This result generalizes the Karhunen Loe ve transform. An important example of a centered real stochastic process on [0, 1] is the Wiener process; the Karhunen Loe ve theorem can be used to provide a canonical orthogonal representation for it. In this case the expansion consists of sinusoidal functions. The above expansion into uncorrelated random variables is also known as the Karhunen Loe ve expansion or Karhunen Loe ve decomposition. The empirical version (i.e., with the coefficients computed from a sample) is known as the Karhunen Loe ve transform (KLT), principal component analysis, proper orthogonal decomposition (POD), Empirical orthogonal functions (a term used in meteorology and geophysics), or the Hotelling transform.
In probability theory and statistics, the index of dispersion, dispersion index, coefficient of dispersion, relative variance, or variance-to-mean ratio (VMR), like the coefficient of variation, is a normalized measure of the dispersion of a probability distribution: it is a measure used to quantify whether a set of observed occurrences are clustered or dispersed compared to a standard statistical model. It is defined as the ratio of the variance  to the mean ,  It is also known as the Fano factor, though this term is sometimes reserved for windowed data (the mean and variance are computed over a subpopulation), where the index of dispersion is used in the special case where the window is infinite. Windowing data is frequently done: the VMR is frequently computed over various intervals in time or small regions in space, which may be called "windows", and the resulting statistic called the Fano factor. It is only defined when the mean  is non-zero, and is generally only used for positive statistics, such as count data or time between events, or where the underlying distribution is assumed to be the exponential distribution or Poisson distribution.
Least-squares spectral analysis (LSSA) is a method of estimating a frequency spectrum, based on a least squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long gapped records; LSSA mitigates such problems. LSSA is also known as the Vani c ek method after Petr Vani c ek, and as the Lomb method (or the Lomb periodogram) and the Lomb Scargle method (or Lomb Scargle periodogram), based on the contributions of Nicholas R. Lomb and, independently, Jeffrey D. Scargle. Closely related methods have been developed by Michael Korenberg and by Scott Chen and David Donoho.
The information bottleneck method is a technique in information theory introduced by Naftali Tishby et al. for finding the best tradeoff between accuracy and complexity (compression) when summarizing (e.g. clustering) a random variable X, given a joint probability distribution between X and an observed relevant variable Y. Other applications include distributional clustering, and dimension reduction. In a well defined sense it generalized the classical notion of minimal sufficient statistics from parametric statistics to arbitrary distributions, not necessarily of exponential form. It does so by relaxing the sufficiency condition to capture some fraction of the mutual information with the relevant variable Y. The compressed variable is  and the algorithm minimizes the following quantity  where  are the mutual information between  and  respectively, and  is a Lagrange multiplier.
In the comparison of various statistical procedures, efficiency is a measure of the optimality of an estimator, of an experimental design, or of a hypothesis testing procedure. Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance. This article primarily deals with efficiency of estimators. The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional "best possible" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure. Efficiencies are often defined using the variance or mean square error as the measure of desirability.
A correlation function is a statistical correlation between random variables at two different points in space or time, usually as a function of the spatial or temporal distance between the points. If one considers the correlation function between random variables representing the same quantity measured at two different points then this is often referred to as an autocorrelation function being made up of autocorrelations. Correlation functions of different random variables are sometimes called cross correlation functions to emphasise that different variables are being considered and because they are made up of cross correlations. Correlation functions are a useful indicator of dependencies as a function of distance in time or space, and they can be used to assess the distance required between sample points for the values to be effectively uncorrelated. In addition, they can form the basis of rules for interpolating values at points for which there are no observations. Correlation functions used in astronomy, financial analysis, and statistical mechanics differ only in the particular stochastic processes they are applied to. In quantum field theory there are correlation functions over quantum distributions.
Matching is a statistical technique which is used to evaluate the effect of a treatment by comparing the treated and the non-treated units in an observational study or quasi-experiment (i.e. when the treatment is not randomly assigned). The goal of matching is, for every treated unit, to find one (or more) non-treated unit(s) with similar observable characteristics against whom the effect of the treatment can be assessed. By matching treated units to similar non-treated units, matching enables a comparison of outcomes among treated and non-treated units to estimate the effect of the treatment reducing bias due to confounding. Propensity score matching, an early matching technique, was developed as part of the Rubin causal model. Matching has been promoted by Donald Rubin. It was prominently criticized in economics by LaLonde (1986), who compared estimates of treatment effects from an experiment to comparable estimates produced with matching methods and showed that matching methods are biased. Dehejia and Wahba (1999) reevaluted LaLonde's critique and show that matching is a good solution. Similar critiques have been raised in political science and sociology journals.
The polytomous Rasch model is generalization of the dichotomous Rasch model. It is a measurement model that has potential application in any context in which the objective is to measure a trait or ability through a process in which responses to items are scored with successive integers. For example, the model is applicable to the use of Likert scales, rating scales, and to educational assessment items for which successively higher integer scores are intended to indicate increasing levels of competence or attainment.
In statistics, the method of estimating equations is a way of specifying how the parameters of a statistical model should be estimated. This can be thought of as a generalisation of many classical methods --- the method of moments, least squares, and maximum likelihood --- as well as some recent methods like M-estimators. The basis of the method is to have, or to find, a set of simultaneous equations involving both the sample data and the unknown model parameters which are to be solved in order to define the estimates of the parameters. Various components of the equations are defined in terms of the set of observed data on which the estimates are to be based. Important examples of estimating equations are the likelihood equations.
In statistics, the reference class problem is the problem of deciding what class to use when calculating the probability applicable to a particular case. For example, to estimate the probability of an aircraft crashing, one might use the frequency of crashes of all aircraft, of this make of aircraft, of aircraft flown by this company in the last ten years, etc. Any case is a member of very many classes, in which the frequency of the attribute of interest (such as crashing) differs, and the reference class problem discusses which is the most appropriate to use. More formally, many arguments in statistics take the form of a statistical syllogism:  proportion of  are   is an  Therefore, the chance that  is a  is   is called the "reference class" and  is the "attribute class" and  is the individual object. How is one to choose an appropriate class   In Bayesian statistics, the problem arises at that of deciding on a prior probability for the outcome in question (or when considering multiple outcomes, a prior probability distribution).
In the analysis of data, a correlogram is an image of correlation statistics. For example, in time series analysis, a correlogram, also known as an autocorrelation plot, is a plot of the sample autocorrelations  versus  (the time lags). If cross-correlation is used, the result is called a cross-correlogram. The correlogram is a commonly used tool for checking randomness in a data set. This randomness is ascertained by computing autocorrelations for data values at varying time lags. If random, such autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero. In addition, correlograms are used in the model identification stage for Box Jenkins autoregressive moving average time series models. Autocorrelations should be near-zero for randomness; if the analyst does not check for randomness, then the validity of many of the statistical conclusions becomes suspect. The correlogram is an excellent way of checking for such randomness. Sometimes, corrgrams, color-mapped matrices of correlation strengths in multivariate analysis, are also called correlograms.
A Barber Johnson diagram is a method of presenting hospital statistics combining four different variables in a unique graph, introduced in 1973. The method constructs a scattergram where length of stay, turnover interval, discharges, and deaths per available bed are combined. These four variables have a common relationship between them and their combination in the diagram permitted a new improved way for analyzing efficiency and performance of the hospital sector. The most complete reference about how to construct the diagram could be found in Yates. In this book, the appendix explains in detail the way for elaborating this kind of diagram.
Parallel coordinates is a common way of visualizing high-dimensional geometry and analyzing multivariate data. To show a set of points in an n-dimensional space, a backdrop is drawn consisting of n parallel lines, typically vertical and equally spaced. A point in n-dimensional space is represented as a polyline with vertices on the parallel axes; the position of the vertex on the i-th axis corresponds to the i-th coordinate of the point. This visualization is closely related to time series visualization, except that it is applied to data where the axes do not correspond to points in time, and therefore do not have a natural order. Therefore, different axis arrangements may be of interest.
In linear algebra and statistics, the pseudo-determinant is the product of all non-zero eigenvalues of a square matrix. It coincides with the regular determinant when the matrix is non-singular.
A Gompertz curve or Gompertz function, named after Benjamin Gompertz, is a sigmoid function. It is a type of mathematical model for a time series, where growth is slowest at the start and end of a time period. The right-hand or future value asymptote of the function is approached much more gradually by the curve than the left-hand or lower valued asymptote, in contrast to the simple logistic function in which both asymptotes are approached by the curve symmetrically. It is a special case of the generalised logistic function.
A scientific control is an experiment or observation designed to minimize the effects of variables other than the independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. An example of a scientific control (sometimes called an "experimental control") might be testing plant fertilizer by giving it to only half the plants in a garden: the plants that receive no fertilizer are the control group, because they establish the baseline level of growth that the fertilizer-treated plants will be compared against. Without a control group, the experiment cannot determine whether the fertilizer-treated plants grow more than they would have if untreated. Ideally, all variables in an experiment will be controlled (accounted for by the control measurements) and none will be uncontrolled. In such an experiment, if all the controls work as expected, it is possible to conclude that the experiment is working as intended and that the results of the experiment are due to the effect of the variable being tested. That is, scientific controls allow an investigator to make a claim like "Two situations were identical until factor X occurred. Since factor X is the only difference between the two situations, the new outcome was caused by factor X."
In probability theory, the multinomial distribution is a generalization of the binomial distribution. For example it models the probability of counts for rolling a k sided dice n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories. When n is 1 and k is 2 the multinomial distribution is the Bernoulli distribution. When k is 2 and number of trials are more than 1 it is the Binomial distribution. When n is 1 it is the categorical distribution. The Bernoulli distribution is the probability distribution of whether a Bernoulli trial is a success. In other words it models the number of heads from flipping a coin one time. The binomial distribution generalizes this to the number of heads from doing n independent flips of the same coin. For the multinomial distribution the analog to the Bernoulli Distribution is the categorical distribution. Instead of flipping one coin, the categorical distribution models the roll of one k sided die. So the multinomial distribution can model n independent rolls of a k sided die. Let k be a fixed finite number. Mathematically, we have k possible mutually exclusive outcomes, with corresponding probabilities p1, ..., pk, and n independent trials. Note that since the k outcomes are mutually exclusive and one must occur we have pi   0 for i = 1, ..., k and . Then if the random variables Xi indicate the number of times outcome number i is observed over the n trials, the vector X = (X1, ..., Xk) follows a multinomial distribution with parameters n and p, where p = (p1, ..., pk). While the trials are independent, their outcomes X are dependent because they must be summed to n. Note that, in some fields, such as natural language processing, the categorical and multinomial distributions are conflated, and it is common to speak of a "multinomial distribution" when a categorical distribution is actually meant. This stems from the fact that it is sometimes convenient to express the outcome of a categorical distribution as a "1-of-K" vector (a vector with one element containing a 1 and all other elements containing a 0) rather than as an integer in the range ; in this form, a categorical distribution is equivalent to a multinomial distribution over a single trial.
This is a list of important publications in statistics, organized by field. Some reasons why a particular publication might be regarded as important: Topic creator   A publication that created a new topic Breakthrough   A publication that changed scientific knowledge significantly Influence   A publication which has significantly influenced the world or has had a massive impact on the teaching of statistics.  
Support curve is a statistical term, coined by A. W. F. Edwards, to describe the graph of the natural logarithm of the likelihood function. The function being plotted is used in the computation of the score and Fisher information, and the graph has a direct interpretation in the context of maximum likelihood estimation and likelihood-ratio tests. The term refers to the hypotheses being tested, i.e. whether or not the data support one hypothesis (or parameter value) more than any other.
In mathematics, Tucker decomposition decomposes a tensor into a set of matrices and one small core tensor. It is named after Ledyard R. Tucker although it goes back to Hitchcock in 1927. Initially described as a three-mode extension of factor analysis and principal component analysis it may actually be generalized to higher mode analysis. It may be regarded as a more flexible PARAFAC (parallel factor analysis ) model. In PARAFAC the core tensor is restricted to be "diagonal".
A ratio distribution (or quotient distribution) is a probability distribution constructed as the distribution of the ratio of random variables having two other known distributions. Given two (usually independent) random variables X and Y, the distribution of the random variable Z that is formed as the ratio  is a ratio distribution. The Cauchy distribution is an example of a ratio distribution. The random variable associated with this distribution comes about as the ratio of two Gaussian (normal) distributed variables with zero mean. Thus the Cauchy distribution is also called the normal ratio distribution. A number of researchers have considered more general ratio distributions. Two distributions often used in test-statistics, the t-distribution and the F-distribution, are also ratio distributions: The t-distributed random variable is the ratio of a Gaussian random variable divided by an independent chi-distributed random variable (i.e., the square root of a chi-squared distribution), while the F-distributed random variable is the ratio of two independent chi-squared distributed random variables. Often the ratio distributions are heavy-tailed, and it may be difficult to work with such distributions and develop an associated statistical test. A method based on the median has been suggested as a "work-around".
In statistics, the two-way analysis of variance (ANOVA) is an extension of the one-way ANOVA that examines the influence of two different categorical independent variables on one continuous dependent variable. The two-way ANOVA not only aims at assessing the main effect of each independent variable but also if there is any interaction between them.
In probability theory, Le vy s continuity theorem, named after the French mathematician Paul Le vy, connects convergence in distribution of the sequence of random variables with pointwise convergence of their characteristic functions. An alternative name sometimes used is Le vy s convergence theorem. This theorem is the basis for one approach to prove the central limit theorem and it is one of the major theorems concerning characteristic functions.
Fisher's exact test is a statistical significance test used in the analysis of contingency tables. Although in practice it is employed when sample sizes are small, it is valid for all sample sizes. It is named after its inventor, Ronald Fisher, and is one of a class of exact tests, so called because the significance of the deviation from a null hypothesis (e.g., P-value) can be calculated exactly, rather than relying on an approximation that becomes exact in the limit as the sample size grows to infinity, as with many statistical tests. Fisher is said to have devised the test following a comment from Dr. Muriel Bristol, who claimed to be able to detect whether the tea or the milk was added first to her cup. He tested her claim in the  lady tasting tea  experiment.
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics and data compression. Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties. Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek         "grape") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning, since they use the same terms and often the same algorithms, but have different goals. Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.
In statistics, a central tendency (or, more commonly, a measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s. The most common measures of central tendency are the arithmetic mean, the median and the mode. A central tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote "the tendency of quantitative data to cluster around some central value."  The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysts may judge whether data has a strong or a weak central tendency based on its dispersion.
In statistics, multiple correspondence analysis (MCA) is a data analysis technique for nominal categorical data, used to detect and represent underlying structures in a data set. It does this by representing data as points in a low-dimensional Euclidean space. The procedure thus appears to be the counterpart of principal component analysis for categorical data. MCA can be viewed as an extension of simple correspondence analysis (CA) in that it is applicable to a large set of categorical variables.
In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled. Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM). As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired (typically by thinning the resulting chain of samples by only taking every nth value, e.g. every 100th value). In addition, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution.
In probability theory, the central limit theorem (CLT) states that, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed, regardless of the underlying distribution. To illustrate what this means, suppose that a sample is obtained containing a large number of observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic average of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the computed values of the average will be distributed according to the normal distribution (commonly known as a "bell curve"). A simple example of this is that if one flips a coin many times, the probability of getting a given number of heads should follow a normal curve, with mean equal to half the total number of flips. The central limit theorem has a number of variants. In its common form, the random variables must be identically distributed. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, given that they comply with certain conditions. In more general usage, a central limit theorem is any of a set of weak-convergence theorems in probability theory. They all express the fact that a sum of many independent and identically distributed (i.i.d.) random variables, or alternatively, random variables with specific types of dependence, will tend to be distributed according to one of a small set of attractor distributions. When the variance of the i.i.d. variables is finite, the attractor distribution is the normal distribution. In contrast, the sum of a number of i.i.d. random variables with power law tail distributions decreasing as |x|   1 where 0 <   < 2 (and therefore having infinite variance) will tend to an alpha-stable distribution with stability parameter (or index of stability) of   as the number of variables grows.
In probability theory, the complement of any event A is the event [not A], i.e. the event that A does not occur. The event A and its complement [not A] are mutually exclusive and exhaustive. Generally, there is only one event B such that A and B are both mutually exclusive and exhaustive; that event is the complement of A. The complement of an event A is usually denoted as A , Ac or A. Given an event, the event and its complementary event define a Bernoulli trial: did the event occur or not  For example, if a typical coin is tossed and one assumes that it cannot land on its edge, then it can either land showing "heads" or "tails." Because these two outcomes are mutually exclusive (i.e. the coin cannot simultaneously show both heads and tails) and collectively exhaustive (i.e. there are no other possible outcomes not represented between these two), they are therefore each other's complements. This means that [heads] is logically equivalent to [not tails], and [tails] is equivalent to [not heads].
In sampling theory, sampling fraction is the ratio of sample size to population size or, in the context of stratified sampling, the ratio of the sample size to the size of the stratum. The formula for the sampling fraction is  where n is the sample size and N is the population size. If the sampling fraction is less than 5% or 0.05, then the finite population multiplier might be ignored.
Epilogism is a style of Inference invented by the ancient Empiric school of medicine. It is a theory-free method of looking at history by accumulating fact with minimal generalization and being conscious of the side effects of making causal claims. Epilogism is an inference which moves entirely within the domain of visible and evident things, it tries not to invoke unobservables. It is tightly knit to the famous "tripos of medicine". See also Doctrines of the Empiric school. See also Causal inference.
This is a list of many lists of countries and territories by various definitions, including FIFA countries, federations and fictional countries. A country or territory is a geographical area, either in the sense of nation (a cultural entity) and/or state (a political entity).
SYSTAT is a statistics and statistical graphics software package, developed by Leland Wilkinson in the late 1970s, who was at the time an assistant professor of psychology at the University of Illinois at Chicago. Systat was incorporated in 1983 and grew to over 50 employees. In 1995 SYSTAT was sold to SPSS Inc., who marketed the product to a scientific audience under the SPSS Science division. By 2002, SPSS had changed its focus to business analytics and decided to sell SYSTAT to Cranes Software in Bangalore, India. Cranes formed Systat Software, Inc. to market and distribute SYSTAT in the US, and a number of other divisions for global distribution. The headquarters are in Chicago, Illinois. By 2005, SYSTAT was in its eleventh version having a revamped codebase completely changed from Fortran into C++. Version 13 came out in 2009, with improvements in the user interface and several new features.
The variance-gamma distribution, generalized Laplace distribution or Bessel function distribution is a continuous probability distribution that is defined as the normal variance-mean mixture where the mixing density is the gamma distribution. The tails of the distribution decrease more slowly than the normal distribution. It is therefore suitable to model phenomena where numerically large values are more probable than is the case for the normal distribution. Examples are returns from financial assets and turbulent wind speeds. The distribution was introduced in the financial literature by Madan and Seneta. The variance-gamma distributions form a subclass of the generalised hyperbolic distributions. The fact that there is a simple expression for the moment generating function implies that simple expressions for all moments are available. The class of variance-gamma distributions is closed under convolution in the following sense. If  and  are independent random variables that are variance-gamma distributed with the same values of the parameters  and , but possibly different values of the other parameters, ,  and  , respectively, then  is variance-gamma distributed with parameters , ,  and . The variance-gamma distribution can also be expressed in terms of three inputs parameters (C,G,M) denoted after the initials of its founders. If the "C",  here, parameter is integer then the distribution has a closed form 2-EPT distribution. See 2-EPT Probability Density Function. Under this restriction closed form option prices can be derived. See also Variance gamma process.
In decision tree learning, Information gain ratio is a ratio of information gain to the intrinsic information. It is used to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.
The power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H1) when it is true that is, the ability of a test to detect an effect, if the effect actually exists. That is,  The power of a test sometimes, less formally, refers to the probability of rejecting the null when it is not correct, though this is not the formal definition stated above. The power is in general a function of the possible distributions, often determined by a parameter, under the alternative hypothesis. As the power increases, there are decreasing chances of a Type II error (false negative), which are also referred to as the false negative rate ( ) since the power is equal to 1  , again, under the alternative hypothesis. A similar concept is Type I error, also referred to as the  false positive rate  or the level of a test under the null hypothesis. Power analysis can be used to calculate the minimum sample size required so that one can be reasonably likely to detect an effect of a given size. For example:  how many times do I need to toss a coin to conclude it is rigged   Power analysis can also be used to calculate the minimum effect size that is likely to be detected in a study using a given sample size. In addition, the concept of power is used to make comparisons between different statistical testing procedures: for example, between a parametric and a nonparametric test of the same hypothesis.
This is a list of scientific journals published in the field of statistics.
The Sargan Hansen test or Sargan's  test is a statistical test used for testing over-identifying restrictions in a statistical model. It was proposed by John Denis Sargan in 1958, and several variants were derived by him in 1975. Lars Peter Hansen re-worked through the derivations and showed that it can be extended to general non-linear GMM in a time series context. The Sargan test is based on the assumption that model parameters are identified via a priori restrictions on the coefficients, and tests the validity of over-identifying restrictions. The test statistic can be computed from residuals from instrumental variables regression by constructing a quadratic form based on the cross-product of the residuals and exogenous variables. Under the null hypothesis that the over-identifying restrictions are valid, the statistic is asymptotically distributed as a chi-square variable with  degrees of freedom (where  is the number of instruments and  is the number of endogenous variables). This version of the Sargan statistic was developed for models estimated using instrumental variables from ordinary time series or cross-sectional data. When longitudinal ("panel data") data are available, it is possible to extend such statistics for testing exogeneity hypotheses for subsets of explanatory variables. Testing of over-identifying assumptions is less important in longitudinal applications because realizations of time varying explanatory variables in different time periods are potential instruments, i.e., over-identifying restrictions are automatically built into models estimated using longitudinal data.
In probability theory, the Girsanov theorem (named after Igor Vladimirovich Girsanov) describes how the dynamics of stochastic processes change when the original measure is changed to an equivalent probability measure. The theorem is especially important in the theory of financial mathematics as it tells how to convert from the physical measure which describes the probability that an underlying instrument (such as a share price or interest rate) will take a particular value or values to the risk-neutral measure which is a very useful tool for pricing derivatives on the underlying.
Bills of mortality were the weekly mortality statistics in London, designed to monitor burials from 1592 to 1595 and then continuously from 1603. The responsibility to produce the statistics was chartered in 1611 to the Worshipful Company of Parish Clerks. The bills covered an area that started to expand as London grew from the City of London, before reaching its maximum extent in 1636. New parishes were then only added where ancient parishes within the area were divided. Factors such as the use of suburban cemeteries outside the area, the exemption of extra-parochial places within the area, the wider growth of the metropolis, and that they recorded burials rather than deaths, made their data invalid. Production of the bills went into decline from 1819 as parishes ceased to provide returns, with the last surviving weekly bill dating from 1858. They were superseded by the weekly returns of the Registrar General from 1840, taking in further parishes until 1847. This area became the district of the Metropolitan Board of Works in 1855, the County of London in 1889 and Inner London in 1965.
In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process. It is named after the Russian mathematician Andrey Markov. A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process. The term strong Markov property is similar to the Markov property, except that the meaning of "present" is defined in terms of a random variable known as a stopping time. Both the terms "Markov property" and "strong Markov property" have been used in connection with a particular "memoryless" property of the exponential distribution. The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model. A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model. A discrete-time stochastic process satisfying the Markov property is known as a Markov chain.  
In statistics, the standard deviation (SD, also represented by the Greek letter sigma   or s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values. A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. The standard deviation of a random variable, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same units as the data. There are also other measures of deviation from the norm, including mean absolute deviation, which provide different mathematical properties from standard deviation. In addition to expressing the variability of a population, the standard deviation is commonly used to measure confidence in statistical conclusions. For example, the margin of error in polling data is determined by calculating the expected standard deviation in the results if the same poll were to be conducted multiple times. This derivation of a standard deviation is often called the "standard error" of the estimate or "standard error of the mean" when referring to a mean. It is computed as the standard deviation of all the means that would be computed from that population if an infinite number of samples were drawn and a mean for each sample were computed. It is very important to note that the standard deviation of a population and the standard error of a statistic derived from that population (such as the mean) are quite different but related (related by the inverse of the square root of the number of observations). The reported margin of error of a poll is computed from the standard error of the mean (or alternatively from the product of the standard deviation of the population and the inverse of the square root of the sample size, which is the same thing) and is typically about twice the standard deviation the half-width of a 95 percent confidence interval. In science, researchers commonly report the standard deviation of experimental data, and only effects that fall much farther than two standard deviations away from what would have been expected are considered statistically significant normal random error or variation in the measurements is in this way distinguished from causal variation. The standard deviation is also important in finance, where the standard deviation on the rate of return on an investment is a measure of the volatility of the investment. When only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data or to a modified quantity that is a better estimate of the population standard deviation (the standard deviation of the entire population).
In queueing theory, a discipline within the mathematical theory of probability, the arrival theorem (also referred to as the random observer property, ROP or job observer property) states that "upon arrival at a station, a job observes the system as if in steady state at an arbitrary instant for the system without that job." The arrival theorem always holds in open product-form networks with unbounded queues at each node, but it also holds in more general networks. A necessary and sufficient condition for the arrival theorem to be satisfied in product-form networks is given in terms of Palm probabilities in Boucherie & Dijk, 1997. A similar result also holds in some closed networks. Examples of product-form networks where the arrival theorem does not hold include reversible Kingman networks and networks with a delay protocol. Mitrani offers the intuition that "The state of node i as seen by an incoming job has a different distribution from the state seen by a random observer. For instance, an incoming job can never see all 'k jobs present at node i, because it itself cannot be among the jobs already present."
In probability theory, a Markov kernel (or stochastic kernel) is a map that plays the role, in the general theory of Markov processes, that the transition matrix does in the theory of Markov processes with a finite state space.
The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below. This was used to devise betting strategies called martingales. The odds-algorithm applies to a class of problems called last-success-problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a "specific event"). This identification must be done at the time of observation. No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of "stopping" to take a well-defined action. Such problems are encountered in several situations.
In probability theory, two events R and B are conditionally independent given a third event Y precisely if the occurrence or non-occurrence of R and the occurrence or non-occurrence of B are independent events in their conditional probability distribution given Y. In other words, R and B are conditionally independent given Y if and only if, given knowledge that Y occurs, knowledge of whether R occurs provides no information on the likelihood of B occurring, and knowledge of whether B occurs provides no information on the likelihood of R occurring.
In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model's parameters.  
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.
In statistics, rankits of a set of data are the expected values of the order statistics of a sample from the standard normal distribution the same size as the data. They are primarily used in the normal probability plot, a graphical technique for normality testing.
In mathematics, quadratic variation is used in the analysis of stochastic processes such as Brownian motion and other martingales. Quadratic variation is just one kind of variation of a process.
In psychometrics, content validity (also known as logical validity) refers to the extent to which a measure represents all facets of a given social construct. For example, a depression scale may lack content validity if it only assesses the affective dimension of depression but fails to take into account the behavioral dimension. An element of subjectivity exists in relation to determining content validity, which requires a degree of agreement about what a particular personality trait such as extraversion represents. A disagreement about a personality trait will prevent the gain of a high content validity.  
Simpson's paradox, or the Yule Simpson effect, is a paradox in probability and statistics, in which a trend appears in different groups of data but disappears or reverses when these groups are combined. It is sometimes given the impersonal title reversal paradox or amalgamation paradox. This result is often encountered in social-science and medical-science statistics, and is particularly confounding when frequency data is unduly given causal interpretations. The paradoxical elements disappear when causal relations are brought into consideration. Many statisticians believe that the mainstream public should be informed of the counter-intuitive results in statistics such as Simpson's paradox. Edward H. Simpson first described this phenomenon in a technical paper in 1951, but the statisticians Karl Pearson, et al., in 1899, and Udny Yule, in 1903, had mentioned similar effects earlier. The name Simpson's paradox was introduced by Colin R. Blyth in 1972.  
In statistics, Crame r's V (sometimes referred to as Crame r's phi and denoted as  c) is a measure of association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson's chi-squared statistic and was published by Harald Crame r in 1946.
Events are often triggered when a stochastic or random process first encounters a threshold. The threshold can be a barrier, boundary or specified state of a system. The amount of time required for a stochastic process, starting from some initial state, to encounter a threshold for the first time is referred to variously as a first hitting time. In statistics, first-hitting-time models are a sub-class of survival models. The first hitting time, also called first passage time, of the barrier set  with respect to an instance of a stochastic process is the time until the stochastic process first enters . More colloquially, a first passage time in a stochastic system, is the time taken for a state variable to reach a certain value. Understanding this metric allows one to further understand the physical system under observation, and as such has been the topic of research in very diverse fields, from Economics to Ecology. The idea that a first hitting time of a stochastic process might describe the time to occurrence of an event has a long history, starting with an interest in the first passage time of Wiener diffusion processes in economics and then in physics in the early 1900s. Modeling the probability of financial ruin as a first passage time was an early application in the field of insurance. An interest in the mathematical properties of first-hitting-times and statistical models and methods for analysis of survival data appeared steadily between the middle and end of the 20th century.
In the theory of stochastic processes in probability theory and statistics, a nuisance variable is a random variable that is fundamental to the probabilistic model, but that is of no particular interest in itself or is no longer of interest: one such usage arises for the Chapman Kolmogorov equation. For example, a model for a stochastic process may be defined conceptually using intermediate variables that are not observed in practice. If the problem is to derive the theoretical properties, such as the mean, variance and covariances of quantities that would be observed, then the intermediate variables are nuisance variables. The related term nuisance factor has been used in the context of block experiments, where the terms in the model representing block-means, often called "factors", are of no interest. Many approaches to the analysis of such experiments, particularly where the experimental design is subject to randomization, treat these factors as random variables. More recently, "nuisance variable" has been used in the same context. "Nuisance variable" has been used in the context of statistical surveys to refer information that is not of direct interest but which needs to be taken into account in an analysis. In the context of stochastic models, the treatment of nuisance variables does not necessarily involve working with the full joint distribution of all the random variables involved, although this is one approach. Instead, an analysis may proceed directly to the quantities of interest. The term nuisance variable is sometimes also used in more general contexts, simply to designate those variables that are marginalised over when finding a marginal distribution. In particular, the term may sometimes be used in the context of Bayesian analysis as an alternative to nuisance parameter, given that Bayesian statistics allows parameters to be treated as having probability distributions. However this is usually avoided as the term nuisance parameter has a specific meaning in statistical theory.
In statistics, the multivariate t-distribution (or multivariate Student distribution) is a multivariate probability distribution. It is a generalization to random vectors of the Student's t-distribution, which is a distribution applicable to univariate random variables. While the case of a random matrix could be treated within this structure, the matrix t-distribution is distinct and makes particular use of the matrix structure.
Location estimation in wireless sensor networks is the problem of estimating the location of an object from a set of noisy measurements. These measurements are acquired in a distributed manner by a set of sensors.
In statistics, quality assurance, and survey methodology, sampling is concerned with the selection of a subset of individuals from within a statistical population to estimate characteristics of the whole population. Each observation measures one or more properties (such as weight, location, color) of observable bodies distinguished as independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. The sampling process comprises several stages: Defining the population of concern Specifying a sampling frame, a set of items or events possible to measure Specifying a sampling method for selecting items or events from the frame Determining the sample size Implementing the sampling plan Sampling and data collecting Data which can be selected
The conditionality principle is a Fisherian principle of statistical inference that Allan Birnbaum formally defined and studied in his 1962 JASA article. Informally, the conditionality principle can be taken as the claim that experiments which were not actually performed are statistically irrelevant. Together with the sufficiency principle, Birnbaum's version of the principle implies the famous likelihood principle. Although the relevance of the proof to data analysis remains controversial among statisticians, many Bayesians and likelihoodists consider the likelihood principle foundational for statistical inference.
FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyva rinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.
In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation. Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the "kernel trick". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors. Algorithms capable of operating with kernels include the kernel perceptron, support vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others. Any linear model can be turned into a non-linear model by applying the kernel trick to the model: replacing its features (predictors) by a kernel function. Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity).
In probability theory, a zero one law is a result that states that an event must have probability 0 or 1 and no intermediate value. Sometimes, the statement is that the limit of certain probabilities must be 0 or 1. It may refer to: Borel Cantelli lemma Blumenthal's zero one law for Markov processes, Engelbert Schmidt zero one law for continuous, nondecreasing additive functionals of Brownian motion, Hewitt Savage zero one law for exchangeable sequences, Kolmogorov's zero one law for the tail  -algebra, Le vy's zero one law, related to martingale convergence. topological zero one law related to meager sets
In statistics, Fieller's theorem allows the calculation of a confidence interval for the ratio of two means.
Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if we exactly knew the speed, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense. Many problems in the natural sciences and engineering are also rife with sources of uncertainty. Computer experiments on computer simulations are the most common approach to study problems in uncertainty quantification.
In statistics, the Breusch Pagan test, developed in 1979 by Trevor Breusch and Adrian Pagan, is used to test for heteroskedasticity in a linear regression model. It was independently suggested with some extension by R. Dennis Cook and Sanford Weisberg in 1983. It tests whether the estimated variance of the residuals from a regression are dependent on the values of the independent variables. In that case, heteroskedasticity is present. Suppose that we estimate the regression model  and obtain from this fitted model a set of values for , the residuals. Ordinary least squares constrains these so that their mean is 0 and so, given the assumption that their variance does not depend on the independent variables, an estimate of this variance can be obtained from the average of the squared values of the residuals. If the assumption is not held to be true, a simple model might be that the variance is linearly related to independent variables. Such a model can be examined by regressing the squared residuals on the independent variables, using an auxiliary regression equation of the form  This is the basis of the Breusch Pagan test. If an F-test confirms that the independent variables are jointly significant then the null hypothesis of homoskedasticity can be rejected. The Breusch Pagan test tests for conditional heteroskedasticity. It is a chi-squared test: the test statistic is n 2 with k degrees of freedom. It tests the null hypothesis of homoskedasticity. If the Chi Squared value is significant with p-value below an appropriate threshold (e.g. p<0.05) then the null hypothesis of homoskedasticity is rejected and heteroskedasticity assumed. If the Breusch Pagan test shows that there is conditional heteroskedasticity, the original regression can be corrected by using the Hansen method, using robust standard errors, or re-thinking the regression equation by changing and/or transforming independent variables.
A time-use survey is a statistical survey which aims to report data on how, on average, people spend their time.
In probability theory, reflected Brownian motion (or regulated Brownian motion, both with the acronym RBM) is a Wiener process in a space with reflecting boundaries. RBMs have been shown to describe queueing models experiencing heavy traffic as first proposed by Kingman and proven by Iglehart and Whitt.
In mathematics, the Hausdorff moment problem, named after Felix Hausdorff, asks for necessary and sufficient conditions that a given sequence { mn : n = 0, 1, 2, ... } be the sequence of moments  of some Borel measure   supported on the closed unit interval [0, 1]. In the case m0 = 1, this is equivalent to the existence of a random variable X supported on [0, 1], such that E Xn = mn. The essential difference between this and other well-known moment problems is that this is on a bounded interval, whereas in the Stieltjes moment problem one considers a half-line [0,  ), and in the Hamburger moment problem one considers the whole line (  ,  ). The Stieltjes moment problems and the Hamburger moment problems, if they are solvable, may have infinitely many solutions (indeterminate moment problem) whereas a Hausdorff moment problem always has a unique solution if it is solvable (determinate moment problem). In the indeterminate moment problem case, there are infinitely measures correspond to the same prescribed moments and they consist of a convex set. The set of polynomials may or may not be dense in the associated Hilbert spaces if the moment problem is indeterminate, and it depends on whether measure is extremal or not. But in the determinate moment problem case, the set of polynomials are dense in the associated Hilbert space. In 1921, Hausdorff showed that { mn : n = 0, 1, 2, ... } is such a moment sequence if and only if the sequence is completely monotonic, i.e., its difference sequences satisfy the equation  for all n,k   0. Here,   is the difference operator given by  The necessity of this condition is easily seen by the identity  which is   0, being the integral of an almost sure non-negative function. For example, it is necessary to have
In probability theory, two sequences of probability measures are said to be contiguous if asymptotically they share the same support. Thus the notion of contiguity extends the concept of absolute continuity to the sequences of measures. The concept was originally introduced by Le Cam (1960) as part of his contribution to the development of abstract general asymptotic theory in mathematical statistics. Le Cam was instrumental during the period in the development of abstract general asymptotic theory in mathematical statistics. He is best known for the general concepts of local asymptotic normality and contiguity.
Markov chain geostatistics refer to the Markov chain spatial models, simulation algorithms and associated spatial correlation measures (e.g., transiogram) based on the Markov chain random field theory, which extends a single Markov chain into a multi-dimensional random field for geostatistical modeling. A Markov chain random field is still a single spatial Markov chain. The spatial Markov chain moves or jumps in a space and decides its state at any unobserved location through interactions with its nearest known neighbors in different directions. The data interaction process can be well explained as a local sequential Bayesian updating process within a neighborhood. Because single-step transition probability matrices are difficult to estimate from sparse sample data and are impractical in representing the complex spatial heterogeneity of states, the transiogram, which is defined as a transition probability function over the distance lag, is proposed as the accompanying spatial measure of Markov chain random fields.
In statistics, an association is any relationship between two measured quantities that renders them statistically dependent. The term "association" is closely related to the term "correlation." Both terms imply that two or more variables vary according to some pattern. However, correlation is more rigidly defined by some correlation coefficient which measures the degree to which the association of the variables tends to a certain pattern. Sometimes the pattern of association is a simple linear relationship (as in the case of the popular Pearson product moment correlation coefficient (commonly called simply "the correlation coefficient"), although other forms of correlation are better suited to non-linear associations.
In the theory of finite population sampling, a sampling design specifies for every possible sample its probability of being drawn.
In probability theory and statistics, the Laplace distribution is a continuous probability distribution named after Pierre-Simon Laplace. It is also sometimes called the double exponential distribution, because it can be thought of as two exponential distributions (with an additional location parameter) spliced together back-to-back, although the term 'double exponential distribution' is also sometimes used to refer to the Gumbel distribution. The difference between two independent identically distributed exponential random variables is governed by a Laplace distribution, as is a Brownian motion evaluated at an exponentially distributed random time. Increments of Laplace motion or a variance gamma process evaluated over the time scale also have a Laplace distribution.
In statistics, count data is a statistical data type, a type of data in which the observations can take only the non-negative integer values {0, 1, 2, 3, ...}, and where these integers arise from counting rather than ranking. The statistical treatment of count data is distinct from that of binary data, in which the observations can take only two values, usually represented by 0 and 1, and from ordinal data, which may also consist of integers but where the individual values fall on an arbitrary scale and only the relative ranking is important. Statistical analyses involving count data can take several forms depending on the context in which the data arise.  simple counts, such as the number of occurrences of thunderstorms in a calendar year, observed for several years. categorical data in which the counts represent the numbers of items falling into each of several categories.  The latter are treated separately as different methodologies apply, and the following applies to simple counts.
Now discredited, Meadow's Law was a precept much in use until recently in the field of child protection, specifically by those investigating cases of multiple cot or crib death   SIDS   within a single family.
In statistics, Goodman and Kruskal's gamma is a measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level. It makes no adjustment for either table size or ties. Values range from  1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association. This statistic (which is distinct from Goodman and Kruskal's lambda) is named after Leo Goodman and William Kruskal, who proposed it in a series of papers from 1954 to 1972.
A hidden semi-Markov model (HSMM) is a statistical model with the same structure as a hidden Markov model except that the unobservable process is semi-Markov rather than Markov. This means that the probability of there being a change in the hidden state depends on the amount of time that has elapsed since entry into the current state. This is in contrast to hidden Markov models where there is a constant probability of changing state given survival in the state up to that time. For instance Sanson & Thomson (2001) modelled daily rainfall using a hidden semi-Markov model. If the underlying process (e.g. weather system) does not have a geometrically distributed duration, an HSMM may be more appropriate. The model was first published by Leonard E. Baum and Ted Petrie in 1966. Statistical inference for hidden semi-Markov models is more difficult than in hidden Markov models, since algorithms like the Baum-Welch algorithm are not directly applicable, and must be adapted requiring more resources.
Stein's example is an important result in decision theory which can be stated as The ordinary decision rule for estimating the mean of a multivariate Gaussian distribution is inadmissible under mean squared error risk in dimension at least 3. The following is an outline of its proof. The reader is referred to the main article for more information.
A time-varying covariate (also called time-dependent covariate) is a term used in statistics, particularly in survival analyses. It reflects the phenomenon that a covariate is not necessarily constant through the whole study. For instance, if one wishes to examine the link between area of residence and cancer, this would be complicated by the fact that study subjects move from one area to another. The area of residency could then be introduced in the statistical model as a time-varying covariate. In survival analysis, this would be done by splitting each study subject into several observations, one for each area of residence. For example, if a person is born at time 0 in area A, moves to area B at time 5, and is diagnosed with cancer at time 8, two observations would be made. One with a length of 5 (5   0) in area A, and one with a length of 3 (8   5) in area B.
Maximum likelihood sequence estimation (MLSE) is a mathematical algorithm to extract useful data out of a noisy data stream.
The (one-dimensional) Holtsmark distribution is a continuous probability distribution. The Holtsmark distribution is a special case of a stable distribution with the index of stability or shape parameter  equal to 3/2 and skewness parameter  of zero. Since  equals zero, the distribution is symmetric, and thus an example of a symmetric alpha-stable distribution. The Holtsmark distribution is one of the few examples of a stable distribution for which a closed form expression of the probability density function is known. However, its probability density function is not expressible in terms of elementary functions; rather, the probability density function is expressed in terms of hypergeometric functions. The Holtsmark distribution has applications in plasma physics and astrophysics. In 1919, Norwegian physicist J. Holtsmark proposed the distribution as a model for the fluctuating fields in plasma due to chaotic motion of charged particles. It is also applicable to other types of Coulomb forces, in particular to modeling of gravitating bodies, and thus is important in astrophysics.
Descriptive statistics is the discipline of quantitatively describing the main features of a collection of information, or the quantitative description itself. Descriptive statistics are distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, are not developed on the basis of probability theory. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example in a paper reporting on a study involving human subjects, there typically appears a table giving the overall sample size, sample sizes in important subgroups (e.g., for each treatment or exposure group), and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, and the proportion of subjects with related comorbidities. Some measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness.
Semantic mapping (SM) is a method in statistics for dimensionality reduction that can be used in a set of multidimensional vectors of features to extract a few new features that preserves the main data characteristics. SM performs dimensionality reduction by clustering the original features in semantic clusters and combining features mapped in the same cluster to generate an extracted feature. Given a data set, this method constructs a projection matrix that can be used to map a data element from a high-dimensional space into a reduced dimensional space. SM can be applied in construction of text mining and information retrieval systems, as well as systems managing vectors of high dimensionality. SM is an alternative to random mapping, principal components analysis and latent semantic indexing methods.  
In statistics, when performing multiple comparisons, the term false positive ratio, also known as the false alarm ratio, usually refers to the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate (or "false alarm rate") usually refers to the expectancy of the false positive ratio.
Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements. For example, it is desired to estimate the proportion of a population of voters who will vote for a particular candidate. That proportion is the parameter sought; the estimate is based on a small random sample of voters. Or, for example, in radar the goal is to estimate the range of objects (airplanes, boats, etc.) by analyzing the two-way transit timing of received echoes of transmitted pulses. Since the reflected pulses are unavoidably embedded in electrical noise, their measured values are randomly distributed, so that the transit time must be estimated. In estimation theory, two approaches are generally considered.  The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector. For example, in electrical communication theory, the measurements which contain information regarding the parameters of interest are often associated with a noisy signal. Without randomness, or noise, the problem would be deterministic and estimation would not be needed.
In mathematics, the Cheeger bound is a bound of the second largest eigenvalue of the transition matrix of a finite-state, discrete-time, reversible stationary Markov chain. It can be seen as a special case of Cheeger inequalities in expander graphs. Let  be a finite set and let  be the transition probability for a reversible Markov chain on . Assume this chain has stationary distribution . Define  and for  define  Define the constant  as  The operator  acting on the space of functions from  to , defined by  has eigenvalues . It is known that . The Cheeger bound is a bound on the second largest eigenvalue . Theorem (Cheeger bound):
In mathematics, the second moment method is a technique used in probability theory and analysis to show that a random variable has positive probability of being positive. More generally, the "moment method" consists of bounding the probability that a random variable fluctuates far from its mean, by using its moments. The method is often quantitative, in that one can often deduce a lower bound on the probability that the random variable is larger than some constant times its expectation. The method involves comparing the second moment of random variables to the square of the first moment.
Standardized rates are a statistical measure of any rates in a population.
In statistical theory, a U-statistic is a class of statistics that is especially important in estimation theory; the letter "U" stands for unbiased. In elementary statistics, U-statistics arise naturally in producing minimum-variance unbiased estimators. The theory of U-statistics allows a minimum-variance unbiased estimator to be derived from each unbiased estimator of an estimable parameter (alternatively, statistical functional) for large classes of probability distributions. An estimable parameter is a measurable function of the population's cumulative probability distribution: For example, for every probability distribution, the population median is an estimable parameter. The theory of U-statistics applies to general classes of probability distributions. Many statistics originally derived for particular parametric families have been recognized as U-statistics for general distributions. In non-parametric statistics, the theory of U-statistics is used to establish for statistical procedures (such as estimators and tests) and estimators relating to the asymptotic normality and to the variance (in finite samples) of such quantities. The theory has been used to study more general statistics as well as stochastic processes, such as random graphs. Suppose that a problem involves independent and identically-distributed random variables and that estimation of a certain parameter is required. Suppose that a simple unbiased estimate can be constructed based on only a few observations: this defines the basic estimator based on a given number of observations. For example, a single observation is itself an unbiased estimate of the mean and a pair of observations can be used to derive an unbiased estimate of the variance. The U-statistic based on this estimator is defined as the average (across all combinatorial selections of the given size from the full set of observations) of the basic estimator applied to the sub-samples. Sen (1992) provides a review of the paper by Wassily Hoeffding (1948), which introduced U-statistics and set out the theory relating to them, and in doing so Sen outlines the importance U-statistics have in statistical theory. Sen says "The impact of Hoeffding (1948) is overwhelming at the present time and is very likely to continue in the years to come". Note that the theory of U-statistics is not limited to the case of independent and identically-distributed random variables or to scalar random-variables.
Mixed logit is a fully general statistical model for examining discrete choices. The motivation for the mixed logit model arises from the limitations of the standard logit model. The standard logit model has three primary limitations, which mixed logit solves: "It [Mixed Logit] obviates the three limitations of standard logit by allowing for random taste variation, unrestricted substitution patterns, and correlation in unobserved factors over time." Mixed logit can also utilize any distribution for the random coefficients, unlike probit which is limited to the normal distribution. It has been shown that a mixed logit model can approximate to any degree of accuracy any true random utility model of discrete choice, given an appropriate specification of variables and distribution of coefficients." The following discussion draws from Ch. 6 of Discrete Choice Methods with Simulation, by Kenneth Train (Cambridge University Press), to which the reader is referred for more details and citations. See also the article on discrete choice for information on how the mixed logit relates to discrete choice analysis in general and to other specific types of choice models.
In mathematics, a Markov information source, or simply, a Markov source, is an information source whose underlying dynamics are given by a stationary finite Markov chain.
In statistics, the topic of location testing for Gaussian scale mixture distributions arises in some particular types of situations where the more standard Student's t-test is inapplicable. Specifically, these cases allow tests of location to be made where the assumption that sample observations arise from populations having a normal distribution can be replaced by the assumption that they arise from a Gaussian scale mixture distribution. The class of Gaussian scale mixture distributions contains all symmetric stable distributions, Laplace distributions, logistic distributions, and exponential power distributions, etc. Introduce tGn(x), the counterpart of Student's t-distribution for Gaussian scale mixtures. This means that if we test the null hypothesis that the center of a Gaussian scale mixture distribution is 0, say, then tnG(x) (x   0) is the infimum of all monotone nondecreasing functions u(x)   1/2, x   0 such that if the critical values of the test are u 1(1    ), then the significance level is at most     1/2 for all Gaussian scale mixture distributions [tGn(x) = 1   tGn( x),for x < 0]. An explicit formula for tGn(x), is given in the papers in the references in terms of Student s t-distributions, tk, k = 1, 2, ..., n. Introduce  G(x):= limn     tGn(x), the Gaussian scale mixture counterpart of the standard normal cumulative distribution function,  (x). Theorem.  G(x) = 1/2 for 0   x < 1,  G(1) = 3/4,  G(x) = C(x/(2   x2)1/2) for quantiles between 1/2 and 0.875, where C(x) is the standard Cauchy cumulative distribution function. This is the convex part of the curve  G(x), x   0 which is followed by a linear section  G(x) = x/(2 3) + 1/2 for 1.3136... < x < 1.4282.... Thus the 90% quantile is exactly 4 3/5. Most importantly,  G(x) =  (x) for x    3. Note that  ( 3) = 0.958..., thus the classical 95% confidence interval for the unknown expected value of Gaussian distributions covers the center of symmetry with at least 95% probability for Gaussian scale mixture distributions. On the other hand, the 90% quantile of  G(x) is 4 3/5 = 1.385... >   1(0.9) = 1.282... The following critical values are important in applications: 0.95 =  (1.645) =  G(1.651), and 0.9 =  (1.282) =  G(1.386). For the extension of the Theorem to all symmetric unimodal distributions one can start with a classical result of Aleksandr Khinchin: namely that all symmetric unimodal distributions are scale mixtures of symmetric uniform distributions.
In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. Otherwise the estimator is said to be biased. In statistics, "bias" is an objective statement about a function, and while not a desired property, it is not pejorative, unlike the ordinary English use of the term "bias". Bias can also be measured with respect to the median, rather than the mean (expected value), in which case one distinguishes median-unbiased from the usual mean-unbiasedness property. Bias is related to consistency in that consistent estimators are convergent and asymptotically unbiased (hence converge to the correct value), though individual estimators in a consistent sequence may be biased (so long as the bias converges to zero); see bias versus consistency. All else equal, an unbiased estimator is preferable to a biased estimator, but in practice all else is not equal, and biased estimators are frequently used, generally with small bias. When a biased estimator is used, the bias is also estimated. A biased estimator may be used for various reasons: because an unbiased estimator does not exist without further assumptions about a population or is difficult to compute (as in unbiased estimation of standard deviation); because an estimator is median-unbiased but not mean-unbiased (or the reverse); because a biased estimator reduces some loss function (particularly mean squared error) compared with unbiased estimators (notably in shrinkage estimators); or because in some cases being unbiased is too strong a condition, and the only unbiased estimators are not useful. Further, mean-unbiasedness is not preserved under non-linear transformations, though median-unbiasedness is (see effect of transformations); for example, the sample variance is an unbiased estimator for the population variance, but its square root, the sample standard deviation, is a biased estimator for the population standard deviation. These are all illustrated below.
In combinatorial mathematics, a block design is a set together with a family of subsets (repeated subsets are allowed at times) whose members are chosen to satisfy some set of properties that are deemed useful for a particular application. These applications come from many areas, including experimental design, finite geometry, software testing, cryptography, and algebraic geometry. Many variations have been examined, but the most intensely studied are the balanced incomplete block designs (BIBDs or 2-designs) which historically were related to statistical issues in the design of experiments. A block design in which all the blocks have the same size is called uniform. The designs discussed in this article are all uniform. Pairwise balanced designs (PBDs) are examples of block designs that are not necessarily uniform.
Stein's example (or phenomenon or paradox), in decision theory and estimation theory, is the phenomenon that when three or more parameters are estimated simultaneously, there exist combined estimators more accurate on average (that is, having lower expected mean squared error) than any method that handles the parameters separately. It is named after Charles Stein of Stanford University, who discovered the phenomenon in 1955. An intuitive explanation is that optimizing for the mean-squared error of a combined estimator is not the same as optimizing for the errors of separate estimators of the individual parameters. In practical terms, if the combined error is in fact of interest, then a combined estimator should be used, even if the underlying parameters are independent; this occurs in channel estimation in telecommunications, for instance (different factors affect overall channel performance). On the other hand, if one is instead interested in estimating an individual parameter, then using a combined estimator does not help and is in fact worse.
A time series is a sequence of data points made: 1) over a continuous time interval 2) out of successive measurements across that interval 3) using equal spacing between every two consecutive measurements 4) with each time unit within the time interval having at most one data point Examples of time series are ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average. Non-Examples: The height measurements of a group of people where each height is recorded over a period of time and each person has only one record in the data set. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). Yet a data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate. Time series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, intelligent transport and trajectory forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements. Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called "time series analysis", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Time series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.) Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).
The International Statistical Institute (ISI) is a professional association of statisticians. It was founded in 1885, although there had been international statistical congresses since 1853. The Institute has about 4,000 elected members from government, academia, and the private sector. The affiliated Associations have membership open to any professional statistician. The Institute publishes a variety of books and journals, and holds an international conference every two years. The biennial convention was commonly known as the ISI Session; however, since 2011, it is now referred to as the ISI World Statistics Congress. The permanent office of the Institute is located in the Statistics Netherlands building in Den Haag - Leidschenveen (The Hague), in the Netherlands.
In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable. Bayes' theorem calculates the renormalized pointwise product of the prior and the likelihood function, to produce the posterior probability distribution, which is the conditional distribution of the uncertain quantity given the data. Similarly, the prior probability of a random event or an uncertain proposition is the unconditional probability that is assigned before any relevant evidence is taken into account. Priors can be created using a number of methods. A prior can be determined from past information, such as previous experiments. A prior can be elicited from the purely subjective assessment of an experienced expert. An uninformative prior can be created to reflect a balance among outcomes when no information is available. Priors can also be chosen according to some principle, such as symmetry or maximizing entropy given constraints; examples are the Jeffreys prior or Bernardo's reference prior. When a family of conjugate priors exists, choosing a prior from that family simplifies calculation of the posterior distribution. Parameters of prior distributions are a kind of hyperparameter. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then: p is a parameter of the underlying system (Bernoulli distribution), and   and   are parameters of the prior distribution (beta distribution); hence hyperparameters. Hyperparameters themselves may have hyperprior distributions expressing beliefs about their values. A Bayesian model with more than one level of prior this like is called a hierarchical Bayes model.
Seismic inversion, in Geophysics (primarily Oil and Gas exploration/development), is the process of transforming seismic reflection data into a quantitative rock-property description of a reservoir. Seismic inversion may be pre- or post-stack, deterministic, random or geostatistical, and typically includes other reservoir measurements such as well logs and cores.
In probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution whereby a finite number of values are equally likely to be observed; every one of n values has equal probability 1/n. Another way of saying "discrete uniform distribution" would be "a known, finite number of outcomes equally likely to happen". A simple example of the discrete uniform distribution is throwing a fair dice. The possible values are 1, 2, 3, 4, 5, 6, and each time the die is thrown the probability of a given score is 1/6. If two dice are thrown and their values added, the resulting distribution is no longer uniform since not all sums have equal probability. The discrete uniform distribution itself is inherently non-parametric. It is convenient, however, to represent its values generally by an integer interval [a,b], so that a,b become the main parameters of the distribution (often one simply considers the interval [1,n] with the single parameter n). With these conventions, the cumulative distribution function (CDF) of the discrete uniform distribution can be expressed, for any k   [a,b], as
Publication bias is a type of bias with regard to what academic research is likely to be published, among what is available to be published. Publication bias is of interest because literature reviews of claims about support for a hypothesis, or values for a parameter will themselves be biased if the original literature is contaminated by publication bias. While some preferences are desirable for instance a bias against publication of flawed studies a tendency of researchers and journal editors to prefer some outcomes rather than others e.g. results showing a significant finding, leads to a problematic bias in the published literature. Studies with significant results often do not appear to be superior to studies with a null result with respect to quality of design. However, statistically significant results have been shown to be three times more likely to be published compared to papers with null results. Multiple factors contribute to publication bias. For instance, once a result is well established, it may become newsworthy to publish papers affirming the null result. It has been found that the most common reason for non-publication is investigators declining to submit results for publication. Factors cited as underlying this effect include investigators assuming they must have made a mistake, to not find a known finding, loss of interest in the topic, or anticipation that others will be uninterested in the null results. Attempts to identify unpublished studies often prove difficult or are unsatisfactory. One effort to decrease this problem is reflected in the move by some journals to require that studies submitted for publication are pre-registered (registering a study prior to collection of data and analysis). Several such registries exist, for instance the Center for Open Science. Strategies are being developed to detect and control for publication bias, for instance down-weighting small and non-randomised studies because of their demonstrated high susceptibility to error and bias, and p-curve analysis
Queueing theory is the mathematical study of waiting lines, or queues. In queueing theory a model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service. Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing and the design of factories, shops, offices and hospitals.
In the study of stochastic processes in mathematics, a hitting time (or first hit time) is the first time at which a given process "hits" a given subset of the state space. Exit times and return times are also examples of hitting times.
Prevalence in epidemiology is the proportion of a population found to have a condition (typically a disease or a risk factor such as smoking or seat-belt use). It is arrived at by comparing the number of people found to have the condition with the total number of people studied, and is usually expressed as a fraction, as a percentage or as the number of cases per 10,000 or 100,000 people. Point prevalence is the proportion of a population that has the condition at a specific point in time. Period prevalence is the proportion of a population that has the condition at some time during a given period (e.g., 12 month prevalence), and includes people who already have the condition at the start of the study period as well as those who acquire it during that period. Lifetime prevalence (LTP) is the proportion of a population that at some point in their life (up to the time of assessment) have experienced the condition. Prevalence estimates are used by epidemiologists, health care providers, government agencies, and insurers. Prevalence is contrasted with incidence, which is a measure of new cases arising in a population over a given period (month, year, etc.). The difference between prevalence and incidence can be summarized thus: prevalence answers "How many people have this disease right now " and incidence answers "How many people per year newly acquire this disease ".
In statistics, the Mate rn covariance (named after the Swedish forestry statistician Bertil Mate rn) is a covariance function used in spatial statistics, geostatistics, machine learning, image analysis, and other applications of multivariate statistical analysis on metric spaces. It is commonly used to define the statistical covariance between measurements made at two points that are d units distant from each other. Since the covariance only depends on distances between points, it is stationary. If the distance is Euclidean distance, the Mate rn covariance is also isotropic.  
In probability theory and statistics, the continuous uniform distribution or rectangular distribution is a family of symmetric probability distributions such that for each member of the family, all intervals of the same length on the distribution's support are equally probable. The support is defined by the two parameters, a and b, which are its minimum and maximum values. The distribution is often abbreviated U(a,b). It is the maximum entropy probability distribution for a random variate X under no constraint other than that it is contained in the distribution's support.
WinBUGS is statistical software for Bayesian analysis using Markov chain Monte Carlo (MCMC) methods. It is based on the BUGS (Bayesian inference Using Gibbs Sampling) project started in 1989. It runs under Microsoft Windows, though it can also be run on Linux or Mac using Wine. It was developed by the BUGS Project, a team of UK researchers at the MRC Biostatistics Unit, Cambridge, and Imperial College School of Medicine, London. The last version of WinBUGS was version 1.4.3, released in August 2007. Development is now focused on OpenBUGS, an open source version of the package. WinBUGS 1.4.3 remains available as a stable version for routine use, but is no longer being developed.
In applied probability, a population process is a Markov chain in which the state of the chain is analogous to the number of individuals in a population (0, 1, 2, etc.), and changes to the state are analogous to the addition or removal of individuals from the population. Although named by analogy to biological populations, population processes find application in a much wider range of fields than just ecology and other biological sciences. These other applications include telecommunications and queueing theory, chemical kinetics and financial mathematics, and hence the 'population' could be of packets in a computer network, of molecules in a chemical reaction, or even of units in a financial index. Population processes are typically characterized by processes of birth and immigration, and of death, emigration and catastrophe, which correspond to the basic demographic processes and broad environmental effects to which a population is subject. However, population processes are also often equivalent to other processes that may typically be characterised under other paradigms (in the literal sense of 'patterns'). Queues, for example, are often characterised by an arrivals process, a service process, and the number of servers. In appropriate circumstances, however, arrivals at a queue are functionally equivalent to births or immigration and the service of waiting 'customers' is equivalent to death or emigration. Typical population processes include birth-death processes and birth, death and catastrophe processes.
StatPlus is a software product for basic univariate and multivariate statistical analysis (MANOVA, GLM, Latin squares), as well as time series analysis, nonparametric statistics, survival analysis and statistical charts including control charts. It was originally developed for use in the biomedical sciences. The original version is now known as BioStat. It is mostly used in biomedicine and natural sciences. The software has a version for the Mac OS X known as StatPlus:mac. The software may also be used as an add-on to the Microsoft Excel.
In stochastic processes, Variable-order Markov (VOM) models are an important class of models that extend the well known Markov chain models. In contrast to the Markov chain models, where each random variable in a sequence with a Markov property depends on a fixed number of random variables, in VOM models this number of conditioning random variables may vary based on the specific observed realization. This realization sequence is often called the context; therefore the VOM models are also called context trees. The flexibility in the number of conditioning random variables turns out to be of real advantage for many applications, such as statistical analysis, classification and prediction.
Extrapolation domain analysis (EDA) is a methodology for identifying geographical areas that seem suitable for adoption of innovative ecosystem management practices on the basis of sites exhibiting similarity in conditions such as climatic, land use and socioeconomic indicators. Whilst it has been applied to water research projects in nine pilot basins, the concept is generic and can be applied to any project where accelerating change being considered as a central development objective. The outputs of the method thus far have been used to quantify the global economic impact of implementing particular innovations together with its effect on water resources. The research has stimulated members of several of the Challenge Program for Water and Food projects to explore potential areas for scaling out. Such is the case of the Quesungual agroforestry system in Honduras, which is moving towards new areas in parallel with areas identified by the EDA method. EDA is a combined approach that incorporates a number of spatial analysis techniques. It was first investigated in 2006, when it was applied to assess how similarity analysis can be used to scale out research findings within seven Andes pilot systems of basins. The method developed further the research around Jones' 'Homologue' analysis by incorporating socio-economic variables in the search for similar sites around the Tropics. It has since been used to evaluate  Impact pathways  and Global Impact Analysis. 'Homologue' was developed to determine the similarity of climatic conditions across a geographical area to those exhibited by the pilot site; the pixel resolution at which this is processed is 2.43 arc minutes, or 4.5 km at the equator. To derive the extrapolation domains, Bayesian and frequentist statistical modelling techniques are used. The weights-of-evidence (WofE) methodology is applied; this is based largely on the concepts of Bayesian probabilistic reasoning. In essence, statistical inference is based on determining the probability of target sites adopting the change demonstrated in pilot areas. The assumption is that a collection of training points will, in aggregate, have common characteristics that will allow their presence in other similar sites to be predicted. It is based on the collection of factors (used to create evidential theme data layers) that prove to be consistent with successful implementation at pilot sites and assumes that if target sites exhibit similar socio-economic, together with climatic and landscapes attributes to pilot sites, then there is strong evidence to suggest that out-scaling to these sites will succeed.
In statistics, an optimality criterion provides a measure of the fit of the data to a given hypothesis, to aid in model selection. A model is designated as the "best" of the candidate models if it gives the best value of an objective function measuring the degree of satisfaction of the criterion used to evaluate the alternative hypotheses. The term has been used to identify the different criteria that are used to evaluate a phylogenetic tree. For example, in order to determine the best topology between two phylogenetic trees using the maximum likelihood optimality criterion, one would calculate the maximum likelihood score of each tree and choose the one that had the better score. However, different optimality criteria can select different hypotheses. In such circumstances caution should be exercised when making strong conclusions. Many other disciplines use similar criteria or have specific measures geared toward the objectives of the field. Optimality criteria include maximum likelihood, Bayesian, maximum parsimony, sum of squared residuals, least absolute deviations, and many others.
In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with "mixture distributions" relate to deriving the properties of the overall population from those of the sub-populations, "mixture models" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information. Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps. Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size of the population has been normalized to 1.
Many statistical analyses are based on distributional assumptions about the population from which the data have been obtained. However, distributional families can have radically different shapes depending on the value of the shape parameter. Therefore, finding a reasonable choice for the shape parameter is a necessary step in the analysis. In many analyses, finding a good distributional model for the data is the primary focus of the analysis. The probability plot correlation coefficient (PPCC) plot is a graphical technique for identifying the shape parameter for a distributional family that best describes the data set. This technique is appropriate for families, such as the Weibull, that are defined by a single shape parameter and location and scale parameters, and it is not appropriate or even possible for distributions, such as the normal, that are defined only by location and scale parameters. The technique is simply "plot the probability plot correlation coefficients for different values of the shape parameter, and choose whichever value yields the best fit".
In the evolutionary biology of sexual reproduction, Operational sex ratio (OSR), is the ratio of sexually competing males that are ready to mate to sexually competing females that are ready to mate, or alternatively the local ratio of fertilizable females to sexually active males at any given time. Its difference from the physical sex ratio, is that it does not take into account sexually inactive or non-competitive individuals (individuals that do not compete for mates). The theory of OSR hypothesizes that the operational sex ratio affects the mating competition of males and females in a population. This concept is especially useful in the study of sexual selection since it is a measure of how intense sexual competition is in a species, and also in the study of the relationship of sexual selection to sexual dimorphism. The OSR is closely linked to the "potential rate of reproduction" of the two sexes; that is, how fast they each could reproduce in ideal circumstances. Usually variation in potential reproductive rates creates bias in the OSR and this in turn will affect the strength of selection. The OSR is said to be biased toward a particular sex when sexually ready members of that sex are more abundant. For example, a male-biased OSR means that there are more sexually competing males than sexually competing females.
In electrical engineering, computer science, statistical computing and bioinformatics, the Baum Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm and is named for Leonard E. Baum and Lloyd R. Welch.
In probability theory and applications, Bayes's rule relates the odds of event  to the odds of event , before (prior to) and after (posterior to) conditioning on another event . The odds on  to event  is simply the ratio of the probabilities of the two events. The prior odds is the ratio of the unconditional or prior probabilities, the posterior odds is the ratio of conditional or posterior probabilities given the event . The relationship is expressed in terms of the likelihood ratio or Bayes factor, . By definition, this is the ratio of the conditional probabilities of the event  given that  is the case or that  is the case, respectively. The rule simply states: posterior odds equals prior odds times Bayes factor (Gelman et al., 2005, Chapter 1). When arbitrarily many events  are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood,  where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as  varies, for fixed or given  (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005). Bayes' rule is an equivalent way to formulate Bayes' theorem. If we know the odds for and against  we also know the probabilities of . It may be preferred to Bayes' theorem in practice for a number of reasons. Bayes' rule is widely used in statistics, science and engineering, for instance in model selection, probabilistic expert systems based on Bayes networks, statistical proof in legal proceedings, email spam filters, and so on (Rosenthal, 2005; Bertsch McGrayne, 2012). As an elementary fact from the calculus of probability, Bayes' rule tells us how unconditional and conditional probabilities are related whether we work with a frequentist interpretation of probability or a Bayesian interpretation of probability. Under the Bayesian interpretation it is frequently applied in the situation where  and  are competing hypotheses, and  is some observed evidence. The rule shows how one's judgement on whether  or  is true should be updated on observing the evidence  (Gelman et al., 2003).
Repeated measures design uses the same subjects with every branch of research, including the control. For instance, repeated measurements are collected in a longitudinal study in which change over time is assessed. Other (non-repeated measures) studies compare the same measure under two or more different conditions. For instance, to test the effects of caffeine on cognitive function, a subject's math ability might be tested once after they consume caffeine and another time when they consume a placebo.
The one-factor-at-a-time method (or OFAT) is a method of designing experiments involving the testing of factors, or causes, one at a time instead of all simultaneously. Prominent text books and academic papers currently favor factorial experimental designs, a method pioneered by Sir Ronald A. Fisher, where multiple factors are changed at once. The reasons stated for favoring the use of factorial design over OFAT are: 1. OFAT requires more runs for the same precision in effect estimation 2. OFAT cannot estimate interactions 3. OFAT can miss optimal settings of factors Despite these criticisms, some researchers have articulated a role for OFAT and showed they can be more effective than fractional factorials under certain conditions (number of runs is limited, primary goal is to attain improvements in the system, and experimental error is not large compared to factor effects, which must be additive and independent of each other). Designed experiments remain nearly always preferred to OFAT with many types and methods available, in addition to fractional factorials which, though usually requiring more runs than OFAT, do address the three concerns above. One modern design over which OFAT has no advantage in number of runs is the Plackett-Burman which, by having all factors vary simultaneously (an important quality in experimental designs), gives generally greater precision in effect estimation.
Bilinear time frequency distributions, or quadratic time frequency distributions, arise in a sub-field of signal analysis and signal processing called time frequency signal processing, and, in the statistical analysis of time series data. Such methods are used where one needs to deal with a situation where the frequency composition of a signal may be changing over time; this sub-field used to be called time frequency signal analysis, and is now more often called time frequency signal processing due to the progress in using these methods to a wide range of signal-processing problems.
Cognitive interviewing is a field research method used primarily in pre-testing survey instruments developed in collaboration by psychologists and survey researchers. It allows survey researchers to collect verbal information regarding survey responses and is used in evaluating whether the question is measuring the construct the researcher intends. The data collected is then used to adjust problematic questions in the questionnaire before fielding the survey instrument to the full sample. Although survey researchers do not totally agree as to what a cognitive interview entails, it in general collects the following information from participants: evaluations on how the subject constructed his or her answers; explanations on what the subject interprets the questions to mean; reporting of any difficulties the subject had in answering the questions; and anything else that reveals the circumstances to the subject s answers. In general, there are two methods practiced when conducting a cognitive interview. The first method, called the think-aloud method, encourages participants participating in the interview to verbalize their thoughts while responding to the survey questions. This method is considered purer as it reduces the possibility of the interviewer introducing any bias into the participants  answers. In contrast, the disadvantage to this method is that it does require training on the part of the participant on the think-aloud process which can be burdensome to the interviewee. The second method has the interviewer ask detailed probes after subjects answer a survey question (called the probing-method). An example of a probe question is:  In your own words, what is this question asking   or  How did you arrive at your answer   Advocates for this method suggest that follow-up probes do not interfere with the actual process of responding to survey questions while requiring very little training on the part of the respondent. In order to conduct a cognitive interview on a survey instrument, the researcher should recruit a minimum of 10 participants to a maximum of 25 participants. The participants recruited should reflect the diversity in the population being studied. Cognitive interviewing is regularly practiced by U.S. Federal Agencies, including the Census Bureau, National Center for Health Statistics and the Bureau of Labor Statistics.
In statistics and regression analysis, moderation occurs when the relationship between two variables depends on a third variable. The third variable is referred to as the moderator variable or simply the moderator. The effect of a moderating variable is characterized statistically as an interaction; that is, a categorical (e.g., sex, race, class) or quantitative (e.g., level of reward) variable that affects the direction and/or strength of the relation between dependent and independent variables. Specifically within a correlational analysis framework, a moderator is a third variable that affects the zero-order correlation between two other variables, or the value of the slope of the dependent variable on the independent variable. In analysis of variance (ANOVA) terms, a basic moderator effect can be represented as an interaction between a focal independent variable and a factor that specifies the appropriate conditions for its operation.
A compound Poisson process is a continuous-time (random) stochastic process with jumps. The jumps arrive randomly according to a Poisson process and the size of the jumps is also random, with a specified probability distribution. A compound Poisson process, parameterised by a rate  and jump size distribution G, is a process  given by  where,  is a Poisson process with rate , and  are independent and identically distributed random variables, with distribution function G, which are also independent of  When  are non-negative integer-valued random variables, then this compound Poisson process is known as a stuttering Poisson process which has the feature that two or more events occur in a very short time .
The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). It can be used as an alternative to the paired Student's t-test, t-test for matched pairs, or the t-test for dependent samples when the population cannot be assumed to be normally distributed.
Functional data analysis (FDA) is a branch of statistics that analyses data providing information about curves, surfaces or anything else varying over a continuum. In its most general form, under an FDA framework each sample element is considered to be a function. The physical continuum over which these functions are defined is often time, but may also be spatial location, wavelength, probability, etc.
Structural estimation is a technique for estimating deep "structural" parameters of theoretical economic models. The term is inherited from the simultaneous equations model. In this sense, "structural estimation" is contrasted with "reduced-form estimation," which is the statistical relationship between observed variables. The difference between a structural parameter and a reduced-form parameter was formalized in the work of the Cowles Foundation. A structural parameter is also said to be "policy invariant" whereas the value of reduced-form parameter can depend on exogenously determined parameters set by public policy makers. The distinction between structural and reduced-form estimation within "microeconometrics" is related to the Lucas critique of reduced-form macroeconomic policy predictions. The original distinction between structure and reduced-form was between the underlying system and the direct relationship between observables implied by the system. Different combinations of structural parameters can imply the same reduced-form parameters, so structural estimation must go beyond the direct relationship between variables. Many economists now use the term "reduced form" to mean statistical estimation without reference to a specific economic model. For example, a regression is often called a reduced-form equation even when no standard economic model would generate it as the reduced form relationship between variables. These conflicting distinctions between structural and reduced form estimation arose from the increasing complexity of economic theory since the formalization of simultaneous equations estimation. A structural model often involves sequential decisions making under uncertainty or strategic environments where beliefs about other agents' actions matter. Parameters of such models are estimated not with regression analysis but non-linear techniques such as generalized method of moments, maximum likelihood, and indirect inference. The reduced-form of such models may result in a regression relationship but often only for special or trivial cases of the structural parameters. Structural estimation is used by economists, econometricians, and statisticians.
Chemometrics is the science of extracting information from chemical systems by data-driven means. Chemometrics is inherently interdisciplinary, using methods frequently employed in core data-analytic disciplines such as multivariate statistics, applied mathematics, and computer science, in order to address problems in chemistry, biochemistry, medicine, biology and chemical engineering. In this way, it mirrors other interdisciplinary fields, such as psychometrics and econometrics.
The generalized entropy index has been proposed as a measure of income inequality in a population. It is derived from information theory as a measure of redundancy in data. In information theory a measure of redundancy can be interpreted as non-randomness or data compression; thus this interpretation also applies to this index. In additional interpretation of the index is as biodiversity as entropy has also been proposed as a measure of diversity.
Fisher's z-distribution is the statistical distribution of half the logarithm of an F-distribution variate:  It was first described by Ronald Fisher in a paper delivered at the International Mathematical Congress of 1924 in Toronto. Nowadays one usually uses the F-distribution instead. The probability density function and cumulative distribution function can be found by using the F-distribution at the value of . However, the mean and variance do not follow the same transformation. The probability density function is  where B is the beta function. When the degrees of freedom becomes large () the distribution approach normality with mean  and variance
SDMX, which stands for Statistical Data and Metadata eXchange is an international initiative that aims at standardising and modernising ( industrialising ) the mechanisms and processes for the exchange of statistical data and metadata among international organisations and their member countries. The SDMX sponsoring institutions are the Bank for International Settlements (BIS), the European Central Bank (ECB), Eurostat (the statistical office of the European Union), the International Monetary Fund (IMF), the Organisation for Economic Co-operation and Development (OECD), the United Nations Statistics Division (UNSD), and the World Bank. These organisations are the main players at world and regional levels in the collection of official statistics in a large variety of domains (agriculture statistics, economic and financial statistics, social statistics, environment statistics etc.). In 2013 SDMX was approved by ISO as an International Standard (ISO 17369:2013). The latest version of the standard - SDMX 2.1 - was released in April 2011. People who are new to SDMX are invited to consult the  Learning about SDMX Basics  page which will provide them with the necessary basic material for understanding SDMX. Users who are already familiar with the SDMX standard will find on the SDMX.org website all material, such as the technical standards and guidelines necessary for properly implementing SDMX in a statistical domain.
An a priori probability is a probability that is derived purely by deductive reasoning. One way of deriving a priori probabilities is the principle of indifference, which has the character of saying that, if there are N mutually exclusive and exhaustive events and if they are equally likely, then the probability of a given event occurring is 1/N. Similarly the probability of one of a given collection of K events is K/N. One disadvantage of defining probabilities in the above way is that it applies only to finite collections of events. In Bayesian inference, the terms "uninformative priors" or "objective priors" refer to particular choices of a priori probabilities. Note that "prior probability" is a broader concept. Similar to the distinction in philosophy between a priori and a posteriori, in Bayesian inference a priori denotes general knowledge about the data distribution before making an inference, while a posteriori denotes knowledge that incorporates the results of making an inference.
Time domain is the analysis of mathematical functions, physical signals or time series of economic or environmental data, with respect to time. In the time domain, the signal or function's value is known for all real numbers, for the case of continuous time, or at various separate instants in the case of discrete time. An oscilloscope is a tool commonly used to visualize real-world signals in the time domain. A time-domain graph shows how a signal changes with time, whereas a frequency-domain graph shows how much of the signal lies within each given frequency band over a range of frequencies.
In statistics, semiparametric regression includes regression models that combine parametric and nonparametric models. They are often used in situations where the fully nonparametric model may not perform well or when the researcher wants to use a parametric model but the functional form with respect to a subset of the regressors or the density of the errors is not known. Semiparametric regression models are a particular type of semiparametric modelling and, since semiparametric models contain a parametric component, they rely on parametric assumptions and may be misspecified and inconsistent, just like a fully parametric model.
The speed prior is a complexity measure similar to Kolmogorov complexity, except that it is based on computation speed as well as program length. The speed prior complexity of a program is its size in bits plus the logarithm of the maximum time we are willing to run it to get a prediction. When compared to traditional measures, use of the Speed Prior has the disadvantage of leading to less optimal predictions, and the advantage of providing computable predictions.
In mathematics and signal processing, the Z-transform converts a discrete-time signal, which is a sequence of real or complex numbers, into a complex frequency domain representation. It can be considered as a discrete-time equivalent of the Laplace transform. This similarity is explored in the theory of time scale calculus.
In probability theory and statistics, a shape parameter is a kind of numerical parameter of a parametric family of probability distributions.
Measurement invariance or measurement equivalence is a statistical property of measurement that indicates that the same construct is being measured across some specified groups. For example, measurement invariance can be used to study whether a given measure is interpreted in a conceptually similar manner by respondents representing different genders or cultural backgrounds. Violations of measurement invariance may preclude meaningful interpretation of measurement data. Tests of measurement invariance are increasingly used in fields such as psychology to supplement evaluation of measurement quality rooted in classical test theory. Measurement invariance is often tested in the framework of multiple-group confirmatory factor analysis (CFA). In the context of structural equation models, including CFA, measurement invariance is often termed factorial invariance.
The Weibull modulus is a dimensionless parameter of the Weibull distribution which is used to describe variability in measured material strength of brittle materials. For ceramics and other brittle materials, the maximum stress that a sample can be measured to withstand before failure may vary from specimen to specimen, even under identical testing conditions. This is related to the distribution of physical flaws present in the surface or body of the brittle specimen, since brittle failure processes originate at these weak points. When flaws are consistent and evenly distributed, samples will behave more uniformly than when flaws are clustered inconsistently. This must be taken into account when describing the strength of the material, so strength is best represented as a distribution of values rather than as one specific value. The Weibull modulus is a shape parameter for the Weibull distribution model which, in this case, maps the probability of failure of a component at varying stresses. Consider strength measurements made on many small samples of a brittle ceramic material. If the measurements show little variation from sample to sample, the calculated Weibull modulus will be high and a single strength value would serve as a good description of the sample-to-sample performance. It may be concluded that its physical flaws, whether inherent to the material itself or resulting from the manufacturing process, are distributed uniformly throughout the material. If the measurements show high variation, the calculated Weibull modulus will be low; this reveals that flaws are clustered inconsistently and the measured strength will be generally weak and variable. Products made from components of low Weibull modulus will exhibit low reliability and their strengths will be broadly distributed. Test procedures for determining the Weibull modulus are specified in DIN EN 843-5 and DIN 51 110-3. A further method to determine the strength of brittle materials has been described by the Wikibook contribution Weakest link determination by use of three parameter Weibull statistics.
In probability theory, Kolmogorov's inequality is a so-called "maximal inequality" that gives a bound on the probability that the partial sums of a finite collection of independent random variables exceed some specified bound. The inequality is named after the Russian mathematician Andrey Kolmogorov.
In mathematics, the Bhatia Davis inequality, named after Rajendra Bhatia and Chandler Davis, is an upper bound on the variance of any bounded probability distribution on the real line. Suppose a distribution has minimum m, maximum M, and expected value  . Then the inequality says:  Equality holds precisely if all of the probability is concentrated at the endpoints m and M. The Bhatia Davis inequality is stronger than Popoviciu's inequality on variances.
ADMB or AD Model Builder is a free and open source software suite for non-linear statistical modeling. It was created by David Fournier and now being developed by the ADMB Project, a creation of the non-profit ADMB Foundation. The "AD" in AD Model Builder refers to the automatic differentiation capabilities that come from the AUTODIF Library, a C++ language extension also created by David Fournier, which implements reverse mode automatic differentiation. A related software package, ADMB-RE, provides additional support for modeling random effects.
The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau, is a Monte Carlo method designed to calculate the density of states of a system. The method performs a non-markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation. The Wang Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. For instance, it has been applied to the solution of numerical integrals and the folding of proteins. The Wang-Landau sampling is related to the metadynamics algorithm.
Pairwise comparison generally is any process of comparing entities in pairs to judge which of each entity is preferred, or has a greater amount of some quantitative property, or whether or not the two entities are identical. The method of pairwise comparison is used in the scientific study of preferences, attitudes, voting systems, social choice, public choice, and multiagent AI systems. In psychology literature, it is often referred to as paired comparison. Prominent psychometrician L. L. Thurstone first introduced a scientific approach to using pairwise comparisons for measurement in 1927, which he referred to as the law of comparative judgment. Thurstone linked this approach to psychophysical theory developed by Ernst Heinrich Weber and Gustav Fechner. Thurstone demonstrated that the method can be used to order items along a dimension such as preference or importance using an interval-type scale.
Social network analysis (SNA) is the process of investigating social structures through the use of network and graph theories. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties or edges (relationships or interactions) that connect them. Examples of social structures commonly visualized through social network analysis include social media networks, friendship and acquaintance networks, collaboration graphs, kinship, disease transmission,and sexual relationships. These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines. Social network analysis has emerged as a key technique in modern sociology. It has also gained a significant following in anthropology, biology, communication studies, economics, geography, history, information science, organizational studies, political science, social psychology, development studies, and sociolinguistics and is now commonly available as a consumer tool.
In machine learning, a maximum-entropy Markov model (MEMM), or conditional Markov model (CMM), is a graphical model for sequence labeling that combines features of hidden Markov models (HMMs) and maximum entropy (MaxEnt) models. An MEMM is a discriminative model that extends a standard maximum entropy classifier by assuming that the unknown values to be learnt are connected in a Markov chain rather than being conditionally independent of each other. MEMMs find applications in natural language processing, specifically in part-of-speech tagging and information extraction.
In statistics, asymptotic theory, or large sample theory, is a generic framework for assessment of properties of estimators and statistical tests. Within this framework it is typically assumed that the sample size n grows indefinitely, and the properties of statistical procedures are evaluated in the limit as n    . In practical applications, asymptotic theory is applied by treating the asymptotic results as approximately valid for finite sample sizes as well. Such approach is often criticized for not having any mathematical grounds behind it, yet it is used ubiquitously anyway. The importance of the asymptotic theory is that it often makes possible to carry out the analysis and state many results which cannot be obtained within the standard  finite-sample theory .
In bioinformatics a dot plot is a graphical method that allows the comparison of two biological sequences and identify regions of close similarity between them. It is a type of recurrence plot.
Trend analysis is the practice of collecting information and attempting to spot a pattern, or trend, in the information. In some fields of study, the term "trend analysis" has more formally defined meanings. Although trend analysis is often used to predict future events, it could be used to estimate uncertain events in the past, such as how many ancient kings probably ruled between two dates, based on data such as the average years which other known kings reigned.
In geometric data analysis and statistical shape analysis, principal geodesic analysis is a generalization of principal component analysis to a non-Euclidean, non-linear setting of manifolds suitable for use with shape descriptors such as medial representations.
An index of qualitative variation (IQV) is a measure of statistical dispersion in nominal distributions. There are a variety of these, but they have been relatively little-studied in the statistics literature. The simplest is the variation ratio, while more complex indices include the information entropy.
In statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.
In probability theory and statistics, the Weibull distribution / ve b l/ is a continuous probability distribution. It is named after Swedish mathematician Waloddi Weibull, who described it in detail in 1951, although it was first identified by Fre chet (1927) and first applied by Rosin & Rammler (1933) to describe a particle size distribution.  
The application software BV4.1 is an easy-to-use tool for decomposing and seasonally adjusting monthly or quarterly economic time series by version 4.1 of the so-called Berlin procedure. It is being developed by the Federal Statistical Office of Germany. The software is released as freeware for non-commercial purposes.
In probability theory and statistics, the Weibull distribution / ve b l/ is a continuous probability distribution. It is named after Swedish mathematician Waloddi Weibull, who described it in detail in 1951, although it was first identified by Fre chet (1927) and first applied by Rosin & Rammler (1933) to describe a particle size distribution.  
Signed differential mapping or SDM is a statistical technique for meta-analyzing studies on differences in brain activity or structure which used neuroimaging techniques such as fMRI, VBM, DTI or PET. It may also refer to a specific piece of software created by the SDM Project to carry out such meta-analyses.
In statistics and econometrics, and in particular in time series analysis, an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model. These models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting). They are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the "integrated" part of the model) can be applied to reduce the non-stationarity. Non-seasonal ARIMA models are generally denoted  where parameters p, d, and q are non-negative integers,  is the order of the Autoregressive model,  is the degree of differencing, and  is the order of the Moving-average model. Seasonal ARIMA models are usually denoted , where  refers to the number of periods in each season, and the uppercase  refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model. ARIMA models form an important part of the Box-Jenkins approach to time-series modelling. When two out of the three terms are zeros, the model may be referred to based on the non-zero parameter, dropping "AR", "I" or "MA" from the acronym describing the model. For example, ARIMA (1,0,0) is AR(1), ARIMA(0,1,0) is I(1), and ARIMA(0,0,1) is MA(1).
Spatial econometrics is the field where spatial analysis and econometrics intersect. In general, econometrics differs from other branches of statistics in focusing on theoretical models, whose parameters are estimated using regression analysis. Spatial econometrics is a refinement of this, where either the theoretical model involves interactions between different entities, or the data observations are not truly independent. Thus, models incorporating spatial auto-correlation or neighborhood effects can be estimated using spatial econometric methods. Such models are common in regional science, real estate economics, and education economics.
In probability theory and statistics, the Jensen Shannon divergence is a popular method of measuring the similarity between two probability distributions. It is also known as information radius (IRad) or total divergence to the average. It is based on the Kullback Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it is always a finite value. The square root of the Jensen Shannon divergence is a metric often referred to as Jensen-Shannon distance.
System dynamics (SD) is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, and time delays.
In probability theory, the arcsine laws are a collection of results for one-dimensional random walks and Brownian motion (the Wiener process). The best known of these is attributed to Paul Le vy (1939). All three laws relate path properties of the Wiener process to the arcsine distribution. A random variable X on [0,1] is arcsine-distributed if
The decomposition of time series is a statistical method that deconstructs a time series into notional components. There are two principal types of decomposition which are outlined below.
Bayes linear statistics is a subjectivist statistical methodology and framework. Traditional subjective Bayesian analysis is based upon fully specified probability distributions, which are very difficult to specify at the necessary level of detail. Bayes linear analysis attempts to solve this problem by developing theory and practise for using partially specified probability models. Bayes linear in its current form has been primarily developed by Michael Goldstein. Mathematically and philosophically it extends Bruno de Finetti's Operational Subjective approach to probability and statistics. Consider first a traditional Bayesian Analysis where you expect to shortly know D and you would like to know more about some other observable B. In the traditional Bayesian approach it is required that every possible outcome is enumerated i.e. every possible outcome is the cross product of the partition of a set of B and D. If represented on a computer where B requires n bits and D m bits then the number of states required is . The first step to such an analysis is to determine a persons subjective probabilities e.g. by asking about their betting behaviour for each of these outcomes. When we learn D conditional probabilities for B are determined by the application of Bayes' rule. Practitioners of subjective Bayesian statistics routinely analyse datasets where the size of this set is large enough that subjective probabilities cannot be meaningfully determined for every element of D   B. This is normally accomplished by assuming exchangeability and then the use of parameterized models with prior distributions over parameters and appealing to the de Finetti's theorem to justify that this produces valid operational subjective probabilities over D   B. The difficulty with such an approach is that the validity of the statistical analysis requires that the subjective probabilities are a good representation of an individual's beliefs however this method results in a very precise specification over D   B and it is often difficult to articulate what it would mean to adopt these belief specifications. In contrast to the traditional Bayesian paradigm Bayes linear statistics following de Finetti uses Prevision or subjective expectation as a primitive, probability is then defined as the expectation of an indicator variable. Instead of specifying a subjective probability for every element in the partition D   B the analyst specifies subjective expectations for just a few quantities that they are interested in or feel knowledgeable about. Then instead of conditioning an adjusted expectation is computed by a rule that is a generalization of Bayes' rule that is based upon expectation. The use of the word linear in the title refers to de Finetti's arguments that probability theory is a linear theory (de Finetti argued against the more common measure theory approach).
Bayesian statistics, named for Thomas Bayes (1701-1761), is a theory in the field of statistics in which the evidence about the true state of the world is expressed in terms of 'degrees of belief' called Bayesian probabilities. Such an interpretation is only one of a number of interpretations of probability and there are other statistical techniques that are not based on 'degrees of belief'. One of the key ideas of Bayesian statistics is that "probability is orderly opinion, and that inference from data is nothing other than the revision of such opinion in the light of relevant new information."
The term normal score is used with two different meanings in statistics. One of them relates to creating a single value which can be treated as if it had arisen from a standard (zero mean, unit variance) normal distribution. The second relates to assigning alternative values to data points within a dataset, with the broad intention of creating data values than can be interpreted as being approximations for values that might have been observed had the data arisen from a standard normal distribution. The first meaning is as an alternative name for the standard score or z score, where values are standardised by subtracting the sample or estimated mean and dividing by the sample or other estimate of the standard deviation. Particularly in applications where the name "normal score" is used, there is usually a presumption that the value can be referred to a table of standard normal probabilities as a means of providing an informal significance test of some hypothesis, such as a difference in means. The second meaning of normal score is associated with data values derived from the ranks of the observations within the dataset. A given data point is assigned a value which is either exactly, or an approximation, to the expectation of the order statistic of the same rank in a sample of standard normal random variables of the same size as the observed data set. Thus the meaning of a normal score of this type is essentially the same as a rankit, although the term "rankit" is becoming obsolete. In this case the transformation creates a set of values which is matched in a certain way to what would be expected had the original set of data values arisen from a normal distribution.
Difference in differences (sometimes abbreviated DID or DD) is a statistical technique used in econometrics and quantitative research in the social sciences that attempts to mimic an experimental research design using observational study data, by studying the differential effect of a treatment on a 'treatment group' versus a 'control group' in a natural experiment. It calculates the effect of a treatment (i.e., an explanatory variable or an independent variable) on an outcome (i.e., a response variable or dependent variable) by comparing the average change over time in the outcome variable for the treatment group, compared to the average change over time for the control group. Although it is intended to mitigate the effects of extraneous factors and selection bias, depending on how the treatment group is chosen, this method may still be subject to certain biases (e.g. mean regression, reverse causality and omitted variable bias). In contrast to a time-series estimate of the treatment effect on subjects (which analyzes differences over time) or a cross-section estimate of the treatment effect (which measures the difference between treatment and control groups), difference in differences uses panel data to measure the differences, between the treatment and control group, of the changes in the outcome variable that occur over time.
The sensitivity index or d' (pronounced 'dee-prime') is a statistic used in signal detection theory. It provides the separation between the means of the signal and the noise distributions, compared against the standard deviation of the signal plus noise distributions. For normally distributed signal and noise with mean and standard deviations  and , and  and , respectively, d' is defined as:  An estimate of d' can be also found from measurements of the hit rate and false-alarm rate. It is calculated as:  d' = Z(hit rate)   Z(false alarm rate),  where function Z(p), p   [0,1], is the inverse of the cumulative distribution function of the Gaussian distribution. d' is a dimensionless statistic. A higher d' indicates that the signal can be more readily detected.
The Rand index or Rand measure (named after William M. Rand) in statistics, and in particular in data clustering, is a measure of the similarity between two data clusterings. A form of the Rand index may be defined that is adjusted for the chance grouping of elements, this is the adjusted Rand index. From a mathematical standpoint, Rand index is related to the accuracy, but is applicable even when class labels are not used.  
In mathematics, a Feller-continuous process is a continuous-time stochastic process for which the expected value of suitable statistics of the process at a given time in the future depend continuously on the initial condition of the process. The concept is named after Croatian-American mathematician William Feller.
In probability theory and statistics, a copula is a multivariate probability distribution for which the marginal probability distribution of each variable is uniform. Copulas are used to describe the dependence between random variables. Their name comes from the Latin for "link" or "tie", similar but unrelated to grammatical copulas in linguistics. Sklar's Theorem states that any multivariate joint distribution can be written in terms of univariate marginal distribution functions and a copula which describes the dependence structure between the variables. Copulas are popular in high-dimensional statistical applications as they allow one to easily model and estimate the distribution of random vectors by estimating marginals and copulae separately. There are many parametric copula families available, which usually have parameters that control the strength of dependence. Some popular parametric copula models are outlined below.
In statistics, cumulative distribution function (CDF)-based nonparametric confidence intervals are a general class of confidence intervals around statistical functionals of a distribution. To calculate these confidence intervals, all that is required is an independently and identically distributed (iid) sample from the distribution and known bounds on the support of the distribution. The latter requirement simply means that all the nonzero probability mass of the distribution must be contained in some known interval .
Combinatorial data analysis (CDA) is the study of data sets where the arrangement of objects is important. CDA can be used either to determine how well a given combinatorial construct reflects the observed data, or to search for a suitable combinatorial construct that does fit the data.
In econometrics, autoregressive conditional heteroscedasticity(ARCH) models are used to characterize and model time series. They are used at any point in a series where the error terms are thought to have a characteristic size or variance. In particular ARCH models assume the variance of the current error term or innovation to be a function of the actual sizes of the previous time periods' error terms: often the variance is related to the squares of the previous innovations. Such models are often called ARCH models (Engle, 1982), although a variety of other acronyms are applied to particular structures that have a similar basis. ARCH models are commonly employed in modeling financial time series that exhibit time-varying volatility clustering, i.e. periods of swings interspersed with periods of relative calm. ARCH-type models are sometimes considered to be in the family of stochastic volatility models, although this is strictly incorrect since at time t the volatility is completely pre-determined (deterministic) given previous values.
The following is a list of national and international statistical services.
In statistics, the Kolmogorov Smirnov test (K S test or KS test) is a nonparametric test of the equality of continuous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K S test), or to compare two samples (two-sample K S test). The Kolmogorov Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case). In each case, the distributions considered under the null hypothesis are continuous distributions but are otherwise unrestricted. The two-sample K S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The Kolmogorov Smirnov test can be modified to serve as a goodness of fit test. In the special case of testing for normality of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic: see below. Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the Shapiro Wilk test or Anderson Darling test. However, other tests have their own disadvantages. For instance the Shapiro Wilk test is known not to work well with many ties (many identical values).
Inverse transform sampling (also known as inversion sampling, the inverse probability integral transform, the inverse transformation method, Smirnov transform, golden rule,) is a basic method for pseudo-random number sampling, i.e. for generating sample numbers at random from any probability distribution given its cumulative distribution function. Inverse transformation sampling takes uniform samples of a number  between 0 and 1, interpreted as a probability, and then return the largest number  from the domain of the distribution  such that . For example, imagine that  is the standard normal distribution with mean zero and standard deviation one. The table below shows samples taken from the uniform distribution and their representation on the standard normal distribution.  We are randomly choosing a proportion of the area under the curve and returning the number in the domain such that exactly this proportion of the area occurs to the left of that number. Intuitively, we are unlikely to choose a number in the far end of tails because there is very little area in them which would require choosing a number very close to zero or one. Computationally, this method involves computing the quantile function of the distribution   in other words, computing the cumulative distribution function (CDF) of the distribution (which maps a number in the domain to a probability between 0 and 1) and then inverting that function. This is the source of the term "inverse" or "inversion" in most of the names for this method. Note that for a discrete distribution, computing the CDF is not in general too difficult: We simply add up the individual probabilities for the various points of the distribution. For a continuous distribution, however, we need to integrate the probability density function (PDF) of the distribution, which is impossible to do analytically for most distributions (including the normal distribution). As a result, this method may be computationally inefficient for many distributions and other methods are preferred; however, it is a useful method for building more generally applicable samplers such as those based on rejection sampling. For the normal distribution, the lack of an analytical expression for the corresponding quantile function means that other methods (e.g. the Box Muller transform) may be preferred computationally. It is often the case that, even for simple distributions, the inverse transform sampling method can be improved on: see, for example, the ziggurat algorithm and rejection sampling. On the other hand, it is possible to approximate the quantile function of the normal distribution extremely accurately using moderate-degree polynomials, and in fact the method of doing this is fast enough that inversion sampling is now the default method for sampling from a normal distribution in the statistical package R.
In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a conditional distribution, which gives the probabilities contingent upon the values of the other variables. The term marginal variable is used to refer to those variables in the subset of variables being retained. These terms are dubbed "marginal" because they used to be found by summing values in a table along rows or columns, and writing the sum in the margins of the table. The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out. The context here is that the theoretical studies being undertaken, or the data analysis being done, involves a wider set of random variables but that attention is being limited to a reduced number of those variables. In many applications an analysis may start with a given collection of random variables, then first extend the set by defining new ones (such as the sum of the original random variables) and finally reduce the number by placing interest in the marginal distribution of a subset (such as the sum). Several different analyses may be done, each treating a different subset of variables as the marginal variables.
In probability theory and statistics, the generalized extreme value (GEV) distribution is a family of continuous probability distributions developed within extreme value theory to combine the Gumbel, Fre chet and Weibull families also known as type I, II and III extreme value distributions. By the extreme value theorem the GEV distribution is the only possible limit distribution of properly normalized maxima of a sequence of independent and identically distributed random variables. Note that a limit distribution need not exist: this requires regularity conditions on the tail of the distribution. Despite this, the GEV distribution is often used as an approximation to model the maxima of long (finite) sequences of random variables. In some fields of application the generalized extreme value distribution is known as the Fisher Tippett distribution, named after Ronald Fisher and L. H. C. Tippett who recognised three function forms outlined below. However usage of this name is sometimes restricted to mean the special case of the Gumbel distribution.
In statistics, a sequence or a vector of random variables is homoscedastic / ho mo sk  d st k/ if all random variables in the sequence or vector have the same finite variance. This is also known as homogeneity of variance. The complementary notion is called heteroscedasticity. The spellings homoskedasticity and heteroskedasticity are also frequently used. The assumption of homoscedasticity simplifies mathematical and computational treatment. Serious violations in homoscedasticity (assuming a distribution of data is homoscedastic when in reality it is heteroscedastic / h t ro sk  d st k/) may result in overestimating the goodness of fit as measured by the Pearson coefficient.
In natural sciences and social sciences, quantitative research is the systematic empirical investigation of observable phenomena via statistical, mathematical or computational techniques. The objective of quantitative research is to develop and employ mathematical models, theories and/or hypotheses pertaining to phenomena. The process of measurement is central to quantitative research because it provides the fundamental connection between empirical observation and mathematical expression of quantitative relationships. Quantitative data is any data that is in numerical form such as statistics, percentages, etc. The researcher analyzes the data with the help of statistics. The researcher is hoping the numbers will yield an unbiased result that can be generalized to some larger population. Qualitative research, on the other hand, asks broad questions and collects word data from phenomena or participants. The researcher looks for themes and describes the information in themes and patterns exclusive to that set of participants. In social sciences, quantitative research is widely used in psychology, economics, demography, sociology, marketing, community health, health & human development, gender and political science, and less frequently in anthropology and history. Research in mathematical sciences such as physics is also 'quantitative' by definition, though this use of the term differs in context. In the social sciences, the term relates to empirical methods, originating in both philosophical positivism and the history of statistics, which contrast with qualitative research methods. Qualitative produce information only on the particular cases studied, and any more general conclusions are only hypotheses. Quantitative methods can be used to verify which of such hypotheses are true. A comprehensive analysis of 1274 articles published in the top two American sociology journals between 1935 and 2005 found that roughly two thirds of these articles used quantitative methods.
Probability theory and statistics have some commonly used conventions, in addition to standard mathematical notation and mathematical symbols.
In statistics, the Kendall rank correlation coefficient, commonly referred to as Kendall's tau coefficient (after the Greek letter  ), is a statistic used to measure the ordinal association between two measured quantities. A tau test is a non-parametric hypothesis test for statistical dependence based on the tau coefficient. It is a measure of rank correlation: the similarity of the orderings of the data when ranked by each of the quantities. It is named after Maurice Kendall, who developed it in 1938, though Gustav Fechner had proposed a similar measure in the context of time series in 1897.
Distance sampling is a widely used group of closely related methods for estimating the density and/or abundance of populations. The main methods are based on line transects or point transects. In this method of sampling, the data collected are the distances of the objects being surveyed from these randomly placed lines or points, and the objective is to estimate the average density of the objects within a region.
In probability theory and statistics, the generalized extreme value (GEV) distribution is a family of continuous probability distributions developed within extreme value theory to combine the Gumbel, Fre chet and Weibull families also known as type I, II and III extreme value distributions. By the extreme value theorem the GEV distribution is the only possible limit distribution of properly normalized maxima of a sequence of independent and identically distributed random variables. Note that a limit distribution need not exist: this requires regularity conditions on the tail of the distribution. Despite this, the GEV distribution is often used as an approximation to model the maxima of long (finite) sequences of random variables. In some fields of application the generalized extreme value distribution is known as the Fisher Tippett distribution, named after Ronald Fisher and L. H. C. Tippett who recognised three function forms outlined below. However usage of this name is sometimes restricted to mean the special case of the Gumbel distribution.
CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy. Evolution strategies (ES) are stochastic, derivative-free methods for numerical optimization of non-linear or non-convex continuous optimization problems. They belong to the class of evolutionary algorithms and evolutionary computation. An evolutionary algorithm is broadly based on the principle of biological evolution, namely the repeated interplay of variation (via recombination and mutation) and selection: in each generation (iteration) new individuals (candidate solutions, denoted as ) are generated by variation, usually in a stochastic way, of the current parental individuals. Then, some individuals are selected to become the parents in the next generation based on their fitness or objective function value . Like this, over the generation sequence, individuals with better and better -values are generated. In an evolution strategy, new candidate solutions are sampled according to a multivariate normal distribution in the . Recombination amounts to selecting a new mean value for the distribution. Mutation amounts to adding a random vector, a perturbation with zero mean. Pairwise dependencies between the variables in the distribution are represented by a covariance matrix. The covariance matrix adaptation (CMA) is a method to update the covariance matrix of this distribution. This is particularly useful, if the function  is ill-conditioned. Adaptation of the covariance matrix amounts to learning a second order model of the underlying objective function similar to the approximation of the inverse Hessian matrix in the Quasi-Newton method in classical optimization. In contrast to most classical methods, fewer assumptions on the nature of the underlying objective function are made. Only the ranking between candidate solutions is exploited for learning the sample distribution and neither derivatives nor even the function values themselves are required by the method.
Statistical geography is the study and practice of collecting, analysing and presenting data that has a geographic or areal dimension, such as census or demographics data. It uses techniques from spatial analysis, but also encompasses geographical activities such as the defining and naming of geographical regions for statistical purposes. For example, for the purposes of statistical geography, the Australian Bureau of Statistics uses the Australian Standard Geographical Classification, a hierarchical regionalisation that divides Australia up into states and territories, then statistical divisions, statistical subdivisions, statistical local areas, and finally census collection districts.
In statistics, data transformation is the application of a deterministic mathematical function to each point in a data set   that is, each data point zi is replaced with the transformed value yi = f(zi), where f is a function. Transforms are usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that is to be applied, or to improve the interpretability or appearance of graphs. Nearly always, the function that is used to transform the data is invertible, and generally is continuous. The transformation is usually applied to a collection of comparable measurements. For example, if we are working with data on peoples' incomes in some currency unit, it would be common to transform each person's income value by the logarithm function.
In statistics, a statistic is sufficient with respect to a statistical model and its associated unknown parameter if "no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter". In particular, a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic, as to which of those probability distributions is that of the population from which the sample was taken. Roughly, given a set  of independent identically distributed data conditioned on an unknown parameter , a sufficient statistic is a function  whose value contains all the information needed to compute any estimate of the parameter (e.g. a maximum likelihood estimate). Due to the factorization theorem (see below), for a sufficient statistic , the joint distribution can be written as . From this factorization, it can easily be seen that the maximum likelihood estimate of  will interact with  only through . Typically, the sufficient statistic is a simple function of the data, e.g. the sum of all the data points. More generally, the "unknown parameter" may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified. In such a case, the sufficient statistic may be a set of functions, called a jointly sufficient statistic. Typically, there are as many functions as there are parameters. For example, for a Gaussian distribution with unknown mean and variance, the jointly sufficient statistic, from which maximum likelihood estimates of both parameters can be estimated, consists of two functions, the sum of all data points and the sum of all squared data points (or equivalently, the sample mean and sample variance). The concept, due to Ronald Fisher, is equivalent to the statement that, conditional on the value of a sufficient statistic for a parameter, the joint probability distribution of the data does not depend on that parameter. Both the statistic and the underlying parameter can be vectors. A related concept is that of linear sufficiency, which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic, although it is restricted to linear estimators. The Kolmogorov structure function deals with individual finite data, the related notion there is the algorithmic sufficient statistic. The concept of sufficiency has fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form (see Pitman Koopman Darmois theorem below), but remains very important in theoretical work.
In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions. In other words, a Dirichlet process is a probability distribution whose domain is itself a set of probability distributions. It is often used in Bayesian inference to describe the prior knowledge about the distribution of random variables how likely it is that the random variables are distributed according to one or another particular distribution. The Dirichlet process is specified by a base distribution  and a positive real number  called the concentration parameter. The base distribution is the expected value of the process, i.e., the Dirichlet process draws distributions "around" the base distribution the way a normal distribution draws real numbers around its mean. However, even if the base distribution is continuous, the distributions drawn from the Dirichlet process are almost surely discrete. The concentration parameter specifies how strong this discretization is: in the limit of , the realizations are all concentrated at a single value, while in the limit of  the realizations become continuous. Between the two extremes the realizations are discrete distributions with less and less concentration as  increases. The Dirichlet process can also be seen as the infinite-dimensional generalization of the Dirichlet distribution. In the same way as the Dirichlet distribution is the conjugate prior for the categorical distribution, the Dirichlet process is the conjugate prior for infinite, nonparametric discrete distributions. A particularly important application of Dirichlet processes is as a prior probability distribution in infinite mixture models. The Dirichlet process was formally introduced by Thomas Ferguson in 1973 and has since been applied in data mining and machine learning, among others for natural language processing, computer vision and bioinformatics.
Genetic epidemiology is the study of the role of genetic factors in determining health and disease in families and in populations, and the interplay of such genetic factors with environmental factors. Genetic epidemiology seeks to derive a statistical and quantitive analysis of how genetics work in large groups.
A graphical model or probabilistic graphical model (PGM) is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics particularly Bayesian statistics and machine learning.
The topic of heteroscedasticity-consistent (HC) standard errors arises in statistics and econometrics in the context of linear regression as well as time series analysis. These are also known as White standard errors, Huber White standard errors, and Eicker Huber White standard errors, to recognize the contributions of Friedhelm Eicker, Peter J. Huber, and Halbert White. In regression and time-series modelling, basic forms of models make use of the assumption that the errors or disturbances ui have the same variance across all observation points. When this is not the case, the errors are said to be heteroscedastic, or to have heteroscedasticity, and this behaviour will be reflected in the residuals  estimated from a fitted model. Heteroscedasticity-consistent standard errors are used to allow the fitting of a model that does contain heteroscedastic residuals. The first such approach was proposed by Huber (1967), and further improved procedures have been produced since for cross-sectional data, time-series data and GARCH estimation.
The World Programming System, also known as WPS, is a software product developed by a company called World Programming. WPS allows users to create, edit and run programs written in the language of SAS. The program was the subject of a lawsuit by SAS Institute. The EU Court of Justice ruled in favor of World Programming, stating that the copyright protection does not extend to the software functionality, the programming language used and the format of the data files used by the program. It stated that there is no copyright infringement when a company which does not have access to the source code of a program studies, observes and tests that program to create another program with the same functionality.
A computer experiment or simulation experiment is an experiment used to study a computer simulation, also referred to as an in silico system. This area includes computational physics, computational chemistry, computational biology and other similar disciplines.
In queueing theory, a discipline within the mathematical theory of probability, the M/M/c queue (or Erlang C model) is a multi-server queueing model. In Kendall's notation it describes a system where arrivals form a single queue and are governed by a Poisson process, there are c servers and job service times are exponentially distributed. It is a generalisation of the M/M/1 queue which considers only a single server. The model with infinitely many servers is the M/M/  queue.
In statistics, the standard deviation (SD, also represented by the Greek letter sigma   or s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values. A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. The standard deviation of a random variable, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same units as the data. There are also other measures of deviation from the norm, including mean absolute deviation, which provide different mathematical properties from standard deviation. In addition to expressing the variability of a population, the standard deviation is commonly used to measure confidence in statistical conclusions. For example, the margin of error in polling data is determined by calculating the expected standard deviation in the results if the same poll were to be conducted multiple times. This derivation of a standard deviation is often called the "standard error" of the estimate or "standard error of the mean" when referring to a mean. It is computed as the standard deviation of all the means that would be computed from that population if an infinite number of samples were drawn and a mean for each sample were computed. It is very important to note that the standard deviation of a population and the standard error of a statistic derived from that population (such as the mean) are quite different but related (related by the inverse of the square root of the number of observations). The reported margin of error of a poll is computed from the standard error of the mean (or alternatively from the product of the standard deviation of the population and the inverse of the square root of the sample size, which is the same thing) and is typically about twice the standard deviation the half-width of a 95 percent confidence interval. In science, researchers commonly report the standard deviation of experimental data, and only effects that fall much farther than two standard deviations away from what would have been expected are considered statistically significant normal random error or variation in the measurements is in this way distinguished from causal variation. The standard deviation is also important in finance, where the standard deviation on the rate of return on an investment is a measure of the volatility of the investment. When only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data or to a modified quantity that is a better estimate of the population standard deviation (the standard deviation of the entire population).
Statistical noise is the colloquialism for recognized amounts of unexplained variation in a sample. See errors and residuals in statistics.
In statistics, the Bayesian information criterion (BIC) or Schwarz criterion (also SBC, SBIC) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC). When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC. The BIC was developed by Gideon E. Schwarz and published in a 1978 paper, where he gave a Bayesian argument for adopting it.
In statistics, the generalized canonical correlation analysis (gCCA), is a way of making sense of cross-correlation matrices between the sets of random variables when there are more than two sets. While a conventional CCA generalizes principal component analysis (PCA) to two sets of random variables, a gCCA generalizes PCA to more than two sets of random variables. The canonical variables represent those common factors that can be found by a large PCA of all of the transformed random variables after each set underwent its own PCA.  
Statistical finance, is the application of econophysics to financial markets. Instead of the normative roots of much of the field of finance, it uses a positivist framework including exemplars from statistical physics with an emphasis on emergent or collective properties of financial markets. The starting point for this approach to understanding financial markets are the empirically observed stylized facts.
Sport (UK) or sports (US) are all forms of usually competitive physical activity or games which, through casual or organised participation, aim to use, maintain or improve physical ability and skills while providing enjoyment to participants, and in some cases, entertainment for spectators. Usually the contest or game is between two sides, each attempting to exceed the other. Some sports allow a tie game; others provide tie-breaking methods, to ensure one winner and one loser. A number of such two-sided contests may be arranged in a tournament producing a champion. Many sports leagues make an annual champion by arranging games in a regular sports season, followed in some cases by playoffs. Hundreds of sports exist, from those between single contestants, through to those with hundreds of simultaneous participants, either in teams or competing as individuals. In certain sports such as racing, many contestants may compete, each against all with one winner. Sport is generally recognised as activities which are based in physical athleticism or physical dexterity, with the largest major competitions such as the Olympic Games admitting only sports meeting this definition, and other organisations such as the Council of Europe using definitions precluding activities without a physical element from classification as sports. However, a number of competitive, but non-physical, activities claim recognition as mind sports. The International Olympic Committee (through ARISF) recognises both chess and bridge as bona fide sports, and SportAccord, the international sports federation association, recognises five non-physical sports, although limits the number of mind games which can be admitted as sports. Sports are usually governed by a set of rules or customs, which serve to ensure fair competition, and allow consistent adjudication of the winner. Winning can be determined by physical events such as scoring goals or crossing a line first. It can also be determined by judges who are scoring elements of the sporting performance, including objective or subjective measures such as technical performance or artistic impression. Records of performance are often kept, and for popular sports, this information may be widely announced or reported in sport news. Sport is also a major source of entertainment for non-participants, with spectator sport drawing large crowds to sport venues, and reaching wider audiences through broadcasting. Sports betting is in some cases severely regulated, and in some cases is central to the sport. According to A.T. Kearney, a consultancy, the global sporting industry is worth up to $620 billion as of 2013. The world's most accessible and practised sport is running, while association football is its most popular spectator sport.
In statistics, and especially Bayesian statistics, the posterior predictive distribution is the distribution of unobserved observations (prediction) conditional on the observed data. Described as the distribution that a new i.i.d. data point  would have, given a set of N existing i.i.d. observations  . In a frequentist context, this might be derived by computing the maximum likelihood estimate (or some other estimate) of the parameter(s) given the observed data, and then plugging them into the distribution function of the new observations. However, the concept of posterior predictive distribution is normally used in a Bayesian context, where it makes use of the entire posterior distribution of the parameter(s) given the observed data to yield a probability distribution over an interval rather than simply a point estimate. Specifically, it is computed by marginalising over the parameters, using the posterior distribution:  where  represents the parameter(s) and  the hyperparameter(s). Any of  may be vectors (or equivalently, may stand for multiple parameters). Note that in many cases,  is independent of  given . Note that this is equivalent to the expected value of the distribution of the new data point, when the expectation is taken over the posterior distribution, i.e.:  (To get an intuition for this, keep in mind that expected value is a type of average. The predictive probability of seeing a particular value of a new observation will vary depending on the parameters of the distribution of the observation. In this case, we don't know the exact value of the parameters, but we have a posterior distribution over them, that specifies what we believe the parameters to be, given the data we've already seen. Logically, then, to get "the" predictive probability, we should average all of the various predictive probabilities over the different possible parameter values, weighting them according to how strongly we believe in them. This is exactly what this expected value does. Compare this to the approach in frequentist statistics, where a single estimate of the parameters, e.g. a maximum likelihood estimate, would be computed, and this value plugged in. This is equivalent to averaging over a posterior distribution with no variance, i.e. where we are completely certain of the parameter having a single value. The result is weighted too strongly towards the mode of the posterior, and takes no account of other possible values, unlike in the Bayesian approach.)
In atmospheric sciences and some other applications of statistics, an anomaly time series is the time series of deviations of a quantity from some mean. Similarly a standardized anomaly series contains values of deviations divided by a standard deviation. Location and scale measures that are resistant to the effects of outliers are sometimes used as the basis of the transformation. The location and scale parameters used in forming an anomaly time-series may either be constant or may themselves be time series. For example, if the original time series consisted of temperatures measured every hour, the effect of typical daily cycles of temperature might be remove by subtracting a time series containing mean temperature values for each hour of the day: clearly, this can be extended by including seasonal variations of temperature. In the atmospheric sciences, the climatological annual cycle is often used as the mean value. Famous atmospheric anomaly time series are for instance the Southern Oscillation index (SOI) and the North Atlantic oscillation index. SOI is the atmospheric component of El Nin o, while NAO plays an important role for European weather by modification of the exit of the Atlantic storm track.
In probability and statistics, the generalized beta distribution is a continuous probability distribution with five parameters, including more than thirty named distributions as limiting or special cases. It has been used in the modeling of income distribution, stock returns, as well as in regression analysis. The exponential generalized Beta (EGB) distribution follows directly from the GB and generalizes other common distributions.  
An ABX test is a method of comparing two choices of sensory stimuli to identify detectable differences between them. A subject is presented with two known samples (sample A, the first reference, and sample B, the second reference) followed by one unknown sample X that is randomly selected from either A or B. The subject is then required to identify X as either A or B. If X cannot be identified reliably with a low p-value in a predetermined number of trials, then the null hypothesis cannot be rejected and it cannot be proven that there is a perceptible difference between A and B. ABX tests can easily be performed as double-blind trials, eliminating any possible unconscious influence from the researcher or the test supervisor. Because samples A and B are provided just prior to sample X, the difference does not have to be discerned from assumption based on long-term memory or past experience. Thus, the ABX test answers whether or not, under ideal circumstances, a perceptual difference can be found. ABX tests are commonly used in evaluations of digital audio data compression methods; sample A is typically an uncompressed sample, and sample B is a compressed version of A. Audible compression artifacts that indicate a shortcoming in the compression algorithm can be identified with subsequent testing. ABX tests can also be used to compare the different degrees of fidelity loss between two different audio formats at a given bitrate. ABX tests can be used to audition input, processing, and output components as well as cabling: virtually any audio product or prototype design.
In queueing theory, a discipline within the mathematical theory of probability, traffic equations are equations that describe the mean arrival rate of traffic, allowing the arrival rates at individual nodes to be determined. Mitrani notes "if the network is stable, the traffic equations are valid and can be solved."
In probability theory, optional stopping theorem (or Doob's optional sampling theorem) says that, under certain conditions, the expected value of a martingale at a stopping time is equal to the expected value of its initial value. Since martingales can be used to model the wealth of a gambler participating in a fair game, the optional stopping theorem says that on the average nothing can be gained by stopping to play the game based on the information obtainable so far (i.e., by not looking into the future). Of course, certain conditions are necessary for this result to hold true, in particular doubling strategies have to be excluded. The optional stopping theorem is an important tool of mathematical finance in the context of the fundamental theorem of asset pricing.
In statistics, Levene's test is an inferential statistic used to assess the equality of variances for a variable calculated for two or more groups. Some common statistical procedures assume that variances of the populations from which different samples are drawn are equal. Levene's test assesses this assumption. It tests the null hypothesis that the population variances are equal (called homogeneity of variance or homoscedasticity). If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. Some of the procedures typically assuming homoscedasticity, for which one can use Levene's tests, include analysis of variance and t-tests. Levene's test is often used before a comparison of means. When Levene's test shows significance, one should switch to more generalized tests that is free from homoscedasticity assumptions (sometimes even non-parametric tests). Levene's test may also be used as a main test for answering a stand-alone question of whether two sub-samples in a given population have equal or different variances.
Data dredging (also data fishing, data snooping, equation fitting and p-hacking) is the use of data mining to uncover patterns in data that can be presented as statistically significant, without first devising a specific hypothesis as to the underlying causality. The process of data mining involves automatically testing huge numbers of hypotheses about a single data set by exhaustively searching for combinations of variables that might show a correlation. Conventional tests of statistical significance are based on the probability that an observation arose by chance, and necessarily accept some risk of mistaken test results, called the significance. When large numbers of tests are performed, some produce false results, hence 5% of randomly chosen hypotheses turn out to be significant at the 5% level, 1% turn out to be significant at the 1% significance level, and so on, by chance alone. When enough hypotheses are tested, it is virtually certain that some falsely appear statistically significant, since almost every data set with any degree of randomness is likely to contain some spurious correlations. If they are not cautious, researchers using data mining techniques can be easily misled by these apparently significant results. The multiple comparisons hazard is common in data dredging. Moreover, subgroups are sometimes explored without alerting the reader to the number of questions at issue, which can lead to misinformed conclusions.
In phylogenetics, maximum parsimony is an optimality criterion under which the phylogenetic tree that minimizes the total number of character-state changes is to be preferred. Under the maximum-parsimony criterion, the optimal tree will minimize the amount of homoplasy (i.e., convergent evolution, parallel evolution, and evolutionary reversals). In other words, under this criterion, the shortest possible tree that explains the data is considered best. The principle is akin to Occam's razor, which states that all else being equal the simplest hypothesis that explains the data should be selected. Some of the basic ideas behind maximum parsimony were presented by James S. Farris  in 1970 and Walter M. Fitch in 1971. Maximum parsimony is an intuitive and simple criterion, and it is popular for this reason. However, although it is easy to score a phylogenetic tree (by counting the number of character-state changes), there is no algorithm to quickly generate the most-parsimonious tree. Instead, the most-parsimonious tree must be found in "tree space" (i.e., amongst all possible trees). For a small number of taxa (i.e., fewer than nine) it is possible to do an exhaustive search, in which every possible tree is scored, and the best one is selected. For nine to twenty taxa, it will generally be preferable to use branch-and-bound, which is also guaranteed to return the best tree. For greater numbers of taxa, a heuristic search must be performed. Because the most-parsimonious tree is always the shortest possible tree, this means that in comparison to the "true" tree that actually describes the evolutionary history of the organisms under study the "best" tree according to the maximum-parsimony criterion will often underestimate the actual evolutionary change that has occurred. In addition, maximum parsimony is not statistically consistent. That is, it is not guaranteed to produce the true tree with high probability, given sufficient data. As demonstrated in 1978 by Joe Felsenstein, maximum parsimony can be inconsistent under certain conditions, such as long-branch attraction.
An Essay towards solving a Problem in the Doctrine of Chances is a work on the mathematical theory of probability by the Reverend Thomas Bayes, published in 1763, two years after its author's death, and containing multiple amendments and additions due to his friend Richard Price. The title comes from the contemporary use of the phrase "doctrine of chances" to mean the theory of probability, which had been introduced via the title of a book by Abraham de Moivre. Contemporary reprints of the Essay carry a more specific and significant title: A Method of Calculating the Exact Probability of All Conclusions founded on Induction. The Essay includes theorems of conditional probability which form the basis of what is now called Bayes's Theorem, together with a detailed treatment of the problem of setting a prior probability. Bayes supposed a sequence of independent experiments, each having as its outcome either success or failure, the probability of success being some number p between 0 and 1. But then he supposed p to be an uncertain quantity, whose probability of being in any interval between 0 and 1 is the length of the interval. In modern terms, p would be considered a random variable uniformly distributed between 0 and 1. Conditionally on the value of p, the trials resulting in success or failure are independent, but unconditionally (or "marginally") they are not. That is because if a large number of successes are observed, then p is more likely to be large, so that success on the next trial is more probable. The question Bayes addressed was: what is the conditional probability distribution of p, given the numbers of successes and failures so far observed. The answer is that its probability density function is  (and  (p) = 0 for p < 0 or p > 1) where k is the number of successes so far observed, and n is the number of trials so far observed. This is what today is called the Beta distribution with parameters k + 1 and n   k + 1.
In statistics, a latent class model (LCM) relates a set of observed (usually discrete) multivariate variables to a set of latent variables. It is a type of latent variable model. It is called a latent class model because the latent variable is discrete. A class is characterized by a pattern of conditional probabilities that indicate the chance that variables take on certain values. Latent Class Analysis (LCA) is a subset of structural equation modeling, used to find groups or subtypes of cases in multivariate categorical data. These subtypes are called "latent classes". Confronted with a situation as follows, a researcher might choose to use LCA to understand the data: Imagine that symptoms a-d have been measured in a range of patients with diseases X Y and Z, and that disease X is associated with the presence of symptoms a, b, and c, disease Y with symptoms b, c, d, and disease Z with symptoms a, c and d. The LCA will attempt to detect the presence of latent classes (the disease entities), creating patterns of association in the symptoms. As in factor analysis, the LCA can also be used to classify case according to their maximum likelihood class membership. Because the criterion for solving the LCA is to achieve latent classes within which there is no longer any association of one symptom with another (because the class is the disease which causes their association, and the set of diseases a patient has (or class a case is a member of) causes the symptom association, the symptoms will be "conditionally independent", i.e., conditional on class membership, they are no longer related.
The 5-parameter Fisher Bingham distribution or Kent distribution, named after Ronald Fisher, Christopher Bingham, and John T. Kent, is a probability distribution on the two-dimensional unit sphere  in  . It is the analogue on the two-dimensional unit sphere of the bivariate normal distribution with an unconstrained covariance matrix. The distribution belongs to the field of directional statistics. The Kent distribution was proposed by John T. Kent in 1982, and is used in geology, bioinformatics. The probability density function  of the Kent distribution is given by:  where  is a three-dimensional unit vector and the normalizing constant  is:  Where  is the modified Bessel function. Note that  and , the normalizing constant of the Von Mises Fisher distribution. The parameter  (with  ) determines the concentration or spread of the distribution, while  (with  ) determines the ellipticity of the contours of equal probability. The higher the  and  parameters, the more concentrated and elliptical the distribution will be, respectively. Vector  is the mean direction, and vectors  are the major and minor axes. The latter two vectors determine the orientation of the equal probability contours on the sphere, while the first vector determines the common center of the contours. The 3 3 matrix  must be orthogonal.
The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall.  Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless. In an example predictive model for an insurance fraud application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the validation sample have been carefully checked and it is known which cases are fraudulent. To analyze the quality of the model, the insurance uses the table of confusion. The definition of accuracy, the table of confusion for model M1Fraud, and the calculation of accuracy for model M1Fraud is shown below.  where TN is the number of true negative cases FP is the number of false positive cases FN is the number of false negative cases TP is the number of true positive cases Formula 1: Definition of Accuracy Table 1: Table of Confusion for Fraud Model M1Fraud.  Formula 2: Accuracy for model M1Fraud With an accuracy of 98.0% model M1Fraud appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting "no fraud". The table of confusion and the accuracy for this trivial  always predict negative  model M2Fraud and the accuracy of this model are shown below. Table 2: Table of Confusion for Fraud Model M2Fraud.  Formula 3: Accuracy for model M2Fraud Model M2Fraudreduces the rate of inaccurate predictions from 2% to 1.5%. This is an apparent improvement of 25%. The new model M2Fraud shows fewer incorrect predictions and markedly improved accuracy, as compared to the original model M1Fraud, but is obviously useless. The alternative model M2Fraud does not offer any value to the company for preventing fraud. The less accurate model is more useful than the more accurate model. Model improvements should not be measured in terms of accuracy gains. It may be going too far to say that accuracy is irrelevant, but caution is advised when using accuracy in the evaluation of predictive models.  
The acceptable quality limit (AQL) is the worst tolerable process average (mean) in percentage or ratio that is still considered acceptable; that is, it is at an acceptable quality level. Closely related terms are the rejectable quality limit and rejectable quality level (RQL). In a quality control procedure, a process is said to be at an acceptable quality level if the appropriate statistic used to construct a control chart does not fall outside the bounds of the acceptable quality limits. Otherwise, the process is said to be at a rejectable control level. In 2008 the usage of the abbreviation AQL for the term "acceptable quality limit" was changed in the standards issued by at least one national standards organization (ANSI/ASQ) to relate to the term "acceptance quality level". It is unclear whether this interpretation will be brought into general usage, but the underlying meaning remains the same. An acceptable quality level is a test and/or inspection standard that prescribes the range of the number of defective components that is considered acceptable when random sampling those components during an inspection. The defects found during an electronic or electrical test, or during a physical (mechanical) inspection, are sometimes classified into three levels: critical, major and minor. Critical defects are those that render the product unsafe or hazardous for the end user or that contravene mandatory regulations. Major defects can result in the product's failure, reducing its marketability, usability or saleability. Lastly, minor defects do not affect the product's marketability or usability, but represent workmanship defects that make the product fall short of defined quality standards. Different companies maintain different interpretations of each defect type. In order to avoid argument, buyers and sellers agree on an AQL standard, chosen according to the level of risk each party assumes, which they use as a reference during pre-shipment inspection.
The variation ratio is a simple measure of statistical dispersion in nominal distributions; it is the simplest measure of qualitative variation. It is defined as the proportion of cases which are not the mode:  where fm is the frequency (number of cases) of the mode, and N is the total number of cases. While a simple measure, it is notable in that some texts and guides suggest or imply that the dispersion of nominal measurements cannot be ascertained. It is defined for instance by (Freeman 1965). Just as with the range or standard deviation, the larger the variation ratio, the more differentiated or dispersed the data are; and the smaller the variation ratio, the more concentrated and similar the data are. For example, a group which is 55% female and 45% male has a proportion of 0.55 females and therefore variation ratio of (1.0- 0.55) = 0.45; and is more dispersed in terms of gender than a group which is 95% female and has a variation ratio of only 0.05. Similarly, a group which is 25% Catholic (where Catholic is the modal religious preference) has a variation ratio of 0.75 and is much more dispersed religiously than a group which is 85% Catholic and has a variation ratio of only 0.15.
Spatial analysis or spatial statistics includes any of the formal techniques which study entities using their topological, geometric, or geographic properties. Spatial analysis includes a variety of techniques, many still in their early development, using different analytic approaches and applied in fields as diverse as astronomy, with its studies of the placement of galaxies in the cosmos, to chip fabrication engineering, with its use of "place and route" algorithms to build complex wiring structures. In a more restricted sense, spatial analysis is the technique applied to structures at the human scale, most notably in the analysis of geographic data. Complex issues arise in spatial analysis, many of which are neither clearly defined nor completely resolved, but form the basis for current research. The most fundamental of these is the problem of defining the spatial location of the entities being studied. Classification of the techniques of spatial analysis is difficult because of the large number of different fields of research involved, the different fundamental approaches which can be chosen, and the many forms the data can take.
In medicine, a diagnostic test is any kind of medical test performed to aid in the diagnosis or detection of disease, injury or any other medical condition. For example, such a test may be used to confirm that a person is free from disease, or to fully diagnose a disease, including to sub-classify it regarding severity and susceptibility to treatment. Companion diagnostics have also been developed to preselect patients for specific treatments based on their own biology, where such targeted therapy may hold promise in personalized treatment of diseases such as cancer. A drug test can be a specific medical test to ascertain the presence of a certain drug in the body (for example, in drug addicts).
In probability theory, the Borel Cantelli lemma is a theorem about sequences of events. In general, it is a result in measure theory. It is named after E mile Borel and Francesco Paolo Cantelli, who gave statement to the lemma in the first decades of the 20th century. A related result, sometimes called the second Borel Cantelli lemma, is a partial converse of the first Borel Cantelli lemma. The lemma states that, under certain conditions, an event will have probability either zero or one. As such, it is the best-known of a class of similar theorems, known as zero-one laws. Other examples include the Kolmogorov 0-1 law and the Hewitt Savage zero-one law.  
Extreme value theory or extreme value analysis (EVA) is a branch of statistics dealing with the extreme deviations from the median of probability distributions. It seeks to assess, from a given ordered sample of a given random variable, the probability of events that are more extreme than any previously observed. Extreme value analysis is widely used in many disciplines, such as structural engineering, finance, earth sciences, traffic prediction, and geological engineering. For example, EVA might be used in the field of hydrology to estimate the probability of an unusually large flooding event, such as the 100-year flood. Similarly, for the design of a breakwater, a coastal engineer would seek to estimate the 50-year wave and design the structure accordingly.
In descriptive statistics and chaos theory, a recurrence plot (RP) is a plot showing, for a given moment in time, the times at which a phase space trajectory visits roughly the same area in the phase space. In other words, it is a graph of  showing  on a horizontal axis and  on a vertical axis, where  is a phase space trajectory.
Probable error has two meanings in statistics. The first meaning is a value describing the probability distribution of a given quantity. It defines the half-range of an interval about a central point for the distribution, such that half of the values from the distribution will lie within the interval and half outside. Thus it is equivalent to half the interquartile range, or the median absolute deviation. The term also has an older meaning (sometimes stated as the only meaning), that has been deprecated for some time: it is denoted   and defined as a fixed multiple of the standard deviation,  , where the multiplying factor derives from the normal distribution, more specifically,  Clearly this latter definition requires that at least the second moment of the distribution should exist, whereas the first definition does not. One use of the term probable error in statistics is as the name for the scale parameter of the Cauchy distribution. A third meaning exists in the context of measurement theory and practice, where the probable error of a measurement made on an instrument having a scale, is defined as being one-half of the finest division on that scale. The implication would be that nearly all measurement would be in the range defined by this version of probable error.
Mathematica is a symbolic mathematical computation program, sometimes called a computer algebra program, used in many scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. The Wolfram Language is the programming language used in Mathematica.
Queueing theory is the mathematical study of waiting lines, or queues. In queueing theory a model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service. Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing and the design of factories, shops, offices and hospitals.
In statistics, the inverse Wishart distribution, also called the inverted Wishart distribution, is a probability distribution defined on real-valued positive-definite matrices. In Bayesian statistics it is used as the conjugate prior for the covariance matrix of a multivariate normal distribution. We say  follows an inverse Wishart distribution, denoted as , if its inverse  has a Wishart distribution . Important identities have been derived for Inverse-Wishart distribution.
In the comparison of various statistical procedures, efficiency is a measure of the optimality of an estimator, of an experimental design, or of a hypothesis testing procedure. Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance. This article primarily deals with efficiency of estimators. The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional "best possible" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure. Efficiencies are often defined using the variance or mean square error as the measure of desirability.
Correction for attenuation is a statistical procedure, due to Spearman (1904), to "rid a correlation coefficient from the weakening effect of measurement error" (Jensen, 1998), a phenomenon also known as regression dilution. In measurement and statistics, it is also called disattenuation. The correlation between two sets of parameters or measurements is estimated in a manner that accounts for measurement error contained within the estimates of those parameters.
In science, randomized experiments are the experiments that allow the greatest reliability and validity of statistical estimates of treatment effects. Randomization-based inference is especially important in experimental design and in survey sampling.
Detrended correspondence analysis (DCA) is a multivariate statistical technique widely used by ecologists to find the main factors or gradients in large, species-rich but usually sparse data matrices that typify ecological community data. For example, Hill and Gauch (1980, p. 55) analyse the data of a vegetation survey of southeast England including 876 species in 3270 releve s. After eliminating outliers, DCA is able to identify two main axes: The first axis goes from dry to wet conditions, and the second axis from woodland to weed communities.
A unit in a statistical analysis refers to one member of a set of entities being studied. It is the material source for the mathematical abstraction of a "random variable". Common examples of a unit would be a single person, animal, plant, or manufactured item that belongs to a larger collection of such entities being studied. Units are often referred to as being either experimental units, sampling units or, more generally, units of observation: An "experimental unit" is typically thought of as one member of a set of objects that are initially equivalent, with each object then subjected to one of several experimental treatments. A "sampling unit" is typically thought of as an object that has been sampled from a statistical population. This term is commonly used in opinion polling and survey sampling. In most statistical studies, the goal is to generalize from the observed units to a larger set consisting of all comparable units that exist but are not directly observed. For example, if we randomly sample 100 people and ask them which candidate they intend to vote for in an election, our main interest is in the voting behavior of all eligible voters, not exclusively on the 100 observed units. In some cases, the observed units may not form a sample from any meaningful population, but rather constitute a convenience sample, or may represent the entire population of interest. In this situation, we may study the units descriptively, or we may study their dynamics over time. But it typically does not make sense to talk about generalizing to a larger population of such units. Studies involving countries or business firms are often of this type. Clinical trials also typically use convenience samples, however the aim is often to make inferences about the efficacy of treatments in other patients, and given the inclusion and exclusion criteria for some clinical trials, the sample may not be representative of the majority of patients with the condition or disease. In simple data sets, the units are in one-to-one correspondence with the data values. In more complex data sets, multiple measurements are made for each unit. For example, if blood pressure measurements are made daily for a week on each subject in a study, there would be seven data values for each statistical unit. Multiple measurements taken on an individual are not independent (they will be more alike compared to measurements taken on different individuals). Ignoring these dependencies during the analysis can lead to an inflated sample size or pseudoreplication. While a unit is often the lowest level at which observations are made, in some cases, a unit can be further decomposed as a statistical assembly. Many statistical analyses use quantitative data that have units of measurement. This is a distinct and non-overlapping use of the term "unit."
The partition of sums of squares is a concept that permeates much of inferential statistics and descriptive statistics. More properly, it is the partitioning of sums of squared deviations or errors. Mathematically, the sum of squared deviations is an unscaled, or unadjusted measure of dispersion (also called variability). When scaled for the number of degrees of freedom, it estimates the variance, or spread of the observations about their mean value. Partitioning of the sum of squared deviations into various components allows the overall variability in a dataset to be ascribed to different types or sources of variability, with the relative importance of each being quantified by the size of each component of the overall sum of squares.
In statistics the frequency (or absolute frequency) of an event  is the number  of times the event occurred in an experiment or study. These frequencies are often graphically represented in histograms.
The Nuremberg Code is a set of research ethics principles for human experimentation set as a result of the Subsequent Nuremberg Trials at the end of the Second World War.
In mathematics, a Bessel process, named after Friedrich Bessel, is a type of stochastic process.
In time series analysis, the lag operator or backshift operator operates on an element of a time series to produce the previous element. For example, given some time series  then  for all  or equivalently  for all  where L is the lag operator. Sometimes the symbol B for backshift is used instead. Note that the lag operator can be raised to arbitrary integer powers so that  and
Mortality rate, or death rate, is a measure of the number of deaths (in general, or due to a specific cause) in a particular population, scaled to the size of that population, per unit of time. Mortality rate is typically expressed in units of deaths per 1,000 individuals per year; thus, a mortality rate of 9.5 (out of 1,000) in a population of 1,000 would mean 9.5 deaths per year in that entire population, or 0.95% out of the total. It is distinct from "morbidity", a term used to refer to either the prevalence or incidence of a disease, and also from the incidence rate (the number of newly appearing cases of the disease per unit of time).
In mathematics, Gaussian measure is a Borel measure on finite-dimensional Euclidean space Rn, closely related to the normal distribution in statistics. There is also a generalization to infinite-dimensional spaces. Gaussian measures are named after the German mathematician Carl Friedrich Gauss. One reason why Gaussian measures are so ubiquitous in probability theory is the Central Limit Theorem. Loosely speaking, it states that if a random variable X is obtained by summing a large number N of independent random variables of order 1, then X is of order  and its law is approximately Gaussian.
Calibrated probability assessments are subjective probabilities assigned by individuals who have been trained to assess probabilities in a way that historically represents their uncertainty. In other words, when a calibrated person says they are "80% confident" in each of 100 predictions they made, they will get about 80% of them correct. Likewise, they will be right 90% of the time they say they are 90% certain, and so on. Calibration training improves subjective probabilities because most people are either "overconfident" or "under-confident" (usually the former). By practicing with a series of trivia questions, it is possible for subjects to fine-tune their ability to assess probabilities. For example, a subject may be asked: True or False: "A hockey puck fits in a golf hole" Confidence: Choose the probability that best represents your chance of getting this question right... 50% 60% 70% 80% 90% 100%  If a person has no idea whatsoever, they will say they are only 50% confident. If they are absolutely certain they are correct, they will say 100%. But most people will answer somewhere in between. If a calibrated person is asked a large number of such questions, they will get about as many correct as they expected. An uncalibrated person who is systematically overconfident may say they are 90% confident in a large number of questions where they only get 70% of them correct. On the other hand, an uncalibrated person who is systematically underconfident may say they are 50% confident in a large number of questions where they actually get 70% of them correct. Calibration training generally involves taking a battery of such tests. Feedback is provided between tests and the subjects refine their probabilities. Calibration training may also involve learning other techniques that help to compensate for consistent over- or under-confidence. Since subjects are better at placing odds when they pretend to bet money, subjects are taught how to convert calibration questions into a type of betting game which is shown to improve their subjective probabilities. Various collaborative methods have been developed, such as prediction market, so that subjective estimates from multiple individuals can be taken into account. Stochastic modeling methods such as the Monte Carlo method often use subjective estimates from "subject matter experts". However, since research shows that such experts are very likely to be statistically overconfident, the model will tend to underestimate uncertainty and risk. The Applied Information Economics method systematically uses calibration training as part of a decision modeling process.
"Natural parameter" links here. For the usage of this term in differential geometry, see differential geometry of curves. In probability and statistics, an exponential family is a set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, on account of some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. The concept of exponential families is credited to E. J. G. Pitman, G. Darmois, and B. O. Koopman in 1935 36. The term exponential class is sometimes used in place of "exponential family". The exponential families include many of the most common distributions, including the normal, exponential, gamma, chi-squared, beta, Dirichlet, Bernoulli, categorical, Poisson, Wishart, Inverse Wishart and many others. A number of common distributions are exponential families only when certain parameters are considered fixed and known, e.g. binomial (with fixed number of trials), multinomial (with fixed number of trials), and negative binomial (with fixed number of failures). Examples of common distributions that are not exponential families are Student's t, most mixture distributions, and even the family of uniform distributions with unknown bounds. See the section below on examples for more discussion. Consideration of exponential-family distributions provides a general framework for selecting a possible alternative parameterisation of the distribution, in terms of natural parameters, and for defining useful sample statistics, called the natural sufficient statistics of the family. For more information, see below.
In statistics, the concept of being an invariant estimator is a criterion that can be used to compare the properties of different estimators for the same quantity. It is a way of formalising the idea that an estimator should have certain intuitively appealing qualities. Strictly speaking, "invariant" would mean that the estimates themselves are unchanged when both the measurements and the parameters are transformed in a compatible way, but the meaning has been extended to allow the estimates to change in appropriate ways with such transformations. The term equivariant estimator is used in formal mathematical contexts that include a precise description of the relation of the way the estimator changes in response to changes to the dataset and parameterisation: this corresponds to the use of "equivariance" in more general mathematics.
A questionnaire is a research instrument consisting of a series of questions and other prompts for the purpose of gathering information from respondents. Although they are often designed for statistical analysis of the responses, this is not always the case. The questionnaire was invented by the Statistical Society of London in 1838. A copy of the instrument is published in the Journal of the Statistical Society, Volume 1, Issue 1, 1838, pages 5 13. Questionnaires have advantages over some other types of surveys in that they are cheap, do not require as much effort from the questioner as verbal or telephone surveys, and often have standardized answers that make it simple to compile data. However, such standardized answers may frustrate users. Questionnaires are also sharply limited by the fact that respondents must be able to read the questions and respond to them. Thus, for some demographic groups conducting a survey by questionnaire may not be concrete. As a type of survey, questionnaires also have many of the same problems relating to question construction and wording that exist in other types of opinion polls.
Log-linear analysis is a technique used in statistics to examine the relationship between more than two categorical variables. The technique is used for both hypothesis testing and model building. In both these uses, models are tested to find the most parsimonious (i.e., least complex) model that best accounts for the variance in the observed frequencies. (A Pearson's chi-square test could be used instead of log-linear analysis, but that technique only allows for two of the variables to be compared at a time.)
The hierarchical hidden Markov model (HHMM) is a statistical model derived from the hidden Markov model (HMM). In an HHMM each state is considered to be a self-contained probabilistic model. More precisely each state of the HHMM is itself an HHMM. HHMMs and HMMs are useful in many fields, including pattern recognition.
Small area estimation is any of several statistical techniques involving the estimation of parameters for small sub-populations, generally used when the sub-population of interest is included in a larger survey. The term "small area" in this context generally refers to a small geographical area such as a county. It may also refer to a "small domain", i.e. a particular demographic within an area. If a survey has been carried out for the population as a whole (for example, a nation or statewide survey), the sample size within any particular small area may be too small to generate accurate estimates from the data. To deal with this problem, it may be possible to use additional data (such as census records) that exists for these small areas in order to obtain estimates. One of the more common small area models in use today is the 'nested area unit level regression model', first used in 1988 to model corn and soybean crop areas in Iowa. The initial survey data, in which farmers reported the area they had growing either corn or soybeans, was compared to estimates obtained from satellite mapping of the farms. The final model resulting from this for unit/farm 'j' in county 'i' is, where 'y' denotes the reported crop area,  is the regression coefficient, 'x' is the farm-level estimate for either corn or soybean usage from the satellite data and  represents the county-level effect of any area characteristics unaccounted for.
In statistical quality control, the  and R chart is a type of control chart used to monitor variables data when samples are collected at regular intervals from a business or industrial process. The chart is advantageous in the following situations: The sample size is relatively small (say, n   10  and s charts are typically used for larger sample sizes) The sample size is constant Humans must perform the calculations for the chart The "chart" actually consists of a pair of charts: One to monitor the process standard deviation (as approximated by the sample moving range) and another to monitor the process mean, as is done with the  and s and individuals control charts. The  and R chart plots the mean value for the quality characteristic across all units in the sample, , plus the range of the quality characteristic across all units in the sample as follows: R = xmax - xmin. The normal distribution is the basis for the charts and requires the following assumptions: The quality characteristic to be monitored is adequately modeled by a normally distributed random variable The parameters   and   for the random variable are the same for each unit and each unit is independent of its predecessors or successors The inspection procedure is same for each sample and is carried out consistently from sample to sample The control limits for this chart type are:  (lower) and  (upper) for monitoring the process variability  for monitoring the process mean where  and  are the estimates of the long-term process mean and range established during control-chart setup and A2, D3, and D4 are sample size-specific anti-biasing constants. The anti-biasing constants are typically found in the appendices of textbooks on statistical process control. As with the  and s and individuals control charts, the  chart is only valid if the within-sample variability is constant. Thus, the R chart is examined before the  chart; if the R chart indicates the sample variability is in statistical control, then the  chart is examined to determine if the sample mean is also in statistical control. If on the other hand, the sample variability is not in statistical control, then the entire process is judged to be not in statistical control regardless of what the  chart indicates.
SAS (Statistical Analysis System) is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics. SAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.
A timeline of probability and statistics  
In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags. This function plays an important role in data analyses aimed at identifying the extent of the lag in an autoregressive model. The use of this function was introduced as part of the Box Jenkins approach to time series modelling, where by plotting the partial autocorrelative functions one could determine the appropriate lags p in an AR (p) model or in an extended ARIMA (p,d,q) model.
The random walk hypothesis is a financial theory stating that stock market prices evolve according to a random walk and thus cannot be predicted. It is consistent with the efficient-market hypothesis. The concept can be traced to French broker Jules Regnault who published a book in 1863, and then to French mathematician Louis Bachelier whose Ph.D. dissertation titled "The Theory of Speculation" (1900) included some remarkable insights and commentary. The same ideas were later developed by MIT Sloan School of Management professor Paul Cootner in his 1964 book The Random Character of Stock Market Prices. The term was popularized by the 1973 book, A Random Walk Down Wall Street, by Burton Malkiel, a Professor of Economics at Princeton University, and was used earlier in Eugene Fama's 1965 article "Random Walks In Stock Market Prices", which was a less technical version of his Ph.D. thesis. The theory that stock prices move randomly was earlier proposed by Maurice Kendall in his 1953 paper, The Analysis of Economic Time Series, Part 1: Prices.
In statistics, a proper linear model is a linear regression model in which the weights given to the predictor variables are chosen in such a way as to optimize the relationship between the prediction and the criterion. Simple regression analysis is the most common example of a proper linear model. Unit-weighted regression is the most common example of an improper linear model.
A pie chart (or a circle chart) is a circular statistical graphic, which is divided into slices to illustrate numerical proportion. In a pie chart, the arc length of each slice (and consequently its central angle and area), is proportional to the quantity it represents. While it is named for its resemblance to a pie which has been sliced, there are variations on the way it can be presented. The earliest known pie chart is generally credited to William Playfair's Statistical Breviary of 1801. Pie charts are very widely used in the business world and the mass media. However, they have been criticized, and many experts recommend avoiding them, pointing out that research has shown it is difficult to compare different sections of a given pie chart, or to compare data across different pie charts. Pie charts can be replaced in most cases by other plots such as the bar chart, box plot or dot plots. In some very rare instances, pie charts can be found in square form. These kinds of pie charts are sometimes referred to as "pizza charts", as the square resembles the shape of a pizza box. These square pie charts serve the same purpose as circular pie charts and all percentages are taken from a 100% total.
A confidence band is used in statistical analysis to represent the uncertainty in an estimate of a curve or function based on limited or noisy data. Similarly, a prediction band is used to represent the uncertainty about the value of a new data-point on the curve, but subject to noise. Confidence and prediction bands are often used as part of the graphical presentation of results of a regression analysis. Confidence bands are closely related to confidence intervals, which represent the uncertainty in an estimate of a single numerical value. "As confidence intervals, by construction, only refer to a single point, they are narrower (at this point) than a confidence band which is supposed to hold simultaneously at many points."
In statistics, Smooth Transition Autoregressive (STAR) models are typically applied to time series data as an extension of autoregressive models, in order to allow for higher degree of flexibility in model parameters through a smooth transition. Given a time series of data xt, the STAR model is a tool for understanding and, perhaps, predicting future values in this series, assuming that the behaviour of the series changes depending on the value of the transition variable. The transition might depend on the past values of the x series (similar to the SETAR models), or exogenous variables. The model consists of 2 autoregressive (AR) parts linked by the transition function. The model is usually referred to as the STAR(p) models proceeded by the letter describing the transition function (see below) and p is the order of the autoregressive part. Most popular transition function include exponential function and first and second-order logistic functions. They give rise to Logistic STAR (LSTAR) and Exponential STAR (ESTAR) models.  
An ogive (/ o d a v/ OH-jyv) is the roundly tapered end of a two-dimensional or three-dimensional object. Villard de Honnecourt, a 13th-century itinerant master-builder from the Picardy in the north of France, was the first writer to use the word ogive. The OED considers the French term's origin obscure; it might come from the Late Latin obviata, the feminine perfect passive participle of obviare, meaning the one who has met or encountered the other.
In statistics, several scatterplot smoothing methods are available to fit a function through the points of a scatterplot to best represent the relationship between the variables. Scatterplots may be smoothed by fitting a line to the data points in a diagram. This line attempts to display the non-random component of the association between the variables in a 2D scatter plot. Smoothing attempts to separate the non-random behaviour in the data from the random fluctuations, removing or reducing these fluctuations, and allows prediction of the response based value of the explanatory variable. Smoothing is normally accomplished by using any one of the techniques mentioned below. A straight line (simple linear regression) A quadratic or a polynomial curve Local regression Smoothing splines The smoothing curve is chosen so as to provide the best fit in some sense, often defined as the fit that results in the minimum sum of the squared errors (a least squares criterion).
In statistics and research, internal consistency is typically a measure based on the correlations between different items on the same test (or the same subscale on a larger test). It measures whether several items that propose to measure the same general construct produce similar scores. For example, if a respondent expressed agreement with the statements "I like to ride bicycles" and "I've enjoyed riding bicycles in the past", and disagreement with the statement "I hate bicycles", this would be indicative of good internal consistency of the test.
In statistics and business, a long tail of some distributions of numbers is the portion of the distribution having a large number of occurrences far from the "head" or central part of the distribution. The distribution could involve popularities, random numbers of occurrences of events with various probabilities, etc. The term is often used loosely, with no definition or arbitrary definition, but precise definitions are possible. In statistics, the term long-tailed distribution has a narrow technical meaning, and is a subtype of heavy-tailed distribution; see that article for details. Intuitively, a distribution is (right) long-tailed if, for any fixed amount, when a quantity exceeds a high level, it almost certainly exceeds it by at least that amount: big quantities are probably even bigger. Note that statistically, there is no sense of the "long tail" of a distribution, but only the property of a distribution being long-tailed. In business, the term long tail is applied to rank-size distributions or rank-frequency distributions (primarily of popularity), which often form power laws and are thus long-tailed distributions in the statistical sense. This is used to describe the retailing strategy of selling a large number of unique items with relatively small quantities sold of each (the "long tail")   usually in addition to selling fewer popular items in large quantities (the "head"). Sometimes an intermediate category is also included, variously called the body, belly, torso, or middle. The specific cutoff of what part of a distribution is the "long tail" is often arbitrary, but in some cases may be specified objectively; see segmentation of rank-size distributions. The long tail concept has found some ground for application, research, and experimentation. It is a term used in online business, mass media, micro-finance (Grameen Bank, for example), user-driven innovation (Eric von Hippel), and social network mechanisms (e.g. crowdsourcing, crowdcasting, peer-to-peer), economic models, and marketing (viral marketing).
Recurrence period density entropy (RPDE) is a method, in the fields of dynamical systems, stochastic processes, and time series analysis, for determining the periodicity, or repetitiveness of a signal.
Concurrent validity is a type of evidence that can be gathered to defend the use of a test for predicting other outcomes. It is a parameter used in sociology, psychology, and other psychometric or behavioral sciences. Concurrent validity is demonstrated when a test correlates well with a measure that has previously been validated. The two measures may be for the same construct, but more often used for different, but presumably related, constructs. The two measures in the study are taken at the same time. This is in contrast to predictive validity, where one measure occurs earlier and is meant to predict some later measure.  In both cases, the (concurrent) predictive power of the test is analyzed using a simple correlation or linear regression.
In probability and statistics, the K-distribution is a three-parameter family of continuous probability distributions. The distribution arises by compounding two gamma distributions. In each case, a re-parametrization of the usual form of the family of gamma distributions is used, such that the parameters are:  the mean of the distribution, and the usual shape parameter.
In statistics, Redescending M-estimators are  -type M-estimators which have   functions that are non-decreasing near the origin, but decreasing toward 0 far from the origin. Their   functions can be chosen to redescend smoothly to zero, so that they usually satisfy  (x) = 0 for all x with |x| > r, where r is referred to as the minimum rejection point. Due to these properties of the   function, these kinds of estimators are very efficient, have a high breakdown point and, unlike other outlier rejection techniques, they do not suffer from a masking effect. They are efficient because they completely reject gross outliers, and do not completely ignore moderately large outliers (like median).
A correlation inequality is any of a number of inequalities satisfied by the correlation functions of a model. Such inequalities are of particular use in statistical mechanics and in percolation theory. Examples include: Bell's inequality FKG inequality Griffiths inequality, and its generalisation, the Ginibre inequality  ^ Ginibre, J. (1972). "Correlation inequalities in statistical mechanics.". Mathematical aspects of statistical mechanics. Providence, R. I.: Amer. Math. Soc. pp. 27 45. MR 0421547.
A Dynamic Bayesian Network (DBN) is a Bayesian network which relates variables to each other over adjacent time steps. This is often called a Two-Timeslice BN (2TBN) because it says that at any point in time T, the value of a variable can be calculated from the internal regressors and the immediate prior value (time T-1). DBNs were developed by Paul Dagum in the early 1990s when he led research funded by two National Science Foundation grants at Stanford University's Section on Medical Informatics. Dagum developed DBNs to unify and extend traditional linear state-space models such as Kalman filters, linear and normal forecasting models such as ARMA and simple dependency models such as hidden Markov models into a general probabilistic representation and inference mechanism for arbitrary nonlinear and non-normal time-dependent domains. Today, DBNs are common in robotics, and have shown potential for a wide range of data mining applications. For example, they have been used in speech recognition, digital forensics, protein sequencing, and bioinformatics. DBN is a generalization of hidden Markov models and Kalman filters.
Probabilistic causation designates a group of philosophical theories that aim to characterize the relationship between cause and effect using the tools of probability theory. The central idea behind these theories is that causes raise the probabilities of their effects, all else being equal.
In statistics, extensions of Fisher's method are a group of approaches that allow approximately valid statistical inferences to be made when the assumptions required for the direct application of Fisher's method are not valid. Fisher's method is a way of combining the information in the p-values from different statistical tests so as to form a single overall test: this method requires that the individual test statistics (or, more immediately, their resulting p-values) should be statistically independent.
In science, a parameter space is the set of all possible combinations of values for all the different parameters contained in a particular mathematical model. The ranges of values of the parameters may form the axes of a plot, and particular outcomes of the model may be plotted against these axes to illustrate how different regions of the parameter space produce different types of behaviour in the model. Often the parameters are inputs of a function, in which case the technical term for the parameter space is domain of a function. Parameter spaces are particularly useful for describing families of probability distributions that depend on parameters. More generally in science, the term parameter space is used to describe experimental variables. For example, the concept has been used in the science of soccer in the article "Parameter space for successful soccer kicks." In the study, "Success rates are determined through the use of four-dimensional parameter space volumes." In the context of statistics, parameter spaces form the background for parameter estimation. As Ross describes in his book: Parameter space is a subset of p-dimensional space consisting of the set of values of   which are allowable in a particular model. The values may sometimes be constrained, say to the positive quadrant or the unit square, or in case of symmetry, to the triangular region where, say  The idea of intentionally truncating the parameter space has also been advanced elsewhere.
Psychological statistics is the application of formulas, theorems, numbers and laws to psychology. Some of the more common applications include: psychometrics learning theory perception human development abnormal psychology Personality test psychological tests Some of the more commonly used statistical tests in psychology are: Parametric tests Student's t-test analysis of variance (ANOVA) ANCOVA (Analysis of Covariance) MANOVA (Multivariate Analysis of Variance)  regression analysis linear regression hierarchical linear modelling  correlation Pearson product-moment correlation coefficient Spearman's rank correlation coefficient  Non-parametric tests chi-square Mann Whitney U
In probability theory and statistics, the beta rectangular distribution is a probability distribution that is a finite mixture distribution of the beta distribution and the continuous uniform distribution. The support is of the distribution is indicated by the parameters a and b, which are the minimum and maximum values respectively. The distribution provides an alternative to the beta distribution such that it allows more density to be placed at the extremes of the bounded interval of support. Thus it is a bounded distribution that allows for outliers to have a greater chance of occurring than does the beta distribution.
Matching pursuit is a type of sparse approximation which involves finding the "best matching" projections of multidimensional data onto an over-complete dictionary . The basic idea is to represent a signal  from Hilbert space  as a weighted sum of functions  (called atoms) taken from :  where  indexes the atoms that have been chosen, and  a weighting factor (an amplitude) for each atom. Given a fixed dictionary, matching pursuit will first find the one atom that has the biggest inner product with the signal, then subtract the contribution due to that atom, and repeat the process until the signal is satisfactorily decomposed. For comparison, consider the Fourier series representation of a signal - this can be described in the terms given above, where the dictionary is built from sinusoidal basis functions (the smallest possible complete dictionary). The main disadvantage of Fourier analysis in signal processing is that it extracts only global features of signals and does not adapt to analysed signals . By taking an extremely redundant dictionary we can look in it for functions that best match a signal . Finding a representation where most of the coefficients in the sum are close to 0 (sparse representation) is desirable for signal coding and compression.
In statistics, scale analysis is a set of methods to analyse survey data, in which responses to questions are combined to measure a latent variable. These items can be dichotomous (e.g. yes/no, agree/disagree, correct/incorrect) or polytomous (e.g. disagree strongly/disagree/neutral/agree/agree strongly). Any measurement for such data is required to be reliable, valid, and homogeneous with comparable results over different studies.
In statistics, per-comparison error rate (PCER) is the probability of a result in the absence of any formal multiple hypothesis testing correction. Typically, when considering a result under many hypotheses, some tests will give false positives; many statisticians make use of Bonferroni correction, false discovery rate, and other methods to determine the odds of a negative result appearing to be positive.
Social statistics is the use of statistical measurement systems to study human behavior in a social environment. This can be accomplished through polling a group of people, evaluating a subset of data obtained about a group of people, or by observation and statistical analysis of a set of data that relates to people and their behaviors. Social scientists use social statistics for many purposes, including: the evaluation of the quality of services available to a group or organization, analyzing behaviors of groups of people in their environment and special situations, determining the wants of people through statistical sampling.
Logit analysis is a statistical technique used by marketers to assess the scope of customer acceptance of a product, particularly a new product. It attempts to determine the intensity or magnitude of customers' purchase intentions and translates that into a measure of actual buying behaviour. Logit analysis assumes that an unmet need in the marketplace has already been detected, and that the product has been designed to meet that need. The purpose of logit analysis is to quantify the potential sales of that product. It takes survey data on consumers' purchase intentions and converts it into actual purchase probabilities. Logit analysis defines the functional relationship between stated purchase intentions and preferences, and the actual probability of purchase. A preference regression is performed on the survey data. This is then modified with actual historical observations of purchase behavior. The resultant functional relationship defines purchase probability. This is the most useful of the purchase intention/rating translations because explicit measures of confidence level and statistical significance can be calculated. Other purchase intention/rating translations include the preference-rank translation and the intent scale translation. The logit function is the reciprocal function to the sigmoid logistic function.
In mathematics, fuzzy measure theory considers generalized measures in which the additive property is replaced by the weaker property of monotonicity. The central concept of fuzzy measure theory is the fuzzy measure (also capacity, see ) which was introduced by Choquet in 1953 and independently defined by Sugeno in 1974 in the context of fuzzy integrals. There exists a number of different classes of fuzzy measures including plausibility/belief measures; possibility/necessity measures; and probability measures which are a subset of classical measures.
In statistics, confirmatory factor analysis (CFA) is a special form of factor analysis, most commonly used in social research. It is used to test whether measures of a construct are consistent with a researcher's understanding of the nature of that construct (or factor). As such, the objective of confirmatory factor analysis is to test whether the data fit a hypothesized measurement model. This hypothesized model is based on theory and/or previous analytic research. CFA was first developed by Jo reskog and has built upon and replaced older methods of analyzing construct validity such as the MTMM Matrix as described in Campbell & Fiske (1959). In confirmatory factor analysis, the researcher first develops a hypothesis about what factors s/he believes are underlying the measures s/he has used (e.g., "Depression" being the factor underlying the Beck Depression Inventory and the Hamilton Rating Scale for Depression) and may impose constraints on the model based on these a priori hypotheses. By imposing these constraints, the researcher is forcing the model to be consistent with his/her theory. For example, if it is posited that there are two factors accounting for the covariance in the measures, and that these factors are unrelated to one another, the researcher can create a model where the correlation between factor A and factor B is constrained to zero. Model fit measures could then be obtained to assess how well the proposed model captured the covariance between all the items or measures in the model. If the constraints the researcher has imposed on the model are inconsistent with the sample data, then the results of statistical tests of model fit will indicate a poor fit, and the model will be rejected. If the fit is poor, it may be due to some items measuring multiple factors. It might also be that some items within a factor are more related to each other than others. For some applications, the requirement of "zero loadings" (for indicators not supposed to load on a certain factor) has been regarded as too strict. A newly developed analysis method, "exploratory structural equation modeling", specifies hypotheses about the relation between observed indicators and their supposed primary latent factors while allowing for estimation of loadings with other latent factors as well.
In statistics, a rank correlation is any of several statistics that measure the relationship between rankings of different ordinal variables or different rankings of the same variable, where a "ranking" is the assignment of the labels "first", "second", "third", etc. to different observations of a particular variable. A rank correlation coefficient measures the degree of similarity between two rankings, and can be used to assess the significance of the relation between them. For example, two common nonparametric methods of significance that use rank correlation are the Mann Whitney U test and the Wilcoxon signed-rank test.
In statistics, the mid-range or mid-extreme of a set of statistical data values is the arithmetic mean of the maximum and minimum values in a data set, defined as:  The mid-range is the midpoint of the range; as such, it is a measure of central tendency. The mid-range is rarely used in practical statistical analysis, as it lacks efficiency as an estimator for most distributions of interest, because it ignores all intermediate points, and lacks robustness, as outliers change it significantly. Indeed, it is one of the least efficient and least robust statistics. However, it finds some use in special cases: it is the maximally efficient estimator for the center of a uniform distribution, trimmed mid-ranges address robustness, and as an L-estimator, it is simple to understand and compute.
Sparse principal component analysis (sparse PCA) is a specialised technique used in statistical analysis and, in particular, in the analysis of multivariate data sets. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables. Ordinary principal component analysis (PCA) uses a vector space transform to reduce multidimensional data sets to lower dimensions. It finds linear combinations of input variables, and transforms them into new variables (called principal components) that correspond to directions of maximal variance in the data. The number of new variables created by these linear combinations is usually much lower than the number of input variables in the original dataset, while still explaining most of the variance present in the data. A particular disadvantage of ordinary PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables.
In mathematics, the Clark Ocone theorem (also known as the Clark Ocone Haussmann theorem or formula) is a theorem of stochastic analysis. It expresses the value of some function F defined on the classical Wiener space of continuous paths starting at the origin as the sum of its mean value and an Ito  integral with respect to that path. It is named after the contributions of mathematicians J.M.C. Clark (1970), Daniel Ocone (1984) and U.G. Haussmann (1978).
The National Health Interview Survey (NHIS) is an annual, cross-sectional survey intended to provide nationally representative estimates on a wide range of health status and utilization measures among the nonmilitary, noninstitutionalized population of the United States. Each annual data set can be used to examine the disease burden and access to care that individuals and families are currently experiencing in the United States. NHIS is designed by the CDC's National Center for Health Statistics (NCHS)   the government agency tasked to monitor the population's health status and behavior   and administered by the U.S. Census Bureau. NHIS has been administered since 1957, although the core content and questionnaires undergo major revisions every 10 15 years. NHIS allows both governmental and outside researchers to obtain estimates on a variety of health-related topics among either the entire nation or specific demographic groups of the population. Also, since the survey design is cross-sectional rather than longitudinal, health information can be trended for demographic groups and the country as a whole, but not for individuals or families.
In statistics, the rational quadratic covariance function is used in spatial statistics, geostatistics, machine learning, image analysis, and other fields where multivariate statistical analysis is conducted on metric spaces. It is commonly used to define the statistical covariance between measurements made at two points that are d units distant from each other. Since the covariance only depends on distances between points, it is stationary. If the distance is Euclidean distance, the rational quadratic covariance function is also isotropic. The rational quadratic covariance between two points separated by d distance units is given by  where   and k are non-negative parameters of the covariance.
External validity is the validity of generalized (causal) inferences in scientific research, usually based on experiments as experimental validity. In other words, it is the extent to which the results of a study can be generalized to other situations and to other people. Mathematical analysis of external validity concerns a determination of whether generalization across heterogeneous populations is feasible, and devising statistical and computational methods that produce valid generalizations.
Semidefinite embedding (SDE) or maximum variance unfolding (MVU) is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data. MVU can be viewed as a non-linear generalization of Principal component analysis. Non-linear dimensionality reduction algorithms attempt to map high-dimensional data onto a low-dimensional Euclidean vector space. Maximum variance Unfolding is a member of the manifold learning family, which also include algorithms such as isomap and locally linear embedding. In manifold learning, the input data is assumed to be sampled from a low dimensional manifold that is embedded inside of a higher-dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighborhoods at every point of the underlying manifold. MVU creates a mapping from the high dimensional input vectors to some low dimensional Euclidean vector space in the following steps: A neighborhood graph is created. Each input is connected with its k-nearest input vectors (according to Euclidean distance metric) and all k-nearest neighbors are connected with each other. If the data is sampled well enough, the resulting graph is a discrete approximation of the underlying manifold. The neighborhood graph is "unfolded" with the help of semidefinite programming. Instead of learning the output vectors directly, the semidefinite programming aims to find an inner product matrix that maximizes the pairwise distances between any two inputs that are not connected in the neighborhood graph while preserving the nearest neighbors distances. The low-dimensional embedding is finally obtained by application of multidimensional scaling on the learned inner product matrix. The steps of applying semidefinite programming followed by a linear dimensionality reduction step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich.
Population statistics is the use of statistics to analyze characteristics or changes to a population. It is related to social demography and demography. Population statistics can analyze anything from global demographic changes to local small scale changes. For example, an analysis of global change shows that population growth has slowed, infant mortality rates have declined and there have been small increases in the aged.
The smoothing spline is a method of fitting a smooth curve to a set of noisy observations using a spline function.
In probability theory and statistics, the probit function is the quantile function associated with the standard normal distribution, which is commonly denoted as N(0,1). Mathematically, it is the inverse of the cumulative distribution function of the standard normal distribution, which is denoted as , so the probit is denoted as . It has applications in exploratory statistical graphics and specialized regression modeling of binary response variables. Largely because of the central limit theorem, the standard normal distribution plays a fundamental role in probability theory and statistics. If we consider the familiar fact that the standard normal distribution places 95% of probability between  1.96 and 1.96, and is symmetric around zero, it follows that  The probit function gives the 'inverse' computation, generating a value of an N(0,1) random variable, associated with specified cumulative probability. Continuing the example, . In general,  and
Isomap is a nonlinear dimensionality reduction method. It is one of several widely used low-dimensional embedding methods. Isomap is used for computing a quasi-isometric, low-dimensional embedding of a set of high-dimensional data points. The algorithm provides a simple method for estimating the intrinsic geometry of a data manifold based on a rough estimate of each data point s neighbors on the manifold. Isomap is highly efficient and generally applicable to a broad range of data sources and dimensionalities.
Inverse Distance Weighting (IDW) is a type of deterministic method for multivariate interpolation with a known scattered set of points. The assigned values to unknown points are calculated with a weighted average of the values available at the known points. The name given to this type of methods was motivated by the weighted average applied, since it resorts to the inverse of the distance to each known point ("amount of proximity") when assigning weights.
Sliced inverse regression (SIR) is a tool for dimension reduction in the field of multivariate statistics. In statistics, regression analysis is a popular way of studying the relationship between a response variable y and its explanatory variable , which is a p-dimensional vector. There are several approaches which come under the term of regression. For example parametric methods include multiple linear regression; non-parametric techniques include local smoothing. With high-dimensional data (as p grows), the number of observations needed to use local smoothing methods escalates exponentially. Reducing the number of dimensions makes the operation computable. Dimension reduction aims to show only the most important directions of the data. SIR uses the inverse regression curve,  to perform a weighted principal component analysis, with which one identifies the effective dimension reducing directions. This article first introduces the reader to the subject of dimension reduction and how it is performed using the model here. There is then a short review on inverse regression, which later brings these pieces together.
Latin hypercube sampling (LHS) is a statistical method for generating a sample of plausible collections of parameter values from a multidimensional distribution. The sampling method is often used to construct computer experiments. The LHS was described by McKay in 1979. An independently equivalent technique was proposed by Egla js in 1977. It was further elaborated by Ronald L. Iman and coauthors in 1981. Detailed computer codes and manuals were later published. In the context of statistical sampling, a square grid containing sample positions is a Latin square if (and only if) there is only one sample in each row and each column. A Latin hypercube is the generalisation of this concept to an arbitrary number of dimensions, whereby each sample is the only one in each axis-aligned hyperplane containing it. When sampling a function of  variables, the range of each variable is divided into  equally probable intervals.  sample points are then placed to satisfy the Latin hypercube requirements; note that this forces the number of divisions, , to be equal for each variable. Also note that this sampling scheme does not require more samples for more dimensions (variables); this independence is one of the main advantages of this sampling scheme. Another advantage is that random samples can be taken one at a time, remembering which samples were taken so far. The maximum number of combinations for a Latin Hypercube of  divisions and  variables (i.e., dimensions) can be computed with the following formula:  For example, a Latin hypercube of  divisions with  variables (i.e., a square) will have 24 possible combinations. A Latin hypercube of  divisions with  variables (i.e., a cube) will have 576 possible combinations. Orthogonal sampling adds the requirement that the entire sample space must be sampled evenly. Although more efficient, orthogonal sampling strategy is more difficult to implement since all random samples must be generated simultaneously.  In two dimensions the difference between random sampling, Latin Hypercube sampling and orthogonal sampling can be explained as follows: In random sampling new sample points are generated without taking into account the previously generated sample points. One does not necessarily need to know beforehand how many sample points are needed. In Latin Hypercube sampling one must first decide how many sample points to use and for each sample point remember in which row and column the sample point was taken. In Orthogonal sampling, the sample space is divided into equally probable subspaces. All sample points are then chosen simultaneously making sure that the total ensemble of sample points is a Latin Hypercube sample and that each subspace is sampled with the same density. Thus, orthogonal sampling ensures that the ensemble of random numbers is a very good representative of the real variability, LHS ensures that the ensemble of random numbers is representative of the real variability whereas traditional random sampling (sometimes called brute force) is just an ensemble of random numbers without any guarantees.
In statistics, sieve estimators are a class of non-parametric estimator which use progressively more complex models to estimate an unknown high-dimensional function as more data becomes available, with the aim of asymptotically reducing error towards zero as the amount of data increases. This method is generally attributed to Ulf Grenander.
The sample mean or empirical mean and the sample covariance are statistics computed from a collection (the sample) of data on one or more random variables. The sample mean and sample covariance are estimators of the population mean and population covariance, where the term population refers to the set from which the sample was taken. The sample mean is a vector each of whose elements is the sample mean of one of the random variables   that is, each of whose elements is the arithmetic average of the observed values of one of the variables. The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables. If only one variable has had values observed, then the sample mean is a single number (the arithmetic average of the observed values of that variable) and the sample covariance matrix is also simply a single value (a 1x1 matrix containing a single number, the sample variance of the observed values of that variable). Due to their ease of calculation and other desirable characteristics, the sample mean and sample covariance are widely used in statistics and applications to numerically represent the location and dispersion, respectively, of a distribution.
Gaussian noise is statistical noise having a probability density function (PDF) equal to that of the normal distribution, which is also known as the Gaussian distribution. In other words, the values that the noise can take on are Gaussian-distributed. The probability density function  of a Gaussian random variable  is given by:  where  represents the grey level,  the mean value and  the standard deviation. A special case is white Gaussian noise, in which the values at any pair of times are identically distributed and statistically independent (and hence uncorrelated). In communication channel testing and modelling, Gaussian noise is used as additive white noise to generate additive white Gaussian noise. In telecommunications and computer networking, communication channels can be affected by wideband Gaussian noise coming from many natural sources, such as the thermal vibrations of atoms in conductors (referred to as thermal noise or Johnson-Nyquist noise), shot noise, black body radiation from the earth and other warm objects, and from celestial sources such as the Sun.
RATS, an abbreviation of Regression Analysis of Time Series, is a statistical package for time series analysis and econometrics. RATS is developed and sold by Estima, Inc., located in Evanston, IL.
In probability theory, there exist several different notions of convergence of random variables. The convergence of sequences of random variables to some limit random variable is an important concept in probability theory, and its applications to statistics and stochastic processes. The same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behaviour that is essentially unchanging when items far enough into the sequence are studied. The different possible notions of convergence relate to how such a behaviour can be characterised: two readily understood behaviours are that the sequence eventually takes a constant value, and that values in the sequence continue to change but can be described by an unchanging probability distribution.
A random field is a generalization of a stochastic process such that the underlying parameter need no longer be a simple real or integer valued "time", but can instead take values that are multidimensional vectors, or points on some manifold. At its most basic, discrete case, a random field is a list of random numbers whose indices are mapped into a space (of n dimensions). When used in the natural sciences, values in a random field are often spatially correlated in one way or another. In its most basic form this might mean that adjacent values (i.e. values with adjacent indices) do not differ as much as values that are further apart. This is an example of a covariance structure, many different types of which may be modeled in a random field. More generally, the values might be defined over a continuous domain, and the random field might be thought of as a "function valued" random variable.
The transferable belief model (TBM) is an elaboration on the Dempster Shafer theory of evidence.
The term generalized logistic distribution is used as the name for several different families of probability distributions. For example, Johnson et al. list four forms, which are listed below. One family described here has also been called the skew-logistic distribution. For other families of distributions that have also been called generalized logistic distributions, see the shifted log-logistic distribution, which is a generalization of the log-logistic distribution.
The Bruck Ryser Chowla theorem is a result on the combinatorics of block designs. It states that if a (v, b, r, k,  )-design exists with v = b (a symmetric block design), then: if v is even, then k     is a square; if v is odd, then the following Diophantine equation has a nontrivial solution: x2   (k    )y2   ( 1)(v 1)/2   z2 = 0.  The theorem was proved in the case of projective planes in (Bruck & Ryser 1949). It was extended to symmetric designs in (Ryser & Chowla 1950).
A cartogram is a map in which some thematic mapping variable   such as travel time, population, or Gross National Product   is substituted for land area or distance. The geometry or space of the map is distorted in order to convey the information of this alternate variable. There are two main types of cartograms: area and distance cartograms. Cartograms have a fairly long history, with examples from the mid-1800s.
Proportional hazards models are a class of survival models in statistics. Survival models relate the time that passes before some event occurs to one or more covariates that may be associated with that quantity of time. In a proportional hazards model, the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate. For example, taking a drug may halve one's hazard rate for a stroke occurring, or, changing the material from which a manufactured component is constructed may double its hazard rate for failure. Other types of survival models such as accelerated failure time models do not exhibit proportional hazards. The accelerated failure time model describes a situation where the biological or mechanical life history of an event is accelerated.
In statistics and information theory, the expected formation matrix of a likelihood function  is the matrix inverse of the Fisher information matrix of , while the observed formation matrix of  is the inverse of the observed information matrix of . Currently, no notation for dealing with formation matrices is widely used, but in books and articles by Ole E. Barndorff-Nielsen and Peter McCullagh, the symbol  is used to denote the element of the i-th line and j-th column of the observed formation matrix. The geometric interpretation of the Fisher information matrix (metric) leads to a notation of  following the notation of the (contravariant) metric tensor in differential geometry. The Fisher information metric is denoted by  so that using Einstein notation we have . These matrices appear naturally in the asymptotic expansion of the distribution of many statistics related to the likelihood ratio.
A standard normal table, also called the unit normal table or Z table, is a mathematical table for the values of  , which are the values of the cumulative distribution function of the normal distribution. It is used to find the probability that a statistic is observed below, above, or between values on the standard normal distribution, and by extension, any normal distribution. Since probability tables cannot be printed for every normal distribution, as there are an infinite variety of normal distributions, it is common practice to convert a normal to a standard normal and then use the standard normal table to find probabilities.
In statistical inference, specifically predictive inference, a prediction interval is an estimate of an interval in which future observations will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis. Prediction intervals are used in both frequentist statistics and Bayesian statistics: a prediction interval bears the same relationship to a future observation that a frequentist confidence interval or Bayesian credible interval bears to an unobservable population parameter: prediction intervals predict the distribution of individual future points, whereas confidence intervals and credible intervals of parameters predict the distribution of estimates of the true population mean or other quantity of interest that cannot be observed. Prediction intervals are also present in forecasts. It is difficult to estimate the prediction intervals of forecasts that have contrary series.
This article gives two concrete illustrations of the central limit theorem. Both involve the sum of independent and identically-distributed random variables and show how the probability distribution of the sum approaches the normal distribution as the number of terms in the sum increases. The first illustration involves a continuous probability distribution, for which the random variables have a probability density function. The second illustration, for which most of the computation can be done by hand, involves a discrete probability distribution, which is characterized by a probability mass function. A free full-featured interactive simulation that allows the user to set up various distributions and adjust the sampling parameters is available through the External links section at the bottom of this page.
In the statistical analysis of observational data, propensity score matching (PSM) is a statistical matching technique that attempts to estimate the effect of a treatment, policy, or other intervention by accounting for the covariates that predict receiving the treatment. PSM attempts to reduce the bias due to confounding variables that could be found in an estimate of the treatment effect obtained from simply comparing outcomes among units that received the treatment versus those that did not. The technique was first published by Paul Rosenbaum and Donald Rubin in 1983, and implements the Rubin causal model for observational studies. The possibility of bias arises because the apparent difference in outcome between these two groups of units may depend on characteristics that affected whether or not a unit received a given treatment instead of due to the effect of the treatment per se. In randomized experiments, the randomization enables unbiased estimation of treatment effects; for each covariate, randomization implies that treatment-groups will be balanced on average, by the law of large numbers. Unfortunately, for observational studies, the assignment of treatments to research subjects is typically not random. Matching attempts to mimic randomization by creating a sample of units that received the treatment that is comparable on all observed covariates to a sample of units that did not receive the treatment. For example, one may be interested to know the consequences of smoking or the consequences of going to university. The people 'treated' are simply those the smokers, or the university graduates who in the course of everyday life undergo whatever it is that is being studied by the researcher. In both of these cases it is unfeasible (and perhaps unethical) to randomly assign people to smoking or a university education, so observational studies are required. The treatment effect estimated by simply comparing a particular outcome rate of cancer or life time earnings between those who smoked and did not smoke or attended university and did not attend university would be biased by any factors that predict smoking or university attendance, respectively. PSM attempts to control for these differences to make the groups receiving treatment and not-treatment more comparable.
In the study of probability, given at least two random variables X, Y, ..., that are defined on a probability space, the joint probability distribution for X, Y, ... is a probability distribution that gives the probability that each of X, Y, ... falls in any particular range or discrete set of values specified for that variable. In the case of only two random variables, this is called a bivariate distribution, but the concept generalizes to any number of random variables, giving a multivariate distribution. The joint probability distribution can be expressed either in terms of a joint cumulative distribution function or in terms of a joint probability density function (in the case of continuous variables) or joint probability mass function (in the case of discrete variables). These in turn can be used to find two other types of distributions: the marginal distribution giving the probabilities for any one of the variables with no reference to any specific ranges of values for the other variables, and the conditional probability distribution giving the probabilities for any subset of the variables conditional on particular values of the remaining variables.
Variational message passing (VMP) is an approximate inference technique for continuous- or discrete-valued Bayesian networks, with conjugate-exponential parents, developed by John Winn. VMP was developed as a means of generalizing the approximate variational methods used by such techniques as Latent Dirichlet allocation and works by updating an approximate distribution at each node through messages in the node's Markov blanket.
In population genetics, Ewens' sampling formula, describes the probabilities associated with counts of how many different alleles are observed a given number of times in the sample.  
In statistics, the Q-function is the tail probability of the standard normal distribution . In other words, Q(x) is the probability that a normal (Gaussian) random variable will obtain a value larger than x standard deviations above the mean. If the underlying random variable is y, then the proper argument to the tail probability is derived as:  which expresses the number of standard deviations away from the mean. Other definitions of the Q-function, all of which are simple transformations of the normal cumulative distribution function, are also used occasionally. Because of its relation to the cumulative distribution function of the normal distribution, the Q-function can also be expressed in terms of the error function, which is an important function in applied mathematics and physics.
In statistics, and especially in the statistical analysis of psychological data, the counternull is a statistic used to aid the understanding and presentation of research results. It revolves around the effect size, which is the mean magnitude of some effect divided by the standard deviation. The counternull value is the effect size that is just as well supported by the data as the null hypothesis. In particular, when results are drawn from a distribution that is symmetrical about its mean, the counternull value is exactly twice the observed effect size. The null hypothesis is a hypothesis set up to be tested against an alternative. Thus the counternull is an alternative hypothesis that, when used to replace the null hypothesis, generates the same p-value as had the original null hypothesis of  no difference.  Some researchers contend that reporting the counternull, in addition to the p-value, serves to counter two common errors of judgment: assuming that failure to reject the null hypothesis at the chosen level of statistical significance means that the observed size of the "effect" is zero; and assuming that rejection of the null hypothesis at a particular p-value means that the measured "effect" is not only statistically significant, but also scientifically important. These arbitrary statistical thresholds create a discontinuity, causing unnecessary confusion and artificial controversy. Other researchers prefer confidence intervals as a means of countering these common errors.
The Fre chet distribution, also known as inverse Weibull distribution, is a special case of the generalized extreme value distribution. It has the cumulative distribution function  where   > 0 is a shape parameter. It can be generalised to include a location parameter m (the minimum) and a scale parameter s > 0 with the cumulative distribution function  Named for Maurice Fre chet who wrote a related paper in 1927, further work was done by Fisher and Tippett in 1928 and by Gumbel in 1958.
In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into "spam" or "non-spam" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance. Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. "A", "B", "AB" or "O", for blood type), ordinal (e.g. "large", "medium" or "small"), integer-valued (e.g. the number of occurrences of a part word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term "classifier" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term "classification" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.
In statistics, hypotheses about the value of the population correlation coefficient   between variables X and Y can be tested using the Fisher transformation (aka Fisher z-transformation) applied to the sample correlation coefficient.
Path coefficients are standardized versions of linear regression weights which can be used in examining the possible causal linkage between statistical variables in the structural equation modeling approach. The standardization involves multiplying the ordinary regression coefficient by the standard deviations of the corresponding explanatory variable: these can then be compared to assess the relative effects of the variables within the fitted regression model. The idea of standardization can be extended to apply to partial regression coefficients. The term "path coefficient" derives from Wright (1921), where a particular diagram-based approach was used to consider the relations between variables in a multivariate system.
A Thurstonian model is a latent variable model for describing the mapping of some continuous scale onto discrete, possibly ordered categories of response. In the model, each of these categories of response corresponds to a latent variable whose value is drawn from a normal distribution, independently of the other response variables and with constant variance. Thurstonian models have been used as an alternative to generalized linear models in analysis of sensory discrimination tasks. They have also been used to model long-term memory in ranking tasks of ordered alternatives, such as the order of the amendments to the US Constitution. Their main advantage over other models ranking tasks is that they account for non-independence of alternatives.  
The point biserial correlation coefficient (rpb) is a correlation coefficient used when one variable (e.g. Y) is dichotomous; Y can either be "naturally" dichotomous, like gender, or an artificially dichotomized variable. In most situations it is not advisable to dichotomize variables artificially. When you artificially dichotomize a variable the new dichotomous variable may be conceptualized as having an underlying continuity. If this is the case, a biserial correlation would be the more appropriate calculation. The point-biserial correlation is mathematically equivalent to the Pearson (product moment) correlation, that is, if we have one continuously measured variable X and a dichotomous variable Y, rXY = rpb. This can be shown by assigning two distinct numerical values to the dichotomous variable. To calculate rpb, assume that the dichotomous variable Y has the two values 0 and 1. If we divide the data set into two groups, group 1 which received the value "1" on Y and group 2 which received the value "0" on Y, then the point-biserial correlation coefficient is calculated as follows:  where sn is the standard deviation used when you have data for every member of the population:  M1 being the mean value on the continuous variable X for all data points in group 1, and M0 the mean value on the continuous variable X for all data points in group 2. Further, n1 is the number of data points in group 1, n0 is the number of data points in group 2 and n is the total sample size. This formula is a computational formula that has been derived from the formula for rXY in order to reduce steps in the calculation; it is easier to compute than rXY. There is an equivalent formula that uses sn 1:  where sn 1 is the standard deviation used when you only have data for a sample of the population:  It's important to note that this is merely an equivalent formula. It is not a formula for use in the case where you only have sample data. There is no version of the formula for a case where you only have sample data. The version of the formula using sn 1 is useful if you are calculating point-biserial correlation coefficients in a programming language or other development environment where you have a function available for calculating sn 1, but don't have a function available for calculating sn. To clarify:  Glass and Hopkins' book Statistical Methods in Education and Psychology, (3rd Edition) contains a correct version of point biserial formula. Also the square of the point biserial correlation coefficient can be written:  We can test the null hypothesis that the correlation is zero in the population. A little algebra shows that the usual formula for assessing the significance of a correlation coefficient, when applied to rpb, is the same as the formula for an unpaired t-test and so  follows Student's t-distribution with (n1+n0 - 2) degrees of freedom when the null hypothesis is true. One disadvantage of the point biserial coefficient is that the further the distribution of Y is from 50/50, the more constrained will be the range of values which the coefficient can take. If X can be assumed to be normally distributed, a better descriptive index is given by the biserial coefficient  where u is the ordinate of the normal distribution with zero mean and unit variance at the point which divides the distribution into proportions n0/n and n1/n. As you might imagine, this is not the easiest thing in the world to calculate and the biserial coefficient is not widely used in practice. A specific case of biserial correlation occurs where X is the sum of a number of dichotomous variables of which Y is one. An example of this is where X is a person's total score on a test composed of n dichotomously scored items. A statistic of interest (which is a discrimination index) is the correlation between responses to a given item and the corresponding total test scores. There are three computations in wide use, all called the point-biserial correlation: (i) the Pearson correlation between item scores and total test scores including the item scores, (ii) the Pearson correlation between item scores and total test scores excluding the item scores, and (iii) a correlation adjusted for the bias caused by the inclusion of item scores in the test scores. Correlation (iii) is  A slightly different version of the point biserial coefficient is the rank biserial which occurs where the variable X consists of ranks while Y is dichotomous. We could calculate the coefficient in the same way as where X is continuous but it would have the same disadvantage that the range of values it can take on becomes more constrained as the distribution of Y becomes more unequal. To get round this, we note that the coefficient will have its largest value where the smallest ranks are all opposite the 0s and the largest ranks are opposite the 1s. Its smallest value occurs where the reverse is the case. These values are respectively plus and minus (n1 + n0)/2. We can therefore use the reciprocal of this value to rescale the difference between the observed mean ranks on to the interval from plus one to minus one. The result is  where M1 and M0 are respectively the means of the ranks corresponding to the 1 and 0 scores of the dichotomous variable. This formula, which simplifies the calculation from the counting of agreements and inversions, is due to Gene V Glass (1966). It is possible to use this to test the null hypothesis of zero correlation in the population from which the sample was drawn. If rrb is calculated as above then the smaller of  and  is distributed as Mann Whitney U with sample sizes n1 and n0 when the null hypothesis is true.
In statistics, an ancillary statistic is a statistic whose sampling distribution does not depend on the parameters of the model. An ancillary statistic is a pivotal quantity that is also a statistic. Ancillary statistics can be used to construct prediction intervals. This concept was introduced by the statistical geneticist Sir Ronald Fisher.
For a less technical treatment, see Parameter identification problem. In statistics, identifiability is a property which a model must satisfy in order for precise inference to be possible. We say that the model is identifiable if it is theoretically possible to learn the true value of this model s underlying parameter after obtaining an infinite number of observations from it. Mathematically, this is equivalent to saying that different values of the parameter must generate different probability distributions of the observable variables. Usually the model is identifiable only under certain technical restrictions, in which case the set of these requirements is called the identification conditions. A model that fails to be identifiable is said to be non-identifiable or unidentifiable; two or more parametrizations are observationally equivalent. In some cases, even though a model is non-identifiable, it is still possible to learn the true values of a certain subset of the model parameters. In this case we say that the model is partially identifiable. In other cases it may be possible to learn the location of the true parameter up to a certain finite region of the parameter space, in which case the model is set identifiable. Aside from strictly theoretical exploration of the model properties, Identifiability can be referred to in a wider scope when a model is tested with relation of experimental data sets. Usually these tests of Identifiability Analysis are applied when the model fitting of experimental data obtained and serve the detection of non-identifiable and sloppy parameters.
In econometrics, the generalized Tobit model is a generalization of the Tobit model named after James Tobin. It is also called the Heckit model after James Heckman. Another name is "type 2 Tobit model". Tobit models assume that a random variable is censored.
Robust statistics are statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. Robust statistical methods have been developed for many common problems, such as estimating location, scale and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. Another motivation is to provide methods with good performance when there are small departures from parametric distributions. For example, robust methods work well for mixtures of two normal distributions with different standard-deviations; under this model, non-robust methods like a t-test work badly.
Decision theory (or theory of choice) in economics, psychology, sociology, philosophy, mathematics, computer science, and statistics is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision. It is closely related to the field of game theory; decision theory is concerned with the choices of individual agents whereas game theory is concerned with interactions of agents whose decisions affect each other.
In statistics, the backfitting algorithm is a simple iterative procedure used to fit a generalized additive model. It was introduced in 1985 by Leo Breiman and Jerome Friedman along with generalized additive models. In most cases, the backfitting algorithm is equivalent to the Gauss Seidel method algorithm for solving a certain linear system of equations
Economic epidemiology is a field at the intersection of epidemiology and economics. Its premise is to incorporate incentives for healthy behavior and their attendant behavioral responses into an epidemiological context to better understand how diseases are transmitted. This framework should help improve policy responses to epidemic diseases by giving policymakers and health-care providers clear tools for thinking about how certain actions can influence the spread of disease transmission. The main context through which this field emerged was the idea of prevalence-dependence, or disinhibition, which suggests that individuals change their behavior as the prevalence of a disease changes. However, economic epidemiology also encompasses other ideas, including the role of externalities, global disease commons and how individuals  incentives can influence the outcome and cost of health interventions. Strategic epidemiology is a branch of economic epidemiology that adopts an explicitly game theoretic approach to analyzing the interplay between individual behavior and population wide disease dynamics.
Bayesian inference using Gibbs sampling (BUGs) is a software package for performing Bayesian inference using Markov chain Monte Carlo (based on Gibbs sampling). BUGs is used in the following software: Just another Gibbs sampler OpenBUGS WinBUGS  ^ David Lunn, David Spiegelhalter, Andrew Thomas and Nicky Best (2009). The BUGS project: Evolution, critique and future directions, Statistics in Medicine 28 (25), 3049 3067. doi:10.1002/sim.3680 PMID 19630097.
Multiscale geometric analysis or geometric multiscale analysis is an emerging area of high-dimensional signal processing and data analysis.
A surrogate model is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as function of design variables. For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the air flow around the wing for different shape variables (length, curvature, material, ..). For many real world problems, however, a single simulation can take many minutes, hours, or even days to complete. As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis and what-if analysis become impossible since they require thousands or even millions of simulation evaluations. One way of alleviating this burden is by constructing approximation models, known as surrogate models, response surface models, metamodels or emulators, that mimic the behavior of the simulation model as closely as possible while being computationally cheap(er) to evaluate. Surrogate models are constructed using a data-driven, bottom-up approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important. A model is constructed based on modeling the response of the simulator to a limited number of intelligently chosen data points. This approach is also known as behavioral modeling or black-box modeling, though the terminology is not always consistent. When only a single design variable is involved, the process is known as curve fitting. Though using surrogate models in lieu of experiments and simulations in engineering design is more common, surrogate modelling may be used in many other areas of science where there are expensive experiments and/or function evaluations.
In statistics, the mean absolute error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes. The mean absolute error is given by  As the name suggests, the mean absolute error is an average of the absolute errors , where  is the prediction and  the true value. Note that alternative formulations may include relative frequencies as weight factors. The mean absolute error is a common measure of forecast error in time series analysis, where the terms "mean absolute deviation" is sometimes used in confusion with the more standard definition of mean absolute deviation. The same confusion exists more generally.
In the study of stochastic processes, an adapted process (also referred to as a non-anticipating or non-anticipative process) is one that cannot "see into the future". An informal interpretation is that X is adapted if and only if, for every realisation and every n, Xn is known at time n. The concept of an adapted process is essential, for instance, in the definition of the Ito  integral, which only makes sense if the integrand is an adapted process.
The maximal ergodic theorem is a theorem in ergodic theory, a discipline within mathematics. Suppose that  is a probability space, that  is a (possibly noninvertible) measure-preserving transformation, and that . Define  by  Then the maximal ergodic theorem states that  for any     R. This theorem is used to prove the point-wise ergodic theorem.
In mathematics, orthogonality is the relation of two lines at right angles to one another (perpendicularity), and the generalization of this relation into n dimensions; and to a variety of mathematical relations thought of as describing non-overlapping, uncorrelated, or independent objects of some kind. The concept of orthogonality has been broadly generalized in mathematics (including in the areas of mathematical functions, calculus and linear algebra), as well as in areas such as chemistry, and engineering.
In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s. It was introduced under a different name into the text retrieval community in the early 1960s, and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate preprocessing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. In the statistics and computer science literature, Naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method; Russell and Norvig note that "[naive Bayes] is sometimes called a Bayesian classifier, a somewhat careless usage that has prompted some Bayesians to call it the idiot Bayes model."
In probability theory, a log-Cauchy distribution is a probability distribution of a random variable whose logarithm is distributed in accordance with a Cauchy distribution. If X is a random variable with a Cauchy distribution, then Y = exp(X) has a log-Cauchy distribution; likewise, if Y has a log-Cauchy distribution, then X = log(Y) has a Cauchy distribution.
Pejoratively, a kitchen sink regression is a statistical regression which uses a long list of possible independent variables to attempt to explain variance in a dependent variable. In economics, psychology, and other social sciences, regression analysis is typically used deductively to test hypotheses, but a kitchen sink regression does not follow this norm. Instead, the analyst throws "everything but the kitchen sink" into the regression in hopes of finding some statistical pattern. The results of this type of regression may be misleadingly interpreted inductively to suggest that the same pattern of relationships between independent and dependent variables will be found in other data, which can lead to hasty generalizations. The difficulty in valid interpretation is that the more independent variables are included in a regression, the greater is the possibility that one or more will be found to be statistically significant while in fact they have no causal effect on the dependent variable that is, the more likely the results are to be afflicted with Type I error. The kitchen sink regression is an example of the practice of data dredging.
The Multiple Indicator Cluster Surveys (MICS) are surveys implemented by countries under the programme developed by the United Nations Children's Fund to provide internationally comparable, statistically rigorous data on the situation of children and women. The first round of surveys (MICS1) was carried out in over 60 countries in mainly 1995 and 1996 in response to the World Summit for Children and measurement of the mid-decade progress. A second round (MICS2) in 2000 increased the depth of the survey, allowing monitoring of a larger number of globally agreed indicators. A third round (MICS3) started in 2006 and aimed at producing data measuring progress also toward the Millennium Development Goals (MDGs), A World Fit for Children, and other major relevant international commitments. The fourth round, launched in 2009, aimed at most data collection conducted in 2010, but in reality most MICS4s were implemented in 2011 and even into 2012 and 2013. This represented a scale-up of frequency of MICS from UNICEF, now offering the survey programme on a three-year cycle. The fifth round, launched in 2012, is currently being planned or has been implemented by more than 40 countries. The MICS is highly comparable to the Demographic and Health Survey (DHS) and the technical teams developing and supporting the surveys are in close collaboration.
In statistics, the Neyman Pearson lemma, named after Jerzy Neyman and Egon Pearson, states that when performing a hypothesis test between two simple hypotheses H0:   =  0 and H1:   =  1, the likelihood-ratio test which rejects H0 in favour of H1 when  where  is the most powerful test at significance level   for a threshold  . If the test is most powerful for all , it is said to be uniformly most powerful (UMP) for alternatives in the set . In practice, the likelihood ratio is often used directly to construct tests   see Likelihood-ratio test. However it can also be used to suggest particular test-statistics that might be of interest or to suggest simplified tests   for this, one considers algebraic manipulation of the ratio to see if there are key statistics in it related to the size of the ratio (i.e. whether a large statistic corresponds to a small ratio or to a large one).
In statistics, a frequency distribution is a table that displays the frequency of various outcomes in a sample. Each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval, and in this way, the table summarizes the distribution of values in the sample.
Inductive reasoning (as opposed to deductive reasoning or abductive reasoning) is reasoning in which the premises are viewed as supplying strong evidence for the truth of the conclusion. While the conclusion of a deductive argument is certain, the truth of the conclusion of an inductive argument is probable, based upon the evidence given. Many dictionaries define inductive reasoning as reasoning that derives general principles from specific observations, though some sources disagree with this usage. The philosophical definition of inductive reasoning is more nuanced than simple progression from particular/individual instances to broader generalizations. Rather, the premises of an inductive logical argument indicate some degree of support (inductive probability) for the conclusion but do not entail it; that is, they suggest truth but do not ensure it. In this manner, there is the possibility of moving from general statements to individual instances (for example, statistical syllogisms, discussed below).
In probability and statistics, the Gompertz distribution is a continuous probability distribution, named after Benjamin Gompertz (1779 - 1865). The Gompertz distribution is often applied to describe the distribution of adult lifespans by demographers and actuaries. Related fields of science such as biology and gerontology also considered the Gompertz distribution for the analysis of survival. More recently, computer scientists have also started to model the failure rates of computer codes by the Gompertz distribution. In Marketing Science, it has been used as an individual-level simulation for customer lifetime value modeling.
In probability and statistics, an urn problem is an idealized mental exercise in which some objects of real interest (such as atoms, people, cars, etc.) are represented as colored balls in an urn or other container. One pretends to remove one or more balls from the urn; the goal is to determine the probability of drawing one color or another, or some other properties. A number of important variations are described below. An urn model is either a set of probabilities that describe events within an urn problem, or it is a probability distribution, or a family of such distributions, of random variables associated with urn problems.
In Bayesian inference, the Bernstein von Mises theorem provides the basis for the important result that the posterior distribution for unknown quantities in any problem is effectively independent of the prior distribution (assuming it obeys Cromwell's rule) once the amount of information supplied by a sample of data is large enough.
In probability theory and statistics, the noncentral chi-squared or noncentral  distribution is a generalization of the chi-squared distribution. This distribution often arises in the power analysis of statistical tests in which the null distribution is (perhaps asymptotically) a chi-squared distribution; important examples of such tests are the likelihood ratio tests.
In elementary mathematics, a variable is an alphabetic character representing a number, called the value of the variable, which is either arbitrary or not fully specified or unknown. Making algebraic computations with variables as if they were explicit numbers allows one to solve a range of problems in a single computation. A typical example is the quadratic formula, which allows one to solve every quadratic equation by simply substituting the numeric values of the coefficients of the given equation to the variables that represent them. The concept of variable is also fundamental in calculus. Typically, a function y = f(x) involves two variables, y and x, representing respectively the value and the argument of the function. The term "variable" comes from the fact that, when the argument (also called the "variable of the function") varies, then the value varies accordingly. In more advanced mathematics, a variable is a symbol that denotes a mathematical object, which could be a number, a vector, a matrix, or even a function. In this case, the original property of "variability" of a variable is not kept (except, sometimes, for informal explanations). Similarly, in computer science, a variable is a name (commonly an alphabetic character or a word) representing some value represented in computer memory. In mathematical logic, a variable is either a symbol representing an unspecified term of the theory, or a basic object of the theory, which is manipulated without referring to its possible intuitive interpretation.
Ensemble forecasting is a numerical weather prediction method that is used to attempt to generate a representative sample of the possible future states of a dynamical system. Ensemble forecasting is a form of Monte Carlo analysis: multiple numerical predictions are conducted using slightly different initial conditions that are all plausible given the past and current set of observations, or measurements. Sometimes the ensemble of forecasts may use different forecast models for different members, or different formulations of a forecast model. The multiple simulations are conducted to account for the two usual sources of uncertainty in forecast models: (1) the errors introduced by the use of imperfect initial conditions, amplified by the chaotic nature of the evolution equations of the dynamical system, which is often referred to as sensitive dependence on the initial conditions; and (2) errors introduced because of imperfections in the model formulation, such as the approximate mathematical methods to solve the equations. Ideally, the verified future dynamical system state should fall within the predicted ensemble spread, and the amount of spread should be related to the uncertainty (error) of the forecast. Consider the problem of numerical weather prediction. In this case, the dynamic system is the atmosphere, the model is a numerical weather prediction model and the initial condition is represented by an objective analysis of an atmospheric state. Today ensemble predictions are commonly made at most of the major operational weather prediction facilities worldwide, including: National Centers for Environmental Prediction (NCEP of the US) European Centre for Medium-Range Weather Forecasts (ECMWF) United Kingdom Met Office Me te o-France Environment Canada Japan Meteorological Agency Bureau of Meteorology (Australia) China Meteorological Administration (CMA) Korea Meteorological Administration CPTEC (Brazil) Experimental ensemble forecasts are made at a number of universities, such as the University of Washington, and ensemble forecasts in the US are also generated by the US Navy and Air Force. There are various ways of viewing the data such as spaghetti plots, ensemble means or Postage Stamps where a number of different results from the models run can be compared.
SHAZAM is a comprehensive econometrics and statistics package for estimating, testing, simulating and forecasting many types of econometrics and statistical models. SHAZAM was originally created in 1977 by Ken White.
The polar method (attributed to George Marsaglia, 1964) is a pseudo-random number sampling method for generating a pair of independent standard normal random variables. While it is superior to the Box Muller transform, the Ziggurat algorithm is even more efficient. Standard normal random variables are frequently used in computer science, computational statistics, and in particular, in applications of the Monte Carlo method. The polar method works by choosing random points (x, y) in the square  1 < x < 1,  1 < y < 1 until  and then returning the required pair of normal random variables as
In econometrics, a dynamic factor (also known as a diffusion index) is a series which measures the co-movement of many time series. It is used in certain macroeconomic models. Formally  where  is the vector of lagged factors of the variables in the  matrix  (T is the number of observations and N is the number of variables),  are the factor loadings, and  is the factor error.
Coding is an analytical process in which data, in both quantitative form (such as questionnaires results) or qualitative (such as interview transcripts) is categorised to facilitate analysis. Coding means the transformation of data into a form understandable by computer software. The classification of information is an important step in preparation of data for computer processing with statistical software. One code should apply to only one category and categories should be comprehensive. There should be clear guidelines for coders (individual who do the coding) so that code is consistent. Some studies will employ multiple coders working independently on the same data. This minimizes the chance of errors from coding and increases the reliability of data.
In statistics, a weighted median of a sample is the 50% weighted percentile. It was first proposed by F. Y. Edgeworth in 1888. Like the median, it is useful as an estimator of central tendency, robust against outliers. It allows for non-uniform statistical weights related to, e.g., varying precision measurements in the sample.
The invariant extended Kalman filter (IEKF) (not to be confused with the iterated extended Kalman filter) is a new version of the extended Kalman filter (EKF) for nonlinear systems possessing symmetries (or invariances). It combines the advantages of both the EKF and the recently introduced symmetry-preserving filters. Indeed, instead of using a linear correction term based on a linear output error, it uses a geometrically adapted correction term based on an invariant output error; in the same way the gain matrix is not updated from a linear state error, but from an invariant state error. The main benefit is that the gain and covariance equations converge to constant values on a much bigger set of trajectories than equilibrium points that is the case for the EKF, which results in a better convergence of the estimation.
Long-range dependency (LRD), also called long memory or long-range persistence, is a phenomenon that may arise in the analysis of spatial or time series data. It relates to the rate of decay of statistical dependence of two points with increasing time interval or spatial distance between the points. A phenomenon is usually considered to have long-range dependence if the dependence decays more slowly than an exponential decay, typically a power-like decay. LRD is often related to self-similar processes or fields. LRD has been used in various fields such as internet traffic modelling, econometrics, hydrology, linguistics and the earth sciences. Different mathematical definitions of LRD are used for different contexts and purposes.
In signal processing, cross-correlation is a measure of similarity of two series as a function of the lag of one relative to the other. This is also known as a sliding dot product or sliding inner-product. It is commonly used for searching a long signal for a shorter, known feature. It has applications in pattern recognition, single particle analysis, electron tomography, averaging, cryptanalysis, and neurophysiology. For continuous functions f and g, the cross-correlation is defined as:  where  denotes the complex conjugate of  and  is the lag. Similarly, for discrete functions, the cross-correlation is defined as:  The cross-correlation is similar in nature to the convolution of two functions. In an autocorrelation, which is the cross-correlation of a signal with itself, there will always be a peak at a lag of zero, and its size will be the signal power. In probability and statistics, the term cross-correlations is used for referring to the correlations between the entries of two random vectors X and Y, while the autocorrelations of a random vector X are considered to be the correlations between the entries of X itself, those forming the correlation matrix (matrix of correlations) of X. This is analogous to the distinction between autocovariance of a random vector and cross-covariance of two random vectors. One more distinction to point out is that in probability and statistics the definition of correlation always includes a standardising factor in such a way that correlations have values between  1 and +1. If  and  are two independent random variables with probability density functions f and g, respectively, then the probability density of the difference  is formally given by the cross-correlation (in the signal-processing sense) ; however this terminology is not used in probability and statistics. In contrast, the convolution  (equivalent to the cross-correlation of f(t) and g( t) ) gives the probability density function of the sum .
In directional statistics, the von Mises Fisher distribution (named after Ronald Fisher and Richard von Mises), is a probability distribution on the -dimensional sphere in . If  the distribution reduces to the von Mises distribution on the circle. The probability density function of the von Mises Fisher distribution for the random p-dimensional unit vector  is given by:  where  and the normalization constant  is equal to  where  denotes the modified Bessel function of the first kind at order . If , the normalization constant reduces to  The parameters  and  are called the mean direction and concentration parameter, respectively. The greater the value of , the higher the concentration of the distribution around the mean direction . The distribution is unimodal for , and is uniform on the sphere for . The von Mises Fisher distribution for , also called the Fisher distribution, was first used to model the interaction of electric dipoles in an electric field (Mardia, 2000). Other applications are found in geology, bioinformatics, and text mining.
The Newman Keuls or Student Newman Keuls (SNK) method is a stepwise multiple comparisons procedure used to identify sample means that are significantly different from each other. It was named after Student (1927), D. Newman, and M. Keuls. This procedure is often used as a post-hoc test whenever a significant difference between three or more sample means has been revealed by an analysis of variance (ANOVA). The Newman Keuls method is similar to Tukey's range test as both procedures use Studentized range statistics. Unlike Tukey's range test, the Newman Keuls method uses different critical values for different pairs of mean comparisons. Thus, the procedure is more likely to reveal significant differences between group means and to commit type I errors by incorrectly rejecting a null hypothesis when it is true. In other words, the Neuman-Keuls procedure is more powerful but less conservative than Tukey's range test.
In statistical decision theory, where we are faced with the problem of estimating a deterministic parameter (vector)  from observations  an estimator (estimation rule)  is called minimax if its maximal risk is minimal among all estimators of . In a sense this means that  is an estimator which performs best in the worst possible case allowed in the problem.
In mathematics, the theory of optimal stopping is concerned with the problem of choosing a time to take a particular action, in order to maximise an expected reward or minimise an expected cost. Optimal stopping problems can be found in areas of statistics, economics, and mathematical finance (related to the pricing of American options). A key example of an optimal stopping problem is the secretary problem. Optimal stopping problems can often be written in the form of a Bellman equation, and are therefore often solved using dynamic programming.
A probabilistic metric space is a generalization of metric spaces where the distance is no longer valued in non-negative real numbers, but instead is valued in distribution functions. Let D+ be the set of all probability distribution functions F such that F(0) = 0: F is a nondecreasing, right continuous mapping from the real numbers R into [0, 1] such that sup F(x) = 1 where the supremum is taken over all x in R. The ordered pair (S,d) is said to be a probabilistic metric space if S is a nonempty set and d: S S  D+ In the following, d(p, q) is denoted by dp,q for every (p, q)   S   S and is a distribution function dp,q(x). The distance-distribution function satisfies the following conditions: du,v(x) = 1 for all x > 0   u = v (u, v   S). du,v(x) = dv,u(x) for all x and for every u, v   S. du,v(x) = 1 and dv,w(y) = 1   du,w(x + y) = 1 for u, v, w   S and x, y   R.  
In probability theory and statistics, a conditional variance is the variance of a random variable given the given the value(s) of one or more other variables. Particularly in econometrics, the conditional variance is also known as the scedastic function or skedastic function. Conditional variances are important parts of autoregressive conditional heteroskedasticity (ARCH) models.  
In gambling a Dutch book or lock is a set of odds and bets which guarantees a profit, regardless of the outcome of the gamble. It is associated with probabilities implied by the odds not being coherent. In economics a Dutch book usually refers to a sequence of trades that would leave one party strictly worse off and another strictly better off. Typical assumptions in consumer choice theory rule out the possibility that anyone can be Dutch-booked. Alan Hajek discusses the history of usage of the term and shows that there is no agreement on its etymology. He also provides a useful survey of the literature related to this concept.
In statistics, the principle of marginality refers to the fact that the average (or main) effects, of variables in an analysis are marginal to their interaction effect. The principle of marginality argues that, in general, it is wrong to test, estimate, or interpret main effects of explanatory variables where the variables interact or, similarly, to model interaction effects but delete main effects that are marginal to them. While such models are interpretable, they lack applicability, as they ignore the effects of their marginal main effects. Nelder and Venables have argued strongly for the importance of this principle in regression analysis.
In probability theory and statistics, the hyperbolic secant distribution is a continuous probability distribution whose probability density function and characteristic function are proportional to the hyperbolic secant function. The hyperbolic secant function is equivalent to the inverse hyperbolic cosine, and thus this distribution is also called the inverse-cosh distribution.
In data mining and association rule learning, lift is a measure of the performance of a targeting model (association rule) at predicting or classifying cases as having an enhanced response (with respect to the population as a whole), measured against a random choice targeting model. A targeting model is doing a good job if the response within the target is much better than the average for the population as a whole. Lift is simply the ratio of these values: target response divided by average response. For example, suppose a population has an average response rate of 5%, but a certain model (or rule) has identified a segment with a response rate of 20%. Then that segment would have a lift of 4.0 (20%/5%). Typically, the modeller seeks to divide the population into quantiles, and rank the quantiles by lift. Organizations can then consider each quantile, and by weighing the predicted response rate (and associated financial benefit) against the cost, they can decide whether to market to that quantile or not. Lift is analogous to information retrieval's average precision metric, if one treats the precision (fraction of the positives that are true positives) as the target response probability. The lift curve can also be considered a variation on the receiver operating characteristic (ROC) curve, and is also known in econometrics as the Lorenz or power curve. The difference between the lifts observed on two different subgroups is called the uplift. The subtraction of two lift curves forms the uplift curve, which is a metric used in uplift modelling.  It is important to note that in general marketing practice the term Lift is also defined as the difference in response rate between the treatment and control groups, indicating the causal impact of a marketing program (versus not having it as in the control group). As a result, "no lift" often means there is no statistically significant effect of the program. On top of this, uplift modelling is a predictive modeling technique to improve (up) lift over control.
Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in insurance, finance and other industries and professions. Actuaries are professionals who are qualified in this field through intense education and experience. In many countries, actuaries must demonstrate their competence by passing a series of thorough professional examinations. Actuarial science includes a number of interrelated subjects, including mathematics, probability theory, statistics, finance, economics, and computer science. Historically, actuarial science used deterministic models in the construction of tables and premiums. The science has gone through revolutionary changes during the last 30 years due to the proliferation of high speed computers and the union of stochastic actuarial models with modern financial theory (Frees 1990). Many universities have undergraduate and graduate degree programs in actuarial science. In 2010, a study published by job search website CareerCast ranked actuary as the #1 job in the United States (Needleman 2010). The study used five key criteria to rank jobs: environment, income, employment outlook, physical demands, and stress. A similar study by U.S. News & World Report in 2006 included actuaries among the 25 Best Professions that it expects will be in great demand in the future (Nemko 2006).  
In probability theory and statistics, the generalized inverse Gaussian distribution (GIG) is a three-parameter family of continuous probability distributions with probability density function  where Kp is a modified Bessel function of the second kind, a > 0, b > 0 and p a real parameter. It is used extensively in geostatistics, statistical linguistics, finance, etc. This distribution was first proposed by E tienne Halphen. It was rediscovered and popularised by Ole Barndorff-Nielsen, who called it the generalized inverse Gaussian distribution. It is also known as the Sichel distribution, after Herbert Sichel. Its statistical properties are discussed in Bent J rgensen's lecture notes.
An eigenpoll is a type of statistical survey which gathers knowledge from the community. It differs from opinion polls by finding the best solution, rather than finding the most popular opinion.
In queueing theory, a discipline within the mathematical theory of probability, an M/G/1 queue is a queue model where arrivals are Markovian (modulated by a Poisson process), service times have a General distribution and there is a single server. The model name is written in Kendall's notation, and is an extension of the M/M/1 queue, where service times must be exponentially distributed. The classic application of the M/G/1 queue is to model performance of a fixed head hard disk.  
The Greenwood statistic is a spacing statistic and can be used to evaluate clustering of events in time or locations in space.
In mathematics, a Bose Mesner algebra is a special set of matrices which arise from a combinatorial structure known as an association scheme, together with the usual set of rules for combining (forming the products of) those matrices, such that they form an associative algebra, or, more precisely, a unitary commutative algebra. Among these rules are:  the result of a product is also within the set of matrices, there is an identity matrix in the set, and taking products is commutative.  Bose Mesner algebras have applications in physics to spin models, and in statistics to the design of experiments. They are named for R. C. Bose and Dale Marsh Mesner.
A ranking is a relationship between a set of items such that, for any two items, the first is either 'ranked higher than', 'ranked lower than' or 'ranked equal to' the second. In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. By reducing detailed measures to a sequence of ordinal numbers, rankings make it possible to evaluate complex information according to certain criteria. Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their relevance, making it possible for the user quickly to select the pages they are likely to want to see. Analysis of data obtained by ranking commonly requires non-parametric statistics.
In statistical hypothesis testing, p-rep or prep has been proposed as a statistical alternative to the classic p-value. Whereas a p-value is the probability of obtaining a result under the null hypothesis, p-rep computes the probability of replicating an effect. Whether it does so is heavily disputed   some have argued that the concept rests on a mathematical falsehood. For a while, the Association for Psychological Science recommended that articles submitted to Psychological Science and their other journals report p-rep rather than the classic p-value, but this is no longer the case.
In many areas of information science, finding predictive relationships from data is a very important task. Initial discovery of relationships is usually done with a training set while a test set and validation set are used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. Test and training sets are used in intelligent systems, machine learning, genetic programming and statistics.
A t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. It can be used to determine if two sets of data are significantly different from each other, and is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known. When the scaling term is unknown and is replaced by an estimate based on the data, the test statistic (under certain conditions) follows a Student's t distribution.  
In probability theory, Dudley s theorem is a result relating the expected upper bound and regularity properties of a Gaussian process to its entropy and covariance structure.
In probability theory, it is possible to approximate the moments of a function f of a random variable X using Taylor expansions, provided that f is sufficiently differentiable and that the moments of X are finite.
In probability theory and statistics, the characteristic function of any real-valued random variable completely defines its probability distribution. If a random variable admits a probability density function, then the characteristic function is the inverse Fourier transform of the probability density function. Thus it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. There are particularly simple results for the characteristic functions of distributions defined by the weighted sums of random variables. In addition to univariate distributions, characteristic functions can be defined for vector or matrix-valued random variables, and can also be extended to more generic cases. The characteristic function always exists when treated as a function of a real-valued argument, unlike the moment-generating function. There are relations between the behavior of the characteristic function of a distribution and properties of the distribution, such as the existence of moments and the existence of a density function.
Repeatability or test retest reliability is the variation in measurements taken by a single person or instrument on the same item, under the same conditions, and in a short period of time. A less-than-perfect test retest reliability causes test retest variability. Such variability can be caused by, for example, intra-individual variability and intra-observer variability. A measurement may be said to be repeatable when this variation is smaller than a pre-determined acceptance criteria. Test retest variability is practically used, for example, in medical monitoring of conditions. In these situations, there is often a predetermined "critical difference", and for differences in monitored values that are smaller than this critical difference, the possibility of pre-test variability as a sole cause of the difference may be considered in addition to, for examples, changes in diseases or treatments.
In probability and statistics, given two stochastic processes  and , the cross-covariance is a function that gives the covariance of the one process with the other at pairs of time points. With the usual notation E  for the expectation operator, if the processes have the mean functions  and , then the cross-covariance is given by  Cross-covariance is related to the more commonly used cross-correlation of the processes in question. In the case of two random vectors  and , the cross-covariance would be a square n by n matrix  with entries  Thus the term cross-covariance is used in order to distinguish this concept from the "covariance" of a random vector X, which is understood to be the matrix of covariances between the scalar components of X itself. In signal processing, the cross-covariance is often called cross-correlation and is a measure of similarity of two signals, commonly used to find features in an unknown signal by comparing it to a known one. It is a function of the relative time between the signals, is sometimes called the sliding dot product, and has applications in pattern recognition and cryptanalysis.
In 1749 a large-scale census and statistical investigation was conducted in the Crown of Castile (15.000 places including Galicia and Andalusia, but not including the Basque provinces, Navarre or the Crown of Aragon). It included population, territorial properties, buildings, cattle, offices, all kinds of revenue and trades, and even geographical information from each place. It was encouraged by king Ferdinand VI of Spain and his minister the Marquis of Ensenada, and is known today as the Catastro of Ensenada. The general answers of each place to the 40 questions of the Catastro produced a huge volume of documentation that affords historians an opportunity to analyze the economy, the society, the practices of the sen ori o system (manorialism) and environmental data from 18th-century Spain. It is the best statistical register of the pre-statistical age of the Ancien Re gime in Europe. Today the word catastro means  register of the properties , but the etymology comes from  enquire . In the 18th century there was a distinction between a catastro, which was made by central officers who traveled to the places to enquire, and the amillaramiento, which was made by local authorities.
A fat-tailed distribution is a probability distribution that has the property, along with the other heavy-tailed distributions, that it exhibits large skewness or kurtosis. This comparison is often made relative to the normal distribution, or to the exponential distribution. Fat-tailed distributions have been empirically encountered in a variety of areas: economics, physics, and earth sciences. Some fat-tailed distributions have power law decay in the tail of the distribution, but do not necessarily follow a power law everywhere.
In information theory, Pinsker's inequality, named after its inventor Mark Semenovich Pinsker, is an inequality that bounds the total variation distance (or statistical distance) in terms of the Kullback Leibler divergence. The inequality is tight up to constant factors.
In probability theory, two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of the other. Similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other. The concept of independence extends to dealing with collections of more than two events or random variables, in which case the events are pairwise independent if each pair are independent of each other, and the events are mutually independent if each event is independent of each other combination of events.
Heart rate variability (HRV) is the physiological phenomenon of variation in the time interval between heartbeats. It is measured by the variation in the beat-to-beat interval. Other terms used include: "cycle length variability", "RR variability" (where R is a point corresponding to the peak of the QRS complex of the ECG wave; and RR is the interval between successive Rs), and "heart period variability". See also Heart rate turbulence, Sinus rhythm. Methods used to detect beats include: ECG, blood pressure, ballistocardiograms, and the pulse wave signal derived from a photoplethysmograph (PPG). ECG is considered superior because it provides a clear waveform, which makes it easier to exclude heartbeats not originating in the sinoatrial node. The term "NN" is used in place of RR to emphasize the fact that the processed beats are "normal" beats.
The Dunn index (DI) (introduced by J. C. Dunn in 1974) is a metric for evaluating clustering algorithms. This is part of a group of validity indices including the Davies Bouldin index or Silhouette index, in that it is an internal evaluation scheme, where the result is based on the clustered data itself. As do all other such indices, the aim is to identify sets of clusters that are compact, with a small variance between members of the cluster, and well separated, where the means of different clusters are sufficiently far apart, as compared to the within cluster variance. For a given assignment of clusters, a higher Dunn index indicates better clustering. One of the drawbacks of using this is the computational cost as the number of clusters and dimensionality of the data increase.  
Mean square weighted deviation is a statistical method used extensively in geochronology also known as the reduced chi-squared. The Mean Square Weighted Deviation (MSWD) is a measure of goodness of fit that takes into account the relative importance of both the internal and external reproducibility, with most common usage in isotopic dating. This statistic is synonmous with the reduced chi-squared statistic, which has widespread usage in statistics and other fields of science. In general when: MSWD = 1 if the age data fit a univariate normal distribution in t (for the arithmetic mean age) or log(t) (for the geometric mean age) space, or if the compositional data fit a bivariate normal distribution in [log(U/He),log(Th/He)]-space (for the central age). MSWD < 1 if the observed scatter is less than that predicted by the analytical uncertainties. In this case, the data are said to be "underdispersed", indicating that the analytical uncertainties were overestimated. MSWD > 1 if the observed scatter exceeds that predicted by the analytical uncertainties. In this case, the data are said to be "overdispersed". This situation is the rule rather than the exception in (U-Th)/He geochronology, indicating an incomplete understanding of the isotope system. Several reasons have been proposed to explain the overdispersion of (U-Th)/He data, including unevenly distributed U-Th distributions and radiation damage. The MSWD statistic is widely used in the field of isotopic dating. Wendt and Carl (1991) provide an "expert" publication on this statistic. Note however that MSWD is directly equivalent to reduced chi squared statistic. Wendt, I., and Carl, C., 1991,The statistical distribution of the mean squared weighted deviation, Chemical Geology, 275-285. Often the geochronologist will determine a series of age measurements on a single sample, with the measured value  having a weighting  and an associated error  for each age determination. As regards weighting, one can either weight all of the measured ages equally, or weight them by the proportion of the sample that they represent. For example, if two thirds of the sample was used for the first measurement and one third for the second and final measurement then one might weight the first measurement twice that of the second. The arithmetic mean of the age determinations is:  but this value can be misleading unless each determination of the age is of equal significance. When each measured value can be assumed to have the same weighting, or significance, the biased and unbiased (or "sample" and "population", respectively) estimators of the variance are computed as follows:  The standard deviation is the square root of the variance. When individual determinations of an age are not of equal significance it is better to use a weighted mean to obtain an 'average' age, as follows:  The biased weighted estimator of variance can be shown to be:  which can be computed on the fly as  The unbiased weighted estimator of the sample variance can be computed as follows:  Again the corresponding standard deviation is the square root of the variance. The unbiased weighted estimator of the sample variance can also be computed on the fly as follows:  The unweighted mean square of the weighted deviations (unweighted MSWD) can then be computed, as follows: MSWD By analogy the weighted mean square of the weighted deviations (weighted MSWD) can be computed, as follows: MSWD
The phrase "inherent bias" refers to the effect of underlying factors or assumptions that skew viewpoints a subject under discussion. There are multiple formal definitions of "inherent bias" which depend on the particular field of study. In statistics, the phrase is used in relation to an inability to measure accuarately and directly what one would wish to measure, meaning that indirect measurements are used which might be subject to unknown distortions.
In mathematics, a local martingale is a type of stochastic process, satisfying the localized version of the martingale property. Every martingale is a local martingale; every bounded local martingale is a martingale; in particular, every local martingale that is bounded from below is a supermartingale, and every local martingale that is bounded from above is a submartingale; however, in general a local martingale is not a martingale, because its expectation can be distorted by large values of small probability. In particular, a driftless diffusion process is a local martingale, but not necessarily a martingale. Local martingales are essential in stochastic analysis, see Ito  calculus, semimartingale, Girsanov theorem.
In queueing theory, a discipline within the mathematical theory of probability, Kendall's notation (or sometimes Kendall notation) is the standard system used to describe and classify a queueing node. D. G. Kendall proposed describing queueing models using three factors written A/S/c in 1953 where A denotes the time between arrivals to the queue, S the size of jobs and c the number of servers at the node. It has since been extended to A/S/c/K/N/D where K is the capacity of the queue, D is the queueing discipline and N is the size of the population of jobs to be served. When the final three parameters are not specified (e.g. M/M/1 queue), it is assumed K =  , N =   and D = FIFO.
In statistics, the Dickey Fuller test tests whether a unit root is present in an autoregressive model. It is named after the statisticians David Dickey and Wayne Fuller, who developed the test in 1979.
In probability theory, the matrix analytic method is a technique to compute the stationary probability distribution of a Markov chain which has a repeating structure (after some point) and a state space which grows unboundedly in no more than one dimension. Such models are often described as M/G/1 type Markov chains because they can describe transitions in an M/G/1 queue. The method is a more complicated version of the matrix geometric method and is the classical solution method for M/G/1 chains.
Survival analysis is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems. This topic is called reliability theory or reliability analysis in engineering, duration analysis or duration modelling in economics, and event history analysis in sociology. Survival analysis attempts to answer questions such as: what is the proportion of a population which will survive past a certain time  Of those that survive, at what rate will they die or fail  Can multiple causes of death or failure be taken into account  How do particular circumstances or characteristics increase or decrease the probability of survival  To answer such questions, it is necessary to define "lifetime". In the case of biological survival, death is unambiguous, but for mechanical reliability, failure may not be well-defined, for there may well be mechanical systems in which failure is partial, a matter of degree, or not otherwise localized in time. Even in biological problems, some events (for example, heart attack or other organ failure) may have the same ambiguity. The theory outlined below assumes well-defined events at specific times; other cases may be better treated by models which explicitly account for ambiguous events. More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an "event" in the survival analysis literature   traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken. Recurring event or repeated event models relax that assumption. The study of recurring events is relevant in systems reliability, and in many areas of social sciences and medical research.
In the theory of probability, the Glivenko Cantelli theorem, named after Valery Ivanovich Glivenko and Francesco Paolo Cantelli, determines the asymptotic behaviour of the empirical distribution function as the number of independent and identically distributed observations grows. The uniform convergence of more general empirical measures becomes an important property of the Glivenko Cantelli classes of functions or sets. The Glivenko Cantelli classes arise in Vapnik Chervonenkis theory, with applications to machine learning. Applications can be found in econometrics making use of M-estimators. Assume that  are independent and identically-distributed random variables in  with common cumulative distribution function . The empirical distribution function for  is defined by  where  is the indicator function of the set . For every (fixed) ,  is a sequence of random variables which converge to  almost surely by the strong law of large numbers, that is,  converges to  pointwise. Glivenko and Cantelli strengthened this result by proving uniform convergence of  to . Theorem  almost surely. This theorem originates with Valery Glivenko, and Francesco Cantelli, in 1933. Remarks If  is a stationary ergodic process, then  converges almost surely to . The Glivenko Cantelli theorem gives a stronger mode of convergence than this in the iid case. An even stronger uniform convergence result for the empirical distribution function is available in the form of an extended type of law of the iterated logarithm. See asymptotic properties of the Empirical distribution function for this and related results.  ^ Howard G.Tucker (1959). "A Generalization of the Glivenko-Cantelli Theorem". The Annals of Mathematical Statistics 30: 828 830. doi:10.1214/aoms/1177706212.  ^ van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. p. 279. ISBN 0-521-78450-6.  ^ van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. p. 265. ISBN 0-521-78450-6.  ^ Glivenko, V. (1933). Sulla determinazione empirica della legge di probabilita. Giorn. Ist. Ital. Attuari 4, 92-99. ^ Cantelli, F. P. (1933). Sulla determinazione empirica delle leggi di probabilita. Giorn. Ist. Ital. Attuari 4, 221-424. ^ van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. p. 268. ISBN 0-521-78450-6.
In statistics a robust confidence interval is a robust modification of confidence intervals, meaning that one modifies the non-robust calculations of the confidence interval so that they are not badly affected by outlying or aberrant observations in a data-set.
In statistics, the Box Cox distribution (also known as the power-normal distribution) is the distribution of a random variable X for which the Box Cox transformation on X follows a truncated normal distribution. It is a continuous probability distribution having probability density function (pdf) given by  for y > 0, where m is the location parameter of the distribution, s is the dispersion,   is the family parameter, I is the indicator function,   is the cumulative distribution function of the standard normal distribution, and sgn is the sign function.
Point pattern analysis (PPA) is the study of the spatial arrangements of points in (usually 2-dimensional) space. The simplest formulation is a set X = {x   D} where D, which can be called the 'study region,' is a subset of Rn, a n-dimensional Euclidean space.
In statistics, G-tests are likelihood-ratio or maximum likelihood statistical significance tests that are increasingly being used in situations where chi-squared tests were previously recommended. The general formula for G is  where Oi is the observed count in a cell, Ei is the expected count under the null hypothesis, ln denotes the natural logarithm, and the sum is taken over all non-empty cells. G-tests have been recommended at least since the 1981 edition of the popular statistics textbook by Robert R. Sokal and F. James Rohlf.
In mathematics and multivariate statistics, the centering matrix is a symmetric and idempotent matrix, which when multiplied with a vector has the same effect as subtracting the mean of the components of the vector from every component.
In the statistical theory of the design of experiments, blocking is the arranging of experimental units in groups (blocks) that are similar to one another. Typically, a blocking factor is a source of variability that is not of primary interest to the experimenter. An example of a blocking factor might be the sex of a patient; by blocking on sex, this source of variability is controlled for, thus leading to greater accuracy.
In statistics (Classical Test Theory), Cronbach's  (alpha) is used as a (lowerbound) estimate of the reliability of a psychometric test. It has been proposed that  can be viewed as the expected correlation of two tests that measure the same construct. By using this definition, it is implicitly assumed that the average correlation of a set of items is an accurate estimate of the average correlation of all items that pertain to a certain construct. Cronbach's  is a function of the number of items in a test, the average covariance between item-pairs, and the variance of the total score. It was first named alpha by Lee Cronbach in 1951, as he had intended to continue with further coefficients. The measure can be viewed as an extension of the Kuder Richardson Formula 20 (KR-20), which is an equivalent measure for dichotomous items. Alpha is not robust against missing data. Several other Greek letters have been used by later researchers to designate other measures used in a similar context. Somewhat related is the average variance extracted (AVE). This article discusses the use of  in psychology, but Cronbach's alpha statistic is widely used in the social sciences, business, nursing, and other disciplines. The term item is used throughout this article, but items could be anything questions, raters, indicators of which one might ask to what extent they "measure the same thing." Items that are manipulated are commonly referred to as variables.
The Hammersley Clifford theorem is a result in probability theory, mathematical statistics and statistical mechanics, that gives necessary and sufficient conditions under which a positive probability distribution can be represented as a Markov network (also known as a Markov random field). It is the fundamental theorem of random fields. It states that a probability distribution that has a positive mass or density satisfies one of the Markov properties with respect to an undirected graph G if and only if it is a Gibbs random field, that is, its density can be factorized over the cliques (or complete subgraphs) of the graph. The relationship between Markov and Gibbs random fields was initiated by Roland Dobrushin and Frank Spitzer in the context of statistical mechanics. The theorem is named after John Hammersley and Peter Clifford who proved the equivalence in an unpublished paper in 1971. Simpler proofs using the inclusion-exclusion principle were given independently by Geoffrey Grimmett, Preston and Sherman in 1973, with a further proof by Julian Besag in 1974.
The secretary problem is a famous problem of optimal stopping theory. The problem has been studied extensively in the fields of applied probability, statistics, and decision theory. It is also known as the marriage problem, the sultan's dowry problem, the fussy suitor problem, the googol game, and the best choice problem. The basic form of the problem is the following: imagine an administrator willing to hire the best secretary out of  rankable applicants for a position. The applicants are interviewed one by one in random order. A decision about each particular applicant is to be made immediately after the interview. Once rejected, an applicant cannot be recalled. During the interview, the administrator can rank the applicant among all applicants interviewed so far, but is unaware of the quality of yet unseen applicants. The question is about the optimal strategy (stopping rule) to maximize the probability of selecting the best applicant. If the decision can be deferred to the end, this can be solved by the simple maximum selection algorithm of tracking the running maximum (and who achieved it), and selecting the overall maximum at the end. The difficulty is that the decision must be made immediately. The problem has an elegant solution. The optimal stopping rule prescribes always rejecting the first  applicants after the interview (where e is the base of the natural logarithm) and then stopping at the first applicant who is better than every applicant interviewed so far (or continuing to the last applicant if this never occurs). Sometimes this strategy is called the  stopping rule, because the probability of stopping at the best applicant with this strategy is about  already for moderate values of . One reason why the secretary problem has received so much attention is that the optimal policy for the problem (the stopping rule) is simple and selects the single best candidate about 37% of the time, irrespective of whether there are 100 or 100 million applicants. In fact, for any value of  the probability of selecting the best candidate when using the optimal policy is at least .
In statistics, a nuisance parameter is any parameter which is not of immediate interest but which must be accounted for in the analysis of those parameters which are of interest. The classic example of a nuisance parameter is the variance,  2, of a normal distribution, when the mean,  , is of primary interest. Nuisance parameters are often variances, but not always; for example in an errors-in-variables model, the unknown true location of each observation is a nuisance parameter. In general, any parameter which intrudes on the analysis of another may be considered a nuisance parameter. A parameter may also cease to be a "nuisance" if it becomes the object of study, as the variance of a distribution may be.
In linguistics, Heaps' law (also called Herdan's law) is an empirical law which describes the number of distinct words in a document (or set of documents) as a function of the document length (so called type-token relation). It can be formulated as  where VR is the number of distinct words in an instance text of size n. K and   are free parameters determined empirically. With English text corpora, typically K is between 10 and 100, and   is between 0.4 and 0.6. The law is frequently attributed to Harold Stanley Heaps, but was originally discovered by Gustav Herdan (1960). Under mild assumptions, the Herdan Heaps law is asymptotically equivalent to Zipf's law concerning the frequencies of individual words within a text. This is a consequence of the fact that the type-token relation (in general) of a homogenous text can be derived from the distribution of its types. Heaps' law means that as more instance text is gathered, there will be diminishing returns in terms of discovery of the full vocabulary from which the distinct terms are drawn. It is interesting to note that Heaps' law also applies to situations in which the "vocabulary" is just some set of distinct types which are attributes of some collection of objects. For example, the objects could be people, and the types could be country of origin of the person. If persons are selected randomly (that is, we are not selecting based on country of origin), then Heaps' law says we will quickly have representatives from most countries (in proportion to their population) but it will become increasingly difficult to cover the entire set of countries by continuing this method of sampling.
In econometrics, the Frisch Waugh Lovell (FWL) theorem is named after the econometricians Ragnar Frisch, Frederick V. Waugh, and Michael C. Lovell. The Frisch Waugh Lovell theorem states that if the regression we are concerned with is:  where  and  are  and  matrices respectively and where  and  are conformable, then the estimate of  will be the same as the estimate of it from a modified regression of the form:  where  projects onto the orthogonal complement of the image of the projection matrix . Equivalently, MX1 projects onto the orthogonal complement of the column space of X1. Specifically,  known as the annihilator matrix. This result implies that all these secondary regressions are unnecessary: using projection matrices to make the explanatory variables orthogonal to each other will lead to the same results as running the regression with all non-orthogonal explanators included.  
In statistics and social sciences, an antecedent variable is a variable that can help to explain the apparent relationship (or part of the relationship) between other variables that are nominally in a cause and effect relationship. In a regression analysis, an antecedent variable would be one that influences both the independent variable and the dependent variable.
In probability theory, there exist several different notions of convergence of random variables. The convergence of sequences of random variables to some limit random variable is an important concept in probability theory, and its applications to statistics and stochastic processes. The same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behaviour that is essentially unchanging when items far enough into the sequence are studied. The different possible notions of convergence relate to how such a behaviour can be characterised: two readily understood behaviours are that the sequence eventually takes a constant value, and that values in the sequence continue to change but can be described by an unchanging probability distribution.
In statistical modeling, regression analysis is a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables   that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a function of the independent variables called the regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution. Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable; for example, correlation does not imply causation. Many techniques for carrying out regression analysis have been developed. Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data. Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional. The performance of regression analysis methods in practice depends on the form of the data generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results. In a narrower sense, regression may refer specifically to the estimation of continuous response variables, as opposed to the discrete response variables used in classification. The case of a continuous output variable may be more specifically referred to as metric regression to distinguish it from related problems.
Manoba is a genus of moths in the family Nolidae.
In queueing theory, a discipline within the mathematical theory of probability, a G-network (generalized queueing network or Gelenbe network) is an open network of G-queues first introduced by Erol Gelenbe as a model for queueing systems with specific control functions, such as traffic re-routing or traffic destruction, as well as a model for neural networks. A G-queue is a network of queues with several types of novel and useful customers: positive customers, which arrive from other queues or arrive externally as Poisson arrivals, and obey standard service and routing disciplines as in conventional network models, negative customers, which arrive from another queue, or which arrive externally as Poisson arrivals, and remove (or 'kill') customers in a non-empty queue, representing the need to remove traffic when the network is congested, including the removal of "batches" of customers  "triggers", which arrive from other queues or from outside the network, and which displace customers and move them to other queues A product form solution superficially similar in form to Jackson's theorem, but which requires the solution of a system of non-linear equations for the traffic flows, exists for the stationary distribution of G-networks while the traffic equations of a G-network are in fact surprisingly non-linear, and the model does not obey partial balance. This broke previous assumptions that partial balance was a necessary condition for a product form solution. A powerful property of G-networks is that they are universal approximators for continuous and bounded functions, so that they can be used to approximate quite general input-output behaviours.
In probability and statistics, a generative model is a model for randomly generating observable data values, typically given some hidden parameters. It specifies a joint probability distribution over observation and label sequences. Generative models are used in machine learning for either modeling data directly (i.e., modeling observations drawn from a probability density function), or as an intermediate step to forming a conditional probability density function. A conditional distribution can be formed from a generative model through Bayes' rule. Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with "representing and speedily is an good"; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc. Generative models contrast with discriminative models, in that a generative model is a full probabilistic model of all variables, whereas a discriminative model provides a model only for the target variable(s) conditional on the observed variables. Thus a generative model can be used, for example, to simulate (i.e. generate) values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities. Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express more complex relationships between the observed and target variables. They don't necessarily perform better than generative models at classification and regression tasks. In modern applications the two classes are seen as complementary or as different views of the same procedure. Examples of generative models include: Gaussian mixture model and other types of mixture model Hidden Markov model Probabilistic context-free grammar Naive Bayes Averaged one-dependence estimators Latent Dirichlet allocation Restricted Boltzmann machine If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see above), although application-specific details will ultimately dictate which approach is most suitable in any particular case.
A Divisia index is a theoretical construct to create index number series for continuous-time data on prices and quantities of goods exchanged. It is designed to incorporate quantity and price changes over time from subcomponents which are measured in different units -- e.g. labor hours and equipment investment and materials purchases -- and to summarize these in a time series which summarizes the changes in quantities and/or prices. The resulting index number series is unitless, like other index numbers. In practice, economic data are not measured in continuous time so when a series is said to be a Divisia index, it usually means the series follows a procedure that makes a close analogue in discrete time periods, usually the To rnqvist index procedure or the Fisher Ideal Index procedures.
In probability theory, the stability of a random variable is the property that a linear combination of two independent copies of the variable has the same distribution, up to location and scale parameters. The distributions of random variables having this property are said to be "stable distributions". Results available in probability theory show that all possible distributions having this property are members of a four-parameter family of distributions. The article on the stable distribution describes this family together with some of the properties of these distributions. The importance in probability theory of "stability" and of the stable family of probability distributions is that they are "attractors" for properly normed sums of independent and identically distributed random variables. Important special cases of stable distributions are the normal distribution, the Cauchy distribution and the Le vy distribution. For details see stable distribution.
In statistical significance testing, a one-tailed test and a two-tailed test are alternative ways of computing the statistical significance of a parameter inferred from a data set, in terms of a test statistic. A two-tailed test is used if deviations of the estimated parameter in either direction from some benchmark value are considered theoretically possible; in contrast, a one-tailed test is used if only deviations in one direction are considered possible. Alternative names are one-sided and two-sided tests; the terminology "tail" is used because the extreme portions of distributions, where observations lead to rejection of the null hypothesis, are small and often "tail off" toward zero as in the normal distribution or "bell curve", pictured above right.
In statistics and machine learning, discretization refers to the process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals. This can be useful when creating probability mass functions   formally, in density estimation. It is a form of discretization in general and also of binning, as in making a histogram. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand. Typically data is discretized into partitions of K equal lengths/width (equal intervals) or K% of the total data (equal frequencies). Mechanisms for discretizing continuous data include Fayyad & Irani's MDL method, which uses mutual information to recursively define the best bins, CAIM, CACC, Ameva, and many others Many machine learning algorithms are known to produce better models by discretizing continuous attributes.
In probability theory and statistics, the Rayleigh distribution / re li/ is a continuous probability distribution for positive-valued random variables. A Rayleigh distribution is often observed when the overall magnitude of a vector is related to its directional components. One example where the Rayleigh distribution naturally arises is when wind velocity is analyzed into its orthogonal 2-dimensional vector components. Assuming that each component is uncorrelated, normally distributed with equal variance, and zero mean, then the overall wind speed (vector magnitude) will be characterized by a Rayleigh distribution. A second example of the distribution arises in the case of random complex numbers whose real and imaginary components are independently and identically distributed Gaussian with equal variance and zero mean. In that case, the absolute value of the complex number is Rayleigh-distributed. The distribution is named after Lord Rayleigh
PSPP is a free software application for analysis of sampled data, intended as a free alternative for IBM SPSS Statistics. It has a graphical user interface and conventional command-line interface. It is written in C and uses GNU Scientific Library for its mathematical routines. The name has "no official acronymic expansion".
In biology, a substitution model describes the process from which a sequence of characters changes into another set of traits. For example, in cladistics, each position in the sequence might correspond to a property of a species which can either be present or absent. The alphabet could then consist of "0" for absence and "1" for presence. Then the sequence 00110 could mean, for example, that a species does not have feathers or lay eggs, does have fur, is warm-blooded, and cannot breathe underwater. Another sequence 11010 would mean that a species has feathers, lays eggs, does not have fur, is warm-blooded, and cannot breathe underwater. In phylogenetics, sequences are often obtained by firstly obtaining a nucleotide or protein sequence alignment, and then taking the bases or amino acids at corresponding positions in the alignment as the characters. Sequences achieved by this might look like AGCGGAGCTTA and GCCGTAGACGC. Substitution models are used for a number of things: Constructing evolutionary trees in phylogenetics or cladistics. Simulating sequences to test other methods and algorithms.
Qualitative properties are properties that are observed and can generally not be measured with a numerical result. They are contrasted to quantitative properties which have numerical characteristics. Some engineering and scientific properties are qualitative. A test method can result in qualitative data about something. This can be a categorical result or a binary classification (e.g., pass/fail, go/no go, conform/non-conform). It can sometimes be an engineering judgement.
Lag windowing is a technique that consists of windowing the auto-correlation coefficients prior to estimating Linear prediction coefficients (LPC). The windowing in the auto-correlation domain has the same effect as a convolution (smoothing) in the power spectral domain and helps stabilizing the result of the Levinson-Durbin algorithm. The window function is typically a Gaussian function.
DFFITS is a diagnostic meant to show how influential a point is in a statistical regression. It was proposed in 1980. It is defined as the Studentized DFFIT, where the latter is the change in the predicted value for a point, obtained when that point is left out of the regression; Studentization is achieved by dividing by the estimated standard deviation of the fit at that point:  where  and  are the prediction for point i with and without point i included in the regression,  is the standard error estimated without the point in question, and  is the leverage for the point. DFFITS is very similar to the externally Studentized residual, and is in fact equal to the latter times . As when the errors are Gaussian the externally Studentized residual is distributed as Student's t (with a number of degrees of freedom equal to the number of residual degrees of freedom minus one), DFFITS for a particular point will be distributed according to this same Student's t distribution multiplied by the leverage factor  for that particular point. Thus, for low leverage points, DFFITS is expected to be small, whereas as the leverage goes to 1 the distribution of the DFFITS value widens infinitely. For a perfectly balanced experimental design (such as a factorial design or balanced partial factorial design), the leverage for each point is p/n, the number of parameters divided by the number of points. This means that the DFFITS values will be distributed (in the Gaussian case) as  times a t variate. Therefore, the authors suggest investigating those points with DFFITS greater than . Although the raw values resulting from the equations are different, Cook's distance and DFFITS are conceptually identical and there is a closed-form formula to convert one value to the other.
In information theory and statistics, Kullback's inequality is a lower bound on the Kullback Leibler divergence expressed in terms of the large deviations rate function. If P and Q are probability distributions on the real line, such that P is absolutely continuous with respect to Q, i.e. P<<Q, and whose first moments exist, then  where  is the rate function, i.e. the convex conjugate of the cumulant-generating function, of , and  is the first moment of  The Crame r Rao bound is a corollary of this result.
A Pareto chart, named after Vilfredo Pareto, is a type of chart that contains both bars and a line graph, where individual values are represented in descending order by bars, and the cumulative total is represented by the line. The left vertical axis is the frequency of occurrence, but it can alternatively represent cost or another important unit of measure. The right vertical axis is the cumulative percentage of the total number of occurrences, total cost, or total of the particular unit of measure. Because the reasons are in decreasing order, the cumulative function is a concave function. To take the example below, in order to lower the amount of late arrivals by 78%, it is sufficient to solve the first three issues. The purpose of the Pareto chart is to highlight the most important among a (typically large) set of factors. In quality control, it often represents the most common sources of defects, the highest occurring type of defect, or the most frequent reasons for customer complaints, and so on. Wilkinson (2006) devised an algorithm for producing statistically based acceptance limits (similar to confidence intervals) for each bar in the Pareto chart. These charts can be generated by simple spreadsheet programs, such as Apache OpenOffice/LibreOffice Calc  and Microsoft Excel, visualization tools such as Tableau Software, specialized statistical software tools, and online quality charts generators. The Pareto chart is one of the seven basic tools of quality control.
In probability theory and statistics, the Bernoulli distribution, named after Swiss scientist Jacob Bernoulli, is the probability distribution of a random variable which takes the value 1 with success probability of  and the value 0 with failure probability of . It can be used to represent a coin toss where 1 and 0 would represent "head" and "tail" (or vice versa), respectively. In particular, unfair coins would have . The Bernoulli distribution is a special case of the two-point distribution, for which the two possible outcomes need not be 0 and 1. It is also a special case of the binomial distribution; the Bernoulli distribution is a binomial distribution where n=1.
Computer-assisted survey information collection (CASIC) refers to a variety of survey modes that were enabled by the introduction of computer technology. The first CASIC modes were interviewer-administered, while later on computerized self-administered questionnaires (CSAQ) appeared. It was coined in 1990 as a catch-all term for survey technologies that have expanded over time.
MLwiN is a statistical software package for fitting multilevel models. It uses both maximum likelihood estimation and Markov Chain Monte Carlo (MCMC) methods. MLwiN is based on an earlier package, MLn, but with a graphical user interface (as well as other additional features). MLwiN represents multilevel models using mathematical notation including Greek letters and multiple subscripts, so the user needs to be (or become) familiar with such notation. For a tutorial introduction to multilevel models and their applications in medical statistics illustrated using MLwiN, see Goldstein et al.
Optimal matching is a sequence analysis method used in social science, to assess the dissimilarity of ordered arrays of tokens that usually represent a time-ordered sequence of socio-economic states two individuals have experienced. Once such distances have been calculated for a set of observations (e.g. individuals in a cohort) classical tools (such as cluster analysis) can be used. The method was tailored to social sciences from a technique originally introduced to study molecular biology (protein or genetic) sequences (see sequence alignment). Optimal matching uses the Needleman-Wunsch algorithm.
Good Turing frequency estimation is a statistical technique for estimating the probability of encountering an object of a hitherto unseen species, given a set of past observations of objects from different species. (In drawing balls from an urn, the 'objects' would be balls and the 'species' would be the distinct colors of the balls (finite but unknown in number). After drawing  red balls,  black balls and  green balls, we would ask what is the probability of drawing a red ball, a black ball, a green ball or one of a previously unseen color.  

Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball.
In probability theory, the Azuma Hoeffding inequality (named after Kazuoki Azuma and Wassily Hoeffding) gives a concentration result for the values of martingales that have bounded differences. Suppose { Xk : k = 0, 1, 2, 3, ... } is a martingale (or super-martingale) and  almost surely. Then for all positive integers N and all positive reals t,  And symmetrically (when Xk is a sub-martingale):  If X is a martingale, using both inequalities above and applying the union bound allows one to obtain a two-sided bound:  Azuma's inequality applied to the Doob martingale gives the method of bounded differences (MOBD) which is common in the analysis of randomized algorithms.
In probability theory and statistics, a copula is a multivariate probability distribution for which the marginal probability distribution of each variable is uniform. Copulas are used to describe the dependence between random variables. Their name comes from the Latin for "link" or "tie", similar but unrelated to grammatical copulas in linguistics. Sklar's Theorem states that any multivariate joint distribution can be written in terms of univariate marginal distribution functions and a copula which describes the dependence structure between the variables. Copulas are popular in high-dimensional statistical applications as they allow one to easily model and estimate the distribution of random vectors by estimating marginals and copulae separately. There are many parametric copula families available, which usually have parameters that control the strength of dependence. Some popular parametric copula models are outlined below.
Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more precise than those based on a single measurement alone, by using Bayesian inference and estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. Ka lma n, one of the primary developers of its theory. The Kalman filter has numerous applications in technology. A common application is for guidance, navigation and control of vehicles, particularly aircraft and spacecraft. Furthermore, the Kalman filter is a widely applied concept in time series analysis used in fields such as signal processing and econometrics. Kalman filters also are one of the main topics in the field of robotic motion planning and control, and they are sometimes included in trajectory optimization. The Kalman filter has also found use in modeling the central nervous system's control of movement. Due to the time delay between issuing motor commands and receiving sensory feedback, use of the Kalman filter provides the needed model for making estimates of the current state of the motor system and issuing updated commands. The algorithm works in a two-step process. In the prediction step, the Kalman filter produces estimates of the current state variables, along with their uncertainties. Once the outcome of the next measurement (necessarily corrupted with some amount of error, including random noise) is observed, these estimates are updated using a weighted average, with more weight being given to estimates with higher certainty. The algorithm is recursive. It can run in real time, using only the present input measurements and the previously calculated state and its uncertainty matrix; no additional past information is required. The Kalman filter does not require any assumption that the errors are Gaussian. However, the filter yields the exact conditional probability estimate in the special case that all errors are Gaussian-distributed. Extensions and generalizations to the method have also been developed, such as the extended Kalman filter and the unscented Kalman filter which work on nonlinear systems. The underlying model is a Bayesian model similar to a hidden Markov model but where the state space of the latent variables is continuous and where all latent and observed variables have Gaussian distributions.
In statistics, ordinary least squares (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, with the goal of minimizing the differences between the observed responses in some arbitrary dataset and the responses predicted by the linear approximation of the data (visually this is seen as the sum of the vertical distances between each data point in the set and the corresponding point on the regression line - the smaller the differences, the better the model fits the data). The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side. The OLS estimator is consistent when the regressors are exogenous and there is no perfect multicollinearity, and optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors be normally distributed, OLS is the maximum likelihood estimator. OLS is used in economics (econometrics), political science and electrical engineering (control theory and signal processing), among many areas of application. The Multi-fractional order estimator is an expanded version of OLS.
Historiometry is the historical study of human progress or individual personal characteristics, using statistics to analyze references to geniuses, their statements, behavior and discoveries in relatively neutral texts. Historiometry combines techniques from cliometrics, which studies the history of economics and from psychometrics, the psychological study of an individual's personality and abilities.
A random function   of either one variable (a random process), or two or more variables (a random field)   is called Gaussian if every finite-dimensional distribution is a multivariate normal distribution. Gaussian random fields on the sphere are useful (for example) when analysing the anomalies in the cosmic microwave background radiation (see, pp. 8 9); brain images obtained by positron emission tomography (see, pp. 9 10). Sometimes, a value of a Gaussian random function deviates from its expected value by several standard deviations. This is a large deviation. Though rare in a small domain (of space or/and time), large deviations may be quite usual in a large domain.
In queueing theory, a discipline within the mathematical theory of probability, a Markovian arrival process (MAP or MArP) is a mathematical model for the time between job arrivals to a system. The simplest such process is a Poisson process where the time between each arrival is exponentially distributed. The processes were first suggested by Neuts in 1979.
In statistics, a floor effect (also known as a basement effect) arises when a data-gathering instrument has a lower limit to the data values it can reliably specify. This lower limit is known as the "floor". Floor effects are occasionally encountered in psychological testing, when a test designed to estimate some psychological trait has a minimum standard score that may not distinguish some test-takers who differ in their responses on the test item content. Giving preschool children an IQ test designed for adults would likely show many of the test-takers with scores near the lowest standard score for adult test-takers (IQ 40 on most tests that were currently normed as of 2010). To indicate differences in current intellectual functioning among young children, IQ tests specifically for young children are developed, on which many test-takers can score well above the floor score. An IQ test designed to help assess intellectually disabled persons might intentionally be designed with easier item content and a lower floor score to better distinguish among individuals taking the test as part of an assessment process.
In probability theory and its applications, such as statistics and cryptography, a random function is a function chosen randomly from a family of possible functions. Each realisation of a random function would result in a different function. Thus the concept of a random function is one example of a random element and hence is a generalization of the simpler idea of a random variable. In probability and statistics, one important type of random function is studied under the name of stochastic processes, for which there are a variety of models describing systems where an observation is a random function of time or space. However, there are other applications where there is a need to describe the uncertainty with which a function is known and where the state of knowledge about the true function can be expressed by saying that it is an unknown realisation of a random function, for example in the Dirichlet process. A special case of a random function is a random permutation, where a realisation can be interpreted as being in the form of a function on the set of integers describing the original location of an item, where the value of the function provides the new (permuted) location of the item that was in a given location. In cryptography, a random function can be a useful building block in enabling cryptographic protocols.
In probability theory and statistics, the factorial moment generating function of the probability distribution of a real-valued random variable X is defined as  for all complex numbers t for which this expected value exists. This is the case at least for all t on the unit circle , see characteristic function. If X is a discrete random variable taking values only in the set {0,1, ...} of non-negative integers, then  is also called probability-generating function of X and  is well-defined at least for all t on the closed unit disk . The factorial moment generating function generates the factorial moments of the probability distribution. Provided  exists in a neighbourhood of t = 1, the nth factorial moment is given by   where the Pochhammer symbol (x)n is the falling factorial  (Many mathematicians, especially in the field of special functions, use the same notation to represent the rising factorial.)
In probability theory, the conditional expectation of a random variable is another random variable equal to the average of the former over each possible "condition". In the case when the random variable is defined over a discrete probability space, the "conditions" are a partition of this probability space. This definition is then generalized to any probability space using measure theory. Conditional expectation is also known as conditional expected value or conditional mean. In modern probability theory the concept of conditional probability is defined in terms of conditional expectation.
This is a list of timelines currently on Wikipedia.
Cross-sectional data, or a cross section of a study population, in statistics and econometrics is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the same point of time, or without regard to differences in time. Analysis of cross-sectional data usually consists of comparing the differences among the subjects. For example, if we want to measure current obesity levels in a population, we could draw a sample of 1,000 people randomly from that population (also known as a cross section of that population), measure their weight and height, and calculate what percentage of that sample is categorized as obese. This cross-sectional sample provides us with a snapshot of that population, at that one point in time. Note that we do not know based on one cross-sectional sample if obesity is increasing or decreasing; we can only describe the current proportion. Cross-sectional data differs from time series data, in which the same small-scale or aggregate entity is observed at various points in time for example, longitudinal data, which follows one subject's changes over the course of time. Another variant, panel data (or time-series cross-sectional (TSCS) data), combines both and looks at multiple subjects and how they change over the course of time. Panel analysis uses panel data to examine changes in variables over time and differences in variables between subjects. In a rolling cross-section, both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly. For example, a political poll may decide to interview 1000 individuals. It first selects these individuals randomly from the entire population. It then assigns a random date to each individual. This is the random date that the individual will be interviewed, and thus included in the survey. Cross-sectional data can be used in cross-sectional regression, which is regression analysis of cross-sectional data. For example, the consumption expenditures of various individuals in a fixed month could be regressed on their incomes, accumulated wealth levels, and their various demographic features to find out how differences in those features lead to differences in consumer behavior.
Frequentist inference is a type of statistical inference that draws conclusions from sample data by emphasizing the frequency or proportion of the data. An alternative name is frequentist statistics. This is the inference framework in which the well-established methodologies of statistical hypothesis testing and confidence intervals are based. Other than frequentistic inference, the main alternative approach to statistical inference is Bayesian inference, while another is fiducial inference. While "Bayesian inference" is sometimes held to include the approach to inference leading to optimal decisions, a more restricted view is taken here for simplicity.
In statistics, imputation is the process of replacing missing data with substituted values. When substituting for a data point, it is known as "unit imputation"; when substituting for a component of a data point, it is known as "item imputation". Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, which may introduce bias or affect the representativeness of the results. Imputation preserves all cases by replacing missing data with an estimated value based on other available information. Once all missing values have been imputed, the data set can then be analysed using standard techniques for complete data.
In statistics and Markov modeling, an ancestral graph is a type of mixed graph to provide a graphical representation for the result of marginalizing one or more vertices in a graphical model that takes the form of a directed acyclic graph.
In statistics, Welch's t-test, or unequal variances t-test, is a two-sample location test which is used to test the hypothesis that two populations have equal means. Welch's t-test is an adaptation of Student's t-test, that is, it has been derived with the help of Student's t-test and is more reliable when the two samples have unequal variances and unequal sample sizes. These tests are often referred to as "unpaired" or "independent samples" t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping. Given that Welch's t-test has been less popular than Student's t-test and may be less familiar to readers, a more informative name is "Welch's unequal variances t-test" or "unequal variances t-test" for brevity.
Quantitative psychological research is defined as psychological research which performs mathematical modeling and statistical estimation or statistical inference or a means for testing objective theories by examining the relationship between variables. The first definition distinguishes it from qualitative psychological research; however, there has been a long debate on the difference between quantitative and qualitative research. It has been argued that because this debated has not found an end, the differences are enough that both quantitative and qualitative research is valuable in ways that both should be used in the gathering of data.
In mathematics, the entropy power inequality is a result in information theory that relates to so-called "entropy power" of random variables. It shows that the entropy power of suitably well-behaved random variables is a superadditive function. The entropy power inequality was proved in 1948 by Claude Shannon in his seminal paper "A Mathematical Theory of Communication". Shannon also provided a sufficient condition for equality to hold; Stam (1959) showed that the condition is in fact necessary.
Biostatistics (or biometry) is the application of statistics to a wide range of topics in biology. The science of biostatistics encompasses the design of biological experiments, especially in medicine, pharmacy, agriculture and fishery; the collection, summarization, and analysis of data from those experiments; and the interpretation of, and inference from, the results. A major branch of this is medical biostatistics, which is exclusively concerned with medicine and health.
In the comparison of various statistical procedures, efficiency is a measure of the optimality of an estimator, of an experimental design, or of a hypothesis testing procedure. Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance. This article primarily deals with efficiency of estimators. The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional "best possible" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure. Efficiencies are often defined using the variance or mean square error as the measure of desirability.
In mathematics, the Hamburger moment problem, named after Hans Ludwig Hamburger, is formulated as follows: given a sequence { mn : n = 1, 2, 3, ... }, does there exist a positive Borel measure   on the real line such that  In other words, an affirmative answer to the problem means that { mn : n = 0, 1, 2, ... } is the sequence of moments of some positive Borel measure  . The Stieltjes moment problem, Vorobyev moment problem, and the Hausdorff moment problem are similar but replace the real line by  (Stieltjes and Vorobyev; but Vorobyev formulates the problem in the terms of matrix theory), or a bounded interval (Hausdorff).
A pseudocount is an amount (not generally an integer, despite its name) added to the number of observed cases in order to change the expected probability in a model of those data, when not known to be zero. Depending on the prior knowledge, which is sometimes a subjective value, a pseudocount may have any non-negative finite value. It may only be zero (or the possibility ignored) if impossible by definition, such as the possibility of a decimal digit of pi being a letter, or a physical possibility that would be rejected and so not counted, such as a computer printing a letter when a valid program for pi is run, or excluded and not counted because of no interest, such as if only interested in the zeros and ones. Generally, there is also a possibility that no value may be computable or observable in a finite time (see Turing's halting problem). But at least one possibility must have a non-zero pseudocount, otherwise no prediction could be computed before the first observation. The relative values of pseudocounts represent the relative prior expected probabilities of their possibilities. The sum of the pseudocounts, which may be very large, represents the estimated weight of the prior knowledge compared with all the actual observations (one for each) when determining the expected probability. In any observed data set or sample there is the possibility, especially with low-probability events and with small data sets, of a possible event not occurring. Its observed frequency is therefore zero, apparently implying a probability of zero. This is an oversimplification, which is inaccurate and often unhelpful, particularly in probability-based machine learning techniques such as artificial neural networks and hidden Markov models. By artificially adjusting the probability of rare (but not impossible) events so those probabilities are not exactly zero, zero-frequency problems are avoided. Also see Cromwell's Rule. The simplest approach is to add one to each observed number of events including the zero-count possibilities. This is sometimes called Laplace's Rule of Succession. It is a type of additive smoothing. More generally and accurately, the pseudocounts should be set in proportion to the prior estimate of their probabilities; equal only if there is no prior reason to prefer one over another; each being one only when there is no prior knowledge at all   see the principle of indifference. However, given appropriate prior knowledge, the sum should be adjusted in proportion to the expectation that the prior probabilities should be considered correct, despite evidence to the contrary   see further analysis. Higher values are appropriate inasmuch as there is prior knowledge of the true values (for a mint condition coin, say); lower values inasmuch as there is prior knowledge that there is probable bias, but of unknown degree (for a bent coin, say). A more complex approach is to estimate the probability of the events from other factors and adjust accordingly.
In statistics, the hypergeometric distribution is the discrete probability distribution generated by picking colored balls at random from an urn without replacement. Various generalizations to this distribution exist for cases where the picking of colored balls is biased so that balls of one color are more likely to be picked than balls of another color. This can be illustrated by the following example. Assume that an opinion poll is conducted by calling random telephone numbers. Unemployed people are more likely to be home and answer the phone than employed people are. Therefore, unemployed respondents are likely to be over-represented in the sample. The probability distribution of employed versus unemployed respondents in a sample of n respondents can be described as a noncentral hypergeometric distribution. The description of biased urn models is complicated by the fact that there is more than one noncentral hypergeometric distribution. Which distribution you get depends on whether items (e.g. colored balls) are sampled one by one in a manner where there is competition between the items, or they are sampled independently of each other. There is widespread confusion about this fact. The name noncentral hypergeometric distribution has been used for two different distributions, and several scientists have used the wrong distribution or erroneously believed that the two distributions were identical. The use of the same name for two different distributions has been possible because these two distributions were studied by two different groups of scientists with hardly any contact with each other. Agner Fog (2007, 2008) has suggested that the best way to avoid confusion is to use the name Wallenius' noncentral hypergeometric distribution for the distribution of a biased urn model where a predetermined number of items are drawn one by one in a competitive manner, while the name Fisher's noncentral hypergeometric distribution is used where items are drawn independently of each other, so that the total number of items drawn is known only after the experiment. The names refer to Kenneth Ted Wallenius and R. A. Fisher who were the first to describe the respective distributions. Fisher's noncentral hypergeometric distribution has previously been given the name extended hypergeometric distribution, but this name is rarely used in the scientific literature, except in handbooks that need to distinguish between the two distributions. Some scientists are strongly opposed to using this name. A thorough explanation of the difference between the two noncentral hypergeometric distributions is obviously needed here.
Pairwise comparison generally is any process of comparing entities in pairs to judge which of each entity is preferred, or has a greater amount of some quantitative property, or whether or not the two entities are identical. The method of pairwise comparison is used in the scientific study of preferences, attitudes, voting systems, social choice, public choice, and multiagent AI systems. In psychology literature, it is often referred to as paired comparison. Prominent psychometrician L. L. Thurstone first introduced a scientific approach to using pairwise comparisons for measurement in 1927, which he referred to as the law of comparative judgment. Thurstone linked this approach to psychophysical theory developed by Ernst Heinrich Weber and Gustav Fechner. Thurstone demonstrated that the method can be used to order items along a dimension such as preference or importance using an interval-type scale.
In probability theory, Maxwell's theorem, named in honor of James Clerk Maxwell, states that if the probability distribution of a vector-valued random variable X = ( X1, ..., Xn )T is the same as the distribution of GX for every n n orthogonal matrix G and the components are independent, then the components X1, ..., Xn are normally distributed with expected value 0, all have the same variance, and all are independent. This theorem is one of many characterizations of the normal distribution. Since a multiplication by an orthogonal matrix is a rotation, the theorem says that if the probability distribution of a random vector is unchanged by rotations and if the components are independent, then the components are identically distributed and normally distributed. In other words, the only rotationally invariant probability distributions on Rn that have independent components are multivariate normal distributions with expected value 0 and variance  2In, (where In = the n n identity matrix), for some positive number  2.
